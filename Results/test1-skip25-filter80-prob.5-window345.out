
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=80

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507653618

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-10T11:40:21.577789: step 1, loss 7.31983, acc 0.234375, learning_rate 0.005
2017-10-10T11:40:21.694716: step 2, loss 5.26642, acc 0.25, learning_rate 0.00498
2017-10-10T11:40:21.815025: step 3, loss 4.59737, acc 0.328125, learning_rate 0.00496008
2017-10-10T11:40:21.934910: step 4, loss 4.06619, acc 0.46875, learning_rate 0.00494024
2017-10-10T11:40:22.052563: step 5, loss 4.65874, acc 0.390625, learning_rate 0.00492049
2017-10-10T11:40:22.170821: step 6, loss 4.69366, acc 0.375, learning_rate 0.00490081
2017-10-10T11:40:22.288083: step 7, loss 4.299, acc 0.390625, learning_rate 0.00488121
2017-10-10T11:40:22.408999: step 8, loss 3.14747, acc 0.40625, learning_rate 0.0048617
2017-10-10T11:40:22.529718: step 9, loss 3.66085, acc 0.359375, learning_rate 0.00484226
2017-10-10T11:40:22.647393: step 10, loss 2.9865, acc 0.53125, learning_rate 0.00482291
2017-10-10T11:40:22.768609: step 11, loss 3.64407, acc 0.421875, learning_rate 0.00480363
2017-10-10T11:40:22.886079: step 12, loss 2.66544, acc 0.515625, learning_rate 0.00478443
2017-10-10T11:40:23.004976: step 13, loss 3.20813, acc 0.46875, learning_rate 0.00476531
2017-10-10T11:40:23.119811: step 14, loss 2.28294, acc 0.546875, learning_rate 0.00474627
2017-10-10T11:40:23.235980: step 15, loss 4.39812, acc 0.421875, learning_rate 0.0047273
2017-10-10T11:40:23.353876: step 16, loss 1.34007, acc 0.6875, learning_rate 0.00470841
2017-10-10T11:40:23.467699: step 17, loss 2.43526, acc 0.578125, learning_rate 0.0046896
2017-10-10T11:40:23.587870: step 18, loss 2.44593, acc 0.5625, learning_rate 0.00467087
2017-10-10T11:40:23.706633: step 19, loss 3.2889, acc 0.4375, learning_rate 0.00465221
2017-10-10T11:40:23.834963: step 20, loss 1.74267, acc 0.671875, learning_rate 0.00463363
2017-10-10T11:40:23.966783: step 21, loss 2.07068, acc 0.5625, learning_rate 0.00461513
2017-10-10T11:40:24.094810: step 22, loss 1.74703, acc 0.671875, learning_rate 0.0045967
2017-10-10T11:40:24.207776: step 23, loss 1.59717, acc 0.65625, learning_rate 0.00457834
2017-10-10T11:40:24.322085: step 24, loss 1.48179, acc 0.640625, learning_rate 0.00456006
2017-10-10T11:40:24.442853: step 25, loss 2.12908, acc 0.609375, learning_rate 0.00454186
2017-10-10T11:40:24.566785: step 26, loss 0.941187, acc 0.6875, learning_rate 0.00452373
2017-10-10T11:40:24.679885: step 27, loss 1.50323, acc 0.640625, learning_rate 0.00450567
2017-10-10T11:40:24.799434: step 28, loss 1.18913, acc 0.65625, learning_rate 0.00448769
2017-10-10T11:40:24.922523: step 29, loss 1.94535, acc 0.640625, learning_rate 0.00446978
2017-10-10T11:40:25.040554: step 30, loss 1.90112, acc 0.65625, learning_rate 0.00445194
2017-10-10T11:40:25.160739: step 31, loss 1.71276, acc 0.671875, learning_rate 0.00443418
2017-10-10T11:40:25.279413: step 32, loss 1.55421, acc 0.671875, learning_rate 0.00441649
2017-10-10T11:40:25.393081: step 33, loss 1.73564, acc 0.703125, learning_rate 0.00439887
2017-10-10T11:40:25.514476: step 34, loss 1.30585, acc 0.65625, learning_rate 0.00438132
2017-10-10T11:40:25.630562: step 35, loss 1.33926, acc 0.6875, learning_rate 0.00436385
2017-10-10T11:40:25.750843: step 36, loss 1.41804, acc 0.625, learning_rate 0.00434644
2017-10-10T11:40:25.878274: step 37, loss 1.04468, acc 0.703125, learning_rate 0.00432911
2017-10-10T11:40:26.000188: step 38, loss 1.80907, acc 0.65625, learning_rate 0.00431185
2017-10-10T11:40:26.119440: step 39, loss 1.06897, acc 0.734375, learning_rate 0.00429465
2017-10-10T11:40:26.239428: step 40, loss 1.57907, acc 0.609375, learning_rate 0.00427753

Evaluation:
2017-10-10T11:40:26.567566: step 40, loss 0.407161, acc 0.853237

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-40

2017-10-10T11:40:27.228982: step 41, loss 1.40937, acc 0.75, learning_rate 0.00426048
2017-10-10T11:40:27.350092: step 42, loss 0.976604, acc 0.75, learning_rate 0.0042435
2017-10-10T11:40:27.467270: step 43, loss 1.17245, acc 0.6875, learning_rate 0.00422659
2017-10-10T11:40:27.581654: step 44, loss 1.33906, acc 0.65625, learning_rate 0.00420974
2017-10-10T11:40:27.702775: step 45, loss 1.43706, acc 0.734375, learning_rate 0.00419297
2017-10-10T11:40:27.819273: step 46, loss 1.90919, acc 0.671875, learning_rate 0.00417626
2017-10-10T11:40:27.940544: step 47, loss 0.974811, acc 0.796875, learning_rate 0.00415962
2017-10-10T11:40:28.059240: step 48, loss 1.43213, acc 0.625, learning_rate 0.00414305
2017-10-10T11:40:28.174795: step 49, loss 0.866488, acc 0.78125, learning_rate 0.00412655
2017-10-10T11:40:28.293203: step 50, loss 0.780119, acc 0.734375, learning_rate 0.00411011
2017-10-10T11:40:28.412568: step 51, loss 1.35097, acc 0.734375, learning_rate 0.00409375
2017-10-10T11:40:28.523668: step 52, loss 1.10948, acc 0.765625, learning_rate 0.00407744
2017-10-10T11:40:28.640723: step 53, loss 0.89925, acc 0.71875, learning_rate 0.00406121
2017-10-10T11:40:28.756614: step 54, loss 0.584526, acc 0.78125, learning_rate 0.00404504
2017-10-10T11:40:28.874110: step 55, loss 1.32595, acc 0.703125, learning_rate 0.00402894
2017-10-10T11:40:28.995474: step 56, loss 0.678131, acc 0.796875, learning_rate 0.0040129
2017-10-10T11:40:29.114710: step 57, loss 1.44699, acc 0.71875, learning_rate 0.00399693
2017-10-10T11:40:29.231670: step 58, loss 0.9475, acc 0.734375, learning_rate 0.00398102
2017-10-10T11:40:29.349865: step 59, loss 0.725829, acc 0.8125, learning_rate 0.00396518
2017-10-10T11:40:29.470728: step 60, loss 1.06244, acc 0.6875, learning_rate 0.00394941
2017-10-10T11:40:29.590716: step 61, loss 1.02891, acc 0.78125, learning_rate 0.00393369
2017-10-10T11:40:29.702642: step 62, loss 0.819614, acc 0.84375, learning_rate 0.00391804
2017-10-10T11:40:29.825801: step 63, loss 0.803943, acc 0.78125, learning_rate 0.00390246
2017-10-10T11:40:29.946666: step 64, loss 1.15649, acc 0.734375, learning_rate 0.00388694
2017-10-10T11:40:30.070196: step 65, loss 1.18808, acc 0.765625, learning_rate 0.00387148
2017-10-10T11:40:30.192626: step 66, loss 0.856321, acc 0.796875, learning_rate 0.00385609
2017-10-10T11:40:30.312999: step 67, loss 0.571892, acc 0.875, learning_rate 0.00384076
2017-10-10T11:40:30.433170: step 68, loss 0.951795, acc 0.65625, learning_rate 0.00382549
2017-10-10T11:40:30.551385: step 69, loss 0.880807, acc 0.734375, learning_rate 0.00381028
2017-10-10T11:40:30.670254: step 70, loss 0.65704, acc 0.8125, learning_rate 0.00379514
2017-10-10T11:40:30.788371: step 71, loss 0.796692, acc 0.828125, learning_rate 0.00378005
2017-10-10T11:40:30.904270: step 72, loss 0.546187, acc 0.84375, learning_rate 0.00376503
2017-10-10T11:40:31.021775: step 73, loss 0.672188, acc 0.828125, learning_rate 0.00375007
2017-10-10T11:40:31.130948: step 74, loss 0.427122, acc 0.875, learning_rate 0.00373517
2017-10-10T11:40:31.251106: step 75, loss 0.393984, acc 0.875, learning_rate 0.00372034
2017-10-10T11:40:31.371401: step 76, loss 0.615773, acc 0.859375, learning_rate 0.00370556
2017-10-10T11:40:31.490050: step 77, loss 0.90511, acc 0.796875, learning_rate 0.00369084
2017-10-10T11:40:31.607325: step 78, loss 0.544252, acc 0.859375, learning_rate 0.00367619
2017-10-10T11:40:31.726180: step 79, loss 0.778023, acc 0.796875, learning_rate 0.00366159
2017-10-10T11:40:31.841976: step 80, loss 0.666334, acc 0.75, learning_rate 0.00364705

Evaluation:
2017-10-10T11:40:32.166422: step 80, loss 0.389771, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-80

2017-10-10T11:40:32.790597: step 81, loss 0.672887, acc 0.8125, learning_rate 0.00363257
2017-10-10T11:40:32.915066: step 82, loss 0.89256, acc 0.8125, learning_rate 0.00361815
2017-10-10T11:40:33.033380: step 83, loss 0.794956, acc 0.78125, learning_rate 0.00360379
2017-10-10T11:40:33.151676: step 84, loss 1.07545, acc 0.75, learning_rate 0.00358949
2017-10-10T11:40:33.271188: step 85, loss 0.79591, acc 0.796875, learning_rate 0.00357525
2017-10-10T11:40:33.400126: step 86, loss 0.580325, acc 0.859375, learning_rate 0.00356106
2017-10-10T11:40:33.514853: step 87, loss 0.936073, acc 0.84375, learning_rate 0.00354694
2017-10-10T11:40:33.634528: step 88, loss 0.483678, acc 0.859375, learning_rate 0.00353287
2017-10-10T11:40:33.753089: step 89, loss 0.676906, acc 0.8125, learning_rate 0.00351885
2017-10-10T11:40:33.873996: step 90, loss 0.724828, acc 0.796875, learning_rate 0.0035049
2017-10-10T11:40:33.999386: step 91, loss 0.786332, acc 0.765625, learning_rate 0.003491
2017-10-10T11:40:34.118252: step 92, loss 0.580862, acc 0.84375, learning_rate 0.00347716
2017-10-10T11:40:34.232389: step 93, loss 0.523475, acc 0.828125, learning_rate 0.00346338
2017-10-10T11:40:34.350817: step 94, loss 0.474609, acc 0.828125, learning_rate 0.00344965
2017-10-10T11:40:34.466751: step 95, loss 0.984087, acc 0.71875, learning_rate 0.00343597
2017-10-10T11:40:34.588805: step 96, loss 0.826328, acc 0.828125, learning_rate 0.00342236
2017-10-10T11:40:34.706552: step 97, loss 0.945947, acc 0.828125, learning_rate 0.0034088
2017-10-10T11:40:34.812231: step 98, loss 1.26568, acc 0.705882, learning_rate 0.00339529
2017-10-10T11:40:34.929686: step 99, loss 0.736948, acc 0.765625, learning_rate 0.00338184
2017-10-10T11:40:35.058993: step 100, loss 0.901416, acc 0.8125, learning_rate 0.00336844
2017-10-10T11:40:35.179234: step 101, loss 0.459822, acc 0.78125, learning_rate 0.0033551
2017-10-10T11:40:35.297557: step 102, loss 0.478544, acc 0.8125, learning_rate 0.00334182
2017-10-10T11:40:35.414015: step 103, loss 0.744809, acc 0.75, learning_rate 0.00332858
2017-10-10T11:40:35.532228: step 104, loss 0.670209, acc 0.828125, learning_rate 0.00331541
2017-10-10T11:40:35.648597: step 105, loss 0.971389, acc 0.75, learning_rate 0.00330228
2017-10-10T11:40:35.766715: step 106, loss 0.470069, acc 0.890625, learning_rate 0.00328921
2017-10-10T11:40:35.888949: step 107, loss 0.544597, acc 0.84375, learning_rate 0.00327619
2017-10-10T11:40:36.003404: step 108, loss 0.695924, acc 0.796875, learning_rate 0.00326323
2017-10-10T11:40:36.123247: step 109, loss 0.555434, acc 0.8125, learning_rate 0.00325032
2017-10-10T11:40:36.237714: step 110, loss 0.597252, acc 0.84375, learning_rate 0.00323746
2017-10-10T11:40:36.354831: step 111, loss 0.571049, acc 0.84375, learning_rate 0.00322465
2017-10-10T11:40:36.473981: step 112, loss 0.377585, acc 0.890625, learning_rate 0.0032119
2017-10-10T11:40:36.588315: step 113, loss 0.696166, acc 0.75, learning_rate 0.0031992
2017-10-10T11:40:36.704185: step 114, loss 0.721773, acc 0.796875, learning_rate 0.00318655
2017-10-10T11:40:36.820334: step 115, loss 0.414784, acc 0.859375, learning_rate 0.00317395
2017-10-10T11:40:36.937448: step 116, loss 1.01064, acc 0.765625, learning_rate 0.0031614
2017-10-10T11:40:37.051229: step 117, loss 0.364983, acc 0.84375, learning_rate 0.0031489
2017-10-10T11:40:37.167334: step 118, loss 0.499498, acc 0.890625, learning_rate 0.00313646
2017-10-10T11:40:37.284097: step 119, loss 0.620714, acc 0.84375, learning_rate 0.00312407
2017-10-10T11:40:37.400001: step 120, loss 0.624426, acc 0.875, learning_rate 0.00311172

Evaluation:
2017-10-10T11:40:37.721414: step 120, loss 0.350576, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-120

2017-10-10T11:40:38.339450: step 121, loss 0.306973, acc 0.90625, learning_rate 0.00309943
2017-10-10T11:40:38.459385: step 122, loss 0.423157, acc 0.828125, learning_rate 0.00308719
2017-10-10T11:40:38.574422: step 123, loss 0.327777, acc 0.828125, learning_rate 0.00307499
2017-10-10T11:40:38.692299: step 124, loss 0.471411, acc 0.828125, learning_rate 0.00306285
2017-10-10T11:40:38.803804: step 125, loss 0.543435, acc 0.890625, learning_rate 0.00305076
2017-10-10T11:40:38.921944: step 126, loss 0.625906, acc 0.796875, learning_rate 0.00303871
2017-10-10T11:40:39.039060: step 127, loss 0.631385, acc 0.84375, learning_rate 0.00302672
2017-10-10T11:40:39.156466: step 128, loss 0.477383, acc 0.859375, learning_rate 0.00301477
2017-10-10T11:40:39.272824: step 129, loss 0.460378, acc 0.828125, learning_rate 0.00300287
2017-10-10T11:40:39.388467: step 130, loss 0.334388, acc 0.921875, learning_rate 0.00299102
2017-10-10T11:40:39.504850: step 131, loss 0.400123, acc 0.90625, learning_rate 0.00297922
2017-10-10T11:40:39.619914: step 132, loss 0.376672, acc 0.890625, learning_rate 0.00296747
2017-10-10T11:40:39.738637: step 133, loss 0.377952, acc 0.875, learning_rate 0.00295577
2017-10-10T11:40:39.858546: step 134, loss 0.255334, acc 0.90625, learning_rate 0.00294411
2017-10-10T11:40:39.974216: step 135, loss 0.518018, acc 0.859375, learning_rate 0.0029325
2017-10-10T11:40:40.093150: step 136, loss 0.396346, acc 0.84375, learning_rate 0.00292094
2017-10-10T11:40:40.210817: step 137, loss 0.637229, acc 0.828125, learning_rate 0.00290943
2017-10-10T11:40:40.333072: step 138, loss 0.809967, acc 0.78125, learning_rate 0.00289796
2017-10-10T11:40:40.453847: step 139, loss 0.513463, acc 0.828125, learning_rate 0.00288654
2017-10-10T11:40:40.570991: step 140, loss 0.2454, acc 0.90625, learning_rate 0.00287516
2017-10-10T11:40:40.685327: step 141, loss 0.568079, acc 0.828125, learning_rate 0.00286384
2017-10-10T11:40:40.802413: step 142, loss 0.422159, acc 0.890625, learning_rate 0.00285256
2017-10-10T11:40:40.927682: step 143, loss 0.646654, acc 0.765625, learning_rate 0.00284132
2017-10-10T11:40:41.044444: step 144, loss 0.415193, acc 0.90625, learning_rate 0.00283013
2017-10-10T11:40:41.160834: step 145, loss 0.519668, acc 0.875, learning_rate 0.00281899
2017-10-10T11:40:41.279523: step 146, loss 0.642794, acc 0.828125, learning_rate 0.00280789
2017-10-10T11:40:41.391756: step 147, loss 0.456906, acc 0.828125, learning_rate 0.00279684
2017-10-10T11:40:41.502333: step 148, loss 0.347829, acc 0.890625, learning_rate 0.00278583
2017-10-10T11:40:41.621185: step 149, loss 0.368134, acc 0.84375, learning_rate 0.00277486
2017-10-10T11:40:41.736924: step 150, loss 0.521579, acc 0.78125, learning_rate 0.00276395
2017-10-10T11:40:41.857115: step 151, loss 0.350969, acc 0.890625, learning_rate 0.00275307
2017-10-10T11:40:41.982472: step 152, loss 0.455211, acc 0.828125, learning_rate 0.00274224
2017-10-10T11:40:42.101048: step 153, loss 0.56034, acc 0.890625, learning_rate 0.00273146
2017-10-10T11:40:42.221198: step 154, loss 0.682333, acc 0.765625, learning_rate 0.00272072
2017-10-10T11:40:42.342010: step 155, loss 0.391816, acc 0.84375, learning_rate 0.00271002
2017-10-10T11:40:42.456033: step 156, loss 0.738684, acc 0.875, learning_rate 0.00269937
2017-10-10T11:40:42.567798: step 157, loss 0.168909, acc 0.9375, learning_rate 0.00268876
2017-10-10T11:40:42.684781: step 158, loss 0.217968, acc 0.90625, learning_rate 0.00267819
2017-10-10T11:40:42.802930: step 159, loss 0.9552, acc 0.78125, learning_rate 0.00266767
2017-10-10T11:40:42.922318: step 160, loss 0.465668, acc 0.828125, learning_rate 0.00265719

Evaluation:
2017-10-10T11:40:43.256379: step 160, loss 0.314453, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-160

2017-10-10T11:40:43.869379: step 161, loss 0.106938, acc 0.953125, learning_rate 0.00264675
2017-10-10T11:40:43.984084: step 162, loss 0.312692, acc 0.890625, learning_rate 0.00263635
2017-10-10T11:40:44.102264: step 163, loss 0.688525, acc 0.796875, learning_rate 0.002626
2017-10-10T11:40:44.221266: step 164, loss 0.616963, acc 0.8125, learning_rate 0.00261569
2017-10-10T11:40:44.341974: step 165, loss 0.422114, acc 0.90625, learning_rate 0.00260542
2017-10-10T11:40:44.459232: step 166, loss 0.394857, acc 0.875, learning_rate 0.0025952
2017-10-10T11:40:44.578637: step 167, loss 0.299128, acc 0.9375, learning_rate 0.00258501
2017-10-10T11:40:44.691837: step 168, loss 0.255081, acc 0.921875, learning_rate 0.00257487
2017-10-10T11:40:44.808201: step 169, loss 0.232011, acc 0.890625, learning_rate 0.00256477
2017-10-10T11:40:44.931623: step 170, loss 0.645739, acc 0.78125, learning_rate 0.0025547
2017-10-10T11:40:45.046670: step 171, loss 0.454158, acc 0.859375, learning_rate 0.00254469
2017-10-10T11:40:45.165160: step 172, loss 0.657834, acc 0.8125, learning_rate 0.00253471
2017-10-10T11:40:45.285295: step 173, loss 0.350359, acc 0.859375, learning_rate 0.00252477
2017-10-10T11:40:45.405642: step 174, loss 0.554912, acc 0.828125, learning_rate 0.00251487
2017-10-10T11:40:45.521330: step 175, loss 0.489795, acc 0.828125, learning_rate 0.00250501
2017-10-10T11:40:45.638148: step 176, loss 0.176746, acc 0.9375, learning_rate 0.0024952
2017-10-10T11:40:45.757346: step 177, loss 0.492639, acc 0.859375, learning_rate 0.00248542
2017-10-10T11:40:45.881844: step 178, loss 0.742977, acc 0.8125, learning_rate 0.00247568
2017-10-10T11:40:46.000760: step 179, loss 0.61663, acc 0.765625, learning_rate 0.00246599
2017-10-10T11:40:46.118985: step 180, loss 0.683718, acc 0.8125, learning_rate 0.00245633
2017-10-10T11:40:46.236105: step 181, loss 0.467104, acc 0.890625, learning_rate 0.00244671
2017-10-10T11:40:46.356559: step 182, loss 0.49494, acc 0.84375, learning_rate 0.00243713
2017-10-10T11:40:46.473913: step 183, loss 0.478829, acc 0.84375, learning_rate 0.00242759
2017-10-10T11:40:46.590444: step 184, loss 0.556861, acc 0.859375, learning_rate 0.00241809
2017-10-10T11:40:46.707876: step 185, loss 0.499034, acc 0.890625, learning_rate 0.00240863
2017-10-10T11:40:46.821893: step 186, loss 0.222636, acc 0.953125, learning_rate 0.00239921
2017-10-10T11:40:46.939595: step 187, loss 0.47053, acc 0.90625, learning_rate 0.00238982
2017-10-10T11:40:47.057335: step 188, loss 0.360998, acc 0.890625, learning_rate 0.00238048
2017-10-10T11:40:47.174230: step 189, loss 0.248747, acc 0.90625, learning_rate 0.00237117
2017-10-10T11:40:47.293924: step 190, loss 0.284808, acc 0.921875, learning_rate 0.0023619
2017-10-10T11:40:47.410253: step 191, loss 0.664165, acc 0.8125, learning_rate 0.00235267
2017-10-10T11:40:47.530165: step 192, loss 0.544399, acc 0.828125, learning_rate 0.00234347
2017-10-10T11:40:47.643643: step 193, loss 0.372978, acc 0.90625, learning_rate 0.00233431
2017-10-10T11:40:47.760544: step 194, loss 0.35501, acc 0.890625, learning_rate 0.00232519
2017-10-10T11:40:47.880467: step 195, loss 0.205864, acc 0.921875, learning_rate 0.00231611
2017-10-10T11:40:47.976352: step 196, loss 0.742009, acc 0.823529, learning_rate 0.00230707
2017-10-10T11:40:48.091660: step 197, loss 0.208543, acc 0.921875, learning_rate 0.00229806
2017-10-10T11:40:48.209848: step 198, loss 0.162941, acc 0.9375, learning_rate 0.00228908
2017-10-10T11:40:48.324181: step 199, loss 0.517314, acc 0.875, learning_rate 0.00228015
2017-10-10T11:40:48.444626: step 200, loss 0.268195, acc 0.890625, learning_rate 0.00227125

Evaluation:
2017-10-10T11:40:48.769255: step 200, loss 0.344373, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-200

2017-10-10T11:40:49.376111: step 201, loss 0.386191, acc 0.859375, learning_rate 0.00226239
2017-10-10T11:40:49.496394: step 202, loss 0.498176, acc 0.875, learning_rate 0.00225356
2017-10-10T11:40:49.617000: step 203, loss 0.195674, acc 0.9375, learning_rate 0.00224477
2017-10-10T11:40:49.732328: step 204, loss 0.415948, acc 0.875, learning_rate 0.00223602
2017-10-10T11:40:49.850749: step 205, loss 0.203596, acc 0.921875, learning_rate 0.0022273
2017-10-10T11:40:49.965697: step 206, loss 0.283678, acc 0.890625, learning_rate 0.00221862
2017-10-10T11:40:50.089777: step 207, loss 0.412114, acc 0.90625, learning_rate 0.00220997
2017-10-10T11:40:50.206172: step 208, loss 0.169636, acc 0.9375, learning_rate 0.00220136
2017-10-10T11:40:50.324475: step 209, loss 0.318476, acc 0.90625, learning_rate 0.00219278
2017-10-10T11:40:50.437011: step 210, loss 0.32915, acc 0.90625, learning_rate 0.00218424
2017-10-10T11:40:50.550570: step 211, loss 0.240119, acc 0.9375, learning_rate 0.00217573
2017-10-10T11:40:50.672242: step 212, loss 0.102451, acc 0.96875, learning_rate 0.00216726
2017-10-10T11:40:50.792451: step 213, loss 0.224777, acc 0.921875, learning_rate 0.00215882
2017-10-10T11:40:50.907716: step 214, loss 0.612299, acc 0.828125, learning_rate 0.00215041
2017-10-10T11:40:51.019233: step 215, loss 0.432869, acc 0.890625, learning_rate 0.00214204
2017-10-10T11:40:51.135186: step 216, loss 0.427569, acc 0.84375, learning_rate 0.00213371
2017-10-10T11:40:51.254808: step 217, loss 0.43661, acc 0.9375, learning_rate 0.00212541
2017-10-10T11:40:51.375129: step 218, loss 0.607119, acc 0.828125, learning_rate 0.00211714
2017-10-10T11:40:51.491942: step 219, loss 0.433458, acc 0.875, learning_rate 0.00210891
2017-10-10T11:40:51.606268: step 220, loss 0.243346, acc 0.921875, learning_rate 0.00210071
2017-10-10T11:40:51.725794: step 221, loss 0.228851, acc 0.9375, learning_rate 0.00209254
2017-10-10T11:40:51.843340: step 222, loss 0.229674, acc 0.90625, learning_rate 0.00208441
2017-10-10T11:40:51.960591: step 223, loss 0.281791, acc 0.875, learning_rate 0.00207631
2017-10-10T11:40:52.082491: step 224, loss 0.428918, acc 0.859375, learning_rate 0.00206824
2017-10-10T11:40:52.200403: step 225, loss 0.301711, acc 0.9375, learning_rate 0.00206021
2017-10-10T11:40:52.316043: step 226, loss 0.150377, acc 0.921875, learning_rate 0.00205221
2017-10-10T11:40:52.435938: step 227, loss 0.160496, acc 0.9375, learning_rate 0.00204424
2017-10-10T11:40:52.554843: step 228, loss 0.42681, acc 0.875, learning_rate 0.0020363
2017-10-10T11:40:52.673628: step 229, loss 0.301394, acc 0.90625, learning_rate 0.0020284
2017-10-10T11:40:52.789589: step 230, loss 0.277165, acc 0.921875, learning_rate 0.00202053
2017-10-10T11:40:52.906544: step 231, loss 0.745979, acc 0.78125, learning_rate 0.00201269
2017-10-10T11:40:53.026000: step 232, loss 0.239473, acc 0.921875, learning_rate 0.00200488
2017-10-10T11:40:53.140904: step 233, loss 0.304019, acc 0.890625, learning_rate 0.00199711
2017-10-10T11:40:53.258105: step 234, loss 0.224455, acc 0.96875, learning_rate 0.00198936
2017-10-10T11:40:53.372280: step 235, loss 0.249243, acc 0.921875, learning_rate 0.00198165
2017-10-10T11:40:53.490927: step 236, loss 0.226267, acc 0.9375, learning_rate 0.00197397
2017-10-10T11:40:53.610129: step 237, loss 0.358243, acc 0.875, learning_rate 0.00196632
2017-10-10T11:40:53.722702: step 238, loss 0.391664, acc 0.875, learning_rate 0.0019587
2017-10-10T11:40:53.840790: step 239, loss 0.433369, acc 0.828125, learning_rate 0.00195112
2017-10-10T11:40:53.959576: step 240, loss 0.305075, acc 0.875, learning_rate 0.00194356

Evaluation:
2017-10-10T11:40:54.280542: step 240, loss 0.29303, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-240

2017-10-10T11:40:54.896928: step 241, loss 0.339932, acc 0.859375, learning_rate 0.00193604
2017-10-10T11:40:55.009766: step 242, loss 0.284347, acc 0.90625, learning_rate 0.00192854
2017-10-10T11:40:55.127693: step 243, loss 0.289946, acc 0.90625, learning_rate 0.00192108
2017-10-10T11:40:55.244689: step 244, loss 0.371703, acc 0.890625, learning_rate 0.00191364
2017-10-10T11:40:55.356329: step 245, loss 0.345414, acc 0.8125, learning_rate 0.00190624
2017-10-10T11:40:55.472162: step 246, loss 0.218244, acc 0.921875, learning_rate 0.00189887
2017-10-10T11:40:55.590932: step 247, loss 0.402591, acc 0.90625, learning_rate 0.00189153
2017-10-10T11:40:55.707104: step 248, loss 0.339619, acc 0.90625, learning_rate 0.00188421
2017-10-10T11:40:55.823707: step 249, loss 0.342108, acc 0.90625, learning_rate 0.00187693
2017-10-10T11:40:55.943653: step 250, loss 0.24561, acc 0.875, learning_rate 0.00186968
2017-10-10T11:40:56.064171: step 251, loss 0.261911, acc 0.875, learning_rate 0.00186245
2017-10-10T11:40:56.181792: step 252, loss 0.127161, acc 0.96875, learning_rate 0.00185526
2017-10-10T11:40:56.297848: step 253, loss 0.310904, acc 0.875, learning_rate 0.0018481
2017-10-10T11:40:56.414573: step 254, loss 0.310318, acc 0.890625, learning_rate 0.00184096
2017-10-10T11:40:56.534658: step 255, loss 0.199367, acc 0.90625, learning_rate 0.00183385
2017-10-10T11:40:56.651834: step 256, loss 0.50162, acc 0.8125, learning_rate 0.00182678
2017-10-10T11:40:56.766580: step 257, loss 0.189161, acc 0.921875, learning_rate 0.00181973
2017-10-10T11:40:56.888489: step 258, loss 0.219315, acc 0.9375, learning_rate 0.00181271
2017-10-10T11:40:57.000154: step 259, loss 0.151834, acc 0.921875, learning_rate 0.00180572
2017-10-10T11:40:57.116547: step 260, loss 0.311595, acc 0.921875, learning_rate 0.00179876
2017-10-10T11:40:57.234073: step 261, loss 0.411134, acc 0.859375, learning_rate 0.00179182
2017-10-10T11:40:57.349929: step 262, loss 0.360805, acc 0.875, learning_rate 0.00178492
2017-10-10T11:40:57.466241: step 263, loss 0.337382, acc 0.875, learning_rate 0.00177804
2017-10-10T11:40:57.588157: step 264, loss 0.248356, acc 0.90625, learning_rate 0.00177119
2017-10-10T11:40:57.708565: step 265, loss 0.231044, acc 0.921875, learning_rate 0.00176437
2017-10-10T11:40:57.827944: step 266, loss 0.226732, acc 0.90625, learning_rate 0.00175758
2017-10-10T11:40:57.944191: step 267, loss 0.271023, acc 0.921875, learning_rate 0.00175081
2017-10-10T11:40:58.057673: step 268, loss 0.456568, acc 0.875, learning_rate 0.00174407
2017-10-10T11:40:58.172452: step 269, loss 0.487106, acc 0.765625, learning_rate 0.00173736
2017-10-10T11:40:58.293173: step 270, loss 0.0699613, acc 0.96875, learning_rate 0.00173068
2017-10-10T11:40:58.413944: step 271, loss 0.25556, acc 0.921875, learning_rate 0.00172402
2017-10-10T11:40:58.535133: step 272, loss 0.259837, acc 0.90625, learning_rate 0.00171739
2017-10-10T11:40:58.651169: step 273, loss 0.162776, acc 0.921875, learning_rate 0.00171079
2017-10-10T11:40:58.772467: step 274, loss 0.485487, acc 0.828125, learning_rate 0.00170422
2017-10-10T11:40:58.896742: step 275, loss 0.51224, acc 0.828125, learning_rate 0.00169767
2017-10-10T11:40:59.015790: step 276, loss 0.199109, acc 0.90625, learning_rate 0.00169115
2017-10-10T11:40:59.134681: step 277, loss 0.492744, acc 0.84375, learning_rate 0.00168465
2017-10-10T11:40:59.250490: step 278, loss 0.234811, acc 0.90625, learning_rate 0.00167818
2017-10-10T11:40:59.366422: step 279, loss 0.369514, acc 0.84375, learning_rate 0.00167174
2017-10-10T11:40:59.486776: step 280, loss 0.121634, acc 0.953125, learning_rate 0.00166533

Evaluation:
2017-10-10T11:40:59.803030: step 280, loss 0.258628, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-280

2017-10-10T11:41:00.417304: step 281, loss 0.433378, acc 0.859375, learning_rate 0.00165894
2017-10-10T11:41:00.536802: step 282, loss 0.655631, acc 0.796875, learning_rate 0.00165257
2017-10-10T11:41:00.662344: step 283, loss 0.204366, acc 0.9375, learning_rate 0.00164624
2017-10-10T11:41:00.787652: step 284, loss 0.373532, acc 0.859375, learning_rate 0.00163993
2017-10-10T11:41:00.906950: step 285, loss 0.245091, acc 0.90625, learning_rate 0.00163364
2017-10-10T11:41:01.029748: step 286, loss 0.253273, acc 0.921875, learning_rate 0.00162738
2017-10-10T11:41:01.144295: step 287, loss 0.173202, acc 0.921875, learning_rate 0.00162115
2017-10-10T11:41:01.263167: step 288, loss 0.253194, acc 0.90625, learning_rate 0.00161494
2017-10-10T11:41:01.380470: step 289, loss 0.283485, acc 0.859375, learning_rate 0.00160875
2017-10-10T11:41:01.501313: step 290, loss 0.400139, acc 0.859375, learning_rate 0.00160259
2017-10-10T11:41:01.623402: step 291, loss 0.169506, acc 0.9375, learning_rate 0.00159646
2017-10-10T11:41:01.743063: step 292, loss 0.159892, acc 0.953125, learning_rate 0.00159035
2017-10-10T11:41:01.861196: step 293, loss 0.32948, acc 0.921875, learning_rate 0.00158427
2017-10-10T11:41:01.959666: step 294, loss 0.256269, acc 0.921569, learning_rate 0.00157821
2017-10-10T11:41:02.074731: step 295, loss 0.115205, acc 0.9375, learning_rate 0.00157218
2017-10-10T11:41:02.193489: step 296, loss 0.252478, acc 0.875, learning_rate 0.00156617
2017-10-10T11:41:02.309249: step 297, loss 0.262048, acc 0.921875, learning_rate 0.00156018
2017-10-10T11:41:02.420397: step 298, loss 0.274766, acc 0.921875, learning_rate 0.00155422
2017-10-10T11:41:02.540194: step 299, loss 0.383861, acc 0.828125, learning_rate 0.00154829
2017-10-10T11:41:02.651344: step 300, loss 0.0985028, acc 0.96875, learning_rate 0.00154238
2017-10-10T11:41:02.777789: step 301, loss 0.375161, acc 0.84375, learning_rate 0.00153649
2017-10-10T11:41:02.902816: step 302, loss 0.225642, acc 0.9375, learning_rate 0.00153063
2017-10-10T11:41:03.018680: step 303, loss 0.255826, acc 0.921875, learning_rate 0.00152479
2017-10-10T11:41:03.135985: step 304, loss 0.323109, acc 0.875, learning_rate 0.00151897
2017-10-10T11:41:03.251963: step 305, loss 0.188998, acc 0.921875, learning_rate 0.00151318
2017-10-10T11:41:03.373263: step 306, loss 0.514644, acc 0.859375, learning_rate 0.00150741
2017-10-10T11:41:03.494336: step 307, loss 0.319136, acc 0.890625, learning_rate 0.00150167
2017-10-10T11:41:03.609274: step 308, loss 0.152542, acc 0.953125, learning_rate 0.00149594
2017-10-10T11:41:03.728462: step 309, loss 0.239492, acc 0.921875, learning_rate 0.00149025
2017-10-10T11:41:03.844763: step 310, loss 0.098005, acc 0.96875, learning_rate 0.00148457
2017-10-10T11:41:03.963074: step 311, loss 0.213473, acc 0.921875, learning_rate 0.00147892
2017-10-10T11:41:04.083643: step 312, loss 0.292889, acc 0.921875, learning_rate 0.00147329
2017-10-10T11:41:04.201076: step 313, loss 0.168257, acc 0.9375, learning_rate 0.00146769
2017-10-10T11:41:04.317255: step 314, loss 0.510759, acc 0.828125, learning_rate 0.0014621
2017-10-10T11:41:04.435480: step 315, loss 0.256604, acc 0.921875, learning_rate 0.00145654
2017-10-10T11:41:04.551660: step 316, loss 0.32473, acc 0.890625, learning_rate 0.00145101
2017-10-10T11:41:04.673382: step 317, loss 0.231379, acc 0.890625, learning_rate 0.00144549
2017-10-10T11:41:04.795314: step 318, loss 0.261046, acc 0.90625, learning_rate 0.00144
2017-10-10T11:41:04.911352: step 319, loss 0.467489, acc 0.875, learning_rate 0.00143453
2017-10-10T11:41:05.032686: step 320, loss 0.277551, acc 0.890625, learning_rate 0.00142908

Evaluation:
2017-10-10T11:41:05.371491: step 320, loss 0.251982, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-320

2017-10-10T11:41:05.992796: step 321, loss 0.369868, acc 0.859375, learning_rate 0.00142366
2017-10-10T11:41:06.112984: step 322, loss 0.220904, acc 0.90625, learning_rate 0.00141826
2017-10-10T11:41:06.228408: step 323, loss 0.268052, acc 0.921875, learning_rate 0.00141288
2017-10-10T11:41:06.343377: step 324, loss 0.247515, acc 0.921875, learning_rate 0.00140752
2017-10-10T11:41:06.457630: step 325, loss 0.27459, acc 0.921875, learning_rate 0.00140218
2017-10-10T11:41:06.582572: step 326, loss 0.158574, acc 0.90625, learning_rate 0.00139686
2017-10-10T11:41:06.702984: step 327, loss 0.260408, acc 0.953125, learning_rate 0.00139157
2017-10-10T11:41:06.821971: step 328, loss 0.35792, acc 0.84375, learning_rate 0.0013863
2017-10-10T11:41:06.944864: step 329, loss 0.166118, acc 0.953125, learning_rate 0.00138105
2017-10-10T11:41:07.060916: step 330, loss 0.195444, acc 0.9375, learning_rate 0.00137582
2017-10-10T11:41:07.175464: step 331, loss 0.221093, acc 0.953125, learning_rate 0.00137061
2017-10-10T11:41:07.292182: step 332, loss 0.313233, acc 0.9375, learning_rate 0.00136543
2017-10-10T11:41:07.406905: step 333, loss 0.233397, acc 0.953125, learning_rate 0.00136026
2017-10-10T11:41:07.524825: step 334, loss 0.339987, acc 0.90625, learning_rate 0.00135512
2017-10-10T11:41:07.645128: step 335, loss 0.108516, acc 0.96875, learning_rate 0.00134999
2017-10-10T11:41:07.759722: step 336, loss 0.454613, acc 0.84375, learning_rate 0.00134489
2017-10-10T11:41:07.879542: step 337, loss 0.3553, acc 0.890625, learning_rate 0.00133981
2017-10-10T11:41:07.993813: step 338, loss 0.362707, acc 0.84375, learning_rate 0.00133475
2017-10-10T11:41:08.116368: step 339, loss 0.234918, acc 0.890625, learning_rate 0.00132971
2017-10-10T11:41:08.228826: step 340, loss 0.470771, acc 0.859375, learning_rate 0.00132469
2017-10-10T11:41:08.343551: step 341, loss 0.127902, acc 0.96875, learning_rate 0.00131969
2017-10-10T11:41:08.457582: step 342, loss 0.199826, acc 0.90625, learning_rate 0.00131471
2017-10-10T11:41:08.572013: step 343, loss 0.115916, acc 0.953125, learning_rate 0.00130975
2017-10-10T11:41:08.693242: step 344, loss 0.152423, acc 0.96875, learning_rate 0.00130482
2017-10-10T11:41:08.806711: step 345, loss 0.293441, acc 0.859375, learning_rate 0.0012999
2017-10-10T11:41:08.925750: step 346, loss 0.311231, acc 0.890625, learning_rate 0.001295
2017-10-10T11:41:09.040571: step 347, loss 0.26258, acc 0.90625, learning_rate 0.00129012
2017-10-10T11:41:09.157995: step 348, loss 0.236857, acc 0.9375, learning_rate 0.00128527
2017-10-10T11:41:09.274130: step 349, loss 0.329793, acc 0.921875, learning_rate 0.00128043
2017-10-10T11:41:09.392569: step 350, loss 0.238852, acc 0.90625, learning_rate 0.00127561
2017-10-10T11:41:09.505678: step 351, loss 0.177231, acc 0.921875, learning_rate 0.00127081
2017-10-10T11:41:09.623198: step 352, loss 0.146991, acc 0.953125, learning_rate 0.00126603
2017-10-10T11:41:09.743462: step 353, loss 0.345325, acc 0.875, learning_rate 0.00126127
2017-10-10T11:41:09.864610: step 354, loss 0.340244, acc 0.84375, learning_rate 0.00125653
2017-10-10T11:41:09.982631: step 355, loss 0.137265, acc 0.96875, learning_rate 0.00125181
2017-10-10T11:41:10.095373: step 356, loss 0.220076, acc 0.90625, learning_rate 0.00124711
2017-10-10T11:41:10.209045: step 357, loss 0.230789, acc 0.90625, learning_rate 0.00124243
2017-10-10T11:41:10.326321: step 358, loss 0.191312, acc 0.921875, learning_rate 0.00123777
2017-10-10T11:41:10.443947: step 359, loss 0.197514, acc 0.9375, learning_rate 0.00123312
2017-10-10T11:41:10.560238: step 360, loss 0.516372, acc 0.84375, learning_rate 0.0012285

Evaluation:
2017-10-10T11:41:10.884178: step 360, loss 0.252463, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-360

2017-10-10T11:41:11.506458: step 361, loss 0.162526, acc 0.921875, learning_rate 0.00122389
2017-10-10T11:41:11.619272: step 362, loss 0.361698, acc 0.875, learning_rate 0.0012193
2017-10-10T11:41:11.734369: step 363, loss 0.107788, acc 0.96875, learning_rate 0.00121473
2017-10-10T11:41:11.853602: step 364, loss 0.341687, acc 0.890625, learning_rate 0.00121018
2017-10-10T11:41:11.971776: step 365, loss 0.318427, acc 0.921875, learning_rate 0.00120565
2017-10-10T11:41:12.083767: step 366, loss 0.42138, acc 0.875, learning_rate 0.00120114
2017-10-10T11:41:12.202137: step 367, loss 0.185849, acc 0.921875, learning_rate 0.00119664
2017-10-10T11:41:12.315480: step 368, loss 0.142463, acc 0.9375, learning_rate 0.00119217
2017-10-10T11:41:12.430873: step 369, loss 0.159042, acc 0.9375, learning_rate 0.00118771
2017-10-10T11:41:12.544194: step 370, loss 0.22334, acc 0.921875, learning_rate 0.00118327
2017-10-10T11:41:12.659605: step 371, loss 0.175438, acc 0.9375, learning_rate 0.00117885
2017-10-10T11:41:12.776665: step 372, loss 0.319439, acc 0.890625, learning_rate 0.00117445
2017-10-10T11:41:12.891548: step 373, loss 0.144621, acc 0.96875, learning_rate 0.00117006
2017-10-10T11:41:13.009641: step 374, loss 0.346886, acc 0.921875, learning_rate 0.00116569
2017-10-10T11:41:13.124623: step 375, loss 0.233213, acc 0.890625, learning_rate 0.00116134
2017-10-10T11:41:13.244586: step 376, loss 0.180098, acc 0.953125, learning_rate 0.00115701
2017-10-10T11:41:13.362923: step 377, loss 0.192702, acc 0.953125, learning_rate 0.0011527
2017-10-10T11:41:13.479697: step 378, loss 0.196096, acc 0.953125, learning_rate 0.0011484
2017-10-10T11:41:13.597143: step 379, loss 0.212984, acc 0.9375, learning_rate 0.00114412
2017-10-10T11:41:13.707989: step 380, loss 0.181553, acc 0.9375, learning_rate 0.00113986
2017-10-10T11:41:13.826248: step 381, loss 0.327747, acc 0.90625, learning_rate 0.00113561
2017-10-10T11:41:13.942192: step 382, loss 0.197823, acc 0.9375, learning_rate 0.00113139
2017-10-10T11:41:14.056715: step 383, loss 0.341935, acc 0.90625, learning_rate 0.00112718
2017-10-10T11:41:14.174112: step 384, loss 0.171797, acc 0.9375, learning_rate 0.00112298
2017-10-10T11:41:14.285270: step 385, loss 0.124237, acc 0.96875, learning_rate 0.00111881
2017-10-10T11:41:14.401253: step 386, loss 0.196967, acc 0.921875, learning_rate 0.00111465
2017-10-10T11:41:14.525853: step 387, loss 0.249627, acc 0.890625, learning_rate 0.00111051
2017-10-10T11:41:14.641203: step 388, loss 0.196658, acc 0.953125, learning_rate 0.00110638
2017-10-10T11:41:14.758949: step 389, loss 0.310072, acc 0.890625, learning_rate 0.00110228
2017-10-10T11:41:14.876379: step 390, loss 0.294733, acc 0.875, learning_rate 0.00109818
2017-10-10T11:41:14.993303: step 391, loss 0.247597, acc 0.9375, learning_rate 0.00109411
2017-10-10T11:41:15.090479: step 392, loss 0.399127, acc 0.843137, learning_rate 0.00109005
2017-10-10T11:41:15.211216: step 393, loss 0.138321, acc 0.953125, learning_rate 0.00108601
2017-10-10T11:41:15.331997: step 394, loss 0.133136, acc 0.953125, learning_rate 0.00108199
2017-10-10T11:41:15.451203: step 395, loss 0.13864, acc 0.953125, learning_rate 0.00107798
2017-10-10T11:41:15.568001: step 396, loss 0.123026, acc 0.953125, learning_rate 0.00107399
2017-10-10T11:41:15.686764: step 397, loss 0.185146, acc 0.921875, learning_rate 0.00107001
2017-10-10T11:41:15.800179: step 398, loss 0.197702, acc 0.921875, learning_rate 0.00106605
2017-10-10T11:41:15.918787: step 399, loss 0.150016, acc 0.921875, learning_rate 0.00106211
2017-10-10T11:41:16.040184: step 400, loss 0.161556, acc 0.921875, learning_rate 0.00105818

Evaluation:
2017-10-10T11:41:16.379825: step 400, loss 0.239968, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-400

2017-10-10T11:41:16.920282: step 401, loss 0.265431, acc 0.890625, learning_rate 0.00105427
2017-10-10T11:41:17.037856: step 402, loss 0.261504, acc 0.890625, learning_rate 0.00105037
2017-10-10T11:41:17.148518: step 403, loss 0.146633, acc 0.96875, learning_rate 0.0010465
2017-10-10T11:41:17.270002: step 404, loss 0.353578, acc 0.84375, learning_rate 0.00104263
2017-10-10T11:41:17.389432: step 405, loss 0.281122, acc 0.921875, learning_rate 0.00103878
2017-10-10T11:41:17.503511: step 406, loss 0.234264, acc 0.921875, learning_rate 0.00103495
2017-10-10T11:41:17.620286: step 407, loss 0.283574, acc 0.90625, learning_rate 0.00103114
2017-10-10T11:41:17.734485: step 408, loss 0.15095, acc 0.9375, learning_rate 0.00102734
2017-10-10T11:41:17.856278: step 409, loss 0.244807, acc 0.90625, learning_rate 0.00102355
2017-10-10T11:41:17.974239: step 410, loss 0.351032, acc 0.875, learning_rate 0.00101978
2017-10-10T11:41:18.095170: step 411, loss 0.293956, acc 0.90625, learning_rate 0.00101603
2017-10-10T11:41:18.222084: step 412, loss 0.190805, acc 0.9375, learning_rate 0.00101229
2017-10-10T11:41:18.341847: step 413, loss 0.178004, acc 0.9375, learning_rate 0.00100856
2017-10-10T11:41:18.459952: step 414, loss 0.182124, acc 0.921875, learning_rate 0.00100486
2017-10-10T11:41:18.581141: step 415, loss 0.133596, acc 0.953125, learning_rate 0.00100116
2017-10-10T11:41:18.698089: step 416, loss 0.174241, acc 0.921875, learning_rate 0.000997483
2017-10-10T11:41:18.815884: step 417, loss 0.147417, acc 0.9375, learning_rate 0.00099382
2017-10-10T11:41:18.931920: step 418, loss 0.162745, acc 0.953125, learning_rate 0.000990172
2017-10-10T11:41:19.044295: step 419, loss 0.106381, acc 0.953125, learning_rate 0.000986538
2017-10-10T11:41:19.159708: step 420, loss 0.159765, acc 0.9375, learning_rate 0.00098292
2017-10-10T11:41:19.278874: step 421, loss 0.0761123, acc 0.984375, learning_rate 0.000979316
2017-10-10T11:41:19.398395: step 422, loss 0.165063, acc 0.953125, learning_rate 0.000975727
2017-10-10T11:41:19.516948: step 423, loss 0.200762, acc 0.953125, learning_rate 0.000972152
2017-10-10T11:41:19.638212: step 424, loss 0.261433, acc 0.953125, learning_rate 0.000968592
2017-10-10T11:41:19.759770: step 425, loss 0.266642, acc 0.90625, learning_rate 0.000965047
2017-10-10T11:41:19.875632: step 426, loss 0.152098, acc 0.96875, learning_rate 0.000961516
2017-10-10T11:41:19.989169: step 427, loss 0.101985, acc 0.953125, learning_rate 0.000958
2017-10-10T11:41:20.110301: step 428, loss 0.189448, acc 0.953125, learning_rate 0.000954497
2017-10-10T11:41:20.229519: step 429, loss 0.317632, acc 0.875, learning_rate 0.00095101
2017-10-10T11:41:20.344895: step 430, loss 0.17913, acc 0.921875, learning_rate 0.000947536
2017-10-10T11:41:20.456713: step 431, loss 0.307311, acc 0.90625, learning_rate 0.000944076
2017-10-10T11:41:20.576149: step 432, loss 0.175417, acc 0.9375, learning_rate 0.000940631
2017-10-10T11:41:20.697256: step 433, loss 0.10834, acc 0.953125, learning_rate 0.0009372
2017-10-10T11:41:20.814942: step 434, loss 0.204966, acc 0.9375, learning_rate 0.000933783
2017-10-10T11:41:20.929797: step 435, loss 0.116296, acc 0.953125, learning_rate 0.000930379
2017-10-10T11:41:21.047587: step 436, loss 0.138322, acc 0.984375, learning_rate 0.00092699
2017-10-10T11:41:21.163086: step 437, loss 0.203125, acc 0.9375, learning_rate 0.000923614
2017-10-10T11:41:21.279346: step 438, loss 0.221684, acc 0.921875, learning_rate 0.000920253
2017-10-10T11:41:21.393869: step 439, loss 0.202014, acc 0.90625, learning_rate 0.000916905
2017-10-10T11:41:21.512438: step 440, loss 0.34579, acc 0.859375, learning_rate 0.00091357

Evaluation:
2017-10-10T11:41:21.842340: step 440, loss 0.232226, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-440

2017-10-10T11:41:22.442975: step 441, loss 0.162934, acc 0.953125, learning_rate 0.000910249
2017-10-10T11:41:22.560630: step 442, loss 0.273583, acc 0.9375, learning_rate 0.000906942
2017-10-10T11:41:22.677559: step 443, loss 0.148202, acc 0.9375, learning_rate 0.000903648
2017-10-10T11:41:22.795499: step 444, loss 0.344551, acc 0.859375, learning_rate 0.000900368
2017-10-10T11:41:22.912369: step 445, loss 0.256261, acc 0.890625, learning_rate 0.000897101
2017-10-10T11:41:23.023542: step 446, loss 0.27097, acc 0.921875, learning_rate 0.000893848
2017-10-10T11:41:23.145663: step 447, loss 0.140922, acc 0.9375, learning_rate 0.000890607
2017-10-10T11:41:23.263748: step 448, loss 0.377825, acc 0.921875, learning_rate 0.00088738
2017-10-10T11:41:23.380441: step 449, loss 0.253386, acc 0.953125, learning_rate 0.000884166
2017-10-10T11:41:23.496899: step 450, loss 0.104505, acc 0.953125, learning_rate 0.000880966
2017-10-10T11:41:23.608502: step 451, loss 0.0842662, acc 0.96875, learning_rate 0.000877778
2017-10-10T11:41:23.726252: step 452, loss 0.199561, acc 0.890625, learning_rate 0.000874603
2017-10-10T11:41:23.842052: step 453, loss 0.251689, acc 0.90625, learning_rate 0.000871441
2017-10-10T11:41:23.960890: step 454, loss 0.16049, acc 0.953125, learning_rate 0.000868293
2017-10-10T11:41:24.079327: step 455, loss 0.404263, acc 0.890625, learning_rate 0.000865157
2017-10-10T11:41:24.198143: step 456, loss 0.196924, acc 0.921875, learning_rate 0.000862033
2017-10-10T11:41:24.313073: step 457, loss 0.228451, acc 0.890625, learning_rate 0.000858923
2017-10-10T11:41:24.430256: step 458, loss 0.30305, acc 0.90625, learning_rate 0.000855825
2017-10-10T11:41:24.549742: step 459, loss 0.166178, acc 0.96875, learning_rate 0.00085274
2017-10-10T11:41:24.664283: step 460, loss 0.159916, acc 0.953125, learning_rate 0.000849668
2017-10-10T11:41:24.783394: step 461, loss 0.192574, acc 0.921875, learning_rate 0.000846608
2017-10-10T11:41:24.898906: step 462, loss 0.178943, acc 0.9375, learning_rate 0.00084356
2017-10-10T11:41:25.016878: step 463, loss 0.219419, acc 0.90625, learning_rate 0.000840525
2017-10-10T11:41:25.127999: step 464, loss 0.157863, acc 0.9375, learning_rate 0.000837502
2017-10-10T11:41:25.241490: step 465, loss 0.327647, acc 0.859375, learning_rate 0.000834492
2017-10-10T11:41:25.359282: step 466, loss 0.11006, acc 0.984375, learning_rate 0.000831494
2017-10-10T11:41:25.479773: step 467, loss 0.187406, acc 0.921875, learning_rate 0.000828508
2017-10-10T11:41:25.601326: step 468, loss 0.109429, acc 0.953125, learning_rate 0.000825535
2017-10-10T11:41:25.718487: step 469, loss 0.296702, acc 0.890625, learning_rate 0.000822573
2017-10-10T11:41:25.833986: step 470, loss 0.0793627, acc 0.984375, learning_rate 0.000819624
2017-10-10T11:41:25.952742: step 471, loss 0.226005, acc 0.90625, learning_rate 0.000816687
2017-10-10T11:41:26.070985: step 472, loss 0.192621, acc 0.9375, learning_rate 0.000813761
2017-10-10T11:41:26.187062: step 473, loss 0.198326, acc 0.921875, learning_rate 0.000810848
2017-10-10T11:41:26.303252: step 474, loss 0.121847, acc 0.953125, learning_rate 0.000807946
2017-10-10T11:41:26.415996: step 475, loss 0.361742, acc 0.859375, learning_rate 0.000805057
2017-10-10T11:41:26.534453: step 476, loss 0.147049, acc 0.9375, learning_rate 0.000802179
2017-10-10T11:41:26.659451: step 477, loss 0.148587, acc 0.9375, learning_rate 0.000799313
2017-10-10T11:41:26.775807: step 478, loss 0.232699, acc 0.9375, learning_rate 0.000796458
2017-10-10T11:41:26.894154: step 479, loss 0.286242, acc 0.890625, learning_rate 0.000793616
2017-10-10T11:41:27.006427: step 480, loss 0.177195, acc 0.984375, learning_rate 0.000790784

Evaluation:
2017-10-10T11:41:27.338110: step 480, loss 0.234747, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-480

2017-10-10T11:41:28.011854: step 481, loss 0.227648, acc 0.921875, learning_rate 0.000787965
2017-10-10T11:41:28.130812: step 482, loss 0.150355, acc 0.96875, learning_rate 0.000785157
2017-10-10T11:41:28.248034: step 483, loss 0.205998, acc 0.953125, learning_rate 0.00078236
2017-10-10T11:41:28.358899: step 484, loss 0.0748877, acc 0.984375, learning_rate 0.000779575
2017-10-10T11:41:28.469830: step 485, loss 0.104981, acc 0.953125, learning_rate 0.000776801
2017-10-10T11:41:28.584381: step 486, loss 0.34436, acc 0.890625, learning_rate 0.000774038
2017-10-10T11:41:28.701547: step 487, loss 0.192091, acc 0.953125, learning_rate 0.000771287
2017-10-10T11:41:28.817795: step 488, loss 0.315009, acc 0.890625, learning_rate 0.000768547
2017-10-10T11:41:28.936429: step 489, loss 0.413606, acc 0.84375, learning_rate 0.000765818
2017-10-10T11:41:29.036257: step 490, loss 0.176949, acc 0.921569, learning_rate 0.000763101
2017-10-10T11:41:29.155110: step 491, loss 0.148813, acc 0.9375, learning_rate 0.000760394
2017-10-10T11:41:29.268709: step 492, loss 0.0852083, acc 0.96875, learning_rate 0.000757698
2017-10-10T11:41:29.381752: step 493, loss 0.150484, acc 0.9375, learning_rate 0.000755014
2017-10-10T11:41:29.497428: step 494, loss 0.256064, acc 0.921875, learning_rate 0.00075234
2017-10-10T11:41:29.615344: step 495, loss 0.223364, acc 0.921875, learning_rate 0.000749677
2017-10-10T11:41:29.727555: step 496, loss 0.190708, acc 0.9375, learning_rate 0.000747026
2017-10-10T11:41:29.844971: step 497, loss 0.423303, acc 0.859375, learning_rate 0.000744385
2017-10-10T11:41:29.961779: step 498, loss 0.347722, acc 0.921875, learning_rate 0.000741754
2017-10-10T11:41:30.079103: step 499, loss 0.140687, acc 0.96875, learning_rate 0.000739135
2017-10-10T11:41:30.198820: step 500, loss 0.119427, acc 0.953125, learning_rate 0.000736526
2017-10-10T11:41:30.315366: step 501, loss 0.12659, acc 0.953125, learning_rate 0.000733928
2017-10-10T11:41:30.431175: step 502, loss 0.30533, acc 0.890625, learning_rate 0.00073134
2017-10-10T11:41:30.549048: step 503, loss 0.209839, acc 0.859375, learning_rate 0.000728763
2017-10-10T11:41:30.665758: step 504, loss 0.103015, acc 0.9375, learning_rate 0.000726197
2017-10-10T11:41:30.784077: step 505, loss 0.104435, acc 0.96875, learning_rate 0.000723641
2017-10-10T11:41:30.901986: step 506, loss 0.0710424, acc 0.984375, learning_rate 0.000721095
2017-10-10T11:41:31.016737: step 507, loss 0.244238, acc 0.90625, learning_rate 0.00071856
2017-10-10T11:41:31.133888: step 508, loss 0.212378, acc 0.921875, learning_rate 0.000716036
2017-10-10T11:41:31.253951: step 509, loss 0.327167, acc 0.90625, learning_rate 0.000713521
2017-10-10T11:41:31.367121: step 510, loss 0.208693, acc 0.921875, learning_rate 0.000711017
2017-10-10T11:41:31.480863: step 511, loss 0.19146, acc 0.890625, learning_rate 0.000708523
2017-10-10T11:41:31.600478: step 512, loss 0.308442, acc 0.875, learning_rate 0.000706039
2017-10-10T11:41:31.716109: step 513, loss 0.285289, acc 0.921875, learning_rate 0.000703565
2017-10-10T11:41:31.834547: step 514, loss 0.079829, acc 0.984375, learning_rate 0.000701102
2017-10-10T11:41:31.961045: step 515, loss 0.285384, acc 0.859375, learning_rate 0.000698648
2017-10-10T11:41:32.078304: step 516, loss 0.246109, acc 0.890625, learning_rate 0.000696204
2017-10-10T11:41:32.193780: step 517, loss 0.189278, acc 0.9375, learning_rate 0.000693771
2017-10-10T11:41:32.317228: step 518, loss 0.208829, acc 0.890625, learning_rate 0.000691347
2017-10-10T11:41:32.432503: step 519, loss 0.152006, acc 0.953125, learning_rate 0.000688934
2017-10-10T11:41:32.550695: step 520, loss 0.256941, acc 0.9375, learning_rate 0.00068653

Evaluation:
2017-10-10T11:41:32.884817: step 520, loss 0.231784, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-520

2017-10-10T11:41:33.433147: step 521, loss 0.148041, acc 0.953125, learning_rate 0.000684136
2017-10-10T11:41:33.552529: step 522, loss 0.269477, acc 0.875, learning_rate 0.000681751
2017-10-10T11:41:33.666806: step 523, loss 0.302057, acc 0.90625, learning_rate 0.000679377
2017-10-10T11:41:33.784276: step 524, loss 0.132882, acc 0.984375, learning_rate 0.000677012
2017-10-10T11:41:33.902236: step 525, loss 0.120462, acc 0.984375, learning_rate 0.000674657
2017-10-10T11:41:34.016378: step 526, loss 0.225115, acc 0.875, learning_rate 0.000672311
2017-10-10T11:41:34.137889: step 527, loss 0.153321, acc 0.9375, learning_rate 0.000669975
2017-10-10T11:41:34.258835: step 528, loss 0.274601, acc 0.890625, learning_rate 0.000667648
2017-10-10T11:41:34.371641: step 529, loss 0.142379, acc 0.9375, learning_rate 0.000665331
2017-10-10T11:41:34.490962: step 530, loss 0.0825237, acc 0.96875, learning_rate 0.000663024
2017-10-10T11:41:34.609049: step 531, loss 0.215512, acc 0.921875, learning_rate 0.000660726
2017-10-10T11:41:34.724680: step 532, loss 0.500309, acc 0.890625, learning_rate 0.000658437
2017-10-10T11:41:34.847294: step 533, loss 0.275586, acc 0.9375, learning_rate 0.000656158
2017-10-10T11:41:34.968709: step 534, loss 0.146515, acc 0.9375, learning_rate 0.000653888
2017-10-10T11:41:35.081739: step 535, loss 0.207632, acc 0.953125, learning_rate 0.000651627
2017-10-10T11:41:35.202592: step 536, loss 0.114405, acc 0.96875, learning_rate 0.000649375
2017-10-10T11:41:35.319654: step 537, loss 0.161977, acc 0.953125, learning_rate 0.000647133
2017-10-10T11:41:35.439325: step 538, loss 0.230374, acc 0.9375, learning_rate 0.000644899
2017-10-10T11:41:35.556021: step 539, loss 0.148525, acc 0.953125, learning_rate 0.000642675
2017-10-10T11:41:35.675508: step 540, loss 0.100789, acc 0.96875, learning_rate 0.00064046
2017-10-10T11:41:35.795058: step 541, loss 0.219734, acc 0.921875, learning_rate 0.000638254
2017-10-10T11:41:35.914377: step 542, loss 0.198187, acc 0.9375, learning_rate 0.000636057
2017-10-10T11:41:36.028575: step 543, loss 0.121465, acc 0.96875, learning_rate 0.000633869
2017-10-10T11:41:36.147137: step 544, loss 0.474538, acc 0.875, learning_rate 0.00063169
2017-10-10T11:41:36.265851: step 545, loss 0.111603, acc 0.984375, learning_rate 0.00062952
2017-10-10T11:41:36.381801: step 546, loss 0.14522, acc 0.921875, learning_rate 0.000627358
2017-10-10T11:41:36.501389: step 547, loss 0.159704, acc 0.921875, learning_rate 0.000625206
2017-10-10T11:41:36.617888: step 548, loss 0.193541, acc 0.921875, learning_rate 0.000623062
2017-10-10T11:41:36.734788: step 549, loss 0.140344, acc 0.9375, learning_rate 0.000620927
2017-10-10T11:41:36.855674: step 550, loss 0.0910501, acc 0.96875, learning_rate 0.000618801
2017-10-10T11:41:36.971845: step 551, loss 0.15156, acc 0.953125, learning_rate 0.000616683
2017-10-10T11:41:37.091353: step 552, loss 0.145398, acc 0.9375, learning_rate 0.000614574
2017-10-10T11:41:37.210820: step 553, loss 0.148751, acc 0.96875, learning_rate 0.000612474
2017-10-10T11:41:37.326334: step 554, loss 0.140191, acc 0.9375, learning_rate 0.000610382
2017-10-10T11:41:37.440784: step 555, loss 0.0849595, acc 0.96875, learning_rate 0.000608299
2017-10-10T11:41:37.558597: step 556, loss 0.124832, acc 0.953125, learning_rate 0.000606224
2017-10-10T11:41:37.672144: step 557, loss 0.115429, acc 0.953125, learning_rate 0.000604158
2017-10-10T11:41:37.790810: step 558, loss 0.180794, acc 0.9375, learning_rate 0.0006021
2017-10-10T11:41:37.908574: step 559, loss 0.193981, acc 0.9375, learning_rate 0.00060005
2017-10-10T11:41:38.025684: step 560, loss 0.164467, acc 0.953125, learning_rate 0.000598009

Evaluation:
2017-10-10T11:41:38.343728: step 560, loss 0.235939, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-560

2017-10-10T11:41:38.976521: step 561, loss 0.196319, acc 0.9375, learning_rate 0.000595977
2017-10-10T11:41:39.100162: step 562, loss 0.286944, acc 0.890625, learning_rate 0.000593952
2017-10-10T11:41:39.212379: step 563, loss 0.107177, acc 0.984375, learning_rate 0.000591936
2017-10-10T11:41:39.332528: step 564, loss 0.148507, acc 0.953125, learning_rate 0.000589928
2017-10-10T11:41:39.452964: step 565, loss 0.188699, acc 0.953125, learning_rate 0.000587928
2017-10-10T11:41:39.561031: step 566, loss 0.258649, acc 0.9375, learning_rate 0.000585937
2017-10-10T11:41:39.679776: step 567, loss 0.295997, acc 0.84375, learning_rate 0.000583953
2017-10-10T11:41:39.798134: step 568, loss 0.178181, acc 0.9375, learning_rate 0.000581978
2017-10-10T11:41:39.917789: step 569, loss 0.217952, acc 0.9375, learning_rate 0.00058001
2017-10-10T11:41:40.034552: step 570, loss 0.264313, acc 0.90625, learning_rate 0.000578051
2017-10-10T11:41:40.148179: step 571, loss 0.0579102, acc 1, learning_rate 0.0005761
2017-10-10T11:41:40.264319: step 572, loss 0.144115, acc 0.9375, learning_rate 0.000574157
2017-10-10T11:41:40.380352: step 573, loss 0.142533, acc 0.9375, learning_rate 0.000572221
2017-10-10T11:41:40.499882: step 574, loss 0.147642, acc 0.921875, learning_rate 0.000570294
2017-10-10T11:41:40.618378: step 575, loss 0.214761, acc 0.921875, learning_rate 0.000568374
2017-10-10T11:41:40.736770: step 576, loss 0.25734, acc 0.921875, learning_rate 0.000566462
2017-10-10T11:41:40.858994: step 577, loss 0.277965, acc 0.859375, learning_rate 0.000564558
2017-10-10T11:41:40.976795: step 578, loss 0.143762, acc 0.9375, learning_rate 0.000562662
2017-10-10T11:41:41.090781: step 579, loss 0.16482, acc 0.921875, learning_rate 0.000560774
2017-10-10T11:41:41.211133: step 580, loss 0.336834, acc 0.921875, learning_rate 0.000558893
2017-10-10T11:41:41.328610: step 581, loss 0.372651, acc 0.890625, learning_rate 0.00055702
2017-10-10T11:41:41.447998: step 582, loss 0.154004, acc 0.953125, learning_rate 0.000555154
2017-10-10T11:41:41.567884: step 583, loss 0.179678, acc 0.953125, learning_rate 0.000553296
2017-10-10T11:41:41.676457: step 584, loss 0.159776, acc 0.9375, learning_rate 0.000551446
2017-10-10T11:41:41.796197: step 585, loss 0.141007, acc 0.953125, learning_rate 0.000549604
2017-10-10T11:41:41.915666: step 586, loss 0.222291, acc 0.90625, learning_rate 0.000547768
2017-10-10T11:41:42.030457: step 587, loss 0.156879, acc 0.96875, learning_rate 0.000545941
2017-10-10T11:41:42.127499: step 588, loss 0.446174, acc 0.862745, learning_rate 0.00054412
2017-10-10T11:41:42.248969: step 589, loss 0.154219, acc 0.90625, learning_rate 0.000542308
2017-10-10T11:41:42.366221: step 590, loss 0.0858124, acc 0.96875, learning_rate 0.000540502
2017-10-10T11:41:42.481940: step 591, loss 0.153873, acc 0.953125, learning_rate 0.000538704
2017-10-10T11:41:42.603369: step 592, loss 0.140376, acc 0.953125, learning_rate 0.000536914
2017-10-10T11:41:42.722293: step 593, loss 0.245739, acc 0.921875, learning_rate 0.00053513
2017-10-10T11:41:42.831500: step 594, loss 0.165084, acc 0.953125, learning_rate 0.000533354
2017-10-10T11:41:42.949959: step 595, loss 0.105621, acc 0.984375, learning_rate 0.000531585
2017-10-10T11:41:43.067117: step 596, loss 0.16734, acc 0.921875, learning_rate 0.000529824
2017-10-10T11:41:43.184596: step 597, loss 0.204262, acc 0.9375, learning_rate 0.000528069
2017-10-10T11:41:43.303311: step 598, loss 0.300254, acc 0.921875, learning_rate 0.000526322
2017-10-10T11:41:43.424966: step 599, loss 0.192173, acc 0.921875, learning_rate 0.000524582
2017-10-10T11:41:43.540360: step 600, loss 0.118928, acc 0.953125, learning_rate 0.000522849

Evaluation:
2017-10-10T11:41:43.873328: step 600, loss 0.230387, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-600

2017-10-10T11:41:44.591548: step 601, loss 0.151653, acc 0.953125, learning_rate 0.000521123
2017-10-10T11:41:44.712460: step 602, loss 0.173951, acc 0.9375, learning_rate 0.000519404
2017-10-10T11:41:44.829070: step 603, loss 0.122585, acc 0.9375, learning_rate 0.000517692
2017-10-10T11:41:44.948182: step 604, loss 0.228849, acc 0.921875, learning_rate 0.000515987
2017-10-10T11:41:45.063997: step 605, loss 0.179194, acc 0.953125, learning_rate 0.000514289
2017-10-10T11:41:45.184956: step 606, loss 0.136736, acc 0.96875, learning_rate 0.000512598
2017-10-10T11:41:45.301282: step 607, loss 0.069835, acc 1, learning_rate 0.000510914
2017-10-10T11:41:45.419188: step 608, loss 0.160017, acc 0.96875, learning_rate 0.000509237
2017-10-10T11:41:45.535091: step 609, loss 0.309445, acc 0.859375, learning_rate 0.000507566
2017-10-10T11:41:45.656879: step 610, loss 0.228664, acc 0.921875, learning_rate 0.000505903
2017-10-10T11:41:45.775762: step 611, loss 0.207336, acc 0.921875, learning_rate 0.000504246
2017-10-10T11:41:45.896463: step 612, loss 0.352241, acc 0.875, learning_rate 0.000502596
2017-10-10T11:41:46.012732: step 613, loss 0.332081, acc 0.90625, learning_rate 0.000500953
2017-10-10T11:41:46.131993: step 614, loss 0.3217, acc 0.84375, learning_rate 0.000499316
2017-10-10T11:41:46.250833: step 615, loss 0.111985, acc 0.953125, learning_rate 0.000497686
2017-10-10T11:41:46.366988: step 616, loss 0.204233, acc 0.9375, learning_rate 0.000496063
2017-10-10T11:41:46.485577: step 617, loss 0.217667, acc 0.90625, learning_rate 0.000494446
2017-10-10T11:41:46.604242: step 618, loss 0.258935, acc 0.953125, learning_rate 0.000492836
2017-10-10T11:41:46.722060: step 619, loss 0.202447, acc 0.953125, learning_rate 0.000491233
2017-10-10T11:41:46.836224: step 620, loss 0.172398, acc 0.953125, learning_rate 0.000489636
2017-10-10T11:41:46.957512: step 621, loss 0.127161, acc 0.984375, learning_rate 0.000488045
2017-10-10T11:41:47.075376: step 622, loss 0.22013, acc 0.9375, learning_rate 0.000486461
2017-10-10T11:41:47.199885: step 623, loss 0.209832, acc 0.9375, learning_rate 0.000484884
2017-10-10T11:41:47.312942: step 624, loss 0.052771, acc 0.96875, learning_rate 0.000483313
2017-10-10T11:41:47.430822: step 625, loss 0.241908, acc 0.890625, learning_rate 0.000481748
2017-10-10T11:41:47.547772: step 626, loss 0.131913, acc 0.984375, learning_rate 0.00048019
2017-10-10T11:41:47.665936: step 627, loss 0.094085, acc 0.984375, learning_rate 0.000478638
2017-10-10T11:41:47.782661: step 628, loss 0.19463, acc 0.9375, learning_rate 0.000477093
2017-10-10T11:41:47.899382: step 629, loss 0.122797, acc 0.96875, learning_rate 0.000475554
2017-10-10T11:41:48.014662: step 630, loss 0.0615305, acc 1, learning_rate 0.000474021
2017-10-10T11:41:48.133238: step 631, loss 0.226884, acc 0.921875, learning_rate 0.000472494
2017-10-10T11:41:48.256967: step 632, loss 0.175457, acc 0.9375, learning_rate 0.000470974
2017-10-10T11:41:48.368941: step 633, loss 0.176673, acc 0.921875, learning_rate 0.000469459
2017-10-10T11:41:48.481586: step 634, loss 0.161681, acc 0.9375, learning_rate 0.000467951
2017-10-10T11:41:48.599428: step 635, loss 0.219787, acc 0.9375, learning_rate 0.000466449
2017-10-10T11:41:48.718663: step 636, loss 0.101052, acc 0.953125, learning_rate 0.000464954
2017-10-10T11:41:48.835673: step 637, loss 0.24379, acc 0.90625, learning_rate 0.000463464
2017-10-10T11:41:48.952023: step 638, loss 0.121652, acc 0.953125, learning_rate 0.00046198
2017-10-10T11:41:49.069593: step 639, loss 0.265322, acc 0.90625, learning_rate 0.000460503
2017-10-10T11:41:49.188041: step 640, loss 0.20847, acc 0.9375, learning_rate 0.000459031

Evaluation:
2017-10-10T11:41:49.527200: step 640, loss 0.226977, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-640

2017-10-10T11:41:50.077842: step 641, loss 0.137741, acc 0.9375, learning_rate 0.000457566
2017-10-10T11:41:50.201510: step 642, loss 0.0886661, acc 0.984375, learning_rate 0.000456106
2017-10-10T11:41:50.317465: step 643, loss 0.280732, acc 0.90625, learning_rate 0.000454653
2017-10-10T11:41:50.432413: step 644, loss 0.191622, acc 0.9375, learning_rate 0.000453205
2017-10-10T11:41:50.550294: step 645, loss 0.246945, acc 0.875, learning_rate 0.000451764
2017-10-10T11:41:50.672464: step 646, loss 0.322399, acc 0.90625, learning_rate 0.000450328
2017-10-10T11:41:50.787818: step 647, loss 0.0969239, acc 0.96875, learning_rate 0.000448898
2017-10-10T11:41:50.903750: step 648, loss 0.254944, acc 0.953125, learning_rate 0.000447474
2017-10-10T11:41:51.022755: step 649, loss 0.225222, acc 0.9375, learning_rate 0.000446055
2017-10-10T11:41:51.139559: step 650, loss 0.149252, acc 0.9375, learning_rate 0.000444643
2017-10-10T11:41:51.256487: step 651, loss 0.287699, acc 0.921875, learning_rate 0.000443236
2017-10-10T11:41:51.371260: step 652, loss 0.0790787, acc 0.984375, learning_rate 0.000441835
2017-10-10T11:41:51.488006: step 653, loss 0.197849, acc 0.9375, learning_rate 0.00044044
2017-10-10T11:41:51.606800: step 654, loss 0.180502, acc 0.921875, learning_rate 0.00043905
2017-10-10T11:41:51.723233: step 655, loss 0.18871, acc 0.921875, learning_rate 0.000437666
2017-10-10T11:41:51.842418: step 656, loss 0.106039, acc 0.96875, learning_rate 0.000436288
2017-10-10T11:41:51.960766: step 657, loss 0.201931, acc 0.953125, learning_rate 0.000434915
2017-10-10T11:41:52.075083: step 658, loss 0.192055, acc 0.96875, learning_rate 0.000433548
2017-10-10T11:41:52.195228: step 659, loss 0.124858, acc 0.9375, learning_rate 0.000432187
2017-10-10T11:41:52.312949: step 660, loss 0.269639, acc 0.90625, learning_rate 0.000430831
2017-10-10T11:41:52.430305: step 661, loss 0.0735792, acc 0.96875, learning_rate 0.000429481
2017-10-10T11:41:52.546151: step 662, loss 0.0970068, acc 0.96875, learning_rate 0.000428136
2017-10-10T11:41:52.658853: step 663, loss 0.195453, acc 0.953125, learning_rate 0.000426796
2017-10-10T11:41:52.775136: step 664, loss 0.168142, acc 0.9375, learning_rate 0.000425463
2017-10-10T11:41:52.900180: step 665, loss 0.124514, acc 0.96875, learning_rate 0.000424134
2017-10-10T11:41:53.018520: step 666, loss 0.278602, acc 0.859375, learning_rate 0.000422811
2017-10-10T11:41:53.138400: step 667, loss 0.155572, acc 0.953125, learning_rate 0.000421493
2017-10-10T11:41:53.255765: step 668, loss 0.212152, acc 0.9375, learning_rate 0.000420181
2017-10-10T11:41:53.377140: step 669, loss 0.157036, acc 0.9375, learning_rate 0.000418874
2017-10-10T11:41:53.488799: step 670, loss 0.189362, acc 0.890625, learning_rate 0.000417573
2017-10-10T11:41:53.610514: step 671, loss 0.201965, acc 0.9375, learning_rate 0.000416276
2017-10-10T11:41:53.728383: step 672, loss 0.141141, acc 0.953125, learning_rate 0.000414985
2017-10-10T11:41:53.845884: step 673, loss 0.125373, acc 0.953125, learning_rate 0.0004137
2017-10-10T11:41:53.957418: step 674, loss 0.223414, acc 0.921875, learning_rate 0.000412419
2017-10-10T11:41:54.068345: step 675, loss 0.272977, acc 0.921875, learning_rate 0.000411144
2017-10-10T11:41:54.182349: step 676, loss 0.117203, acc 0.96875, learning_rate 0.000409874
2017-10-10T11:41:54.302697: step 677, loss 0.182844, acc 0.921875, learning_rate 0.000408609
2017-10-10T11:41:54.420724: step 678, loss 0.228054, acc 0.9375, learning_rate 0.00040735
2017-10-10T11:41:54.534147: step 679, loss 0.282073, acc 0.9375, learning_rate 0.000406095
2017-10-10T11:41:54.655768: step 680, loss 0.113277, acc 0.953125, learning_rate 0.000404846

Evaluation:
2017-10-10T11:41:54.979880: step 680, loss 0.22807, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-680

2017-10-10T11:41:55.580883: step 681, loss 0.167604, acc 0.921875, learning_rate 0.000403601
2017-10-10T11:41:55.699470: step 682, loss 0.161248, acc 0.921875, learning_rate 0.000402362
2017-10-10T11:41:55.816088: step 683, loss 0.115186, acc 0.9375, learning_rate 0.000401128
2017-10-10T11:41:55.938798: step 684, loss 0.177756, acc 0.9375, learning_rate 0.000399899
2017-10-10T11:41:56.053599: step 685, loss 0.121629, acc 0.984375, learning_rate 0.000398675
2017-10-10T11:41:56.150180: step 686, loss 0.198945, acc 0.960784, learning_rate 0.000397456
2017-10-10T11:41:56.267661: step 687, loss 0.147331, acc 0.9375, learning_rate 0.000396241
2017-10-10T11:41:56.381088: step 688, loss 0.123613, acc 0.9375, learning_rate 0.000395032
2017-10-10T11:41:56.494542: step 689, loss 0.0741467, acc 0.984375, learning_rate 0.000393828
2017-10-10T11:41:56.611968: step 690, loss 0.0899987, acc 0.953125, learning_rate 0.000392629
2017-10-10T11:41:56.734889: step 691, loss 0.256788, acc 0.90625, learning_rate 0.000391434
2017-10-10T11:41:56.848508: step 692, loss 0.221471, acc 0.90625, learning_rate 0.000390245
2017-10-10T11:41:56.963957: step 693, loss 0.134436, acc 0.9375, learning_rate 0.00038906
2017-10-10T11:41:57.082989: step 694, loss 0.115349, acc 0.96875, learning_rate 0.00038788
2017-10-10T11:41:57.199810: step 695, loss 0.0619149, acc 0.984375, learning_rate 0.000386705
2017-10-10T11:41:57.313756: step 696, loss 0.234663, acc 0.90625, learning_rate 0.000385535
2017-10-10T11:41:57.423471: step 697, loss 0.0693609, acc 0.96875, learning_rate 0.000384369
2017-10-10T11:41:57.542395: step 698, loss 0.163708, acc 0.953125, learning_rate 0.000383209
2017-10-10T11:41:57.660692: step 699, loss 0.124032, acc 0.9375, learning_rate 0.000382053
2017-10-10T11:41:57.780853: step 700, loss 0.276698, acc 0.90625, learning_rate 0.000380901
2017-10-10T11:41:57.905051: step 701, loss 0.13814, acc 0.921875, learning_rate 0.000379755
2017-10-10T11:41:58.026018: step 702, loss 0.216168, acc 0.921875, learning_rate 0.000378613
2017-10-10T11:41:58.144518: step 703, loss 0.246992, acc 0.90625, learning_rate 0.000377476
2017-10-10T11:41:58.262797: step 704, loss 0.118444, acc 0.953125, learning_rate 0.000376343
2017-10-10T11:41:58.385476: step 705, loss 0.14046, acc 0.96875, learning_rate 0.000375215
2017-10-10T11:41:58.507069: step 706, loss 0.253178, acc 0.9375, learning_rate 0.000374092
2017-10-10T11:41:58.629080: step 707, loss 0.124761, acc 0.9375, learning_rate 0.000372973
2017-10-10T11:41:58.747942: step 708, loss 0.137324, acc 0.953125, learning_rate 0.000371859
2017-10-10T11:41:58.869631: step 709, loss 0.0787865, acc 1, learning_rate 0.000370749
2017-10-10T11:41:58.987010: step 710, loss 0.182742, acc 0.953125, learning_rate 0.000369644
2017-10-10T11:41:59.102588: step 711, loss 0.0956719, acc 0.96875, learning_rate 0.000368543
2017-10-10T11:41:59.215587: step 712, loss 0.241599, acc 0.90625, learning_rate 0.000367447
2017-10-10T11:41:59.330495: step 713, loss 0.0815597, acc 0.984375, learning_rate 0.000366356
2017-10-10T11:41:59.447062: step 714, loss 0.231158, acc 0.90625, learning_rate 0.000365268
2017-10-10T11:41:59.567087: step 715, loss 0.15205, acc 0.953125, learning_rate 0.000364186
2017-10-10T11:41:59.686279: step 716, loss 0.325221, acc 0.921875, learning_rate 0.000363107
2017-10-10T11:41:59.801890: step 717, loss 0.181886, acc 0.921875, learning_rate 0.000362033
2017-10-10T11:41:59.919817: step 718, loss 0.278002, acc 0.90625, learning_rate 0.000360964
2017-10-10T11:42:00.039095: step 719, loss 0.159228, acc 0.9375, learning_rate 0.000359899
2017-10-10T11:42:00.149102: step 720, loss 0.186364, acc 0.90625, learning_rate 0.000358838

Evaluation:
2017-10-10T11:42:00.477604: step 720, loss 0.22536, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-720

2017-10-10T11:42:01.171530: step 721, loss 0.241258, acc 0.890625, learning_rate 0.000357781
2017-10-10T11:42:01.284886: step 722, loss 0.143638, acc 0.9375, learning_rate 0.000356729
2017-10-10T11:42:01.404290: step 723, loss 0.186137, acc 0.9375, learning_rate 0.000355681
2017-10-10T11:42:01.526520: step 724, loss 0.0944379, acc 0.953125, learning_rate 0.000354637
2017-10-10T11:42:01.642637: step 725, loss 0.160811, acc 0.9375, learning_rate 0.000353598
2017-10-10T11:42:01.755197: step 726, loss 0.173302, acc 0.9375, learning_rate 0.000352563
2017-10-10T11:42:01.871789: step 727, loss 0.211444, acc 0.9375, learning_rate 0.000351532
2017-10-10T11:42:01.983218: step 728, loss 0.32331, acc 0.921875, learning_rate 0.000350505
2017-10-10T11:42:02.100438: step 729, loss 0.170099, acc 0.90625, learning_rate 0.000349483
2017-10-10T11:42:02.215557: step 730, loss 0.165063, acc 0.9375, learning_rate 0.000348465
2017-10-10T11:42:02.337176: step 731, loss 0.170532, acc 0.921875, learning_rate 0.00034745
2017-10-10T11:42:02.462773: step 732, loss 0.0796928, acc 0.984375, learning_rate 0.00034644
2017-10-10T11:42:02.577853: step 733, loss 0.107195, acc 0.984375, learning_rate 0.000345434
2017-10-10T11:42:02.697366: step 734, loss 0.150714, acc 0.96875, learning_rate 0.000344433
2017-10-10T11:42:02.823551: step 735, loss 0.143462, acc 0.921875, learning_rate 0.000343435
2017-10-10T11:42:02.937241: step 736, loss 0.141122, acc 0.953125, learning_rate 0.000342441
2017-10-10T11:42:03.054620: step 737, loss 0.114175, acc 0.984375, learning_rate 0.000341452
2017-10-10T11:42:03.173027: step 738, loss 0.134403, acc 0.921875, learning_rate 0.000340466
2017-10-10T11:42:03.291113: step 739, loss 0.147345, acc 0.953125, learning_rate 0.000339485
2017-10-10T11:42:03.410920: step 740, loss 0.140426, acc 0.953125, learning_rate 0.000338507
2017-10-10T11:42:03.525687: step 741, loss 0.260603, acc 0.875, learning_rate 0.000337534
2017-10-10T11:42:03.648547: step 742, loss 0.205821, acc 0.953125, learning_rate 0.000336564
2017-10-10T11:42:03.768499: step 743, loss 0.241786, acc 0.953125, learning_rate 0.000335598
2017-10-10T11:42:03.894126: step 744, loss 0.153836, acc 0.921875, learning_rate 0.000334637
2017-10-10T11:42:04.028136: step 745, loss 0.19712, acc 0.921875, learning_rate 0.000333679
2017-10-10T11:42:04.160826: step 746, loss 0.136375, acc 0.96875, learning_rate 0.000332725
2017-10-10T11:42:04.287738: step 747, loss 0.129412, acc 0.96875, learning_rate 0.000331775
2017-10-10T11:42:04.410718: step 748, loss 0.170806, acc 0.953125, learning_rate 0.000330829
2017-10-10T11:42:04.530367: step 749, loss 0.0892615, acc 0.984375, learning_rate 0.000329887
2017-10-10T11:42:04.647727: step 750, loss 0.0804024, acc 0.96875, learning_rate 0.000328949
2017-10-10T11:42:04.768913: step 751, loss 0.112386, acc 0.953125, learning_rate 0.000328014
2017-10-10T11:42:04.893022: step 752, loss 0.147864, acc 0.9375, learning_rate 0.000327083
2017-10-10T11:42:05.007063: step 753, loss 0.15144, acc 0.9375, learning_rate 0.000326157
2017-10-10T11:42:05.128497: step 754, loss 0.350865, acc 0.875, learning_rate 0.000325233
2017-10-10T11:42:05.248272: step 755, loss 0.0897318, acc 0.96875, learning_rate 0.000324314
2017-10-10T11:42:05.368627: step 756, loss 0.147092, acc 0.953125, learning_rate 0.000323399
2017-10-10T11:42:05.490385: step 757, loss 0.0921695, acc 0.953125, learning_rate 0.000322487
2017-10-10T11:42:05.611163: step 758, loss 0.22121, acc 0.9375, learning_rate 0.000321579
2017-10-10T11:42:05.731772: step 759, loss 0.170943, acc 0.96875, learning_rate 0.000320674
2017-10-10T11:42:05.848214: step 760, loss 0.127421, acc 0.953125, learning_rate 0.000319773

Evaluation:
2017-10-10T11:42:06.593158: step 760, loss 0.233425, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-760

2017-10-10T11:42:07.390049: step 761, loss 0.115403, acc 0.96875, learning_rate 0.000318876
2017-10-10T11:42:07.510578: step 762, loss 0.108295, acc 0.96875, learning_rate 0.000317983
2017-10-10T11:42:07.632131: step 763, loss 0.103493, acc 0.96875, learning_rate 0.000317093
2017-10-10T11:42:07.745083: step 764, loss 0.0682264, acc 0.984375, learning_rate 0.000316207
2017-10-10T11:42:07.857558: step 765, loss 0.146105, acc 0.9375, learning_rate 0.000315325
2017-10-10T11:42:07.980455: step 766, loss 0.147862, acc 0.921875, learning_rate 0.000314446
2017-10-10T11:42:08.096137: step 767, loss 0.0667896, acc 0.984375, learning_rate 0.00031357
2017-10-10T11:42:08.212010: step 768, loss 0.173082, acc 0.921875, learning_rate 0.000312699
2017-10-10T11:42:08.328976: step 769, loss 0.234734, acc 0.921875, learning_rate 0.00031183
2017-10-10T11:42:08.445394: step 770, loss 0.311178, acc 0.875, learning_rate 0.000310966
2017-10-10T11:42:08.562610: step 771, loss 0.210097, acc 0.953125, learning_rate 0.000310105
2017-10-10T11:42:08.677495: step 772, loss 0.182689, acc 0.9375, learning_rate 0.000309247
2017-10-10T11:42:08.798716: step 773, loss 0.327591, acc 0.921875, learning_rate 0.000308393
2017-10-10T11:42:08.918179: step 774, loss 0.230939, acc 0.921875, learning_rate 0.000307542
2017-10-10T11:42:09.037062: step 775, loss 0.225221, acc 0.90625, learning_rate 0.000306695
2017-10-10T11:42:09.150412: step 776, loss 0.0805241, acc 1, learning_rate 0.000305852
2017-10-10T11:42:09.267020: step 777, loss 0.13236, acc 0.96875, learning_rate 0.000305011
2017-10-10T11:42:09.383984: step 778, loss 0.187253, acc 0.953125, learning_rate 0.000304174
2017-10-10T11:42:09.499617: step 779, loss 0.190256, acc 0.921875, learning_rate 0.000303341
2017-10-10T11:42:09.619889: step 780, loss 0.118733, acc 0.96875, learning_rate 0.000302511
2017-10-10T11:42:09.736979: step 781, loss 0.106423, acc 0.96875, learning_rate 0.000301684
2017-10-10T11:42:09.864032: step 782, loss 0.239103, acc 0.953125, learning_rate 0.000300861
2017-10-10T11:42:09.979241: step 783, loss 0.084155, acc 0.96875, learning_rate 0.000300041
2017-10-10T11:42:10.076828: step 784, loss 0.112725, acc 0.960784, learning_rate 0.000299225
2017-10-10T11:42:10.194053: step 785, loss 0.146718, acc 0.9375, learning_rate 0.000298412
2017-10-10T11:42:10.315344: step 786, loss 0.0870739, acc 0.984375, learning_rate 0.000297602
2017-10-10T11:42:10.432573: step 787, loss 0.118856, acc 0.984375, learning_rate 0.000296795
2017-10-10T11:42:10.551195: step 788, loss 0.344615, acc 0.890625, learning_rate 0.000295992
2017-10-10T11:42:10.668267: step 789, loss 0.201941, acc 0.90625, learning_rate 0.000295192
2017-10-10T11:42:10.783814: step 790, loss 0.21325, acc 0.890625, learning_rate 0.000294395
2017-10-10T11:42:10.918129: step 791, loss 0.0925856, acc 0.953125, learning_rate 0.000293602
2017-10-10T11:42:11.037773: step 792, loss 0.143206, acc 0.921875, learning_rate 0.000292812
2017-10-10T11:42:11.150349: step 793, loss 0.138022, acc 0.953125, learning_rate 0.000292025
2017-10-10T11:42:11.270293: step 794, loss 0.188529, acc 0.96875, learning_rate 0.000291241
2017-10-10T11:42:11.387899: step 795, loss 0.11894, acc 0.984375, learning_rate 0.00029046
2017-10-10T11:42:11.503394: step 796, loss 0.274911, acc 0.921875, learning_rate 0.000289683
2017-10-10T11:42:11.629675: step 797, loss 0.128758, acc 0.96875, learning_rate 0.000288908
2017-10-10T11:42:11.740784: step 798, loss 0.154965, acc 0.96875, learning_rate 0.000288137
2017-10-10T11:42:11.863218: step 799, loss 0.219232, acc 0.921875, learning_rate 0.000287369
2017-10-10T11:42:11.977960: step 800, loss 0.148669, acc 0.9375, learning_rate 0.000286605

Evaluation:
2017-10-10T11:42:12.310103: step 800, loss 0.225459, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-800

2017-10-10T11:42:12.919146: step 801, loss 0.182114, acc 0.921875, learning_rate 0.000285843
2017-10-10T11:42:13.038024: step 802, loss 0.10574, acc 0.953125, learning_rate 0.000285084
2017-10-10T11:42:13.152539: step 803, loss 0.168383, acc 0.953125, learning_rate 0.000284329
2017-10-10T11:42:13.269822: step 804, loss 0.276701, acc 0.90625, learning_rate 0.000283577
2017-10-10T11:42:13.381944: step 805, loss 0.315533, acc 0.90625, learning_rate 0.000282827
2017-10-10T11:42:13.501085: step 806, loss 0.141932, acc 0.953125, learning_rate 0.000282081
2017-10-10T11:42:13.621652: step 807, loss 0.0883457, acc 0.953125, learning_rate 0.000281338
2017-10-10T11:42:13.737110: step 808, loss 0.0732398, acc 0.984375, learning_rate 0.000280598
2017-10-10T11:42:13.861194: step 809, loss 0.233705, acc 0.953125, learning_rate 0.00027986
2017-10-10T11:42:13.977849: step 810, loss 0.236323, acc 0.921875, learning_rate 0.000279126
2017-10-10T11:42:14.090009: step 811, loss 0.163141, acc 0.953125, learning_rate 0.000278395
2017-10-10T11:42:14.209028: step 812, loss 0.213241, acc 0.9375, learning_rate 0.000277667
2017-10-10T11:42:14.328222: step 813, loss 0.217711, acc 0.921875, learning_rate 0.000276942
2017-10-10T11:42:14.446306: step 814, loss 0.110594, acc 0.96875, learning_rate 0.00027622
2017-10-10T11:42:14.565153: step 815, loss 0.0768467, acc 1, learning_rate 0.0002755
2017-10-10T11:42:14.681336: step 816, loss 0.110359, acc 0.953125, learning_rate 0.000274784
2017-10-10T11:42:14.798693: step 817, loss 0.0907453, acc 0.96875, learning_rate 0.000274071
2017-10-10T11:42:14.918326: step 818, loss 0.155496, acc 0.984375, learning_rate 0.00027336
2017-10-10T11:42:15.036985: step 819, loss 0.179189, acc 0.953125, learning_rate 0.000272652
2017-10-10T11:42:15.153534: step 820, loss 0.137079, acc 0.9375, learning_rate 0.000271948
2017-10-10T11:42:15.271195: step 821, loss 0.20668, acc 0.9375, learning_rate 0.000271246
2017-10-10T11:42:15.390045: step 822, loss 0.114524, acc 0.953125, learning_rate 0.000270547
2017-10-10T11:42:15.513967: step 823, loss 0.141005, acc 0.953125, learning_rate 0.000269851
2017-10-10T11:42:15.634984: step 824, loss 0.163516, acc 0.953125, learning_rate 0.000269157
2017-10-10T11:42:15.756750: step 825, loss 0.113514, acc 0.9375, learning_rate 0.000268467
2017-10-10T11:42:15.878544: step 826, loss 0.174784, acc 0.9375, learning_rate 0.000267779
2017-10-10T11:42:15.996501: step 827, loss 0.324564, acc 0.875, learning_rate 0.000267094
2017-10-10T11:42:16.118250: step 828, loss 0.136864, acc 0.953125, learning_rate 0.000266412
2017-10-10T11:42:16.234988: step 829, loss 0.0394474, acc 1, learning_rate 0.000265733
2017-10-10T11:42:16.354435: step 830, loss 0.117671, acc 0.953125, learning_rate 0.000265057
2017-10-10T11:42:16.472414: step 831, loss 0.226244, acc 0.921875, learning_rate 0.000264383
2017-10-10T11:42:16.600493: step 832, loss 0.0781585, acc 0.984375, learning_rate 0.000263712
2017-10-10T11:42:16.714636: step 833, loss 0.150861, acc 0.921875, learning_rate 0.000263044
2017-10-10T11:42:16.830901: step 834, loss 0.114822, acc 0.96875, learning_rate 0.000262378
2017-10-10T11:42:16.948992: step 835, loss 0.0655634, acc 1, learning_rate 0.000261715
2017-10-10T11:42:17.070560: step 836, loss 0.275712, acc 0.90625, learning_rate 0.000261055
2017-10-10T11:42:17.190267: step 837, loss 0.203166, acc 0.921875, learning_rate 0.000260398
2017-10-10T11:42:17.311365: step 838, loss 0.261679, acc 0.921875, learning_rate 0.000259743
2017-10-10T11:42:17.429221: step 839, loss 0.0810204, acc 0.96875, learning_rate 0.000259091
2017-10-10T11:42:17.549894: step 840, loss 0.196968, acc 0.90625, learning_rate 0.000258442

Evaluation:
2017-10-10T11:42:17.882496: step 840, loss 0.22638, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-840

2017-10-10T11:42:18.583835: step 841, loss 0.174363, acc 0.921875, learning_rate 0.000257795
2017-10-10T11:42:18.701340: step 842, loss 0.355106, acc 0.90625, learning_rate 0.000257151
2017-10-10T11:42:18.819246: step 843, loss 0.223245, acc 0.90625, learning_rate 0.00025651
2017-10-10T11:42:18.938976: step 844, loss 0.147714, acc 0.953125, learning_rate 0.000255871
2017-10-10T11:42:19.054197: step 845, loss 0.139584, acc 0.9375, learning_rate 0.000255235
2017-10-10T11:42:19.171886: step 846, loss 0.196808, acc 0.953125, learning_rate 0.000254601
2017-10-10T11:42:19.282782: step 847, loss 0.0654721, acc 1, learning_rate 0.00025397
2017-10-10T11:42:19.395045: step 848, loss 0.287485, acc 0.90625, learning_rate 0.000253341
2017-10-10T11:42:19.513843: step 849, loss 0.201109, acc 0.9375, learning_rate 0.000252716
2017-10-10T11:42:19.633781: step 850, loss 0.163455, acc 0.953125, learning_rate 0.000252092
2017-10-10T11:42:19.751030: step 851, loss 0.124851, acc 0.96875, learning_rate 0.000251471
2017-10-10T11:42:19.872300: step 852, loss 0.202999, acc 0.90625, learning_rate 0.000250853
2017-10-10T11:42:19.989917: step 853, loss 0.124328, acc 0.953125, learning_rate 0.000250237
2017-10-10T11:42:20.106277: step 854, loss 0.126988, acc 0.96875, learning_rate 0.000249624
2017-10-10T11:42:20.220754: step 855, loss 0.164273, acc 0.921875, learning_rate 0.000249013
2017-10-10T11:42:20.339565: step 856, loss 0.110518, acc 0.9375, learning_rate 0.000248405
2017-10-10T11:42:20.458670: step 857, loss 0.320658, acc 0.890625, learning_rate 0.000247799
2017-10-10T11:42:20.575320: step 858, loss 0.107826, acc 0.96875, learning_rate 0.000247196
2017-10-10T11:42:20.699541: step 859, loss 0.0997719, acc 0.953125, learning_rate 0.000246595
2017-10-10T11:42:20.815959: step 860, loss 0.110519, acc 0.984375, learning_rate 0.000245997
2017-10-10T11:42:20.943690: step 861, loss 0.102363, acc 0.96875, learning_rate 0.000245401
2017-10-10T11:42:21.061380: step 862, loss 0.329415, acc 0.890625, learning_rate 0.000244808
2017-10-10T11:42:21.185660: step 863, loss 0.069131, acc 0.96875, learning_rate 0.000244216
2017-10-10T11:42:21.305821: step 864, loss 0.167249, acc 0.953125, learning_rate 0.000243628
2017-10-10T11:42:21.421388: step 865, loss 0.177489, acc 0.9375, learning_rate 0.000243042
2017-10-10T11:42:21.537470: step 866, loss 0.226811, acc 0.90625, learning_rate 0.000242458
2017-10-10T11:42:21.655774: step 867, loss 0.105234, acc 0.953125, learning_rate 0.000241876
2017-10-10T11:42:21.770981: step 868, loss 0.115085, acc 0.953125, learning_rate 0.000241297
2017-10-10T11:42:21.896188: step 869, loss 0.0827316, acc 0.96875, learning_rate 0.00024072
2017-10-10T11:42:22.012627: step 870, loss 0.275499, acc 0.90625, learning_rate 0.000240146
2017-10-10T11:42:22.129022: step 871, loss 0.131486, acc 0.96875, learning_rate 0.000239574
2017-10-10T11:42:22.241088: step 872, loss 0.0922144, acc 0.984375, learning_rate 0.000239004
2017-10-10T11:42:22.362065: step 873, loss 0.0887913, acc 0.984375, learning_rate 0.000238437
2017-10-10T11:42:22.481087: step 874, loss 0.0887923, acc 0.96875, learning_rate 0.000237872
2017-10-10T11:42:22.598125: step 875, loss 0.161819, acc 0.9375, learning_rate 0.000237309
2017-10-10T11:42:22.715692: step 876, loss 0.11054, acc 0.96875, learning_rate 0.000236749
2017-10-10T11:42:22.831124: step 877, loss 0.123022, acc 0.9375, learning_rate 0.00023619
2017-10-10T11:42:22.951067: step 878, loss 0.137356, acc 0.921875, learning_rate 0.000235635
2017-10-10T11:42:23.069041: step 879, loss 0.201784, acc 0.953125, learning_rate 0.000235081
2017-10-10T11:42:23.183876: step 880, loss 0.192122, acc 0.921875, learning_rate 0.00023453

Evaluation:
2017-10-10T11:42:23.521867: step 880, loss 0.22591, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-880

2017-10-10T11:42:24.071343: step 881, loss 0.141514, acc 0.953125, learning_rate 0.00023398
2017-10-10T11:42:24.172472: step 882, loss 0.0791098, acc 0.980392, learning_rate 0.000233434
2017-10-10T11:42:24.291078: step 883, loss 0.0878024, acc 0.953125, learning_rate 0.000232889
2017-10-10T11:42:24.404939: step 884, loss 0.21966, acc 0.890625, learning_rate 0.000232346
2017-10-10T11:42:24.530100: step 885, loss 0.102831, acc 0.953125, learning_rate 0.000231806
2017-10-10T11:42:24.648471: step 886, loss 0.106548, acc 0.96875, learning_rate 0.000231268
2017-10-10T11:42:24.765100: step 887, loss 0.107202, acc 0.953125, learning_rate 0.000230732
2017-10-10T11:42:24.878587: step 888, loss 0.278801, acc 0.90625, learning_rate 0.000230199
2017-10-10T11:42:24.995930: step 889, loss 0.310421, acc 0.9375, learning_rate 0.000229667
2017-10-10T11:42:25.110799: step 890, loss 0.141845, acc 0.9375, learning_rate 0.000229138
2017-10-10T11:42:25.230531: step 891, loss 0.115267, acc 0.96875, learning_rate 0.000228611
2017-10-10T11:42:25.349539: step 892, loss 0.328293, acc 0.921875, learning_rate 0.000228086
2017-10-10T11:42:25.468388: step 893, loss 0.190939, acc 0.921875, learning_rate 0.000227563
2017-10-10T11:42:25.583198: step 894, loss 0.182424, acc 0.953125, learning_rate 0.000227043
2017-10-10T11:42:25.700537: step 895, loss 0.199272, acc 0.921875, learning_rate 0.000226524
2017-10-10T11:42:25.815106: step 896, loss 0.0710467, acc 0.96875, learning_rate 0.000226008
2017-10-10T11:42:25.933789: step 897, loss 0.219178, acc 0.9375, learning_rate 0.000225493
2017-10-10T11:42:26.053725: step 898, loss 0.0936745, acc 0.96875, learning_rate 0.000224981
2017-10-10T11:42:26.169332: step 899, loss 0.20105, acc 0.921875, learning_rate 0.000224471
2017-10-10T11:42:26.288530: step 900, loss 0.249613, acc 0.90625, learning_rate 0.000223963
2017-10-10T11:42:26.411197: step 901, loss 0.0278339, acc 1, learning_rate 0.000223457
2017-10-10T11:42:26.528009: step 902, loss 0.0855953, acc 0.96875, learning_rate 0.000222953
2017-10-10T11:42:26.646977: step 903, loss 0.119062, acc 0.96875, learning_rate 0.000222451
2017-10-10T11:42:26.766240: step 904, loss 0.0758818, acc 0.984375, learning_rate 0.000221951
2017-10-10T11:42:26.893915: step 905, loss 0.220337, acc 0.921875, learning_rate 0.000221453
2017-10-10T11:42:27.006639: step 906, loss 0.300388, acc 0.890625, learning_rate 0.000220958
2017-10-10T11:42:27.126607: step 907, loss 0.199378, acc 0.953125, learning_rate 0.000220464
2017-10-10T11:42:27.250310: step 908, loss 0.253135, acc 0.890625, learning_rate 0.000219972
2017-10-10T11:42:27.372281: step 909, loss 0.120487, acc 0.953125, learning_rate 0.000219483
2017-10-10T11:42:27.486625: step 910, loss 0.159761, acc 0.96875, learning_rate 0.000218995
2017-10-10T11:42:27.603229: step 911, loss 0.116576, acc 0.953125, learning_rate 0.000218509
2017-10-10T11:42:27.719822: step 912, loss 0.186346, acc 0.921875, learning_rate 0.000218025
2017-10-10T11:42:27.839198: step 913, loss 0.141944, acc 0.953125, learning_rate 0.000217544
2017-10-10T11:42:27.957520: step 914, loss 0.261918, acc 0.90625, learning_rate 0.000217064
2017-10-10T11:42:28.075481: step 915, loss 0.142819, acc 0.953125, learning_rate 0.000216586
2017-10-10T11:42:28.192247: step 916, loss 0.175121, acc 0.96875, learning_rate 0.00021611
2017-10-10T11:42:28.311047: step 917, loss 0.0802632, acc 0.984375, learning_rate 0.000215636
2017-10-10T11:42:28.432099: step 918, loss 0.0828041, acc 0.984375, learning_rate 0.000215164
2017-10-10T11:42:28.549142: step 919, loss 0.101636, acc 0.96875, learning_rate 0.000214694
2017-10-10T11:42:28.666170: step 920, loss 0.299238, acc 0.875, learning_rate 0.000214226

Evaluation:
2017-10-10T11:42:29.009435: step 920, loss 0.223719, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-920

2017-10-10T11:42:29.614235: step 921, loss 0.11632, acc 0.953125, learning_rate 0.00021376
2017-10-10T11:42:29.726378: step 922, loss 0.272889, acc 0.921875, learning_rate 0.000213295
2017-10-10T11:42:29.846198: step 923, loss 0.135059, acc 0.953125, learning_rate 0.000212833
2017-10-10T11:42:29.965216: step 924, loss 0.130076, acc 0.9375, learning_rate 0.000212372
2017-10-10T11:42:30.085712: step 925, loss 0.123184, acc 0.953125, learning_rate 0.000211914
2017-10-10T11:42:30.204442: step 926, loss 0.0634362, acc 0.984375, learning_rate 0.000211457
2017-10-10T11:42:30.315904: step 927, loss 0.0749964, acc 0.984375, learning_rate 0.000211002
2017-10-10T11:42:30.432916: step 928, loss 0.194808, acc 0.9375, learning_rate 0.000210549
2017-10-10T11:42:30.554534: step 929, loss 0.0592743, acc 1, learning_rate 0.000210098
2017-10-10T11:42:30.674233: step 930, loss 0.172353, acc 0.9375, learning_rate 0.000209648
2017-10-10T11:42:30.789456: step 931, loss 0.301173, acc 0.890625, learning_rate 0.000209201
2017-10-10T11:42:30.904471: step 932, loss 0.107253, acc 0.96875, learning_rate 0.000208755
2017-10-10T11:42:31.024065: step 933, loss 0.0890567, acc 0.984375, learning_rate 0.000208311
2017-10-10T11:42:31.142234: step 934, loss 0.196185, acc 0.921875, learning_rate 0.000207869
2017-10-10T11:42:31.263250: step 935, loss 0.238959, acc 0.921875, learning_rate 0.000207429
2017-10-10T11:42:31.381349: step 936, loss 0.247163, acc 0.890625, learning_rate 0.00020699
2017-10-10T11:42:31.500321: step 937, loss 0.0183333, acc 1, learning_rate 0.000206554
2017-10-10T11:42:31.615688: step 938, loss 0.0799353, acc 1, learning_rate 0.000206119
2017-10-10T11:42:31.735821: step 939, loss 0.134773, acc 0.9375, learning_rate 0.000205685
2017-10-10T11:42:31.853610: step 940, loss 0.152796, acc 0.96875, learning_rate 0.000205254
2017-10-10T11:42:31.967905: step 941, loss 0.197678, acc 0.921875, learning_rate 0.000204824
2017-10-10T11:42:32.087503: step 942, loss 0.305047, acc 0.890625, learning_rate 0.000204397
2017-10-10T11:42:32.203408: step 943, loss 0.163353, acc 0.9375, learning_rate 0.00020397
2017-10-10T11:42:32.323357: step 944, loss 0.190515, acc 0.921875, learning_rate 0.000203546
2017-10-10T11:42:32.441408: step 945, loss 0.164796, acc 0.953125, learning_rate 0.000203123
2017-10-10T11:42:32.561001: step 946, loss 0.242695, acc 0.890625, learning_rate 0.000202702
2017-10-10T11:42:32.675725: step 947, loss 0.206784, acc 0.9375, learning_rate 0.000202283
2017-10-10T11:42:32.789968: step 948, loss 0.0908625, acc 0.953125, learning_rate 0.000201866
2017-10-10T11:42:32.914592: step 949, loss 0.0928884, acc 0.984375, learning_rate 0.00020145
2017-10-10T11:42:33.029453: step 950, loss 0.174226, acc 0.90625, learning_rate 0.000201036
2017-10-10T11:42:33.144689: step 951, loss 0.170592, acc 0.9375, learning_rate 0.000200623
2017-10-10T11:42:33.261861: step 952, loss 0.0643591, acc 0.984375, learning_rate 0.000200213
2017-10-10T11:42:33.381941: step 953, loss 0.16826, acc 0.953125, learning_rate 0.000199804
2017-10-10T11:42:33.496415: step 954, loss 0.224551, acc 0.921875, learning_rate 0.000199396
2017-10-10T11:42:33.614290: step 955, loss 0.234498, acc 0.875, learning_rate 0.000198991
2017-10-10T11:42:33.733603: step 956, loss 0.122508, acc 0.96875, learning_rate 0.000198587
2017-10-10T11:42:33.849156: step 957, loss 0.22468, acc 0.90625, learning_rate 0.000198184
2017-10-10T11:42:33.968951: step 958, loss 0.093886, acc 0.953125, learning_rate 0.000197783
2017-10-10T11:42:34.084062: step 959, loss 0.0887362, acc 0.96875, learning_rate 0.000197384
2017-10-10T11:42:34.198383: step 960, loss 0.308103, acc 0.9375, learning_rate 0.000196987

Evaluation:
2017-10-10T11:42:34.548473: step 960, loss 0.22567, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-960

2017-10-10T11:42:35.168295: step 961, loss 0.0964112, acc 0.96875, learning_rate 0.000196591
2017-10-10T11:42:35.288492: step 962, loss 0.173233, acc 0.921875, learning_rate 0.000196197
2017-10-10T11:42:35.404768: step 963, loss 0.168877, acc 0.984375, learning_rate 0.000195804
2017-10-10T11:42:35.523025: step 964, loss 0.269854, acc 0.90625, learning_rate 0.000195413
2017-10-10T11:42:35.644979: step 965, loss 0.141476, acc 0.984375, learning_rate 0.000195023
2017-10-10T11:42:35.759569: step 966, loss 0.268122, acc 0.921875, learning_rate 0.000194636
2017-10-10T11:42:35.876667: step 967, loss 0.135492, acc 0.9375, learning_rate 0.000194249
2017-10-10T11:42:35.995613: step 968, loss 0.272105, acc 0.890625, learning_rate 0.000193865
2017-10-10T11:42:36.112914: step 969, loss 0.122143, acc 0.96875, learning_rate 0.000193482
2017-10-10T11:42:36.224336: step 970, loss 0.152598, acc 0.921875, learning_rate 0.0001931
2017-10-10T11:42:36.339428: step 971, loss 0.115226, acc 0.984375, learning_rate 0.00019272
2017-10-10T11:42:36.457906: step 972, loss 0.129851, acc 0.953125, learning_rate 0.000192341
2017-10-10T11:42:36.576940: step 973, loss 0.1101, acc 0.96875, learning_rate 0.000191965
2017-10-10T11:42:36.692956: step 974, loss 0.108631, acc 0.953125, learning_rate 0.000191589
2017-10-10T11:42:36.809586: step 975, loss 0.154705, acc 0.953125, learning_rate 0.000191215
2017-10-10T11:42:36.950464: step 976, loss 0.0781907, acc 0.96875, learning_rate 0.000190843
2017-10-10T11:42:37.069794: step 977, loss 0.104793, acc 0.953125, learning_rate 0.000190472
2017-10-10T11:42:37.185107: step 978, loss 0.230832, acc 0.9375, learning_rate 0.000190103
2017-10-10T11:42:37.301709: step 979, loss 0.0926149, acc 0.96875, learning_rate 0.000189735
2017-10-10T11:42:37.398060: step 980, loss 0.131806, acc 0.980392, learning_rate 0.000189369
2017-10-10T11:42:37.516998: step 981, loss 0.236028, acc 0.921875, learning_rate 0.000189004
2017-10-10T11:42:37.632192: step 982, loss 0.179175, acc 0.921875, learning_rate 0.000188641
2017-10-10T11:42:37.750542: step 983, loss 0.127546, acc 0.96875, learning_rate 0.000188279
2017-10-10T11:42:37.869062: step 984, loss 0.169623, acc 0.921875, learning_rate 0.000187919
2017-10-10T11:42:37.986373: step 985, loss 0.0687125, acc 1, learning_rate 0.00018756
2017-10-10T11:42:38.099084: step 986, loss 0.137045, acc 0.96875, learning_rate 0.000187202
2017-10-10T11:42:38.219213: step 987, loss 0.145145, acc 0.953125, learning_rate 0.000186846
2017-10-10T11:42:38.335784: step 988, loss 0.0801311, acc 0.96875, learning_rate 0.000186492
2017-10-10T11:42:38.448578: step 989, loss 0.158822, acc 0.9375, learning_rate 0.000186139
2017-10-10T11:42:38.565413: step 990, loss 0.172002, acc 0.953125, learning_rate 0.000185787
2017-10-10T11:42:38.679040: step 991, loss 0.243623, acc 0.890625, learning_rate 0.000185437
2017-10-10T11:42:38.798538: step 992, loss 0.138239, acc 0.953125, learning_rate 0.000185088
2017-10-10T11:42:38.919437: step 993, loss 0.0721119, acc 0.96875, learning_rate 0.000184741
2017-10-10T11:42:39.036275: step 994, loss 0.0855272, acc 0.953125, learning_rate 0.000184395
2017-10-10T11:42:39.151368: step 995, loss 0.187063, acc 0.9375, learning_rate 0.000184051
2017-10-10T11:42:39.270207: step 996, loss 0.229138, acc 0.890625, learning_rate 0.000183708
2017-10-10T11:42:39.387746: step 997, loss 0.115524, acc 0.96875, learning_rate 0.000183366
2017-10-10T11:42:39.505111: step 998, loss 0.236196, acc 0.9375, learning_rate 0.000183026
2017-10-10T11:42:39.621859: step 999, loss 0.180322, acc 0.953125, learning_rate 0.000182687
2017-10-10T11:42:39.740616: step 1000, loss 0.0800509, acc 0.984375, learning_rate 0.000182349

Evaluation:
2017-10-10T11:42:40.077953: step 1000, loss 0.223547, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1000

2017-10-10T11:42:40.757408: step 1001, loss 0.169009, acc 0.9375, learning_rate 0.000182013
2017-10-10T11:42:40.875732: step 1002, loss 0.0910396, acc 0.953125, learning_rate 0.000181678
2017-10-10T11:42:40.992308: step 1003, loss 0.117794, acc 0.984375, learning_rate 0.000181345
2017-10-10T11:42:41.105742: step 1004, loss 0.165826, acc 0.953125, learning_rate 0.000181013
2017-10-10T11:42:41.225982: step 1005, loss 0.0640453, acc 0.96875, learning_rate 0.000180682
2017-10-10T11:42:41.346215: step 1006, loss 0.16811, acc 0.953125, learning_rate 0.000180353
2017-10-10T11:42:41.462864: step 1007, loss 0.206678, acc 0.90625, learning_rate 0.000180025
2017-10-10T11:42:41.580193: step 1008, loss 0.158198, acc 0.9375, learning_rate 0.000179698
2017-10-10T11:42:41.698732: step 1009, loss 0.17722, acc 0.9375, learning_rate 0.000179373
2017-10-10T11:42:41.813039: step 1010, loss 0.149967, acc 0.9375, learning_rate 0.000179049
2017-10-10T11:42:41.937783: step 1011, loss 0.104065, acc 0.96875, learning_rate 0.000178726
2017-10-10T11:42:42.065246: step 1012, loss 0.109005, acc 0.953125, learning_rate 0.000178405
2017-10-10T11:42:42.206068: step 1013, loss 0.258295, acc 0.921875, learning_rate 0.000178085
2017-10-10T11:42:42.345594: step 1014, loss 0.160831, acc 0.921875, learning_rate 0.000177766
2017-10-10T11:42:42.472269: step 1015, loss 0.347069, acc 0.875, learning_rate 0.000177449
2017-10-10T11:42:42.597623: step 1016, loss 0.100729, acc 0.96875, learning_rate 0.000177133
2017-10-10T11:42:42.724248: step 1017, loss 0.241162, acc 0.90625, learning_rate 0.000176818
2017-10-10T11:42:42.851329: step 1018, loss 0.115608, acc 0.96875, learning_rate 0.000176504
2017-10-10T11:42:42.973585: step 1019, loss 0.145061, acc 0.96875, learning_rate 0.000176192
2017-10-10T11:42:43.094150: step 1020, loss 0.0882928, acc 0.96875, learning_rate 0.000175881
2017-10-10T11:42:43.221000: step 1021, loss 0.219019, acc 0.90625, learning_rate 0.000175571
2017-10-10T11:42:43.337384: step 1022, loss 0.245531, acc 0.9375, learning_rate 0.000175263
2017-10-10T11:42:43.463860: step 1023, loss 0.0971204, acc 0.96875, learning_rate 0.000174956
2017-10-10T11:42:43.602659: step 1024, loss 0.131087, acc 0.953125, learning_rate 0.00017465
2017-10-10T11:42:43.729219: step 1025, loss 0.143898, acc 0.953125, learning_rate 0.000174345
2017-10-10T11:42:43.844005: step 1026, loss 0.139512, acc 0.96875, learning_rate 0.000174042
2017-10-10T11:42:43.970442: step 1027, loss 0.145361, acc 0.9375, learning_rate 0.000173739
2017-10-10T11:42:44.096516: step 1028, loss 0.106215, acc 0.953125, learning_rate 0.000173438
2017-10-10T11:42:44.220985: step 1029, loss 0.0544193, acc 0.984375, learning_rate 0.000173139
2017-10-10T11:42:44.339876: step 1030, loss 0.190376, acc 0.921875, learning_rate 0.00017284
2017-10-10T11:42:44.464107: step 1031, loss 0.119307, acc 0.96875, learning_rate 0.000172543
2017-10-10T11:42:44.592966: step 1032, loss 0.279669, acc 0.859375, learning_rate 0.000172247
2017-10-10T11:42:44.712906: step 1033, loss 0.148801, acc 0.953125, learning_rate 0.000171952
2017-10-10T11:42:44.841315: step 1034, loss 0.0837928, acc 1, learning_rate 0.000171658
2017-10-10T11:42:44.975486: step 1035, loss 0.269721, acc 0.890625, learning_rate 0.000171366
2017-10-10T11:42:45.103575: step 1036, loss 0.213907, acc 0.9375, learning_rate 0.000171074
2017-10-10T11:42:45.230365: step 1037, loss 0.110387, acc 0.96875, learning_rate 0.000170784
2017-10-10T11:42:45.354251: step 1038, loss 0.220974, acc 0.875, learning_rate 0.000170495
2017-10-10T11:42:45.477070: step 1039, loss 0.110847, acc 0.96875, learning_rate 0.000170208
2017-10-10T11:42:45.595946: step 1040, loss 0.175507, acc 0.921875, learning_rate 0.000169921

Evaluation:
2017-10-10T11:42:46.339489: step 1040, loss 0.221996, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1040

2017-10-10T11:42:46.910534: step 1041, loss 0.110923, acc 0.9375, learning_rate 0.000169636
2017-10-10T11:42:47.047122: step 1042, loss 0.0954061, acc 0.96875, learning_rate 0.000169351
2017-10-10T11:42:47.178334: step 1043, loss 0.146183, acc 0.953125, learning_rate 0.000169068
2017-10-10T11:42:47.312886: step 1044, loss 0.126585, acc 0.984375, learning_rate 0.000168786
2017-10-10T11:42:47.439753: step 1045, loss 0.161791, acc 0.921875, learning_rate 0.000168506
2017-10-10T11:42:47.575276: step 1046, loss 0.254462, acc 0.9375, learning_rate 0.000168226
2017-10-10T11:42:47.701741: step 1047, loss 0.15357, acc 0.9375, learning_rate 0.000167947
2017-10-10T11:42:47.832391: step 1048, loss 0.137582, acc 0.953125, learning_rate 0.00016767
2017-10-10T11:42:47.962069: step 1049, loss 0.224295, acc 0.9375, learning_rate 0.000167394
2017-10-10T11:42:48.080781: step 1050, loss 0.0938817, acc 0.96875, learning_rate 0.000167119
2017-10-10T11:42:48.202693: step 1051, loss 0.0987026, acc 0.96875, learning_rate 0.000166845
2017-10-10T11:42:48.323110: step 1052, loss 0.164949, acc 0.984375, learning_rate 0.000166572
2017-10-10T11:42:48.459605: step 1053, loss 0.126775, acc 0.96875, learning_rate 0.0001663
2017-10-10T11:42:48.585764: step 1054, loss 0.0797268, acc 0.96875, learning_rate 0.00016603
2017-10-10T11:42:48.717328: step 1055, loss 0.162104, acc 0.9375, learning_rate 0.00016576
2017-10-10T11:42:48.848462: step 1056, loss 0.123228, acc 0.953125, learning_rate 0.000165492
2017-10-10T11:42:48.969112: step 1057, loss 0.182198, acc 0.9375, learning_rate 0.000165224
2017-10-10T11:42:49.096712: step 1058, loss 0.193108, acc 0.90625, learning_rate 0.000164958
2017-10-10T11:42:49.224611: step 1059, loss 0.21552, acc 0.921875, learning_rate 0.000164693
2017-10-10T11:42:49.345352: step 1060, loss 0.249741, acc 0.90625, learning_rate 0.000164429
2017-10-10T11:42:49.479429: step 1061, loss 0.26441, acc 0.90625, learning_rate 0.000164166
2017-10-10T11:42:49.606700: step 1062, loss 0.091803, acc 0.984375, learning_rate 0.000163904
2017-10-10T11:42:49.732452: step 1063, loss 0.148073, acc 0.953125, learning_rate 0.000163643
2017-10-10T11:42:49.861761: step 1064, loss 0.0698387, acc 0.984375, learning_rate 0.000163383
2017-10-10T11:42:49.985959: step 1065, loss 0.112601, acc 0.96875, learning_rate 0.000163125
2017-10-10T11:42:50.118903: step 1066, loss 0.201013, acc 0.953125, learning_rate 0.000162867
2017-10-10T11:42:50.256691: step 1067, loss 0.147735, acc 0.953125, learning_rate 0.00016261
2017-10-10T11:42:50.383695: step 1068, loss 0.0844225, acc 0.984375, learning_rate 0.000162355
2017-10-10T11:42:50.506959: step 1069, loss 0.126653, acc 0.953125, learning_rate 0.0001621
2017-10-10T11:42:50.634474: step 1070, loss 0.139299, acc 0.9375, learning_rate 0.000161847
2017-10-10T11:42:50.757967: step 1071, loss 0.239015, acc 0.9375, learning_rate 0.000161594
2017-10-10T11:42:50.895946: step 1072, loss 0.145965, acc 0.9375, learning_rate 0.000161343
2017-10-10T11:42:51.027011: step 1073, loss 0.145957, acc 0.96875, learning_rate 0.000161093
2017-10-10T11:42:51.156623: step 1074, loss 0.257945, acc 0.90625, learning_rate 0.000160843
2017-10-10T11:42:51.280918: step 1075, loss 0.201138, acc 0.953125, learning_rate 0.000160595
2017-10-10T11:42:51.410842: step 1076, loss 0.150193, acc 0.953125, learning_rate 0.000160348
2017-10-10T11:42:51.534043: step 1077, loss 0.129235, acc 0.953125, learning_rate 0.000160101
2017-10-10T11:42:51.637503: step 1078, loss 0.120202, acc 0.960784, learning_rate 0.000159856
2017-10-10T11:42:51.765979: step 1079, loss 0.221056, acc 0.90625, learning_rate 0.000159612
2017-10-10T11:42:51.893324: step 1080, loss 0.0993287, acc 0.953125, learning_rate 0.000159368

Evaluation:
2017-10-10T11:42:52.264374: step 1080, loss 0.220343, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1080

2017-10-10T11:42:52.914661: step 1081, loss 0.125939, acc 0.96875, learning_rate 0.000159126
2017-10-10T11:42:53.042958: step 1082, loss 0.0667546, acc 1, learning_rate 0.000158885
2017-10-10T11:42:53.171873: step 1083, loss 0.195927, acc 0.921875, learning_rate 0.000158644
2017-10-10T11:42:53.296252: step 1084, loss 0.0551481, acc 1, learning_rate 0.000158405
2017-10-10T11:42:53.424143: step 1085, loss 0.0979794, acc 0.96875, learning_rate 0.000158167
2017-10-10T11:42:53.554496: step 1086, loss 0.048262, acc 1, learning_rate 0.000157929
2017-10-10T11:42:53.682302: step 1087, loss 0.141063, acc 0.953125, learning_rate 0.000157693
2017-10-10T11:42:53.802415: step 1088, loss 0.139968, acc 0.984375, learning_rate 0.000157457
2017-10-10T11:42:53.930564: step 1089, loss 0.127504, acc 0.96875, learning_rate 0.000157223
2017-10-10T11:42:54.065019: step 1090, loss 0.152527, acc 0.90625, learning_rate 0.000156989
2017-10-10T11:42:54.183633: step 1091, loss 0.302788, acc 0.9375, learning_rate 0.000156757
2017-10-10T11:42:54.304052: step 1092, loss 0.194533, acc 0.90625, learning_rate 0.000156525
2017-10-10T11:42:54.432393: step 1093, loss 0.107976, acc 0.953125, learning_rate 0.000156294
2017-10-10T11:42:54.569623: step 1094, loss 0.255483, acc 0.890625, learning_rate 0.000156064
2017-10-10T11:42:54.706310: step 1095, loss 0.129795, acc 0.953125, learning_rate 0.000155836
2017-10-10T11:42:54.835415: step 1096, loss 0.210259, acc 0.90625, learning_rate 0.000155608
2017-10-10T11:42:54.958424: step 1097, loss 0.088846, acc 0.953125, learning_rate 0.000155381
2017-10-10T11:42:55.083747: step 1098, loss 0.235831, acc 0.921875, learning_rate 0.000155155
2017-10-10T11:42:55.206189: step 1099, loss 0.154624, acc 0.9375, learning_rate 0.000154929
2017-10-10T11:42:55.330957: step 1100, loss 0.218358, acc 0.953125, learning_rate 0.000154705
2017-10-10T11:42:55.458716: step 1101, loss 0.112212, acc 0.953125, learning_rate 0.000154482
2017-10-10T11:42:55.584361: step 1102, loss 0.117646, acc 0.953125, learning_rate 0.00015426
2017-10-10T11:42:55.709234: step 1103, loss 0.252437, acc 0.9375, learning_rate 0.000154038
2017-10-10T11:42:55.836309: step 1104, loss 0.167611, acc 0.9375, learning_rate 0.000153818
2017-10-10T11:42:55.963396: step 1105, loss 0.0830347, acc 0.96875, learning_rate 0.000153598
2017-10-10T11:42:56.087223: step 1106, loss 0.188654, acc 0.9375, learning_rate 0.000153379
2017-10-10T11:42:56.211412: step 1107, loss 0.228448, acc 0.9375, learning_rate 0.000153161
2017-10-10T11:42:56.335170: step 1108, loss 0.170934, acc 0.9375, learning_rate 0.000152944
2017-10-10T11:42:56.464483: step 1109, loss 0.0878464, acc 0.953125, learning_rate 0.000152728
2017-10-10T11:42:56.588437: step 1110, loss 0.138838, acc 0.9375, learning_rate 0.000152513
2017-10-10T11:42:56.710727: step 1111, loss 0.0527626, acc 1, learning_rate 0.000152299
2017-10-10T11:42:56.836090: step 1112, loss 0.254341, acc 0.890625, learning_rate 0.000152085
2017-10-10T11:42:56.970869: step 1113, loss 0.112812, acc 0.953125, learning_rate 0.000151872
2017-10-10T11:42:57.091957: step 1114, loss 0.251107, acc 0.890625, learning_rate 0.000151661
2017-10-10T11:42:57.219335: step 1115, loss 0.0484893, acc 1, learning_rate 0.00015145
2017-10-10T11:42:57.345656: step 1116, loss 0.114677, acc 0.953125, learning_rate 0.00015124
2017-10-10T11:42:57.471700: step 1117, loss 0.257926, acc 0.875, learning_rate 0.000151031
2017-10-10T11:42:57.593286: step 1118, loss 0.157196, acc 0.953125, learning_rate 0.000150822
2017-10-10T11:42:57.719201: step 1119, loss 0.186448, acc 0.953125, learning_rate 0.000150615
2017-10-10T11:42:57.839696: step 1120, loss 0.127648, acc 0.953125, learning_rate 0.000150408

Evaluation:
2017-10-10T11:42:58.234474: step 1120, loss 0.222295, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1120

2017-10-10T11:42:58.932319: step 1121, loss 0.115247, acc 0.96875, learning_rate 0.000150203
2017-10-10T11:42:59.054380: step 1122, loss 0.0810475, acc 0.96875, learning_rate 0.000149998
2017-10-10T11:42:59.181540: step 1123, loss 0.122065, acc 0.96875, learning_rate 0.000149794
2017-10-10T11:42:59.309530: step 1124, loss 0.265843, acc 0.90625, learning_rate 0.00014959
2017-10-10T11:42:59.434473: step 1125, loss 0.0826661, acc 0.984375, learning_rate 0.000149388
2017-10-10T11:42:59.569183: step 1126, loss 0.0941278, acc 0.96875, learning_rate 0.000149186
2017-10-10T11:42:59.691326: step 1127, loss 0.139815, acc 0.9375, learning_rate 0.000148986
2017-10-10T11:42:59.815876: step 1128, loss 0.123201, acc 0.953125, learning_rate 0.000148786
2017-10-10T11:42:59.942298: step 1129, loss 0.191469, acc 0.9375, learning_rate 0.000148587
2017-10-10T11:43:00.064976: step 1130, loss 0.113243, acc 0.96875, learning_rate 0.000148388
2017-10-10T11:43:00.194383: step 1131, loss 0.210427, acc 0.890625, learning_rate 0.000148191
2017-10-10T11:43:00.319531: step 1132, loss 0.165041, acc 0.9375, learning_rate 0.000147994
2017-10-10T11:43:00.441856: step 1133, loss 0.110184, acc 0.96875, learning_rate 0.000147798
2017-10-10T11:43:00.565780: step 1134, loss 0.172737, acc 0.9375, learning_rate 0.000147603
2017-10-10T11:43:00.693276: step 1135, loss 0.118796, acc 0.953125, learning_rate 0.000147409
2017-10-10T11:43:00.817828: step 1136, loss 0.117494, acc 0.96875, learning_rate 0.000147215
2017-10-10T11:43:00.953088: step 1137, loss 0.115959, acc 0.953125, learning_rate 0.000147022
2017-10-10T11:43:01.074143: step 1138, loss 0.0466257, acc 1, learning_rate 0.000146831
2017-10-10T11:43:01.193684: step 1139, loss 0.139066, acc 0.953125, learning_rate 0.000146639
2017-10-10T11:43:01.317242: step 1140, loss 0.214942, acc 0.9375, learning_rate 0.000146449
2017-10-10T11:43:01.455475: step 1141, loss 0.125003, acc 0.953125, learning_rate 0.000146259
2017-10-10T11:43:01.578545: step 1142, loss 0.126487, acc 0.953125, learning_rate 0.000146071
2017-10-10T11:43:01.699181: step 1143, loss 0.118173, acc 0.96875, learning_rate 0.000145883
2017-10-10T11:43:01.821307: step 1144, loss 0.147455, acc 0.921875, learning_rate 0.000145695
2017-10-10T11:43:01.952255: step 1145, loss 0.34001, acc 0.90625, learning_rate 0.000145509
2017-10-10T11:43:02.077755: step 1146, loss 0.216164, acc 0.921875, learning_rate 0.000145323
2017-10-10T11:43:02.198007: step 1147, loss 0.07221, acc 0.984375, learning_rate 0.000145138
2017-10-10T11:43:02.321547: step 1148, loss 0.0933638, acc 0.96875, learning_rate 0.000144954
2017-10-10T11:43:02.444372: step 1149, loss 0.244823, acc 0.921875, learning_rate 0.00014477
2017-10-10T11:43:02.566687: step 1150, loss 0.183117, acc 0.921875, learning_rate 0.000144588
2017-10-10T11:43:02.696455: step 1151, loss 0.122245, acc 0.9375, learning_rate 0.000144406
2017-10-10T11:43:02.819279: step 1152, loss 0.195329, acc 0.921875, learning_rate 0.000144224
2017-10-10T11:43:02.952493: step 1153, loss 0.179202, acc 0.96875, learning_rate 0.000144044
2017-10-10T11:43:03.076361: step 1154, loss 0.147079, acc 0.9375, learning_rate 0.000143864
2017-10-10T11:43:03.197077: step 1155, loss 0.105638, acc 0.984375, learning_rate 0.000143685
2017-10-10T11:43:03.317914: step 1156, loss 0.208278, acc 0.9375, learning_rate 0.000143507
2017-10-10T11:43:03.443882: step 1157, loss 0.132774, acc 0.96875, learning_rate 0.000143329
2017-10-10T11:43:03.562758: step 1158, loss 0.0803477, acc 0.96875, learning_rate 0.000143152
2017-10-10T11:43:03.683351: step 1159, loss 0.131359, acc 0.953125, learning_rate 0.000142976
2017-10-10T11:43:03.805828: step 1160, loss 0.0860811, acc 0.984375, learning_rate 0.000142801

Evaluation:
2017-10-10T11:43:04.189524: step 1160, loss 0.222696, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1160

2017-10-10T11:43:04.782647: step 1161, loss 0.151872, acc 0.921875, learning_rate 0.000142626
2017-10-10T11:43:04.948587: step 1162, loss 0.0510202, acc 1, learning_rate 0.000142452
2017-10-10T11:43:05.114378: step 1163, loss 0.0723106, acc 0.984375, learning_rate 0.000142279
2017-10-10T11:43:05.293090: step 1164, loss 0.117313, acc 0.953125, learning_rate 0.000142106
2017-10-10T11:43:05.478420: step 1165, loss 0.13415, acc 0.96875, learning_rate 0.000141934
2017-10-10T11:43:05.685349: step 1166, loss 0.0741994, acc 0.96875, learning_rate 0.000141763
2017-10-10T11:43:05.884304: step 1167, loss 0.119738, acc 0.9375, learning_rate 0.000141593
2017-10-10T11:43:06.087831: step 1168, loss 0.144042, acc 0.9375, learning_rate 0.000141423
2017-10-10T11:43:06.271841: step 1169, loss 0.0546674, acc 0.984375, learning_rate 0.000141254
2017-10-10T11:43:06.449677: step 1170, loss 0.145041, acc 0.9375, learning_rate 0.000141085
2017-10-10T11:43:06.634146: step 1171, loss 0.0614749, acc 1, learning_rate 0.000140918
2017-10-10T11:43:06.816862: step 1172, loss 0.0967181, acc 0.953125, learning_rate 0.000140751
2017-10-10T11:43:06.992883: step 1173, loss 0.22127, acc 0.921875, learning_rate 0.000140584
2017-10-10T11:43:07.182985: step 1174, loss 0.226672, acc 0.90625, learning_rate 0.000140419
2017-10-10T11:43:07.360188: step 1175, loss 0.103075, acc 0.953125, learning_rate 0.000140254
2017-10-10T11:43:07.510259: step 1176, loss 0.0710468, acc 0.980392, learning_rate 0.000140089
2017-10-10T11:43:07.695709: step 1177, loss 0.0993196, acc 0.96875, learning_rate 0.000139926
2017-10-10T11:43:07.875145: step 1178, loss 0.0847342, acc 0.96875, learning_rate 0.000139763
2017-10-10T11:43:08.054138: step 1179, loss 0.0592564, acc 1, learning_rate 0.0001396
2017-10-10T11:43:08.218948: step 1180, loss 0.125107, acc 0.953125, learning_rate 0.000139439
2017-10-10T11:43:08.393540: step 1181, loss 0.0738137, acc 0.984375, learning_rate 0.000139278
2017-10-10T11:43:08.569372: step 1182, loss 0.216055, acc 0.90625, learning_rate 0.000139118
2017-10-10T11:43:08.753935: step 1183, loss 0.0341888, acc 1, learning_rate 0.000138958
2017-10-10T11:43:08.929550: step 1184, loss 0.0779087, acc 0.953125, learning_rate 0.000138799
2017-10-10T11:43:09.100188: step 1185, loss 0.218085, acc 0.90625, learning_rate 0.00013864
2017-10-10T11:43:09.282425: step 1186, loss 0.0867911, acc 0.984375, learning_rate 0.000138483
2017-10-10T11:43:09.480753: step 1187, loss 0.245276, acc 0.90625, learning_rate 0.000138326
2017-10-10T11:43:09.655079: step 1188, loss 0.174305, acc 0.90625, learning_rate 0.000138169
2017-10-10T11:43:09.849144: step 1189, loss 0.243482, acc 0.90625, learning_rate 0.000138013
2017-10-10T11:43:10.036152: step 1190, loss 0.0771481, acc 0.984375, learning_rate 0.000137858
2017-10-10T11:43:10.228345: step 1191, loss 0.114688, acc 0.953125, learning_rate 0.000137704
2017-10-10T11:43:10.435901: step 1192, loss 0.249357, acc 0.921875, learning_rate 0.00013755
2017-10-10T11:43:10.631867: step 1193, loss 0.273969, acc 0.875, learning_rate 0.000137397
2017-10-10T11:43:10.832008: step 1194, loss 0.151565, acc 0.9375, learning_rate 0.000137244
2017-10-10T11:43:11.033381: step 1195, loss 0.13227, acc 0.953125, learning_rate 0.000137092
2017-10-10T11:43:11.217390: step 1196, loss 0.101282, acc 0.984375, learning_rate 0.000136941
2017-10-10T11:43:11.401867: step 1197, loss 0.218471, acc 0.9375, learning_rate 0.00013679
2017-10-10T11:43:11.589564: step 1198, loss 0.200781, acc 0.90625, learning_rate 0.00013664
2017-10-10T11:43:11.762908: step 1199, loss 0.218095, acc 0.890625, learning_rate 0.00013649
2017-10-10T11:43:11.950027: step 1200, loss 0.159803, acc 0.9375, learning_rate 0.000136341

Evaluation:
2017-10-10T11:43:12.850823: step 1200, loss 0.219862, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1200

2017-10-10T11:43:14.392920: step 1201, loss 0.101653, acc 0.953125, learning_rate 0.000136193
2017-10-10T11:43:14.590089: step 1202, loss 0.111603, acc 0.953125, learning_rate 0.000136045
2017-10-10T11:43:14.777659: step 1203, loss 0.0941004, acc 0.96875, learning_rate 0.000135898
2017-10-10T11:43:14.984702: step 1204, loss 0.185718, acc 0.921875, learning_rate 0.000135751
2017-10-10T11:43:15.178108: step 1205, loss 0.283411, acc 0.9375, learning_rate 0.000135605
2017-10-10T11:43:15.381386: step 1206, loss 0.228067, acc 0.921875, learning_rate 0.00013546
2017-10-10T11:43:15.569365: step 1207, loss 0.240382, acc 0.921875, learning_rate 0.000135315
2017-10-10T11:43:15.755930: step 1208, loss 0.139783, acc 0.9375, learning_rate 0.000135171
2017-10-10T11:43:15.925294: step 1209, loss 0.187893, acc 0.921875, learning_rate 0.000135028
2017-10-10T11:43:16.092436: step 1210, loss 0.117331, acc 0.953125, learning_rate 0.000134885
2017-10-10T11:43:16.313066: step 1211, loss 0.168591, acc 0.953125, learning_rate 0.000134742
2017-10-10T11:43:16.484350: step 1212, loss 0.128843, acc 0.9375, learning_rate 0.0001346
2017-10-10T11:43:16.649724: step 1213, loss 0.0945068, acc 0.96875, learning_rate 0.000134459
2017-10-10T11:43:16.807445: step 1214, loss 0.157599, acc 0.9375, learning_rate 0.000134319
2017-10-10T11:43:17.005816: step 1215, loss 0.223438, acc 0.90625, learning_rate 0.000134178
2017-10-10T11:43:17.159641: step 1216, loss 0.0849685, acc 0.96875, learning_rate 0.000134039
2017-10-10T11:43:17.366966: step 1217, loss 0.171452, acc 0.9375, learning_rate 0.0001339
2017-10-10T11:43:17.550138: step 1218, loss 0.257841, acc 0.875, learning_rate 0.000133762
2017-10-10T11:43:17.724942: step 1219, loss 0.195085, acc 0.953125, learning_rate 0.000133624
2017-10-10T11:43:17.922292: step 1220, loss 0.100496, acc 0.953125, learning_rate 0.000133487
2017-10-10T11:43:18.107084: step 1221, loss 0.124771, acc 0.9375, learning_rate 0.00013335
2017-10-10T11:43:18.314931: step 1222, loss 0.131882, acc 0.953125, learning_rate 0.000133214
2017-10-10T11:43:18.515025: step 1223, loss 0.144891, acc 0.96875, learning_rate 0.000133078
2017-10-10T11:43:18.706299: step 1224, loss 0.11378, acc 0.953125, learning_rate 0.000132943
2017-10-10T11:43:18.888857: step 1225, loss 0.095992, acc 0.96875, learning_rate 0.000132809
2017-10-10T11:43:19.078104: step 1226, loss 0.0810625, acc 0.96875, learning_rate 0.000132675
2017-10-10T11:43:19.241861: step 1227, loss 0.0911509, acc 0.953125, learning_rate 0.000132541
2017-10-10T11:43:19.425039: step 1228, loss 0.158289, acc 0.96875, learning_rate 0.000132409
2017-10-10T11:43:19.628832: step 1229, loss 0.0883074, acc 0.96875, learning_rate 0.000132276
2017-10-10T11:43:19.818253: step 1230, loss 0.220731, acc 0.90625, learning_rate 0.000132145
2017-10-10T11:43:20.006685: step 1231, loss 0.0874173, acc 0.984375, learning_rate 0.000132013
2017-10-10T11:43:20.200038: step 1232, loss 0.125752, acc 0.96875, learning_rate 0.000131883
2017-10-10T11:43:20.401014: step 1233, loss 0.275448, acc 0.875, learning_rate 0.000131753
2017-10-10T11:43:20.573735: step 1234, loss 0.0638059, acc 0.984375, learning_rate 0.000131623
2017-10-10T11:43:20.759205: step 1235, loss 0.146728, acc 0.9375, learning_rate 0.000131494
2017-10-10T11:43:20.931782: step 1236, loss 0.145185, acc 0.9375, learning_rate 0.000131365
2017-10-10T11:43:21.111847: step 1237, loss 0.0821208, acc 0.96875, learning_rate 0.000131237
2017-10-10T11:43:21.293277: step 1238, loss 0.1425, acc 0.96875, learning_rate 0.00013111
2017-10-10T11:43:21.523936: step 1239, loss 0.186829, acc 0.921875, learning_rate 0.000130983
2017-10-10T11:43:21.754510: step 1240, loss 0.126552, acc 0.9375, learning_rate 0.000130856

Evaluation:
2017-10-10T11:43:22.102219: step 1240, loss 0.218379, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1240

2017-10-10T11:43:22.997294: step 1241, loss 0.110739, acc 1, learning_rate 0.00013073
2017-10-10T11:43:23.175441: step 1242, loss 0.224188, acc 0.9375, learning_rate 0.000130605
2017-10-10T11:43:23.375970: step 1243, loss 0.0947984, acc 0.984375, learning_rate 0.00013048
2017-10-10T11:43:23.562120: step 1244, loss 0.085668, acc 0.96875, learning_rate 0.000130356
2017-10-10T11:43:23.735087: step 1245, loss 0.338413, acc 0.84375, learning_rate 0.000130232
2017-10-10T11:43:23.938312: step 1246, loss 0.122724, acc 0.984375, learning_rate 0.000130108
2017-10-10T11:43:24.135385: step 1247, loss 0.062925, acc 0.984375, learning_rate 0.000129985
2017-10-10T11:43:24.341857: step 1248, loss 0.176773, acc 0.9375, learning_rate 0.000129863
2017-10-10T11:43:24.536956: step 1249, loss 0.0899113, acc 0.96875, learning_rate 0.000129741
2017-10-10T11:43:24.735448: step 1250, loss 0.0787609, acc 0.984375, learning_rate 0.00012962
2017-10-10T11:43:24.946516: step 1251, loss 0.210639, acc 0.890625, learning_rate 0.000129499
2017-10-10T11:43:25.136071: step 1252, loss 0.165401, acc 0.953125, learning_rate 0.000129378
2017-10-10T11:43:25.312945: step 1253, loss 0.0449245, acc 1, learning_rate 0.000129259
2017-10-10T11:43:25.495686: step 1254, loss 0.0587667, acc 1, learning_rate 0.000129139
2017-10-10T11:43:25.646659: step 1255, loss 0.0410962, acc 1, learning_rate 0.00012902
2017-10-10T11:43:25.856029: step 1256, loss 0.140708, acc 0.953125, learning_rate 0.000128902
2017-10-10T11:43:26.047969: step 1257, loss 0.0932749, acc 0.96875, learning_rate 0.000128784
2017-10-10T11:43:26.249143: step 1258, loss 0.179738, acc 0.9375, learning_rate 0.000128666
2017-10-10T11:43:26.455136: step 1259, loss 0.141616, acc 0.953125, learning_rate 0.000128549
2017-10-10T11:43:26.669926: step 1260, loss 0.233571, acc 0.9375, learning_rate 0.000128433
2017-10-10T11:43:26.884058: step 1261, loss 0.140689, acc 0.953125, learning_rate 0.000128317
2017-10-10T11:43:27.083805: step 1262, loss 0.103686, acc 0.96875, learning_rate 0.000128201
2017-10-10T11:43:27.274708: step 1263, loss 0.191257, acc 0.953125, learning_rate 0.000128086
2017-10-10T11:43:27.471919: step 1264, loss 0.179357, acc 0.90625, learning_rate 0.000127971
2017-10-10T11:43:27.662015: step 1265, loss 0.0657685, acc 0.984375, learning_rate 0.000127857
2017-10-10T11:43:27.859954: step 1266, loss 0.33243, acc 0.890625, learning_rate 0.000127743
2017-10-10T11:43:28.051053: step 1267, loss 0.0915543, acc 0.984375, learning_rate 0.00012763
2017-10-10T11:43:28.238053: step 1268, loss 0.206854, acc 0.953125, learning_rate 0.000127517
2017-10-10T11:43:28.408199: step 1269, loss 0.0683142, acc 0.984375, learning_rate 0.000127405
2017-10-10T11:43:28.613698: step 1270, loss 0.115523, acc 0.9375, learning_rate 0.000127293
2017-10-10T11:43:28.796089: step 1271, loss 0.193355, acc 0.90625, learning_rate 0.000127182
2017-10-10T11:43:28.984820: step 1272, loss 0.0628021, acc 0.984375, learning_rate 0.000127071
2017-10-10T11:43:29.154700: step 1273, loss 0.145978, acc 0.9375, learning_rate 0.00012696
2017-10-10T11:43:29.289044: step 1274, loss 0.210978, acc 0.921569, learning_rate 0.00012685
2017-10-10T11:43:29.500969: step 1275, loss 0.0601046, acc 0.984375, learning_rate 0.000126741
2017-10-10T11:43:29.769244: step 1276, loss 0.117753, acc 0.9375, learning_rate 0.000126632
2017-10-10T11:43:29.907050: step 1277, loss 0.160239, acc 0.96875, learning_rate 0.000126523
2017-10-10T11:43:30.048376: step 1278, loss 0.214271, acc 0.953125, learning_rate 0.000126415
2017-10-10T11:43:30.181802: step 1279, loss 0.170541, acc 0.953125, learning_rate 0.000126307
2017-10-10T11:43:30.321315: step 1280, loss 0.0606764, acc 0.96875, learning_rate 0.000126199

Evaluation:
2017-10-10T11:43:30.666475: step 1280, loss 0.221813, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1280

2017-10-10T11:43:31.593304: step 1281, loss 0.114183, acc 0.953125, learning_rate 0.000126093
2017-10-10T11:43:31.780847: step 1282, loss 0.298983, acc 0.90625, learning_rate 0.000125986
2017-10-10T11:43:32.381114: step 1283, loss 0.197845, acc 0.9375, learning_rate 0.00012588
2017-10-10T11:43:32.582452: step 1284, loss 0.0927747, acc 0.953125, learning_rate 0.000125774
2017-10-10T11:43:32.792552: step 1285, loss 0.0990405, acc 0.96875, learning_rate 0.000125669
2017-10-10T11:43:32.981892: step 1286, loss 0.146118, acc 0.953125, learning_rate 0.000125564
2017-10-10T11:43:33.172861: step 1287, loss 0.0668065, acc 0.96875, learning_rate 0.00012546
2017-10-10T11:43:33.351770: step 1288, loss 0.25574, acc 0.90625, learning_rate 0.000125356
2017-10-10T11:43:33.537213: step 1289, loss 0.203009, acc 0.890625, learning_rate 0.000125253
2017-10-10T11:43:33.723678: step 1290, loss 0.136277, acc 0.953125, learning_rate 0.00012515
2017-10-10T11:43:33.930906: step 1291, loss 0.16141, acc 0.921875, learning_rate 0.000125047
2017-10-10T11:43:34.127559: step 1292, loss 0.287991, acc 0.875, learning_rate 0.000124945
2017-10-10T11:43:34.297329: step 1293, loss 0.236827, acc 0.890625, learning_rate 0.000124843
2017-10-10T11:43:34.488973: step 1294, loss 0.076947, acc 1, learning_rate 0.000124741
2017-10-10T11:43:34.684861: step 1295, loss 0.167791, acc 0.96875, learning_rate 0.00012464
2017-10-10T11:43:34.891510: step 1296, loss 0.211036, acc 0.921875, learning_rate 0.00012454
2017-10-10T11:43:35.074868: step 1297, loss 0.132587, acc 0.984375, learning_rate 0.00012444
2017-10-10T11:43:35.264344: step 1298, loss 0.219851, acc 0.90625, learning_rate 0.00012434
2017-10-10T11:43:35.456669: step 1299, loss 0.0816174, acc 0.96875, learning_rate 0.000124241
2017-10-10T11:43:35.652057: step 1300, loss 0.0383705, acc 1, learning_rate 0.000124142
2017-10-10T11:43:35.860868: step 1301, loss 0.0857928, acc 0.96875, learning_rate 0.000124043
2017-10-10T11:43:36.057510: step 1302, loss 0.0985059, acc 1, learning_rate 0.000123945
2017-10-10T11:43:36.239102: step 1303, loss 0.147077, acc 0.953125, learning_rate 0.000123847
2017-10-10T11:43:36.425107: step 1304, loss 0.141416, acc 0.953125, learning_rate 0.00012375
2017-10-10T11:43:36.629756: step 1305, loss 0.10633, acc 0.984375, learning_rate 0.000123653
2017-10-10T11:43:36.814687: step 1306, loss 0.0941296, acc 0.96875, learning_rate 0.000123556
2017-10-10T11:43:37.004545: step 1307, loss 0.098831, acc 0.96875, learning_rate 0.00012346
2017-10-10T11:43:37.224857: step 1308, loss 0.109543, acc 0.953125, learning_rate 0.000123364
2017-10-10T11:43:37.481617: step 1309, loss 0.0827257, acc 0.96875, learning_rate 0.000123269
2017-10-10T11:43:37.614057: step 1310, loss 0.155839, acc 0.9375, learning_rate 0.000123174
2017-10-10T11:43:37.737810: step 1311, loss 0.188496, acc 0.953125, learning_rate 0.00012308
2017-10-10T11:43:37.877898: step 1312, loss 0.098346, acc 0.96875, learning_rate 0.000122985
2017-10-10T11:43:37.999987: step 1313, loss 0.304263, acc 0.890625, learning_rate 0.000122892
2017-10-10T11:43:38.137400: step 1314, loss 0.103241, acc 0.984375, learning_rate 0.000122798
2017-10-10T11:43:38.266173: step 1315, loss 0.117675, acc 0.96875, learning_rate 0.000122705
2017-10-10T11:43:38.460649: step 1316, loss 0.0897916, acc 0.96875, learning_rate 0.000122612
2017-10-10T11:43:38.663082: step 1317, loss 0.140972, acc 0.96875, learning_rate 0.00012252
2017-10-10T11:43:38.848795: step 1318, loss 0.159702, acc 0.9375, learning_rate 0.000122428
2017-10-10T11:43:39.043341: step 1319, loss 0.20631, acc 0.90625, learning_rate 0.000122337
2017-10-10T11:43:39.232384: step 1320, loss 0.108865, acc 0.953125, learning_rate 0.000122245

Evaluation:
2017-10-10T11:43:39.720347: step 1320, loss 0.217954, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1320

2017-10-10T11:43:40.783116: step 1321, loss 0.221883, acc 0.890625, learning_rate 0.000122155
2017-10-10T11:43:40.983291: step 1322, loss 0.266951, acc 0.921875, learning_rate 0.000122064
2017-10-10T11:43:41.176044: step 1323, loss 0.0779926, acc 0.984375, learning_rate 0.000121974
2017-10-10T11:43:41.357330: step 1324, loss 0.277787, acc 0.90625, learning_rate 0.000121884
2017-10-10T11:43:41.536851: step 1325, loss 0.0789781, acc 0.984375, learning_rate 0.000121795
2017-10-10T11:43:41.718860: step 1326, loss 0.125688, acc 0.96875, learning_rate 0.000121706
2017-10-10T11:43:41.910288: step 1327, loss 0.0797385, acc 0.984375, learning_rate 0.000121618
2017-10-10T11:43:42.067899: step 1328, loss 0.107394, acc 0.953125, learning_rate 0.000121529
2017-10-10T11:43:42.245791: step 1329, loss 0.16907, acc 0.9375, learning_rate 0.000121441
2017-10-10T11:43:42.435968: step 1330, loss 0.108684, acc 0.953125, learning_rate 0.000121354
2017-10-10T11:43:42.625810: step 1331, loss 0.120297, acc 0.953125, learning_rate 0.000121267
2017-10-10T11:43:42.825113: step 1332, loss 0.153297, acc 0.9375, learning_rate 0.00012118
2017-10-10T11:43:42.993770: step 1333, loss 0.158969, acc 0.9375, learning_rate 0.000121093
2017-10-10T11:43:43.149478: step 1334, loss 0.162836, acc 0.9375, learning_rate 0.000121007
2017-10-10T11:43:43.336884: step 1335, loss 0.240101, acc 0.9375, learning_rate 0.000120922
2017-10-10T11:43:43.508703: step 1336, loss 0.169076, acc 0.953125, learning_rate 0.000120836
2017-10-10T11:43:43.680859: step 1337, loss 0.139695, acc 0.953125, learning_rate 0.000120751
2017-10-10T11:43:43.904983: step 1338, loss 0.163105, acc 0.9375, learning_rate 0.000120666
2017-10-10T11:43:44.076913: step 1339, loss 0.0856505, acc 0.984375, learning_rate 0.000120582
2017-10-10T11:43:44.260682: step 1340, loss 0.249809, acc 0.90625, learning_rate 0.000120498
2017-10-10T11:43:44.449648: step 1341, loss 0.0917922, acc 0.984375, learning_rate 0.000120414
2017-10-10T11:43:44.631191: step 1342, loss 0.128382, acc 0.96875, learning_rate 0.000120331
2017-10-10T11:43:44.817003: step 1343, loss 0.0471511, acc 0.984375, learning_rate 0.000120248
2017-10-10T11:43:45.012051: step 1344, loss 0.0878215, acc 0.953125, learning_rate 0.000120165
2017-10-10T11:43:45.281550: step 1345, loss 0.0455985, acc 1, learning_rate 0.000120083
2017-10-10T11:43:45.463569: step 1346, loss 0.133776, acc 0.953125, learning_rate 0.000120001
2017-10-10T11:43:45.583390: step 1347, loss 0.139101, acc 0.953125, learning_rate 0.00011992
2017-10-10T11:43:45.704250: step 1348, loss 0.23692, acc 0.921875, learning_rate 0.000119838
2017-10-10T11:43:45.830336: step 1349, loss 0.14563, acc 0.953125, learning_rate 0.000119757
2017-10-10T11:43:45.958391: step 1350, loss 0.138478, acc 0.953125, learning_rate 0.000119677
2017-10-10T11:43:46.085263: step 1351, loss 0.216521, acc 0.890625, learning_rate 0.000119596
2017-10-10T11:43:46.205497: step 1352, loss 0.218568, acc 0.9375, learning_rate 0.000119516
2017-10-10T11:43:46.383980: step 1353, loss 0.0841579, acc 0.984375, learning_rate 0.000119437
2017-10-10T11:43:46.564750: step 1354, loss 0.221328, acc 0.9375, learning_rate 0.000119357
2017-10-10T11:43:46.756492: step 1355, loss 0.125095, acc 0.9375, learning_rate 0.000119278
2017-10-10T11:43:46.955583: step 1356, loss 0.0740446, acc 0.984375, learning_rate 0.0001192
2017-10-10T11:43:47.159100: step 1357, loss 0.103817, acc 0.953125, learning_rate 0.000119121
2017-10-10T11:43:47.331684: step 1358, loss 0.103504, acc 0.953125, learning_rate 0.000119043
2017-10-10T11:43:47.510464: step 1359, loss 0.138451, acc 0.9375, learning_rate 0.000118965
2017-10-10T11:43:47.693850: step 1360, loss 0.122107, acc 0.953125, learning_rate 0.000118888

Evaluation:
2017-10-10T11:43:48.123116: step 1360, loss 0.21956, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1360

2017-10-10T11:43:49.208271: step 1361, loss 0.223595, acc 0.890625, learning_rate 0.000118811
2017-10-10T11:43:49.386162: step 1362, loss 0.161189, acc 0.953125, learning_rate 0.000118734
2017-10-10T11:43:49.582995: step 1363, loss 0.360613, acc 0.90625, learning_rate 0.000118658
2017-10-10T11:43:49.792423: step 1364, loss 0.0750376, acc 0.984375, learning_rate 0.000118582
2017-10-10T11:43:49.987288: step 1365, loss 0.134876, acc 0.953125, learning_rate 0.000118506
2017-10-10T11:43:50.170283: step 1366, loss 0.137749, acc 0.953125, learning_rate 0.00011843
2017-10-10T11:43:50.380289: step 1367, loss 0.214932, acc 0.953125, learning_rate 0.000118355
2017-10-10T11:43:50.570881: step 1368, loss 0.106777, acc 0.96875, learning_rate 0.00011828
2017-10-10T11:43:50.772239: step 1369, loss 0.240098, acc 0.890625, learning_rate 0.000118205
2017-10-10T11:43:50.975311: step 1370, loss 0.131963, acc 0.96875, learning_rate 0.000118131
2017-10-10T11:43:51.158212: step 1371, loss 0.150846, acc 0.953125, learning_rate 0.000118057
2017-10-10T11:43:51.322387: step 1372, loss 0.132317, acc 0.960784, learning_rate 0.000117983
2017-10-10T11:43:51.513191: step 1373, loss 0.157413, acc 0.953125, learning_rate 0.00011791
2017-10-10T11:43:51.695508: step 1374, loss 0.047281, acc 1, learning_rate 0.000117837
2017-10-10T11:43:51.871767: step 1375, loss 0.062553, acc 1, learning_rate 0.000117764
2017-10-10T11:43:52.057951: step 1376, loss 0.089696, acc 0.96875, learning_rate 0.000117692
2017-10-10T11:43:52.254640: step 1377, loss 0.0470172, acc 0.984375, learning_rate 0.000117619
2017-10-10T11:43:52.444215: step 1378, loss 0.0965469, acc 0.9375, learning_rate 0.000117547
2017-10-10T11:43:52.647914: step 1379, loss 0.107531, acc 0.984375, learning_rate 0.000117476
2017-10-10T11:43:52.831282: step 1380, loss 0.0773627, acc 0.984375, learning_rate 0.000117404
2017-10-10T11:43:52.978071: step 1381, loss 0.130191, acc 0.96875, learning_rate 0.000117333
2017-10-10T11:43:53.225244: step 1382, loss 0.15957, acc 0.9375, learning_rate 0.000117263
2017-10-10T11:43:53.393468: step 1383, loss 0.141474, acc 0.953125, learning_rate 0.000117192
2017-10-10T11:43:53.514689: step 1384, loss 0.09684, acc 0.96875, learning_rate 0.000117122
2017-10-10T11:43:53.643951: step 1385, loss 0.0578582, acc 0.984375, learning_rate 0.000117052
2017-10-10T11:43:53.774304: step 1386, loss 0.140025, acc 0.9375, learning_rate 0.000116983
2017-10-10T11:43:53.902653: step 1387, loss 0.1486, acc 0.96875, learning_rate 0.000116913
2017-10-10T11:43:54.025528: step 1388, loss 0.0942516, acc 0.96875, learning_rate 0.000116844
2017-10-10T11:43:54.153295: step 1389, loss 0.0868646, acc 0.96875, learning_rate 0.000116775
2017-10-10T11:43:54.283341: step 1390, loss 0.174317, acc 0.9375, learning_rate 0.000116707
2017-10-10T11:43:54.464840: step 1391, loss 0.131242, acc 0.96875, learning_rate 0.000116639
2017-10-10T11:43:54.640847: step 1392, loss 0.230525, acc 0.90625, learning_rate 0.000116571
2017-10-10T11:43:54.839574: step 1393, loss 0.123751, acc 0.953125, learning_rate 0.000116503
2017-10-10T11:43:55.025710: step 1394, loss 0.328145, acc 0.921875, learning_rate 0.000116436
2017-10-10T11:43:55.207931: step 1395, loss 0.140438, acc 0.96875, learning_rate 0.000116369
2017-10-10T11:43:55.419471: step 1396, loss 0.123025, acc 0.953125, learning_rate 0.000116302
2017-10-10T11:43:55.614638: step 1397, loss 0.130416, acc 0.96875, learning_rate 0.000116235
2017-10-10T11:43:55.795856: step 1398, loss 0.174612, acc 0.890625, learning_rate 0.000116169
2017-10-10T11:43:56.001686: step 1399, loss 0.13004, acc 0.953125, learning_rate 0.000116103
2017-10-10T11:43:56.191341: step 1400, loss 0.158828, acc 0.9375, learning_rate 0.000116037

Evaluation:
2017-10-10T11:43:56.637200: step 1400, loss 0.218118, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1400

2017-10-10T11:43:57.798144: step 1401, loss 0.188596, acc 0.921875, learning_rate 0.000115972
2017-10-10T11:43:58.002659: step 1402, loss 0.0967571, acc 0.984375, learning_rate 0.000115907
2017-10-10T11:43:58.189348: step 1403, loss 0.101676, acc 0.96875, learning_rate 0.000115842
2017-10-10T11:43:58.390438: step 1404, loss 0.0875457, acc 0.96875, learning_rate 0.000115777
2017-10-10T11:43:58.567663: step 1405, loss 0.082244, acc 0.984375, learning_rate 0.000115713
2017-10-10T11:43:58.749945: step 1406, loss 0.0978874, acc 0.953125, learning_rate 0.000115649
2017-10-10T11:43:58.925845: step 1407, loss 0.104785, acc 0.984375, learning_rate 0.000115585
2017-10-10T11:43:59.123531: step 1408, loss 0.110967, acc 0.953125, learning_rate 0.000115521
2017-10-10T11:43:59.300942: step 1409, loss 0.169728, acc 0.9375, learning_rate 0.000115458
2017-10-10T11:43:59.471963: step 1410, loss 0.0891858, acc 0.984375, learning_rate 0.000115395
2017-10-10T11:43:59.643115: step 1411, loss 0.151352, acc 0.953125, learning_rate 0.000115332
2017-10-10T11:43:59.831096: step 1412, loss 0.195632, acc 0.9375, learning_rate 0.000115269
2017-10-10T11:44:00.026821: step 1413, loss 0.038364, acc 1, learning_rate 0.000115207
2017-10-10T11:44:00.224557: step 1414, loss 0.0810184, acc 0.984375, learning_rate 0.000115145
2017-10-10T11:44:00.424731: step 1415, loss 0.0620085, acc 1, learning_rate 0.000115083
2017-10-10T11:44:00.635597: step 1416, loss 0.101234, acc 0.96875, learning_rate 0.000115022
2017-10-10T11:44:00.824781: step 1417, loss 0.111847, acc 0.984375, learning_rate 0.00011496
2017-10-10T11:44:01.001483: step 1418, loss 0.114675, acc 0.96875, learning_rate 0.000114899
2017-10-10T11:44:01.184882: step 1419, loss 0.14408, acc 0.953125, learning_rate 0.000114838
2017-10-10T11:44:01.377639: step 1420, loss 0.0519032, acc 0.984375, learning_rate 0.000114778
2017-10-10T11:44:01.673384: step 1421, loss 0.17512, acc 0.921875, learning_rate 0.000114717
2017-10-10T11:44:01.835634: step 1422, loss 0.087564, acc 0.984375, learning_rate 0.000114657
2017-10-10T11:44:01.969734: step 1423, loss 0.0224118, acc 1, learning_rate 0.000114598
2017-10-10T11:44:02.106878: step 1424, loss 0.0926986, acc 0.984375, learning_rate 0.000114538
2017-10-10T11:44:02.243362: step 1425, loss 0.265777, acc 0.90625, learning_rate 0.000114479
2017-10-10T11:44:02.377415: step 1426, loss 0.0983572, acc 0.9375, learning_rate 0.00011442
2017-10-10T11:44:02.512341: step 1427, loss 0.157812, acc 0.9375, learning_rate 0.000114361
2017-10-10T11:44:02.650650: step 1428, loss 0.0655665, acc 0.96875, learning_rate 0.000114302
2017-10-10T11:44:02.839254: step 1429, loss 0.134218, acc 0.9375, learning_rate 0.000114244
2017-10-10T11:44:03.022836: step 1430, loss 0.0945119, acc 0.953125, learning_rate 0.000114186
2017-10-10T11:44:03.203385: step 1431, loss 0.226053, acc 0.90625, learning_rate 0.000114128
2017-10-10T11:44:03.384997: step 1432, loss 0.0686579, acc 0.96875, learning_rate 0.00011407
2017-10-10T11:44:03.567840: step 1433, loss 0.10082, acc 0.96875, learning_rate 0.000114013
2017-10-10T11:44:03.751751: step 1434, loss 0.213017, acc 0.9375, learning_rate 0.000113955
2017-10-10T11:44:03.935814: step 1435, loss 0.188998, acc 0.953125, learning_rate 0.000113898
2017-10-10T11:44:04.125966: step 1436, loss 0.0687966, acc 1, learning_rate 0.000113842
2017-10-10T11:44:04.313954: step 1437, loss 0.0938789, acc 0.984375, learning_rate 0.000113785
2017-10-10T11:44:04.502148: step 1438, loss 0.116756, acc 0.953125, learning_rate 0.000113729
2017-10-10T11:44:04.687212: step 1439, loss 0.220075, acc 0.921875, learning_rate 0.000113673
2017-10-10T11:44:04.885714: step 1440, loss 0.126866, acc 0.953125, learning_rate 0.000113617

Evaluation:
2017-10-10T11:44:05.341621: step 1440, loss 0.216761, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1440

2017-10-10T11:44:06.238110: step 1441, loss 0.229143, acc 0.9375, learning_rate 0.000113561
2017-10-10T11:44:06.420327: step 1442, loss 0.0940209, acc 0.96875, learning_rate 0.000113506
2017-10-10T11:44:06.630514: step 1443, loss 0.155938, acc 0.90625, learning_rate 0.000113451
2017-10-10T11:44:06.828847: step 1444, loss 0.219617, acc 0.90625, learning_rate 0.000113396
2017-10-10T11:44:07.014232: step 1445, loss 0.149113, acc 0.96875, learning_rate 0.000113341
2017-10-10T11:44:07.207171: step 1446, loss 0.133452, acc 0.953125, learning_rate 0.000113287
2017-10-10T11:44:07.405582: step 1447, loss 0.136288, acc 0.9375, learning_rate 0.000113233
2017-10-10T11:44:07.589450: step 1448, loss 0.131957, acc 0.953125, learning_rate 0.000113179
2017-10-10T11:44:07.772391: step 1449, loss 0.0940979, acc 0.984375, learning_rate 0.000113125
2017-10-10T11:44:07.963888: step 1450, loss 0.206231, acc 0.921875, learning_rate 0.000113071
2017-10-10T11:44:08.149669: step 1451, loss 0.0950471, acc 0.96875, learning_rate 0.000113018
2017-10-10T11:44:08.341508: step 1452, loss 0.206012, acc 0.9375, learning_rate 0.000112965
2017-10-10T11:44:08.529748: step 1453, loss 0.114717, acc 0.96875, learning_rate 0.000112912
2017-10-10T11:44:08.716687: step 1454, loss 0.200955, acc 0.953125, learning_rate 0.000112859
2017-10-10T11:44:08.905664: step 1455, loss 0.0835282, acc 0.96875, learning_rate 0.000112807
2017-10-10T11:44:09.086789: step 1456, loss 0.0605975, acc 1, learning_rate 0.000112754
2017-10-10T11:44:09.269242: step 1457, loss 0.148054, acc 0.953125, learning_rate 0.000112702
2017-10-10T11:44:09.436828: step 1458, loss 0.244821, acc 0.921875, learning_rate 0.000112651
2017-10-10T11:44:09.618027: step 1459, loss 0.178532, acc 0.921875, learning_rate 0.000112599
2017-10-10T11:44:09.885020: step 1460, loss 0.158537, acc 0.953125, learning_rate 0.000112547
2017-10-10T11:44:10.073572: step 1461, loss 0.14214, acc 0.953125, learning_rate 0.000112496
2017-10-10T11:44:10.212208: step 1462, loss 0.0752404, acc 0.984375, learning_rate 0.000112445
2017-10-10T11:44:10.347614: step 1463, loss 0.181225, acc 0.953125, learning_rate 0.000112394
2017-10-10T11:44:10.486123: step 1464, loss 0.245246, acc 0.9375, learning_rate 0.000112344
2017-10-10T11:44:10.626413: step 1465, loss 0.17748, acc 0.9375, learning_rate 0.000112293
2017-10-10T11:44:10.749714: step 1466, loss 0.166696, acc 0.9375, learning_rate 0.000112243
2017-10-10T11:44:10.941869: step 1467, loss 0.145249, acc 0.953125, learning_rate 0.000112193
2017-10-10T11:44:11.115435: step 1468, loss 0.0769389, acc 0.984375, learning_rate 0.000112144
2017-10-10T11:44:11.335658: step 1469, loss 0.0802968, acc 0.984375, learning_rate 0.000112094
2017-10-10T11:44:11.492800: step 1470, loss 0.114314, acc 0.941176, learning_rate 0.000112045
2017-10-10T11:44:11.686896: step 1471, loss 0.148575, acc 0.953125, learning_rate 0.000111995
2017-10-10T11:44:11.879410: step 1472, loss 0.189251, acc 0.9375, learning_rate 0.000111946
2017-10-10T11:44:12.068972: step 1473, loss 0.0957553, acc 0.96875, learning_rate 0.000111898
2017-10-10T11:44:12.248243: step 1474, loss 0.0915917, acc 0.953125, learning_rate 0.000111849
2017-10-10T11:44:12.423188: step 1475, loss 0.143063, acc 0.921875, learning_rate 0.000111801
2017-10-10T11:44:12.610064: step 1476, loss 0.123566, acc 0.96875, learning_rate 0.000111753
2017-10-10T11:44:12.789588: step 1477, loss 0.179957, acc 0.9375, learning_rate 0.000111705
2017-10-10T11:44:12.960697: step 1478, loss 0.206823, acc 0.921875, learning_rate 0.000111657
2017-10-10T11:44:13.149718: step 1479, loss 0.130874, acc 0.953125, learning_rate 0.000111609
2017-10-10T11:44:13.340358: step 1480, loss 0.108941, acc 0.96875, learning_rate 0.000111562

Evaluation:
2017-10-10T11:44:13.802981: step 1480, loss 0.215619, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1480

2017-10-10T11:44:14.810791: step 1481, loss 0.10862, acc 0.96875, learning_rate 0.000111515
2017-10-10T11:44:15.014539: step 1482, loss 0.12972, acc 0.96875, learning_rate 0.000111468
2017-10-10T11:44:15.204430: step 1483, loss 0.217727, acc 0.921875, learning_rate 0.000111421
2017-10-10T11:44:15.393076: step 1484, loss 0.0680969, acc 0.96875, learning_rate 0.000111374
2017-10-10T11:44:15.608201: step 1485, loss 0.212988, acc 0.921875, learning_rate 0.000111328
2017-10-10T11:44:15.817697: step 1486, loss 0.181789, acc 0.953125, learning_rate 0.000111282
2017-10-10T11:44:16.014769: step 1487, loss 0.12931, acc 0.9375, learning_rate 0.000111236
2017-10-10T11:44:16.212003: step 1488, loss 0.0901937, acc 0.953125, learning_rate 0.00011119
2017-10-10T11:44:16.399134: step 1489, loss 0.152476, acc 0.953125, learning_rate 0.000111144
2017-10-10T11:44:16.562477: step 1490, loss 0.185005, acc 0.921875, learning_rate 0.000111099
2017-10-10T11:44:16.759934: step 1491, loss 0.0773704, acc 0.96875, learning_rate 0.000111053
2017-10-10T11:44:16.931045: step 1492, loss 0.105047, acc 0.9375, learning_rate 0.000111008
2017-10-10T11:44:17.099036: step 1493, loss 0.128692, acc 0.96875, learning_rate 0.000110963
2017-10-10T11:44:17.269016: step 1494, loss 0.220177, acc 0.953125, learning_rate 0.000110918
2017-10-10T11:44:17.456857: step 1495, loss 0.202701, acc 0.953125, learning_rate 0.000110874
2017-10-10T11:44:17.645015: step 1496, loss 0.0811608, acc 0.984375, learning_rate 0.00011083
2017-10-10T11:44:17.821319: step 1497, loss 0.140826, acc 0.984375, learning_rate 0.000110785
2017-10-10T11:44:18.081615: step 1498, loss 0.204506, acc 0.90625, learning_rate 0.000110741
2017-10-10T11:44:18.272179: step 1499, loss 0.0628995, acc 1, learning_rate 0.000110697
2017-10-10T11:44:18.402879: step 1500, loss 0.228524, acc 0.90625, learning_rate 0.000110654
2017-10-10T11:44:18.527123: step 1501, loss 0.175012, acc 0.921875, learning_rate 0.00011061
2017-10-10T11:44:18.655929: step 1502, loss 0.10003, acc 0.96875, learning_rate 0.000110567
2017-10-10T11:44:18.776209: step 1503, loss 0.105804, acc 0.984375, learning_rate 0.000110524
2017-10-10T11:44:18.899862: step 1504, loss 0.101432, acc 0.96875, learning_rate 0.000110481
2017-10-10T11:44:19.036829: step 1505, loss 0.116069, acc 0.96875, learning_rate 0.000110438
2017-10-10T11:44:19.219515: step 1506, loss 0.114973, acc 0.953125, learning_rate 0.000110396
2017-10-10T11:44:19.382987: step 1507, loss 0.100729, acc 0.984375, learning_rate 0.000110353
2017-10-10T11:44:19.570302: step 1508, loss 0.0289837, acc 1, learning_rate 0.000110311
2017-10-10T11:44:19.769727: step 1509, loss 0.135242, acc 0.96875, learning_rate 0.000110269
2017-10-10T11:44:19.969255: step 1510, loss 0.126803, acc 0.953125, learning_rate 0.000110227
2017-10-10T11:44:20.162720: step 1511, loss 0.120561, acc 0.953125, learning_rate 0.000110185
2017-10-10T11:44:20.346720: step 1512, loss 0.082378, acc 0.96875, learning_rate 0.000110144
2017-10-10T11:44:20.535661: step 1513, loss 0.0477863, acc 1, learning_rate 0.000110102
2017-10-10T11:44:20.729949: step 1514, loss 0.142343, acc 0.96875, learning_rate 0.000110061
2017-10-10T11:44:20.939745: step 1515, loss 0.178307, acc 0.921875, learning_rate 0.00011002
2017-10-10T11:44:21.133454: step 1516, loss 0.199629, acc 0.9375, learning_rate 0.000109979
2017-10-10T11:44:21.328737: step 1517, loss 0.229265, acc 0.90625, learning_rate 0.000109938
2017-10-10T11:44:21.525377: step 1518, loss 0.136985, acc 0.9375, learning_rate 0.000109898
2017-10-10T11:44:21.729815: step 1519, loss 0.101874, acc 0.96875, learning_rate 0.000109857
2017-10-10T11:44:21.916866: step 1520, loss 0.162017, acc 0.921875, learning_rate 0.000109817

Evaluation:
2017-10-10T11:44:22.350522: step 1520, loss 0.217869, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1520

2017-10-10T11:44:23.474744: step 1521, loss 0.16377, acc 0.9375, learning_rate 0.000109777
2017-10-10T11:44:23.669931: step 1522, loss 0.0300128, acc 1, learning_rate 0.000109737
2017-10-10T11:44:23.877210: step 1523, loss 0.089041, acc 0.96875, learning_rate 0.000109697
2017-10-10T11:44:24.069002: step 1524, loss 0.119415, acc 0.953125, learning_rate 0.000109658
2017-10-10T11:44:24.250846: step 1525, loss 0.1235, acc 0.953125, learning_rate 0.000109618
2017-10-10T11:44:24.420895: step 1526, loss 0.132559, acc 0.953125, learning_rate 0.000109579
2017-10-10T11:44:24.592322: step 1527, loss 0.183059, acc 0.96875, learning_rate 0.00010954
2017-10-10T11:44:24.783507: step 1528, loss 0.131703, acc 0.96875, learning_rate 0.000109501
2017-10-10T11:44:24.975648: step 1529, loss 0.160377, acc 0.9375, learning_rate 0.000109462
2017-10-10T11:44:25.153009: step 1530, loss 0.289276, acc 0.921875, learning_rate 0.000109424
2017-10-10T11:44:25.335481: step 1531, loss 0.116255, acc 0.96875, learning_rate 0.000109385
2017-10-10T11:44:25.519317: step 1532, loss 0.0587059, acc 1, learning_rate 0.000109347
2017-10-10T11:44:25.706676: step 1533, loss 0.109217, acc 0.96875, learning_rate 0.000109309
2017-10-10T11:44:25.944879: step 1534, loss 0.18656, acc 0.96875, learning_rate 0.000109271
2017-10-10T11:44:26.178968: step 1535, loss 0.113856, acc 0.96875, learning_rate 0.000109233
2017-10-10T11:44:26.314946: step 1536, loss 0.185126, acc 0.9375, learning_rate 0.000109195
2017-10-10T11:44:26.448826: step 1537, loss 0.168833, acc 0.90625, learning_rate 0.000109158
2017-10-10T11:44:26.575619: step 1538, loss 0.123241, acc 0.9375, learning_rate 0.00010912
2017-10-10T11:44:26.707260: step 1539, loss 0.150444, acc 0.953125, learning_rate 0.000109083
2017-10-10T11:44:26.842860: step 1540, loss 0.0858425, acc 0.96875, learning_rate 0.000109046
2017-10-10T11:44:27.035958: step 1541, loss 0.290276, acc 0.90625, learning_rate 0.000109009
2017-10-10T11:44:27.239082: step 1542, loss 0.170101, acc 0.9375, learning_rate 0.000108972
2017-10-10T11:44:27.425279: step 1543, loss 0.157214, acc 0.921875, learning_rate 0.000108936
2017-10-10T11:44:27.606314: step 1544, loss 0.0764426, acc 0.984375, learning_rate 0.000108899
2017-10-10T11:44:27.779726: step 1545, loss 0.138579, acc 0.984375, learning_rate 0.000108863
2017-10-10T11:44:27.964259: step 1546, loss 0.195169, acc 0.921875, learning_rate 0.000108827
2017-10-10T11:44:28.138356: step 1547, loss 0.138222, acc 0.921875, learning_rate 0.000108791
2017-10-10T11:44:28.316049: step 1548, loss 0.0965125, acc 0.96875, learning_rate 0.000108755
2017-10-10T11:44:28.500652: step 1549, loss 0.0793146, acc 0.984375, learning_rate 0.000108719
2017-10-10T11:44:28.692452: step 1550, loss 0.0582042, acc 1, learning_rate 0.000108683
2017-10-10T11:44:28.901057: step 1551, loss 0.173111, acc 0.90625, learning_rate 0.000108648
2017-10-10T11:44:29.073014: step 1552, loss 0.10653, acc 0.96875, learning_rate 0.000108613
2017-10-10T11:44:29.268768: step 1553, loss 0.125452, acc 0.953125, learning_rate 0.000108577
2017-10-10T11:44:29.464153: step 1554, loss 0.255515, acc 0.90625, learning_rate 0.000108542
2017-10-10T11:44:29.646267: step 1555, loss 0.161011, acc 0.9375, learning_rate 0.000108508
2017-10-10T11:44:29.853262: step 1556, loss 0.0866199, acc 0.953125, learning_rate 0.000108473
2017-10-10T11:44:30.046012: step 1557, loss 0.181084, acc 0.96875, learning_rate 0.000108438
2017-10-10T11:44:30.245447: step 1558, loss 0.0805643, acc 0.984375, learning_rate 0.000108404
2017-10-10T11:44:30.440910: step 1559, loss 0.107177, acc 0.953125, learning_rate 0.00010837
2017-10-10T11:44:30.608724: step 1560, loss 0.0675937, acc 0.984375, learning_rate 0.000108335

Evaluation:
2017-10-10T11:44:31.067862: step 1560, loss 0.219633, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1560

2017-10-10T11:44:31.968913: step 1561, loss 0.0503191, acc 1, learning_rate 0.000108301
2017-10-10T11:44:32.161110: step 1562, loss 0.162651, acc 0.921875, learning_rate 0.000108267
2017-10-10T11:44:32.370670: step 1563, loss 0.0940264, acc 0.984375, learning_rate 0.000108234
2017-10-10T11:44:32.556442: step 1564, loss 0.186381, acc 0.96875, learning_rate 0.0001082
2017-10-10T11:44:32.773579: step 1565, loss 0.104766, acc 0.953125, learning_rate 0.000108167
2017-10-10T11:44:32.984090: step 1566, loss 0.214546, acc 0.921875, learning_rate 0.000108133
2017-10-10T11:44:33.200886: step 1567, loss 0.237103, acc 0.859375, learning_rate 0.0001081
2017-10-10T11:44:33.364857: step 1568, loss 0.188175, acc 0.901961, learning_rate 0.000108067
2017-10-10T11:44:33.573548: step 1569, loss 0.155249, acc 0.953125, learning_rate 0.000108034
2017-10-10T11:44:33.823035: step 1570, loss 0.160308, acc 0.921875, learning_rate 0.000108001
2017-10-10T11:44:34.024943: step 1571, loss 0.192835, acc 0.921875, learning_rate 0.000107969
2017-10-10T11:44:34.152495: step 1572, loss 0.150859, acc 0.953125, learning_rate 0.000107936
2017-10-10T11:44:34.277595: step 1573, loss 0.180419, acc 0.9375, learning_rate 0.000107904
2017-10-10T11:44:34.405945: step 1574, loss 0.084506, acc 0.984375, learning_rate 0.000107871
2017-10-10T11:44:34.532443: step 1575, loss 0.164458, acc 0.9375, learning_rate 0.000107839
2017-10-10T11:44:34.656357: step 1576, loss 0.167774, acc 0.9375, learning_rate 0.000107807
2017-10-10T11:44:34.804875: step 1577, loss 0.169199, acc 0.9375, learning_rate 0.000107775
2017-10-10T11:44:34.981955: step 1578, loss 0.0952385, acc 0.984375, learning_rate 0.000107744
2017-10-10T11:44:35.153035: step 1579, loss 0.089345, acc 0.984375, learning_rate 0.000107712
2017-10-10T11:44:35.329173: step 1580, loss 0.170215, acc 0.9375, learning_rate 0.000107681
2017-10-10T11:44:35.520836: step 1581, loss 0.0978386, acc 0.984375, learning_rate 0.000107649
2017-10-10T11:44:35.683919: step 1582, loss 0.164837, acc 0.90625, learning_rate 0.000107618
2017-10-10T11:44:35.865127: step 1583, loss 0.206754, acc 0.921875, learning_rate 0.000107587
2017-10-10T11:44:36.011003: step 1584, loss 0.0392104, acc 1, learning_rate 0.000107556
2017-10-10T11:44:36.195364: step 1585, loss 0.138372, acc 0.953125, learning_rate 0.000107525
2017-10-10T11:44:36.369203: step 1586, loss 0.102129, acc 0.9375, learning_rate 0.000107494
2017-10-10T11:44:36.568229: step 1587, loss 0.120206, acc 0.9375, learning_rate 0.000107464
2017-10-10T11:44:36.752867: step 1588, loss 0.0732242, acc 0.96875, learning_rate 0.000107433
2017-10-10T11:44:36.946092: step 1589, loss 0.239749, acc 0.90625, learning_rate 0.000107403
2017-10-10T11:44:37.126920: step 1590, loss 0.0422687, acc 1, learning_rate 0.000107373
2017-10-10T11:44:37.335434: step 1591, loss 0.196622, acc 0.90625, learning_rate 0.000107343
2017-10-10T11:44:37.514541: step 1592, loss 0.18682, acc 0.96875, learning_rate 0.000107313
2017-10-10T11:44:37.718078: step 1593, loss 0.133157, acc 0.953125, learning_rate 0.000107283
2017-10-10T11:44:37.904851: step 1594, loss 0.134146, acc 0.953125, learning_rate 0.000107253
2017-10-10T11:44:38.084346: step 1595, loss 0.122345, acc 0.96875, learning_rate 0.000107224
2017-10-10T11:44:38.270106: step 1596, loss 0.155918, acc 0.9375, learning_rate 0.000107194
2017-10-10T11:44:38.456849: step 1597, loss 0.14195, acc 0.9375, learning_rate 0.000107165
2017-10-10T11:44:38.641819: step 1598, loss 0.103504, acc 0.921875, learning_rate 0.000107136
2017-10-10T11:44:38.812450: step 1599, loss 0.197396, acc 0.921875, learning_rate 0.000107106
2017-10-10T11:44:39.005853: step 1600, loss 0.149374, acc 0.9375, learning_rate 0.000107077

Evaluation:
2017-10-10T11:44:39.473340: step 1600, loss 0.216058, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1600

2017-10-10T11:44:40.524147: step 1601, loss 0.185435, acc 0.953125, learning_rate 0.000107048
2017-10-10T11:44:40.711122: step 1602, loss 0.0884683, acc 0.96875, learning_rate 0.00010702
2017-10-10T11:44:40.888899: step 1603, loss 0.0976025, acc 0.96875, learning_rate 0.000106991
2017-10-10T11:44:41.075111: step 1604, loss 0.275259, acc 0.90625, learning_rate 0.000106963
2017-10-10T11:44:41.250912: step 1605, loss 0.100009, acc 0.953125, learning_rate 0.000106934
2017-10-10T11:44:41.434715: step 1606, loss 0.191098, acc 0.921875, learning_rate 0.000106906
2017-10-10T11:44:41.688970: step 1607, loss 0.275718, acc 0.9375, learning_rate 0.000106878
2017-10-10T11:44:41.904332: step 1608, loss 0.147416, acc 0.953125, learning_rate 0.00010685
2017-10-10T11:44:42.030662: step 1609, loss 0.0638012, acc 0.984375, learning_rate 0.000106822
2017-10-10T11:44:42.160748: step 1610, loss 0.0755137, acc 0.984375, learning_rate 0.000106794
2017-10-10T11:44:42.300243: step 1611, loss 0.103657, acc 0.96875, learning_rate 0.000106766
2017-10-10T11:44:42.433085: step 1612, loss 0.15678, acc 0.953125, learning_rate 0.000106738
2017-10-10T11:44:42.567845: step 1613, loss 0.0990242, acc 0.984375, learning_rate 0.000106711
2017-10-10T11:44:42.695810: step 1614, loss 0.0600831, acc 0.984375, learning_rate 0.000106684
2017-10-10T11:44:42.879913: step 1615, loss 0.110831, acc 0.953125, learning_rate 0.000106656
2017-10-10T11:44:43.064864: step 1616, loss 0.166665, acc 0.9375, learning_rate 0.000106629
2017-10-10T11:44:43.248974: step 1617, loss 0.0805192, acc 0.984375, learning_rate 0.000106602
2017-10-10T11:44:43.434413: step 1618, loss 0.159618, acc 0.953125, learning_rate 0.000106575
2017-10-10T11:44:43.629921: step 1619, loss 0.0959025, acc 0.984375, learning_rate 0.000106548
2017-10-10T11:44:43.821186: step 1620, loss 0.166446, acc 0.9375, learning_rate 0.000106521
2017-10-10T11:44:43.998844: step 1621, loss 0.0802962, acc 0.984375, learning_rate 0.000106495
2017-10-10T11:44:44.173119: step 1622, loss 0.233427, acc 0.9375, learning_rate 0.000106468
2017-10-10T11:44:44.369035: step 1623, loss 0.0796152, acc 0.96875, learning_rate 0.000106442
2017-10-10T11:44:44.569402: step 1624, loss 0.112811, acc 0.96875, learning_rate 0.000106416
2017-10-10T11:44:44.762304: step 1625, loss 0.138973, acc 0.953125, learning_rate 0.000106389
2017-10-10T11:44:44.947975: step 1626, loss 0.0887937, acc 0.984375, learning_rate 0.000106363
2017-10-10T11:44:45.132438: step 1627, loss 0.193446, acc 0.921875, learning_rate 0.000106337
2017-10-10T11:44:45.331024: step 1628, loss 0.115617, acc 0.953125, learning_rate 0.000106312
2017-10-10T11:44:45.535789: step 1629, loss 0.166707, acc 0.953125, learning_rate 0.000106286
2017-10-10T11:44:45.731916: step 1630, loss 0.0974986, acc 0.96875, learning_rate 0.00010626
2017-10-10T11:44:45.918109: step 1631, loss 0.269436, acc 0.9375, learning_rate 0.000106235
2017-10-10T11:44:46.103135: step 1632, loss 0.161707, acc 0.921875, learning_rate 0.000106209
2017-10-10T11:44:46.299804: step 1633, loss 0.237133, acc 0.9375, learning_rate 0.000106184
2017-10-10T11:44:46.477195: step 1634, loss 0.149185, acc 0.953125, learning_rate 0.000106159
2017-10-10T11:44:46.673156: step 1635, loss 0.0613992, acc 0.984375, learning_rate 0.000106133
2017-10-10T11:44:46.880553: step 1636, loss 0.0979953, acc 0.984375, learning_rate 0.000106108
2017-10-10T11:44:47.072068: step 1637, loss 0.131011, acc 0.953125, learning_rate 0.000106083
2017-10-10T11:44:47.250545: step 1638, loss 0.207749, acc 0.953125, learning_rate 0.000106059
2017-10-10T11:44:47.440383: step 1639, loss 0.134973, acc 0.9375, learning_rate 0.000106034
2017-10-10T11:44:47.635035: step 1640, loss 0.0976416, acc 0.953125, learning_rate 0.000106009

Evaluation:
2017-10-10T11:44:48.080084: step 1640, loss 0.21354, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1640

2017-10-10T11:44:49.223274: step 1641, loss 0.127184, acc 0.96875, learning_rate 0.000105985
2017-10-10T11:44:49.423591: step 1642, loss 0.186167, acc 0.953125, learning_rate 0.00010596
2017-10-10T11:44:49.676934: step 1643, loss 0.203312, acc 0.9375, learning_rate 0.000105936
2017-10-10T11:44:49.870373: step 1644, loss 0.13936, acc 0.921875, learning_rate 0.000105912
2017-10-10T11:44:49.991903: step 1645, loss 0.0895591, acc 0.96875, learning_rate 0.000105888
2017-10-10T11:44:50.119640: step 1646, loss 0.211676, acc 0.9375, learning_rate 0.000105864
2017-10-10T11:44:50.242817: step 1647, loss 0.0435632, acc 0.984375, learning_rate 0.00010584
2017-10-10T11:44:50.368025: step 1648, loss 0.151449, acc 0.9375, learning_rate 0.000105816
2017-10-10T11:44:50.494329: step 1649, loss 0.13951, acc 0.96875, learning_rate 0.000105792
2017-10-10T11:44:50.696351: step 1650, loss 0.144102, acc 0.953125, learning_rate 0.000105768
2017-10-10T11:44:50.891541: step 1651, loss 0.0884948, acc 0.953125, learning_rate 0.000105745
2017-10-10T11:44:51.086951: step 1652, loss 0.166945, acc 0.90625, learning_rate 0.000105721
2017-10-10T11:44:51.308863: step 1653, loss 0.114043, acc 0.96875, learning_rate 0.000105698
2017-10-10T11:44:51.513022: step 1654, loss 0.0840613, acc 1, learning_rate 0.000105675
2017-10-10T11:44:51.706290: step 1655, loss 0.0831586, acc 1, learning_rate 0.000105652
2017-10-10T11:44:51.936899: step 1656, loss 0.116735, acc 0.953125, learning_rate 0.000105629
2017-10-10T11:44:52.118480: step 1657, loss 0.0900695, acc 0.984375, learning_rate 0.000105606
2017-10-10T11:44:52.305928: step 1658, loss 0.190273, acc 0.921875, learning_rate 0.000105583
2017-10-10T11:44:52.487026: step 1659, loss 0.195064, acc 0.953125, learning_rate 0.00010556
2017-10-10T11:44:52.688218: step 1660, loss 0.132498, acc 0.96875, learning_rate 0.000105537
2017-10-10T11:44:52.909628: step 1661, loss 0.121917, acc 0.96875, learning_rate 0.000105515
2017-10-10T11:44:53.124899: step 1662, loss 0.0394015, acc 1, learning_rate 0.000105492
2017-10-10T11:44:53.329589: step 1663, loss 0.121206, acc 0.9375, learning_rate 0.00010547
2017-10-10T11:44:53.531153: step 1664, loss 0.181991, acc 0.9375, learning_rate 0.000105447
2017-10-10T11:44:53.734788: step 1665, loss 0.0330869, acc 1, learning_rate 0.000105425
2017-10-10T11:44:53.955421: step 1666, loss 0.115085, acc 0.960784, learning_rate 0.000105403
2017-10-10T11:44:54.166561: step 1667, loss 0.0481866, acc 1, learning_rate 0.000105381
2017-10-10T11:44:54.383405: step 1668, loss 0.25311, acc 0.921875, learning_rate 0.000105359
2017-10-10T11:44:54.599516: step 1669, loss 0.150963, acc 0.9375, learning_rate 0.000105337
2017-10-10T11:44:54.808486: step 1670, loss 0.083069, acc 0.96875, learning_rate 0.000105315
2017-10-10T11:44:55.000849: step 1671, loss 0.0890209, acc 0.96875, learning_rate 0.000105294
2017-10-10T11:44:55.252831: step 1672, loss 0.188627, acc 0.921875, learning_rate 0.000105272
2017-10-10T11:44:55.461824: step 1673, loss 0.0863413, acc 0.96875, learning_rate 0.000105251
2017-10-10T11:44:55.659514: step 1674, loss 0.101853, acc 0.96875, learning_rate 0.000105229
2017-10-10T11:44:55.857935: step 1675, loss 0.0482965, acc 0.984375, learning_rate 0.000105208
2017-10-10T11:44:56.072272: step 1676, loss 0.0730561, acc 0.984375, learning_rate 0.000105186
2017-10-10T11:44:56.275049: step 1677, loss 0.144192, acc 0.921875, learning_rate 0.000105165
2017-10-10T11:44:56.468401: step 1678, loss 0.0700374, acc 0.984375, learning_rate 0.000105144
2017-10-10T11:44:56.676635: step 1679, loss 0.0954938, acc 0.984375, learning_rate 0.000105123
2017-10-10T11:44:56.884063: step 1680, loss 0.163907, acc 0.953125, learning_rate 0.000105102

Evaluation:
2017-10-10T11:44:57.667089: step 1680, loss 0.217104, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1680

2017-10-10T11:44:58.898631: step 1681, loss 0.194, acc 0.953125, learning_rate 0.000105081
2017-10-10T11:44:59.109954: step 1682, loss 0.0759884, acc 0.984375, learning_rate 0.000105061
2017-10-10T11:44:59.323306: step 1683, loss 0.054123, acc 1, learning_rate 0.00010504
2017-10-10T11:44:59.523046: step 1684, loss 0.307912, acc 0.90625, learning_rate 0.00010502
2017-10-10T11:44:59.740881: step 1685, loss 0.162637, acc 0.9375, learning_rate 0.000104999
2017-10-10T11:44:59.949043: step 1686, loss 0.0621766, acc 0.984375, learning_rate 0.000104979
2017-10-10T11:45:00.144859: step 1687, loss 0.0821095, acc 1, learning_rate 0.000104958
2017-10-10T11:45:00.364971: step 1688, loss 0.245609, acc 0.921875, learning_rate 0.000104938
2017-10-10T11:45:00.541051: step 1689, loss 0.189612, acc 0.953125, learning_rate 0.000104918
2017-10-10T11:45:00.732838: step 1690, loss 0.0792678, acc 1, learning_rate 0.000104898
2017-10-10T11:45:00.936067: step 1691, loss 0.0652322, acc 0.984375, learning_rate 0.000104878
2017-10-10T11:45:01.110562: step 1692, loss 0.119543, acc 0.953125, learning_rate 0.000104858
2017-10-10T11:45:01.301588: step 1693, loss 0.214923, acc 0.9375, learning_rate 0.000104838
2017-10-10T11:45:01.500859: step 1694, loss 0.0882893, acc 0.96875, learning_rate 0.000104818
2017-10-10T11:45:01.687849: step 1695, loss 0.102667, acc 0.984375, learning_rate 0.000104799
2017-10-10T11:45:01.875222: step 1696, loss 0.209309, acc 0.921875, learning_rate 0.000104779
2017-10-10T11:45:02.100930: step 1697, loss 0.101188, acc 0.953125, learning_rate 0.00010476
2017-10-10T11:45:02.305933: step 1698, loss 0.0636226, acc 0.953125, learning_rate 0.00010474
2017-10-10T11:45:02.494757: step 1699, loss 0.0467563, acc 0.984375, learning_rate 0.000104721
2017-10-10T11:45:02.672922: step 1700, loss 0.0796253, acc 0.96875, learning_rate 0.000104702
2017-10-10T11:45:02.865045: step 1701, loss 0.181981, acc 0.921875, learning_rate 0.000104682
2017-10-10T11:45:03.038184: step 1702, loss 0.13971, acc 0.984375, learning_rate 0.000104663
2017-10-10T11:45:03.217039: step 1703, loss 0.105873, acc 0.96875, learning_rate 0.000104644
2017-10-10T11:45:03.397934: step 1704, loss 0.105806, acc 0.96875, learning_rate 0.000104625
2017-10-10T11:45:03.576329: step 1705, loss 0.271414, acc 0.921875, learning_rate 0.000104606
2017-10-10T11:45:03.755217: step 1706, loss 0.111689, acc 0.96875, learning_rate 0.000104588
2017-10-10T11:45:03.944870: step 1707, loss 0.13451, acc 0.953125, learning_rate 0.000104569
2017-10-10T11:45:04.160840: step 1708, loss 0.143009, acc 0.9375, learning_rate 0.00010455
2017-10-10T11:45:04.326026: step 1709, loss 0.16609, acc 0.953125, learning_rate 0.000104532
2017-10-10T11:45:04.516867: step 1710, loss 0.222679, acc 0.890625, learning_rate 0.000104513
2017-10-10T11:45:04.707224: step 1711, loss 0.112214, acc 0.96875, learning_rate 0.000104495
2017-10-10T11:45:04.888870: step 1712, loss 0.137208, acc 0.953125, learning_rate 0.000104476
2017-10-10T11:45:05.078362: step 1713, loss 0.0521275, acc 0.984375, learning_rate 0.000104458
2017-10-10T11:45:05.253045: step 1714, loss 0.104955, acc 0.96875, learning_rate 0.00010444
2017-10-10T11:45:05.444959: step 1715, loss 0.150827, acc 0.9375, learning_rate 0.000104422
2017-10-10T11:45:05.632244: step 1716, loss 0.104451, acc 0.953125, learning_rate 0.000104404
2017-10-10T11:45:05.830797: step 1717, loss 0.0896376, acc 0.96875, learning_rate 0.000104386
2017-10-10T11:45:06.020978: step 1718, loss 0.232886, acc 0.90625, learning_rate 0.000104368
2017-10-10T11:45:06.280978: step 1719, loss 0.156568, acc 0.921875, learning_rate 0.00010435
2017-10-10T11:45:06.523886: step 1720, loss 0.0763845, acc 0.984375, learning_rate 0.000104332

Evaluation:
2017-10-10T11:45:06.875151: step 1720, loss 0.217875, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1720

2017-10-10T11:45:07.636693: step 1721, loss 0.247455, acc 0.90625, learning_rate 0.000104315
2017-10-10T11:45:07.827089: step 1722, loss 0.0862809, acc 0.96875, learning_rate 0.000104297
2017-10-10T11:45:08.016909: step 1723, loss 0.171021, acc 0.9375, learning_rate 0.000104279
2017-10-10T11:45:08.210690: step 1724, loss 0.142715, acc 0.921875, learning_rate 0.000104262
2017-10-10T11:45:08.383398: step 1725, loss 0.181141, acc 0.921875, learning_rate 0.000104245
2017-10-10T11:45:08.564867: step 1726, loss 0.045619, acc 0.984375, learning_rate 0.000104227
2017-10-10T11:45:08.742558: step 1727, loss 0.150484, acc 0.9375, learning_rate 0.00010421
2017-10-10T11:45:08.965201: step 1728, loss 0.0563619, acc 0.984375, learning_rate 0.000104193
2017-10-10T11:45:09.134614: step 1729, loss 0.184719, acc 0.9375, learning_rate 0.000104176
2017-10-10T11:45:09.316353: step 1730, loss 0.13192, acc 0.9375, learning_rate 0.000104159
2017-10-10T11:45:09.503847: step 1731, loss 0.0801951, acc 0.984375, learning_rate 0.000104142
2017-10-10T11:45:09.684824: step 1732, loss 0.181907, acc 0.9375, learning_rate 0.000104125
2017-10-10T11:45:09.890723: step 1733, loss 0.0417279, acc 1, learning_rate 0.000104108
2017-10-10T11:45:10.074114: step 1734, loss 0.226792, acc 0.921875, learning_rate 0.000104091
2017-10-10T11:45:10.268865: step 1735, loss 0.211059, acc 0.921875, learning_rate 0.000104074
2017-10-10T11:45:10.456849: step 1736, loss 0.114703, acc 0.953125, learning_rate 0.000104058
2017-10-10T11:45:10.648846: step 1737, loss 0.0773588, acc 0.96875, learning_rate 0.000104041
2017-10-10T11:45:10.861003: step 1738, loss 0.258695, acc 0.921875, learning_rate 0.000104025
2017-10-10T11:45:11.039875: step 1739, loss 0.153687, acc 0.953125, learning_rate 0.000104008
2017-10-10T11:45:11.243925: step 1740, loss 0.206742, acc 0.921875, learning_rate 0.000103992
2017-10-10T11:45:11.432860: step 1741, loss 0.0481185, acc 0.984375, learning_rate 0.000103976
2017-10-10T11:45:11.619369: step 1742, loss 0.0793897, acc 0.96875, learning_rate 0.000103959
2017-10-10T11:45:11.885111: step 1743, loss 0.130609, acc 0.96875, learning_rate 0.000103943
2017-10-10T11:45:12.087642: step 1744, loss 0.18662, acc 0.921875, learning_rate 0.000103927
2017-10-10T11:45:12.302949: step 1745, loss 0.154285, acc 0.953125, learning_rate 0.000103911
2017-10-10T11:45:12.524359: step 1746, loss 0.158188, acc 0.96875, learning_rate 0.000103895
2017-10-10T11:45:12.727069: step 1747, loss 0.164035, acc 0.953125, learning_rate 0.000103879
2017-10-10T11:45:12.939484: step 1748, loss 0.0659894, acc 0.984375, learning_rate 0.000103863
2017-10-10T11:45:13.156240: step 1749, loss 0.0510453, acc 0.984375, learning_rate 0.000103848
2017-10-10T11:45:13.387310: step 1750, loss 0.128694, acc 0.9375, learning_rate 0.000103832
2017-10-10T11:45:13.610559: step 1751, loss 0.138769, acc 0.9375, learning_rate 0.000103816
2017-10-10T11:45:13.828629: step 1752, loss 0.094387, acc 0.96875, learning_rate 0.000103801
2017-10-10T11:45:14.040056: step 1753, loss 0.120457, acc 0.953125, learning_rate 0.000103785
2017-10-10T11:45:14.257739: step 1754, loss 0.14725, acc 0.9375, learning_rate 0.00010377
2017-10-10T11:45:14.453101: step 1755, loss 0.0847483, acc 0.984375, learning_rate 0.000103754
2017-10-10T11:45:14.643630: step 1756, loss 0.167203, acc 0.953125, learning_rate 0.000103739
2017-10-10T11:45:14.969011: step 1757, loss 0.186668, acc 0.9375, learning_rate 0.000103724
2017-10-10T11:45:15.128225: step 1758, loss 0.0959671, acc 0.9375, learning_rate 0.000103709
2017-10-10T11:45:15.260974: step 1759, loss 0.0902955, acc 0.96875, learning_rate 0.000103694
2017-10-10T11:45:15.405516: step 1760, loss 0.118109, acc 0.953125, learning_rate 0.000103678

Evaluation:
2017-10-10T11:45:15.785598: step 1760, loss 0.215109, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1760

2017-10-10T11:45:16.888986: step 1761, loss 0.112293, acc 0.953125, learning_rate 0.000103663
2017-10-10T11:45:17.076890: step 1762, loss 0.0571587, acc 0.984375, learning_rate 0.000103648
2017-10-10T11:45:17.272832: step 1763, loss 0.241109, acc 0.953125, learning_rate 0.000103634
2017-10-10T11:45:17.471467: step 1764, loss 0.111055, acc 0.960784, learning_rate 0.000103619
2017-10-10T11:45:17.677778: step 1765, loss 0.111392, acc 0.9375, learning_rate 0.000103604
2017-10-10T11:45:17.897186: step 1766, loss 0.136905, acc 0.96875, learning_rate 0.000103589
2017-10-10T11:45:18.098073: step 1767, loss 0.153163, acc 0.9375, learning_rate 0.000103575
2017-10-10T11:45:18.302066: step 1768, loss 0.0386066, acc 1, learning_rate 0.00010356
2017-10-10T11:45:18.506082: step 1769, loss 0.0535449, acc 1, learning_rate 0.000103545
2017-10-10T11:45:18.706374: step 1770, loss 0.0896688, acc 0.984375, learning_rate 0.000103531
2017-10-10T11:45:18.919002: step 1771, loss 0.146426, acc 0.9375, learning_rate 0.000103517
2017-10-10T11:45:19.105788: step 1772, loss 0.119169, acc 0.9375, learning_rate 0.000103502
2017-10-10T11:45:19.296871: step 1773, loss 0.0529723, acc 0.984375, learning_rate 0.000103488
2017-10-10T11:45:19.493398: step 1774, loss 0.136955, acc 0.953125, learning_rate 0.000103474
2017-10-10T11:45:19.682365: step 1775, loss 0.0866901, acc 0.984375, learning_rate 0.00010346
2017-10-10T11:45:19.881218: step 1776, loss 0.0948379, acc 0.96875, learning_rate 0.000103445
2017-10-10T11:45:20.068566: step 1777, loss 0.0357702, acc 1, learning_rate 0.000103431
2017-10-10T11:45:20.275833: step 1778, loss 0.27562, acc 0.90625, learning_rate 0.000103417
2017-10-10T11:45:20.474511: step 1779, loss 0.117274, acc 0.96875, learning_rate 0.000103403
2017-10-10T11:45:20.692983: step 1780, loss 0.203173, acc 0.921875, learning_rate 0.00010339
2017-10-10T11:45:20.920807: step 1781, loss 0.124253, acc 0.96875, learning_rate 0.000103376
2017-10-10T11:45:21.108853: step 1782, loss 0.101612, acc 0.96875, learning_rate 0.000103362
2017-10-10T11:45:21.330523: step 1783, loss 0.11426, acc 0.984375, learning_rate 0.000103348
2017-10-10T11:45:21.523391: step 1784, loss 0.208018, acc 0.90625, learning_rate 0.000103335
2017-10-10T11:45:21.752169: step 1785, loss 0.16333, acc 0.9375, learning_rate 0.000103321
2017-10-10T11:45:21.960367: step 1786, loss 0.298649, acc 0.890625, learning_rate 0.000103307
2017-10-10T11:45:22.163282: step 1787, loss 0.109825, acc 0.96875, learning_rate 0.000103294
2017-10-10T11:45:22.354498: step 1788, loss 0.110405, acc 0.96875, learning_rate 0.00010328
2017-10-10T11:45:22.552398: step 1789, loss 0.123991, acc 0.9375, learning_rate 0.000103267
2017-10-10T11:45:22.749972: step 1790, loss 0.136035, acc 0.96875, learning_rate 0.000103254
2017-10-10T11:45:22.967551: step 1791, loss 0.0881874, acc 0.984375, learning_rate 0.00010324
2017-10-10T11:45:23.181219: step 1792, loss 0.115789, acc 0.953125, learning_rate 0.000103227
2017-10-10T11:45:23.455344: step 1793, loss 0.121195, acc 0.953125, learning_rate 0.000103214
2017-10-10T11:45:23.755873: step 1794, loss 0.201879, acc 0.921875, learning_rate 0.000103201
2017-10-10T11:45:23.927762: step 1795, loss 0.115906, acc 0.96875, learning_rate 0.000103188
2017-10-10T11:45:24.125640: step 1796, loss 0.100833, acc 0.9375, learning_rate 0.000103175
2017-10-10T11:45:24.304900: step 1797, loss 0.119637, acc 0.953125, learning_rate 0.000103162
2017-10-10T11:45:24.500822: step 1798, loss 0.185032, acc 0.921875, learning_rate 0.000103149
2017-10-10T11:45:24.695502: step 1799, loss 0.22286, acc 0.90625, learning_rate 0.000103136
2017-10-10T11:45:24.888151: step 1800, loss 0.0612358, acc 0.984375, learning_rate 0.000103123

Evaluation:
2017-10-10T11:45:25.589470: step 1800, loss 0.21497, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1800

2017-10-10T11:45:26.608888: step 1801, loss 0.150953, acc 0.9375, learning_rate 0.000103111
2017-10-10T11:45:26.839307: step 1802, loss 0.21554, acc 0.921875, learning_rate 0.000103098
2017-10-10T11:45:27.078315: step 1803, loss 0.152477, acc 0.953125, learning_rate 0.000103085
2017-10-10T11:45:27.309251: step 1804, loss 0.101028, acc 0.96875, learning_rate 0.000103073
2017-10-10T11:45:27.572857: step 1805, loss 0.154994, acc 0.921875, learning_rate 0.00010306
2017-10-10T11:45:27.807316: step 1806, loss 0.0885051, acc 0.984375, learning_rate 0.000103048
2017-10-10T11:45:28.079479: step 1807, loss 0.149026, acc 0.953125, learning_rate 0.000103035
2017-10-10T11:45:28.356176: step 1808, loss 0.0420863, acc 0.984375, learning_rate 0.000103023
2017-10-10T11:45:28.600064: step 1809, loss 0.0771466, acc 0.96875, learning_rate 0.00010301
2017-10-10T11:45:28.858181: step 1810, loss 0.136438, acc 0.953125, learning_rate 0.000102998
2017-10-10T11:45:29.082569: step 1811, loss 0.05348, acc 0.984375, learning_rate 0.000102986
2017-10-10T11:45:29.331541: step 1812, loss 0.312343, acc 0.890625, learning_rate 0.000102974
2017-10-10T11:45:29.583081: step 1813, loss 0.142647, acc 0.90625, learning_rate 0.000102962
2017-10-10T11:45:29.829177: step 1814, loss 0.0820364, acc 0.984375, learning_rate 0.000102949
2017-10-10T11:45:30.065079: step 1815, loss 0.0761464, acc 0.96875, learning_rate 0.000102937
2017-10-10T11:45:30.417094: step 1816, loss 0.0832253, acc 0.96875, learning_rate 0.000102925
2017-10-10T11:45:30.597421: step 1817, loss 0.104559, acc 0.953125, learning_rate 0.000102913
2017-10-10T11:45:30.795613: step 1818, loss 0.163679, acc 0.9375, learning_rate 0.000102902
2017-10-10T11:45:30.988050: step 1819, loss 0.237454, acc 0.921875, learning_rate 0.00010289
2017-10-10T11:45:31.183640: step 1820, loss 0.117739, acc 0.96875, learning_rate 0.000102878
2017-10-10T11:45:31.378203: step 1821, loss 0.160558, acc 0.953125, learning_rate 0.000102866
2017-10-10T11:45:31.608055: step 1822, loss 0.141563, acc 0.953125, learning_rate 0.000102855
2017-10-10T11:45:31.844838: step 1823, loss 0.0585023, acc 1, learning_rate 0.000102843
2017-10-10T11:45:32.142882: step 1824, loss 0.153573, acc 0.953125, learning_rate 0.000102831
2017-10-10T11:45:32.406131: step 1825, loss 0.194277, acc 0.890625, learning_rate 0.00010282
2017-10-10T11:45:32.657800: step 1826, loss 0.265232, acc 0.875, learning_rate 0.000102808
2017-10-10T11:45:32.888774: step 1827, loss 0.0671853, acc 0.984375, learning_rate 0.000102797
2017-10-10T11:45:33.132854: step 1828, loss 0.158036, acc 0.953125, learning_rate 0.000102785
2017-10-10T11:45:33.372979: step 1829, loss 0.105188, acc 0.984375, learning_rate 0.000102774
2017-10-10T11:45:33.597096: step 1830, loss 0.0563451, acc 0.984375, learning_rate 0.000102763
2017-10-10T11:45:33.895619: step 1831, loss 0.0644455, acc 0.984375, learning_rate 0.000102751
2017-10-10T11:45:34.167183: step 1832, loss 0.0661959, acc 0.984375, learning_rate 0.00010274
2017-10-10T11:45:34.445528: step 1833, loss 0.115406, acc 0.9375, learning_rate 0.000102729
2017-10-10T11:45:34.721723: step 1834, loss 0.103693, acc 0.953125, learning_rate 0.000102718
2017-10-10T11:45:34.910220: step 1835, loss 0.0952092, acc 0.984375, learning_rate 0.000102707
2017-10-10T11:45:35.082080: step 1836, loss 0.151741, acc 0.9375, learning_rate 0.000102696
2017-10-10T11:45:35.280932: step 1837, loss 0.182743, acc 0.921875, learning_rate 0.000102685
2017-10-10T11:45:35.469764: step 1838, loss 0.186308, acc 0.9375, learning_rate 0.000102674
2017-10-10T11:45:35.661132: step 1839, loss 0.139652, acc 0.96875, learning_rate 0.000102663
2017-10-10T11:45:35.854270: step 1840, loss 0.143366, acc 0.953125, learning_rate 0.000102652

Evaluation:
2017-10-10T11:45:36.384950: step 1840, loss 0.214458, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1840

2017-10-10T11:45:37.604591: step 1841, loss 0.126845, acc 0.984375, learning_rate 0.000102641
2017-10-10T11:45:37.850097: step 1842, loss 0.106865, acc 0.953125, learning_rate 0.00010263
2017-10-10T11:45:38.104818: step 1843, loss 0.123181, acc 0.9375, learning_rate 0.00010262
2017-10-10T11:45:38.393172: step 1844, loss 0.0936152, acc 0.96875, learning_rate 0.000102609
2017-10-10T11:45:38.669756: step 1845, loss 0.106074, acc 0.953125, learning_rate 0.000102598
2017-10-10T11:45:38.872414: step 1846, loss 0.243489, acc 0.9375, learning_rate 0.000102588
2017-10-10T11:45:39.096160: step 1847, loss 0.167334, acc 0.921875, learning_rate 0.000102577
2017-10-10T11:45:39.308554: step 1848, loss 0.0697697, acc 1, learning_rate 0.000102567
2017-10-10T11:45:39.519623: step 1849, loss 0.175665, acc 0.953125, learning_rate 0.000102556
2017-10-10T11:45:39.750393: step 1850, loss 0.131705, acc 0.984375, learning_rate 0.000102546
2017-10-10T11:45:40.016854: step 1851, loss 0.2003, acc 0.90625, learning_rate 0.000102535
2017-10-10T11:45:40.270314: step 1852, loss 0.12556, acc 0.96875, learning_rate 0.000102525
2017-10-10T11:45:40.528883: step 1853, loss 0.0905599, acc 0.984375, learning_rate 0.000102515
2017-10-10T11:45:40.782293: step 1854, loss 0.0526444, acc 0.984375, learning_rate 0.000102504
2017-10-10T11:45:41.020878: step 1855, loss 0.0985868, acc 0.96875, learning_rate 0.000102494
2017-10-10T11:45:41.283319: step 1856, loss 0.0745673, acc 0.96875, learning_rate 0.000102484
2017-10-10T11:45:41.524717: step 1857, loss 0.150844, acc 0.921875, learning_rate 0.000102474
2017-10-10T11:45:41.746715: step 1858, loss 0.0730696, acc 1, learning_rate 0.000102464
2017-10-10T11:45:42.008821: step 1859, loss 0.0593098, acc 1, learning_rate 0.000102454
2017-10-10T11:45:42.252909: step 1860, loss 0.211047, acc 0.9375, learning_rate 0.000102444
2017-10-10T11:45:42.537068: step 1861, loss 0.0635482, acc 0.984375, learning_rate 0.000102434
2017-10-10T11:45:42.744930: step 1862, loss 0.160953, acc 0.941176, learning_rate 0.000102424
2017-10-10T11:45:43.007771: step 1863, loss 0.0754436, acc 0.984375, learning_rate 0.000102414
2017-10-10T11:45:43.277066: step 1864, loss 0.270502, acc 0.90625, learning_rate 0.000102404
2017-10-10T11:45:43.533307: step 1865, loss 0.0764423, acc 0.96875, learning_rate 0.000102394
2017-10-10T11:45:43.778339: step 1866, loss 0.120952, acc 0.9375, learning_rate 0.000102384
2017-10-10T11:45:44.053016: step 1867, loss 0.153073, acc 0.953125, learning_rate 0.000102375
2017-10-10T11:45:44.308809: step 1868, loss 0.151177, acc 0.921875, learning_rate 0.000102365
2017-10-10T11:45:44.563035: step 1869, loss 0.163102, acc 0.9375, learning_rate 0.000102355
2017-10-10T11:45:44.796132: step 1870, loss 0.0690217, acc 0.984375, learning_rate 0.000102346
2017-10-10T11:45:45.039179: step 1871, loss 0.16712, acc 0.921875, learning_rate 0.000102336
2017-10-10T11:45:45.266393: step 1872, loss 0.111276, acc 0.984375, learning_rate 0.000102327
2017-10-10T11:45:45.564707: step 1873, loss 0.100422, acc 0.96875, learning_rate 0.000102317
2017-10-10T11:45:45.827806: step 1874, loss 0.236216, acc 0.9375, learning_rate 0.000102308
2017-10-10T11:45:46.009371: step 1875, loss 0.110615, acc 0.9375, learning_rate 0.000102298
2017-10-10T11:45:46.215098: step 1876, loss 0.175962, acc 0.953125, learning_rate 0.000102289
2017-10-10T11:45:46.394599: step 1877, loss 0.153503, acc 0.921875, learning_rate 0.000102279
2017-10-10T11:45:46.589084: step 1878, loss 0.11906, acc 0.953125, learning_rate 0.00010227
2017-10-10T11:45:46.897261: step 1879, loss 0.0817392, acc 0.953125, learning_rate 0.000102261
2017-10-10T11:45:47.115317: step 1880, loss 0.139282, acc 0.953125, learning_rate 0.000102252

Evaluation:
2017-10-10T11:45:48.519606: step 1880, loss 0.21506, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1880

2017-10-10T11:45:51.314720: step 1881, loss 0.0649181, acc 0.984375, learning_rate 0.000102242
2017-10-10T11:45:51.588449: step 1882, loss 0.191829, acc 0.890625, learning_rate 0.000102233
2017-10-10T11:45:51.814067: step 1883, loss 0.0979362, acc 0.96875, learning_rate 0.000102224
2017-10-10T11:45:52.062510: step 1884, loss 0.199086, acc 0.90625, learning_rate 0.000102215
2017-10-10T11:45:52.327342: step 1885, loss 0.148, acc 0.9375, learning_rate 0.000102206
2017-10-10T11:45:52.549152: step 1886, loss 0.0940603, acc 0.96875, learning_rate 0.000102197
2017-10-10T11:45:52.890849: step 1887, loss 0.0630742, acc 0.984375, learning_rate 0.000102188
2017-10-10T11:45:53.098827: step 1888, loss 0.125755, acc 0.984375, learning_rate 0.000102179
2017-10-10T11:45:53.295107: step 1889, loss 0.151283, acc 0.953125, learning_rate 0.00010217
2017-10-10T11:45:53.485506: step 1890, loss 0.148443, acc 0.953125, learning_rate 0.000102161
2017-10-10T11:45:53.693128: step 1891, loss 0.17195, acc 0.9375, learning_rate 0.000102153
2017-10-10T11:45:53.890925: step 1892, loss 0.0880063, acc 1, learning_rate 0.000102144
2017-10-10T11:45:54.140863: step 1893, loss 0.119621, acc 0.953125, learning_rate 0.000102135
2017-10-10T11:45:54.406721: step 1894, loss 0.166773, acc 0.953125, learning_rate 0.000102126
2017-10-10T11:45:54.657064: step 1895, loss 0.090154, acc 0.96875, learning_rate 0.000102118
2017-10-10T11:45:54.945395: step 1896, loss 0.115677, acc 0.953125, learning_rate 0.000102109
2017-10-10T11:45:55.227646: step 1897, loss 0.137024, acc 0.953125, learning_rate 0.0001021
2017-10-10T11:45:55.464308: step 1898, loss 0.142338, acc 0.9375, learning_rate 0.000102092
2017-10-10T11:45:55.720596: step 1899, loss 0.12677, acc 0.9375, learning_rate 0.000102083
2017-10-10T11:45:55.983753: step 1900, loss 0.113381, acc 0.953125, learning_rate 0.000102075
2017-10-10T11:45:56.250615: step 1901, loss 0.108253, acc 0.96875, learning_rate 0.000102066
2017-10-10T11:45:56.537046: step 1902, loss 0.0894924, acc 0.953125, learning_rate 0.000102058
2017-10-10T11:45:56.815467: step 1903, loss 0.115506, acc 0.953125, learning_rate 0.00010205
2017-10-10T11:45:57.184350: step 1904, loss 0.0995301, acc 0.953125, learning_rate 0.000102041
2017-10-10T11:45:57.367790: step 1905, loss 0.099035, acc 0.984375, learning_rate 0.000102033
2017-10-10T11:45:57.545587: step 1906, loss 0.107025, acc 0.953125, learning_rate 0.000102025
2017-10-10T11:45:57.740821: step 1907, loss 0.0689469, acc 0.984375, learning_rate 0.000102016
2017-10-10T11:45:57.934811: step 1908, loss 0.101452, acc 0.96875, learning_rate 0.000102008
2017-10-10T11:45:58.108579: step 1909, loss 0.0655187, acc 0.984375, learning_rate 0.000102
2017-10-10T11:45:58.376682: step 1910, loss 0.0799626, acc 0.984375, learning_rate 0.000101992
2017-10-10T11:45:58.624480: step 1911, loss 0.222088, acc 0.90625, learning_rate 0.000101984
2017-10-10T11:45:58.854554: step 1912, loss 0.113791, acc 0.953125, learning_rate 0.000101975
2017-10-10T11:45:59.118427: step 1913, loss 0.146072, acc 0.921875, learning_rate 0.000101967
2017-10-10T11:45:59.365033: step 1914, loss 0.0669804, acc 0.984375, learning_rate 0.000101959
2017-10-10T11:45:59.583639: step 1915, loss 0.0844262, acc 0.984375, learning_rate 0.000101951
2017-10-10T11:45:59.809034: step 1916, loss 0.134684, acc 0.96875, learning_rate 0.000101943
2017-10-10T11:46:00.057875: step 1917, loss 0.0771399, acc 0.984375, learning_rate 0.000101935
2017-10-10T11:46:00.281855: step 1918, loss 0.132589, acc 0.953125, learning_rate 0.000101928
2017-10-10T11:46:00.510105: step 1919, loss 0.112084, acc 0.96875, learning_rate 0.00010192
2017-10-10T11:46:00.754126: step 1920, loss 0.20654, acc 0.90625, learning_rate 0.000101912

Evaluation:
2017-10-10T11:46:01.336870: step 1920, loss 0.215572, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1920

2017-10-10T11:46:02.407045: step 1921, loss 0.0615677, acc 1, learning_rate 0.000101904
2017-10-10T11:46:02.667530: step 1922, loss 0.143419, acc 0.96875, learning_rate 0.000101896
2017-10-10T11:46:02.903500: step 1923, loss 0.274083, acc 0.890625, learning_rate 0.000101889
2017-10-10T11:46:03.173751: step 1924, loss 0.194919, acc 0.9375, learning_rate 0.000101881
2017-10-10T11:46:03.406618: step 1925, loss 0.0645325, acc 0.984375, learning_rate 0.000101873
2017-10-10T11:46:03.672307: step 1926, loss 0.106935, acc 0.96875, learning_rate 0.000101865
2017-10-10T11:46:03.914479: step 1927, loss 0.144533, acc 0.953125, learning_rate 0.000101858
2017-10-10T11:46:04.160939: step 1928, loss 0.1485, acc 0.953125, learning_rate 0.00010185
2017-10-10T11:46:04.434251: step 1929, loss 0.0970862, acc 0.96875, learning_rate 0.000101843
2017-10-10T11:46:04.677106: step 1930, loss 0.106819, acc 0.9375, learning_rate 0.000101835
2017-10-10T11:46:04.950714: step 1931, loss 0.0502266, acc 1, learning_rate 0.000101828
2017-10-10T11:46:05.180908: step 1932, loss 0.113627, acc 0.953125, learning_rate 0.00010182
2017-10-10T11:46:05.404844: step 1933, loss 0.185767, acc 0.9375, learning_rate 0.000101813
2017-10-10T11:46:05.681556: step 1934, loss 0.152886, acc 0.953125, learning_rate 0.000101805
2017-10-10T11:46:05.908545: step 1935, loss 0.150673, acc 0.9375, learning_rate 0.000101798
2017-10-10T11:46:06.122484: step 1936, loss 0.0862827, acc 0.96875, learning_rate 0.000101791
2017-10-10T11:46:06.398028: step 1937, loss 0.104492, acc 0.953125, learning_rate 0.000101783
2017-10-10T11:46:06.651697: step 1938, loss 0.0807055, acc 0.984375, learning_rate 0.000101776
2017-10-10T11:46:06.914510: step 1939, loss 0.110409, acc 0.96875, learning_rate 0.000101769
2017-10-10T11:46:07.151795: step 1940, loss 0.133831, acc 0.953125, learning_rate 0.000101762
2017-10-10T11:46:07.412096: step 1941, loss 0.0661175, acc 0.984375, learning_rate 0.000101754
2017-10-10T11:46:07.732891: step 1942, loss 0.155958, acc 0.953125, learning_rate 0.000101747
2017-10-10T11:46:07.979963: step 1943, loss 0.104716, acc 0.96875, learning_rate 0.00010174
2017-10-10T11:46:08.181235: step 1944, loss 0.169736, acc 0.921875, learning_rate 0.000101733
2017-10-10T11:46:08.372920: step 1945, loss 0.185485, acc 0.9375, learning_rate 0.000101726
2017-10-10T11:46:08.582977: step 1946, loss 0.105543, acc 0.96875, learning_rate 0.000101719
2017-10-10T11:46:08.766798: step 1947, loss 0.214062, acc 0.890625, learning_rate 0.000101712
2017-10-10T11:46:08.947905: step 1948, loss 0.164935, acc 0.953125, learning_rate 0.000101705
2017-10-10T11:46:09.236873: step 1949, loss 0.0898322, acc 0.96875, learning_rate 0.000101698
2017-10-10T11:46:09.720138: step 1950, loss 0.130838, acc 0.9375, learning_rate 0.000101691
2017-10-10T11:46:09.924924: step 1951, loss 0.221287, acc 0.9375, learning_rate 0.000101684
2017-10-10T11:46:10.118130: step 1952, loss 0.10126, acc 0.96875, learning_rate 0.000101677
2017-10-10T11:46:10.326441: step 1953, loss 0.0519494, acc 0.984375, learning_rate 0.00010167
2017-10-10T11:46:10.535676: step 1954, loss 0.0692933, acc 1, learning_rate 0.000101664
2017-10-10T11:46:10.747489: step 1955, loss 0.174747, acc 0.9375, learning_rate 0.000101657
2017-10-10T11:46:11.014395: step 1956, loss 0.0449788, acc 1, learning_rate 0.00010165
2017-10-10T11:46:11.280740: step 1957, loss 0.127424, acc 0.9375, learning_rate 0.000101643
2017-10-10T11:46:11.511421: step 1958, loss 0.106763, acc 0.984375, learning_rate 0.000101637
2017-10-10T11:46:11.776071: step 1959, loss 0.155397, acc 0.921875, learning_rate 0.00010163
2017-10-10T11:46:11.957565: step 1960, loss 0.0875546, acc 0.980392, learning_rate 0.000101623

Evaluation:
2017-10-10T11:46:12.511154: step 1960, loss 0.213374, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-1960

2017-10-10T11:46:13.681109: step 1961, loss 0.0506681, acc 0.984375, learning_rate 0.000101617
2017-10-10T11:46:13.914030: step 1962, loss 0.173631, acc 0.96875, learning_rate 0.00010161
2017-10-10T11:46:14.137717: step 1963, loss 0.145868, acc 0.9375, learning_rate 0.000101604
2017-10-10T11:46:14.375686: step 1964, loss 0.139807, acc 0.96875, learning_rate 0.000101597
2017-10-10T11:46:14.661829: step 1965, loss 0.117228, acc 0.953125, learning_rate 0.00010159
2017-10-10T11:46:14.884560: step 1966, loss 0.166993, acc 0.921875, learning_rate 0.000101584
2017-10-10T11:46:15.136088: step 1967, loss 0.0911508, acc 0.96875, learning_rate 0.000101577
2017-10-10T11:46:15.395537: step 1968, loss 0.114139, acc 0.96875, learning_rate 0.000101571
2017-10-10T11:46:15.681179: step 1969, loss 0.290781, acc 0.890625, learning_rate 0.000101565
2017-10-10T11:46:15.939117: step 1970, loss 0.0357018, acc 1, learning_rate 0.000101558
2017-10-10T11:46:16.176861: step 1971, loss 0.2153, acc 0.921875, learning_rate 0.000101552
2017-10-10T11:46:16.407381: step 1972, loss 0.111625, acc 0.953125, learning_rate 0.000101546
2017-10-10T11:46:16.666844: step 1973, loss 0.254671, acc 0.890625, learning_rate 0.000101539
2017-10-10T11:46:16.913090: step 1974, loss 0.0511493, acc 0.984375, learning_rate 0.000101533
2017-10-10T11:46:17.172313: step 1975, loss 0.100768, acc 0.96875, learning_rate 0.000101527
2017-10-10T11:46:17.436874: step 1976, loss 0.235716, acc 0.921875, learning_rate 0.00010152
2017-10-10T11:46:17.658321: step 1977, loss 0.0951524, acc 0.984375, learning_rate 0.000101514
2017-10-10T11:46:17.857872: step 1978, loss 0.105042, acc 0.984375, learning_rate 0.000101508
2017-10-10T11:46:18.160238: step 1979, loss 0.154821, acc 0.953125, learning_rate 0.000101502
2017-10-10T11:46:18.349690: step 1980, loss 0.0771905, acc 0.984375, learning_rate 0.000101496
2017-10-10T11:46:18.628833: step 1981, loss 0.117134, acc 0.96875, learning_rate 0.00010149
2017-10-10T11:46:18.831081: step 1982, loss 0.106689, acc 0.984375, learning_rate 0.000101484
2017-10-10T11:46:18.968767: step 1983, loss 0.157158, acc 0.921875, learning_rate 0.000101478
2017-10-10T11:46:19.121343: step 1984, loss 0.131466, acc 0.96875, learning_rate 0.000101472
2017-10-10T11:46:19.306497: step 1985, loss 0.106672, acc 0.96875, learning_rate 0.000101466
2017-10-10T11:46:19.516815: step 1986, loss 0.154755, acc 0.953125, learning_rate 0.00010146
2017-10-10T11:46:19.762439: step 1987, loss 0.0985519, acc 0.953125, learning_rate 0.000101454
2017-10-10T11:46:19.977094: step 1988, loss 0.191343, acc 0.9375, learning_rate 0.000101448
2017-10-10T11:46:20.229789: step 1989, loss 0.163742, acc 0.9375, learning_rate 0.000101442
2017-10-10T11:46:20.461232: step 1990, loss 0.0894208, acc 0.984375, learning_rate 0.000101436
2017-10-10T11:46:20.689812: step 1991, loss 0.0740638, acc 0.96875, learning_rate 0.00010143
2017-10-10T11:46:20.952047: step 1992, loss 0.0752536, acc 0.96875, learning_rate 0.000101424
2017-10-10T11:46:21.188914: step 1993, loss 0.124151, acc 0.953125, learning_rate 0.000101418
2017-10-10T11:46:21.428355: step 1994, loss 0.149513, acc 0.96875, learning_rate 0.000101413
2017-10-10T11:46:21.678546: step 1995, loss 0.0616682, acc 0.984375, learning_rate 0.000101407
2017-10-10T11:46:21.914713: step 1996, loss 0.154335, acc 0.96875, learning_rate 0.000101401
2017-10-10T11:46:22.169034: step 1997, loss 0.121211, acc 0.953125, learning_rate 0.000101395
2017-10-10T11:46:22.436187: step 1998, loss 0.119625, acc 0.984375, learning_rate 0.00010139
2017-10-10T11:46:22.690312: step 1999, loss 0.0814059, acc 0.984375, learning_rate 0.000101384
2017-10-10T11:46:22.948875: step 2000, loss 0.109551, acc 0.96875, learning_rate 0.000101378

Evaluation:
2017-10-10T11:46:23.523813: step 2000, loss 0.213906, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2000

2017-10-10T11:46:24.696861: step 2001, loss 0.172223, acc 0.953125, learning_rate 0.000101373
2017-10-10T11:46:24.941960: step 2002, loss 0.217237, acc 0.9375, learning_rate 0.000101367
2017-10-10T11:46:25.207461: step 2003, loss 0.116205, acc 0.96875, learning_rate 0.000101362
2017-10-10T11:46:25.467720: step 2004, loss 0.171185, acc 0.921875, learning_rate 0.000101356
2017-10-10T11:46:25.700592: step 2005, loss 0.103586, acc 0.96875, learning_rate 0.00010135
2017-10-10T11:46:25.938165: step 2006, loss 0.0757411, acc 0.984375, learning_rate 0.000101345
2017-10-10T11:46:26.233276: step 2007, loss 0.114114, acc 0.953125, learning_rate 0.000101339
2017-10-10T11:46:26.508835: step 2008, loss 0.166865, acc 0.9375, learning_rate 0.000101334
2017-10-10T11:46:26.711062: step 2009, loss 0.0881218, acc 0.96875, learning_rate 0.000101328
2017-10-10T11:46:26.902863: step 2010, loss 0.139819, acc 0.953125, learning_rate 0.000101323
2017-10-10T11:46:27.109076: step 2011, loss 0.135783, acc 0.96875, learning_rate 0.000101318
2017-10-10T11:46:27.302592: step 2012, loss 0.147998, acc 0.9375, learning_rate 0.000101312
2017-10-10T11:46:27.566323: step 2013, loss 0.213525, acc 0.9375, learning_rate 0.000101307
2017-10-10T11:46:27.820257: step 2014, loss 0.185826, acc 0.9375, learning_rate 0.000101302
2017-10-10T11:46:28.071503: step 2015, loss 0.133171, acc 0.96875, learning_rate 0.000101296
2017-10-10T11:46:28.326028: step 2016, loss 0.125059, acc 0.9375, learning_rate 0.000101291
2017-10-10T11:46:28.560881: step 2017, loss 0.200141, acc 0.921875, learning_rate 0.000101286
2017-10-10T11:46:28.813021: step 2018, loss 0.113013, acc 0.953125, learning_rate 0.00010128
2017-10-10T11:46:29.177079: step 2019, loss 0.105694, acc 0.953125, learning_rate 0.000101275
2017-10-10T11:46:29.393350: step 2020, loss 0.0683444, acc 0.96875, learning_rate 0.00010127
2017-10-10T11:46:29.583728: step 2021, loss 0.16872, acc 0.921875, learning_rate 0.000101265
2017-10-10T11:46:29.761951: step 2022, loss 0.125875, acc 0.9375, learning_rate 0.00010126
2017-10-10T11:46:29.965203: step 2023, loss 0.0771967, acc 0.96875, learning_rate 0.000101255
2017-10-10T11:46:30.137031: step 2024, loss 0.0605462, acc 0.984375, learning_rate 0.000101249
2017-10-10T11:46:30.364981: step 2025, loss 0.0954674, acc 0.984375, learning_rate 0.000101244
2017-10-10T11:46:30.629378: step 2026, loss 0.117669, acc 0.9375, learning_rate 0.000101239
2017-10-10T11:46:30.874274: step 2027, loss 0.082883, acc 0.96875, learning_rate 0.000101234
2017-10-10T11:46:31.130827: step 2028, loss 0.108687, acc 0.953125, learning_rate 0.000101229
2017-10-10T11:46:31.372394: step 2029, loss 0.20128, acc 0.921875, learning_rate 0.000101224
2017-10-10T11:46:31.599194: step 2030, loss 0.0920089, acc 0.953125, learning_rate 0.000101219
2017-10-10T11:46:31.852632: step 2031, loss 0.104475, acc 0.96875, learning_rate 0.000101214
2017-10-10T11:46:32.096512: step 2032, loss 0.0739543, acc 0.96875, learning_rate 0.000101209
2017-10-10T11:46:32.337265: step 2033, loss 0.101232, acc 0.96875, learning_rate 0.000101204
2017-10-10T11:46:32.574101: step 2034, loss 0.10347, acc 0.96875, learning_rate 0.000101199
2017-10-10T11:46:32.819449: step 2035, loss 0.0989753, acc 0.9375, learning_rate 0.000101194
2017-10-10T11:46:33.088207: step 2036, loss 0.122405, acc 0.953125, learning_rate 0.00010119
2017-10-10T11:46:33.353880: step 2037, loss 0.247225, acc 0.875, learning_rate 0.000101185
2017-10-10T11:46:33.604293: step 2038, loss 0.15772, acc 0.96875, learning_rate 0.00010118
2017-10-10T11:46:33.848892: step 2039, loss 0.144507, acc 0.96875, learning_rate 0.000101175
2017-10-10T11:46:34.097051: step 2040, loss 0.141063, acc 0.953125, learning_rate 0.00010117

Evaluation:
2017-10-10T11:46:34.665786: step 2040, loss 0.213297, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2040

2017-10-10T11:46:35.852994: step 2041, loss 0.07397, acc 0.984375, learning_rate 0.000101166
2017-10-10T11:46:36.099466: step 2042, loss 0.265644, acc 0.921875, learning_rate 0.000101161
2017-10-10T11:46:36.341782: step 2043, loss 0.0907083, acc 0.96875, learning_rate 0.000101156
2017-10-10T11:46:36.585172: step 2044, loss 0.119465, acc 0.953125, learning_rate 0.000101151
2017-10-10T11:46:36.827214: step 2045, loss 0.1487, acc 0.9375, learning_rate 0.000101147
2017-10-10T11:46:37.104625: step 2046, loss 0.0912851, acc 0.96875, learning_rate 0.000101142
2017-10-10T11:46:37.385271: step 2047, loss 0.131287, acc 0.9375, learning_rate 0.000101137
2017-10-10T11:46:37.653379: step 2048, loss 0.0706413, acc 0.96875, learning_rate 0.000101133
2017-10-10T11:46:37.893601: step 2049, loss 0.152598, acc 0.9375, learning_rate 0.000101128
2017-10-10T11:46:38.180823: step 2050, loss 0.153549, acc 0.921875, learning_rate 0.000101123
2017-10-10T11:46:38.427585: step 2051, loss 0.102364, acc 0.953125, learning_rate 0.000101119
2017-10-10T11:46:38.693741: step 2052, loss 0.0703031, acc 0.96875, learning_rate 0.000101114
2017-10-10T11:46:38.921906: step 2053, loss 0.119502, acc 0.984375, learning_rate 0.00010111
2017-10-10T11:46:39.172687: step 2054, loss 0.087905, acc 0.96875, learning_rate 0.000101105
2017-10-10T11:46:39.422222: step 2055, loss 0.219527, acc 0.90625, learning_rate 0.000101101
2017-10-10T11:46:39.744992: step 2056, loss 0.147996, acc 0.921875, learning_rate 0.000101096
2017-10-10T11:46:39.941460: step 2057, loss 0.140175, acc 0.9375, learning_rate 0.000101092
2017-10-10T11:46:40.109502: step 2058, loss 0.244707, acc 0.901961, learning_rate 0.000101087
2017-10-10T11:46:40.291219: step 2059, loss 0.0931195, acc 0.96875, learning_rate 0.000101083
2017-10-10T11:46:40.468820: step 2060, loss 0.0642473, acc 0.984375, learning_rate 0.000101078
2017-10-10T11:46:40.663801: step 2061, loss 0.217202, acc 0.90625, learning_rate 0.000101074
2017-10-10T11:46:40.925017: step 2062, loss 0.144669, acc 0.953125, learning_rate 0.00010107
2017-10-10T11:46:41.200845: step 2063, loss 0.0886254, acc 0.96875, learning_rate 0.000101065
2017-10-10T11:46:41.428322: step 2064, loss 0.0508681, acc 1, learning_rate 0.000101061
2017-10-10T11:46:41.658140: step 2065, loss 0.155769, acc 0.953125, learning_rate 0.000101057
2017-10-10T11:46:41.875952: step 2066, loss 0.167388, acc 0.953125, learning_rate 0.000101052
2017-10-10T11:46:42.128850: step 2067, loss 0.208458, acc 0.90625, learning_rate 0.000101048
2017-10-10T11:46:42.361049: step 2068, loss 0.0629108, acc 0.984375, learning_rate 0.000101044
2017-10-10T11:46:42.590718: step 2069, loss 0.059061, acc 0.96875, learning_rate 0.000101039
2017-10-10T11:46:42.835941: step 2070, loss 0.147661, acc 0.953125, learning_rate 0.000101035
2017-10-10T11:46:43.111650: step 2071, loss 0.159148, acc 0.9375, learning_rate 0.000101031
2017-10-10T11:46:43.366521: step 2072, loss 0.13927, acc 0.953125, learning_rate 0.000101027
2017-10-10T11:46:43.560823: step 2073, loss 0.144851, acc 0.921875, learning_rate 0.000101023
2017-10-10T11:46:43.756412: step 2074, loss 0.210728, acc 0.90625, learning_rate 0.000101018
2017-10-10T11:46:43.963102: step 2075, loss 0.131803, acc 0.9375, learning_rate 0.000101014
2017-10-10T11:46:44.163440: step 2076, loss 0.286965, acc 0.875, learning_rate 0.00010101
2017-10-10T11:46:44.442728: step 2077, loss 0.13607, acc 0.9375, learning_rate 0.000101006
2017-10-10T11:46:44.655692: step 2078, loss 0.179355, acc 0.9375, learning_rate 0.000101002
2017-10-10T11:46:44.909065: step 2079, loss 0.169253, acc 0.9375, learning_rate 0.000100998
2017-10-10T11:46:45.173582: step 2080, loss 0.175136, acc 0.953125, learning_rate 0.000100994

Evaluation:
2017-10-10T11:46:45.771544: step 2080, loss 0.213465, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2080

2017-10-10T11:46:46.775982: step 2081, loss 0.0897641, acc 0.953125, learning_rate 0.00010099
2017-10-10T11:46:47.004776: step 2082, loss 0.153906, acc 0.921875, learning_rate 0.000100986
2017-10-10T11:46:47.240396: step 2083, loss 0.214062, acc 0.921875, learning_rate 0.000100982
2017-10-10T11:46:47.454684: step 2084, loss 0.0871321, acc 0.953125, learning_rate 0.000100978
2017-10-10T11:46:47.696266: step 2085, loss 0.108226, acc 0.953125, learning_rate 0.000100974
2017-10-10T11:46:47.932869: step 2086, loss 0.113242, acc 0.953125, learning_rate 0.00010097
2017-10-10T11:46:48.202647: step 2087, loss 0.162667, acc 0.9375, learning_rate 0.000100966
2017-10-10T11:46:48.484848: step 2088, loss 0.182484, acc 0.953125, learning_rate 0.000100962
2017-10-10T11:46:48.722074: step 2089, loss 0.0879347, acc 0.984375, learning_rate 0.000100958
2017-10-10T11:46:48.971038: step 2090, loss 0.14096, acc 0.96875, learning_rate 0.000100954
2017-10-10T11:46:49.233109: step 2091, loss 0.13636, acc 0.9375, learning_rate 0.00010095
2017-10-10T11:46:49.460768: step 2092, loss 0.165247, acc 0.9375, learning_rate 0.000100946
2017-10-10T11:46:49.727850: step 2093, loss 0.179048, acc 0.953125, learning_rate 0.000100942
2017-10-10T11:46:50.028398: step 2094, loss 0.132393, acc 0.921875, learning_rate 0.000100938
2017-10-10T11:46:50.296584: step 2095, loss 0.116779, acc 0.96875, learning_rate 0.000100935
2017-10-10T11:46:50.465077: step 2096, loss 0.103661, acc 0.96875, learning_rate 0.000100931
2017-10-10T11:46:50.635397: step 2097, loss 0.113611, acc 0.953125, learning_rate 0.000100927
2017-10-10T11:46:50.814432: step 2098, loss 0.188222, acc 0.9375, learning_rate 0.000100923
2017-10-10T11:46:51.004211: step 2099, loss 0.140061, acc 0.953125, learning_rate 0.000100919
2017-10-10T11:46:51.256873: step 2100, loss 0.0801968, acc 0.96875, learning_rate 0.000100916
2017-10-10T11:46:51.402133: step 2101, loss 0.0625393, acc 0.984375, learning_rate 0.000100912
2017-10-10T11:46:51.544582: step 2102, loss 0.0512746, acc 1, learning_rate 0.000100908
2017-10-10T11:46:51.690585: step 2103, loss 0.110428, acc 0.953125, learning_rate 0.000100904
2017-10-10T11:46:51.844251: step 2104, loss 0.127043, acc 0.921875, learning_rate 0.000100901
2017-10-10T11:46:52.051388: step 2105, loss 0.127992, acc 0.953125, learning_rate 0.000100897
2017-10-10T11:46:52.313101: step 2106, loss 0.119886, acc 0.96875, learning_rate 0.000100893
2017-10-10T11:46:52.592531: step 2107, loss 0.143652, acc 0.921875, learning_rate 0.00010089
2017-10-10T11:46:52.817503: step 2108, loss 0.106932, acc 0.953125, learning_rate 0.000100886
2017-10-10T11:46:53.098364: step 2109, loss 0.138987, acc 0.9375, learning_rate 0.000100883
2017-10-10T11:46:53.348340: step 2110, loss 0.0572338, acc 0.984375, learning_rate 0.000100879
2017-10-10T11:46:53.598121: step 2111, loss 0.146019, acc 0.96875, learning_rate 0.000100875
2017-10-10T11:46:53.882947: step 2112, loss 0.173382, acc 0.9375, learning_rate 0.000100872
2017-10-10T11:46:54.112444: step 2113, loss 0.0627373, acc 0.984375, learning_rate 0.000100868
2017-10-10T11:46:54.400407: step 2114, loss 0.0875406, acc 1, learning_rate 0.000100865
2017-10-10T11:46:54.676944: step 2115, loss 0.0929238, acc 0.96875, learning_rate 0.000100861
2017-10-10T11:46:54.849147: step 2116, loss 0.0666043, acc 0.984375, learning_rate 0.000100858
2017-10-10T11:46:55.090862: step 2117, loss 0.0445449, acc 1, learning_rate 0.000100854
2017-10-10T11:46:55.351813: step 2118, loss 0.125611, acc 0.9375, learning_rate 0.000100851
2017-10-10T11:46:55.637525: step 2119, loss 0.120186, acc 0.9375, learning_rate 0.000100847
2017-10-10T11:46:55.876823: step 2120, loss 0.141453, acc 0.953125, learning_rate 0.000100844

Evaluation:
2017-10-10T11:46:56.490871: step 2120, loss 0.214941, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2120

2017-10-10T11:46:57.675245: step 2121, loss 0.0617939, acc 1, learning_rate 0.00010084
2017-10-10T11:46:57.942251: step 2122, loss 0.280463, acc 0.875, learning_rate 0.000100837
2017-10-10T11:46:58.201019: step 2123, loss 0.061725, acc 0.984375, learning_rate 0.000100833
2017-10-10T11:46:58.436247: step 2124, loss 0.0545786, acc 0.984375, learning_rate 0.00010083
2017-10-10T11:46:58.688799: step 2125, loss 0.0368602, acc 1, learning_rate 0.000100827
2017-10-10T11:46:58.937162: step 2126, loss 0.166118, acc 0.9375, learning_rate 0.000100823
2017-10-10T11:46:59.161039: step 2127, loss 0.20405, acc 0.921875, learning_rate 0.00010082
2017-10-10T11:46:59.484054: step 2128, loss 0.0862211, acc 0.96875, learning_rate 0.000100817
2017-10-10T11:46:59.673075: step 2129, loss 0.0855659, acc 0.96875, learning_rate 0.000100813
2017-10-10T11:46:59.885007: step 2130, loss 0.0987298, acc 0.984375, learning_rate 0.00010081
2017-10-10T11:47:00.095382: step 2131, loss 0.103138, acc 0.96875, learning_rate 0.000100807
2017-10-10T11:47:00.311883: step 2132, loss 0.0862164, acc 0.984375, learning_rate 0.000100803
2017-10-10T11:47:00.528483: step 2133, loss 0.0793916, acc 1, learning_rate 0.0001008
2017-10-10T11:47:00.784070: step 2134, loss 0.0705165, acc 0.984375, learning_rate 0.000100797
2017-10-10T11:47:01.100927: step 2135, loss 0.0644853, acc 0.96875, learning_rate 0.000100793
2017-10-10T11:47:01.383666: step 2136, loss 0.10657, acc 0.96875, learning_rate 0.00010079
2017-10-10T11:47:01.569693: step 2137, loss 0.193605, acc 0.953125, learning_rate 0.000100787
2017-10-10T11:47:01.748694: step 2138, loss 0.061307, acc 0.96875, learning_rate 0.000100784
2017-10-10T11:47:01.933456: step 2139, loss 0.134863, acc 0.921875, learning_rate 0.000100781
2017-10-10T11:47:02.110659: step 2140, loss 0.216547, acc 0.921875, learning_rate 0.000100777
2017-10-10T11:47:02.364511: step 2141, loss 0.085126, acc 0.96875, learning_rate 0.000100774
2017-10-10T11:47:02.628387: step 2142, loss 0.10176, acc 0.984375, learning_rate 0.000100771
2017-10-10T11:47:02.873038: step 2143, loss 0.0910231, acc 0.953125, learning_rate 0.000100768
2017-10-10T11:47:03.134848: step 2144, loss 0.0733327, acc 0.984375, learning_rate 0.000100765
2017-10-10T11:47:03.380888: step 2145, loss 0.0491583, acc 1, learning_rate 0.000100762
2017-10-10T11:47:03.597058: step 2146, loss 0.0908591, acc 0.984375, learning_rate 0.000100759
2017-10-10T11:47:03.866549: step 2147, loss 0.0470793, acc 1, learning_rate 0.000100755
2017-10-10T11:47:04.103159: step 2148, loss 0.0682469, acc 0.96875, learning_rate 0.000100752
2017-10-10T11:47:04.338429: step 2149, loss 0.244481, acc 0.9375, learning_rate 0.000100749
2017-10-10T11:47:04.596331: step 2150, loss 0.166082, acc 0.9375, learning_rate 0.000100746
2017-10-10T11:47:04.897904: step 2151, loss 0.104441, acc 0.96875, learning_rate 0.000100743
2017-10-10T11:47:05.150961: step 2152, loss 0.195751, acc 0.953125, learning_rate 0.00010074
2017-10-10T11:47:05.400811: step 2153, loss 0.126489, acc 0.953125, learning_rate 0.000100737
2017-10-10T11:47:05.652959: step 2154, loss 0.131535, acc 0.953125, learning_rate 0.000100734
2017-10-10T11:47:05.917822: step 2155, loss 0.157241, acc 0.96875, learning_rate 0.000100731
2017-10-10T11:47:06.136839: step 2156, loss 0.123508, acc 0.960784, learning_rate 0.000100728
2017-10-10T11:47:06.371757: step 2157, loss 0.108664, acc 0.953125, learning_rate 0.000100725
2017-10-10T11:47:06.634739: step 2158, loss 0.167406, acc 0.953125, learning_rate 0.000100722
2017-10-10T11:47:06.893452: step 2159, loss 0.0863843, acc 0.984375, learning_rate 0.000100719
2017-10-10T11:47:07.143508: step 2160, loss 0.187642, acc 0.953125, learning_rate 0.000100716

Evaluation:
2017-10-10T11:47:07.745734: step 2160, loss 0.213627, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2160

2017-10-10T11:47:08.913164: step 2161, loss 0.143959, acc 0.953125, learning_rate 0.000100713
2017-10-10T11:47:09.149680: step 2162, loss 0.121872, acc 0.9375, learning_rate 0.000100711
2017-10-10T11:47:09.360862: step 2163, loss 0.0314131, acc 1, learning_rate 0.000100708
2017-10-10T11:47:09.578784: step 2164, loss 0.189887, acc 0.921875, learning_rate 0.000100705
2017-10-10T11:47:09.836913: step 2165, loss 0.105619, acc 0.953125, learning_rate 0.000100702
2017-10-10T11:47:10.087278: step 2166, loss 0.126898, acc 0.96875, learning_rate 0.000100699
2017-10-10T11:47:10.308813: step 2167, loss 0.128123, acc 0.953125, learning_rate 0.000100696
2017-10-10T11:47:10.557471: step 2168, loss 0.120382, acc 0.984375, learning_rate 0.000100693
2017-10-10T11:47:10.784000: step 2169, loss 0.0687178, acc 0.984375, learning_rate 0.00010069
2017-10-10T11:47:11.029889: step 2170, loss 0.163937, acc 0.953125, learning_rate 0.000100688
2017-10-10T11:47:11.329001: step 2171, loss 0.128336, acc 0.96875, learning_rate 0.000100685
2017-10-10T11:47:11.634681: step 2172, loss 0.196143, acc 0.921875, learning_rate 0.000100682
2017-10-10T11:47:11.829655: step 2173, loss 0.076122, acc 0.984375, learning_rate 0.000100679
2017-10-10T11:47:12.028476: step 2174, loss 0.161463, acc 0.9375, learning_rate 0.000100677
2017-10-10T11:47:12.219945: step 2175, loss 0.137562, acc 0.953125, learning_rate 0.000100674
2017-10-10T11:47:12.411310: step 2176, loss 0.0839732, acc 0.96875, learning_rate 0.000100671
2017-10-10T11:47:12.593700: step 2177, loss 0.0544188, acc 0.984375, learning_rate 0.000100668
2017-10-10T11:47:12.859549: step 2178, loss 0.127692, acc 0.953125, learning_rate 0.000100666
2017-10-10T11:47:13.109037: step 2179, loss 0.291916, acc 0.875, learning_rate 0.000100663
2017-10-10T11:47:13.374519: step 2180, loss 0.116727, acc 0.96875, learning_rate 0.00010066
2017-10-10T11:47:13.616426: step 2181, loss 0.240786, acc 0.9375, learning_rate 0.000100657
2017-10-10T11:47:13.842347: step 2182, loss 0.187852, acc 0.90625, learning_rate 0.000100655
2017-10-10T11:47:14.071451: step 2183, loss 0.111342, acc 0.953125, learning_rate 0.000100652
2017-10-10T11:47:14.336865: step 2184, loss 0.114448, acc 0.984375, learning_rate 0.000100649
2017-10-10T11:47:14.588882: step 2185, loss 0.0317302, acc 1, learning_rate 0.000100647
2017-10-10T11:47:14.860859: step 2186, loss 0.129942, acc 0.953125, learning_rate 0.000100644
2017-10-10T11:47:15.116515: step 2187, loss 0.098429, acc 0.96875, learning_rate 0.000100641
2017-10-10T11:47:15.333852: step 2188, loss 0.258008, acc 0.890625, learning_rate 0.000100639
2017-10-10T11:47:15.589112: step 2189, loss 0.0515662, acc 0.96875, learning_rate 0.000100636
2017-10-10T11:47:15.821062: step 2190, loss 0.0631974, acc 0.984375, learning_rate 0.000100634
2017-10-10T11:47:16.068824: step 2191, loss 0.12814, acc 0.96875, learning_rate 0.000100631
2017-10-10T11:47:16.343011: step 2192, loss 0.110457, acc 0.96875, learning_rate 0.000100628
2017-10-10T11:47:16.586797: step 2193, loss 0.227349, acc 0.921875, learning_rate 0.000100626
2017-10-10T11:47:16.785728: step 2194, loss 0.0909738, acc 0.96875, learning_rate 0.000100623
2017-10-10T11:47:16.998778: step 2195, loss 0.061776, acc 0.96875, learning_rate 0.000100621
2017-10-10T11:47:17.204542: step 2196, loss 0.0942306, acc 0.984375, learning_rate 0.000100618
2017-10-10T11:47:17.414253: step 2197, loss 0.0413994, acc 1, learning_rate 0.000100616
2017-10-10T11:47:17.624491: step 2198, loss 0.0617262, acc 0.984375, learning_rate 0.000100613
2017-10-10T11:47:17.903297: step 2199, loss 0.128253, acc 0.96875, learning_rate 0.000100611
2017-10-10T11:47:18.157137: step 2200, loss 0.196291, acc 0.90625, learning_rate 0.000100608

Evaluation:
2017-10-10T11:47:18.727247: step 2200, loss 0.212673, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2200

2017-10-10T11:47:19.827563: step 2201, loss 0.137604, acc 0.984375, learning_rate 0.000100606
2017-10-10T11:47:20.080862: step 2202, loss 0.134172, acc 0.96875, learning_rate 0.000100603
2017-10-10T11:47:20.320824: step 2203, loss 0.0714947, acc 0.984375, learning_rate 0.000100601
2017-10-10T11:47:20.536988: step 2204, loss 0.0516187, acc 0.984375, learning_rate 0.000100598
2017-10-10T11:47:20.800835: step 2205, loss 0.0673225, acc 0.984375, learning_rate 0.000100596
2017-10-10T11:47:21.058049: step 2206, loss 0.107332, acc 0.953125, learning_rate 0.000100594
2017-10-10T11:47:21.307269: step 2207, loss 0.121142, acc 0.96875, learning_rate 0.000100591
2017-10-10T11:47:21.553843: step 2208, loss 0.0599241, acc 1, learning_rate 0.000100589
2017-10-10T11:47:21.855446: step 2209, loss 0.193979, acc 0.921875, learning_rate 0.000100586
2017-10-10T11:47:22.132890: step 2210, loss 0.0976997, acc 0.984375, learning_rate 0.000100584
2017-10-10T11:47:22.320045: step 2211, loss 0.0412231, acc 0.984375, learning_rate 0.000100581
2017-10-10T11:47:22.498531: step 2212, loss 0.0681256, acc 0.984375, learning_rate 0.000100579
2017-10-10T11:47:22.673374: step 2213, loss 0.160843, acc 0.96875, learning_rate 0.000100577
2017-10-10T11:47:22.869488: step 2214, loss 0.0600333, acc 0.984375, learning_rate 0.000100574
2017-10-10T11:47:23.044531: step 2215, loss 0.120185, acc 0.9375, learning_rate 0.000100572
2017-10-10T11:47:23.265621: step 2216, loss 0.0872082, acc 0.96875, learning_rate 0.00010057
2017-10-10T11:47:23.516939: step 2217, loss 0.19602, acc 0.9375, learning_rate 0.000100567
2017-10-10T11:47:23.788797: step 2218, loss 0.202699, acc 0.953125, learning_rate 0.000100565
2017-10-10T11:47:24.053254: step 2219, loss 0.108576, acc 0.953125, learning_rate 0.000100563
2017-10-10T11:47:24.304876: step 2220, loss 0.104265, acc 0.96875, learning_rate 0.00010056
2017-10-10T11:47:24.565001: step 2221, loss 0.0720531, acc 0.984375, learning_rate 0.000100558
2017-10-10T11:47:24.889076: step 2222, loss 0.161861, acc 0.9375, learning_rate 0.000100556
2017-10-10T11:47:25.081331: step 2223, loss 0.175722, acc 0.9375, learning_rate 0.000100554
2017-10-10T11:47:25.249187: step 2224, loss 0.0981825, acc 0.953125, learning_rate 0.000100551
2017-10-10T11:47:25.423952: step 2225, loss 0.106927, acc 0.96875, learning_rate 0.000100549
2017-10-10T11:47:25.609512: step 2226, loss 0.157767, acc 0.921875, learning_rate 0.000100547
2017-10-10T11:47:25.844450: step 2227, loss 0.113526, acc 0.953125, learning_rate 0.000100545
2017-10-10T11:47:26.058044: step 2228, loss 0.117309, acc 0.953125, learning_rate 0.000100542
2017-10-10T11:47:26.243116: step 2229, loss 0.13929, acc 0.96875, learning_rate 0.00010054
2017-10-10T11:47:26.441496: step 2230, loss 0.212588, acc 0.921875, learning_rate 0.000100538
2017-10-10T11:47:26.636876: step 2231, loss 0.0991126, acc 0.9375, learning_rate 0.000100536
2017-10-10T11:47:26.908403: step 2232, loss 0.0608732, acc 1, learning_rate 0.000100534
2017-10-10T11:47:27.135887: step 2233, loss 0.0913319, acc 0.96875, learning_rate 0.000100531
2017-10-10T11:47:27.381910: step 2234, loss 0.2228, acc 0.890625, learning_rate 0.000100529
2017-10-10T11:47:27.643162: step 2235, loss 0.0963092, acc 0.96875, learning_rate 0.000100527
2017-10-10T11:47:27.887358: step 2236, loss 0.164011, acc 0.9375, learning_rate 0.000100525
2017-10-10T11:47:28.134558: step 2237, loss 0.106342, acc 0.953125, learning_rate 0.000100523
2017-10-10T11:47:28.381698: step 2238, loss 0.181924, acc 0.921875, learning_rate 0.000100521
2017-10-10T11:47:28.620175: step 2239, loss 0.148715, acc 0.921875, learning_rate 0.000100519
2017-10-10T11:47:28.872665: step 2240, loss 0.0796613, acc 0.984375, learning_rate 0.000100516

Evaluation:
2017-10-10T11:47:29.438394: step 2240, loss 0.213604, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2240

2017-10-10T11:47:30.696842: step 2241, loss 0.140599, acc 0.9375, learning_rate 0.000100514
2017-10-10T11:47:30.963965: step 2242, loss 0.0964701, acc 0.96875, learning_rate 0.000100512
2017-10-10T11:47:31.226567: step 2243, loss 0.176784, acc 0.921875, learning_rate 0.00010051
2017-10-10T11:47:31.495090: step 2244, loss 0.0620677, acc 0.984375, learning_rate 0.000100508
2017-10-10T11:47:31.738997: step 2245, loss 0.139377, acc 0.9375, learning_rate 0.000100506
2017-10-10T11:47:31.996218: step 2246, loss 0.0999579, acc 1, learning_rate 0.000100504
2017-10-10T11:47:32.242208: step 2247, loss 0.133162, acc 0.9375, learning_rate 0.000100502
2017-10-10T11:47:32.572867: step 2248, loss 0.163043, acc 0.953125, learning_rate 0.0001005
2017-10-10T11:47:32.838124: step 2249, loss 0.0865735, acc 1, learning_rate 0.000100498
2017-10-10T11:47:33.045612: step 2250, loss 0.172144, acc 0.953125, learning_rate 0.000100496
2017-10-10T11:47:33.228549: step 2251, loss 0.176737, acc 0.921875, learning_rate 0.000100494
2017-10-10T11:47:33.430154: step 2252, loss 0.11475, acc 0.953125, learning_rate 0.000100492
2017-10-10T11:47:33.716139: step 2253, loss 0.0842406, acc 0.96875, learning_rate 0.00010049
2017-10-10T11:47:33.878713: step 2254, loss 0.189507, acc 0.960784, learning_rate 0.000100488
2017-10-10T11:47:34.078069: step 2255, loss 0.192691, acc 0.921875, learning_rate 0.000100486
2017-10-10T11:47:34.280837: step 2256, loss 0.133707, acc 0.96875, learning_rate 0.000100484
2017-10-10T11:47:34.477537: step 2257, loss 0.15218, acc 0.921875, learning_rate 0.000100482
2017-10-10T11:47:34.666253: step 2258, loss 0.0689579, acc 1, learning_rate 0.00010048
2017-10-10T11:47:34.945099: step 2259, loss 0.0723443, acc 0.984375, learning_rate 0.000100478
2017-10-10T11:47:35.180942: step 2260, loss 0.0483747, acc 1, learning_rate 0.000100476
2017-10-10T11:47:35.464902: step 2261, loss 0.0376318, acc 0.984375, learning_rate 0.000100474
2017-10-10T11:47:35.737984: step 2262, loss 0.0938803, acc 0.953125, learning_rate 0.000100472
2017-10-10T11:47:35.999476: step 2263, loss 0.166677, acc 0.921875, learning_rate 0.00010047
2017-10-10T11:47:36.231351: step 2264, loss 0.0888183, acc 0.96875, learning_rate 0.000100468
2017-10-10T11:47:36.486745: step 2265, loss 0.123537, acc 0.921875, learning_rate 0.000100466
2017-10-10T11:47:36.737492: step 2266, loss 0.136726, acc 0.953125, learning_rate 0.000100464
2017-10-10T11:47:37.020492: step 2267, loss 0.11013, acc 0.953125, learning_rate 0.000100462
2017-10-10T11:47:37.256723: step 2268, loss 0.0704659, acc 0.984375, learning_rate 0.000100461
2017-10-10T11:47:37.491924: step 2269, loss 0.10983, acc 0.96875, learning_rate 0.000100459
2017-10-10T11:47:37.735177: step 2270, loss 0.182314, acc 0.9375, learning_rate 0.000100457
2017-10-10T11:47:37.987255: step 2271, loss 0.146295, acc 0.9375, learning_rate 0.000100455
2017-10-10T11:47:38.236953: step 2272, loss 0.0709585, acc 1, learning_rate 0.000100453
2017-10-10T11:47:38.496925: step 2273, loss 0.181932, acc 0.90625, learning_rate 0.000100451
2017-10-10T11:47:38.757875: step 2274, loss 0.180721, acc 0.921875, learning_rate 0.000100449
2017-10-10T11:47:39.024839: step 2275, loss 0.126486, acc 0.96875, learning_rate 0.000100448
2017-10-10T11:47:39.280465: step 2276, loss 0.118202, acc 0.953125, learning_rate 0.000100446
2017-10-10T11:47:39.509581: step 2277, loss 0.206529, acc 0.9375, learning_rate 0.000100444
2017-10-10T11:47:39.760905: step 2278, loss 0.185252, acc 0.953125, learning_rate 0.000100442
2017-10-10T11:47:40.021166: step 2279, loss 0.153266, acc 0.953125, learning_rate 0.00010044
2017-10-10T11:47:40.302654: step 2280, loss 0.259494, acc 0.953125, learning_rate 0.000100439

Evaluation:
2017-10-10T11:47:40.900888: step 2280, loss 0.212706, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2280

2017-10-10T11:47:42.210365: step 2281, loss 0.122108, acc 0.96875, learning_rate 0.000100437
2017-10-10T11:47:42.454527: step 2282, loss 0.180118, acc 0.921875, learning_rate 0.000100435
2017-10-10T11:47:42.678850: step 2283, loss 0.0725563, acc 0.984375, learning_rate 0.000100433
2017-10-10T11:47:42.924470: step 2284, loss 0.0468853, acc 0.984375, learning_rate 0.000100431
2017-10-10T11:47:43.109537: step 2285, loss 0.181548, acc 0.921875, learning_rate 0.00010043
2017-10-10T11:47:43.293478: step 2286, loss 0.152225, acc 0.921875, learning_rate 0.000100428
2017-10-10T11:47:43.500470: step 2287, loss 0.0746452, acc 0.984375, learning_rate 0.000100426
2017-10-10T11:47:43.684904: step 2288, loss 0.0977722, acc 0.984375, learning_rate 0.000100424
2017-10-10T11:47:43.929148: step 2289, loss 0.180206, acc 0.9375, learning_rate 0.000100423
2017-10-10T11:47:44.206697: step 2290, loss 0.0904468, acc 0.96875, learning_rate 0.000100421
2017-10-10T11:47:44.462055: step 2291, loss 0.191282, acc 0.9375, learning_rate 0.000100419
2017-10-10T11:47:44.742915: step 2292, loss 0.059376, acc 0.96875, learning_rate 0.000100418
2017-10-10T11:47:44.975972: step 2293, loss 0.179314, acc 0.875, learning_rate 0.000100416
2017-10-10T11:47:45.236110: step 2294, loss 0.0973205, acc 0.984375, learning_rate 0.000100414
2017-10-10T11:47:45.457119: step 2295, loss 0.0459352, acc 0.984375, learning_rate 0.000100412
2017-10-10T11:47:45.703266: step 2296, loss 0.0850545, acc 0.96875, learning_rate 0.000100411
2017-10-10T11:47:45.934685: step 2297, loss 0.160468, acc 0.984375, learning_rate 0.000100409
2017-10-10T11:47:46.159785: step 2298, loss 0.159744, acc 0.921875, learning_rate 0.000100407
2017-10-10T11:47:46.438984: step 2299, loss 0.158064, acc 0.9375, learning_rate 0.000100406
2017-10-10T11:47:46.676312: step 2300, loss 0.137854, acc 0.953125, learning_rate 0.000100404
2017-10-10T11:47:46.950631: step 2301, loss 0.139836, acc 0.9375, learning_rate 0.000100402
2017-10-10T11:47:47.232604: step 2302, loss 0.231217, acc 0.9375, learning_rate 0.000100401
2017-10-10T11:47:47.475207: step 2303, loss 0.102523, acc 0.984375, learning_rate 0.000100399
2017-10-10T11:47:47.663095: step 2304, loss 0.0612778, acc 0.984375, learning_rate 0.000100398
2017-10-10T11:47:47.896876: step 2305, loss 0.0919663, acc 0.9375, learning_rate 0.000100396
2017-10-10T11:47:48.146336: step 2306, loss 0.142656, acc 0.953125, learning_rate 0.000100394
2017-10-10T11:47:48.414635: step 2307, loss 0.126062, acc 0.9375, learning_rate 0.000100393
2017-10-10T11:47:48.684896: step 2308, loss 0.14364, acc 0.96875, learning_rate 0.000100391
2017-10-10T11:47:48.939314: step 2309, loss 0.179024, acc 0.921875, learning_rate 0.000100389
2017-10-10T11:47:49.149906: step 2310, loss 0.0578028, acc 0.984375, learning_rate 0.000100388
2017-10-10T11:47:49.412187: step 2311, loss 0.162829, acc 0.9375, learning_rate 0.000100386
2017-10-10T11:47:49.665451: step 2312, loss 0.0775935, acc 0.953125, learning_rate 0.000100385
2017-10-10T11:47:49.933899: step 2313, loss 0.0941108, acc 0.984375, learning_rate 0.000100383
2017-10-10T11:47:50.178438: step 2314, loss 0.111502, acc 0.96875, learning_rate 0.000100382
2017-10-10T11:47:50.531134: step 2315, loss 0.096735, acc 0.953125, learning_rate 0.00010038
2017-10-10T11:47:50.732045: step 2316, loss 0.102078, acc 0.96875, learning_rate 0.000100378
2017-10-10T11:47:50.915009: step 2317, loss 0.0624764, acc 0.984375, learning_rate 0.000100377
2017-10-10T11:47:51.124813: step 2318, loss 0.208149, acc 0.921875, learning_rate 0.000100375
2017-10-10T11:47:51.292825: step 2319, loss 0.132706, acc 0.953125, learning_rate 0.000100374
2017-10-10T11:47:51.520877: step 2320, loss 0.130087, acc 0.96875, learning_rate 0.000100372

Evaluation:
2017-10-10T11:47:52.091644: step 2320, loss 0.212388, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2320

2017-10-10T11:47:53.367402: step 2321, loss 0.150947, acc 0.984375, learning_rate 0.000100371
2017-10-10T11:47:53.565792: step 2322, loss 0.115127, acc 0.96875, learning_rate 0.000100369
2017-10-10T11:47:53.748844: step 2323, loss 0.163451, acc 0.90625, learning_rate 0.000100368
2017-10-10T11:47:53.953163: step 2324, loss 0.0549915, acc 0.984375, learning_rate 0.000100366
2017-10-10T11:47:54.188731: step 2325, loss 0.187252, acc 0.9375, learning_rate 0.000100365
2017-10-10T11:47:54.458991: step 2326, loss 0.0946648, acc 0.953125, learning_rate 0.000100363
2017-10-10T11:47:54.693280: step 2327, loss 0.0785753, acc 0.984375, learning_rate 0.000100362
2017-10-10T11:47:54.930920: step 2328, loss 0.0898704, acc 0.953125, learning_rate 0.00010036
2017-10-10T11:47:55.174387: step 2329, loss 0.0810801, acc 0.96875, learning_rate 0.000100359
2017-10-10T11:47:55.442053: step 2330, loss 0.132081, acc 0.953125, learning_rate 0.000100357
2017-10-10T11:47:55.672167: step 2331, loss 0.0529656, acc 0.984375, learning_rate 0.000100356
2017-10-10T11:47:55.929233: step 2332, loss 0.0433048, acc 0.984375, learning_rate 0.000100354
2017-10-10T11:47:56.168901: step 2333, loss 0.188507, acc 0.921875, learning_rate 0.000100353
2017-10-10T11:47:56.404825: step 2334, loss 0.191489, acc 0.9375, learning_rate 0.000100352
2017-10-10T11:47:56.620949: step 2335, loss 0.061809, acc 0.984375, learning_rate 0.00010035
2017-10-10T11:47:56.860904: step 2336, loss 0.188516, acc 0.96875, learning_rate 0.000100349
2017-10-10T11:47:57.100822: step 2337, loss 0.199668, acc 0.921875, learning_rate 0.000100347
2017-10-10T11:47:57.316885: step 2338, loss 0.215882, acc 0.9375, learning_rate 0.000100346
2017-10-10T11:47:57.550746: step 2339, loss 0.0811686, acc 0.953125, learning_rate 0.000100344
2017-10-10T11:47:57.752373: step 2340, loss 0.147914, acc 0.953125, learning_rate 0.000100343
2017-10-10T11:47:57.995346: step 2341, loss 0.0616883, acc 0.984375, learning_rate 0.000100342
2017-10-10T11:47:58.233380: step 2342, loss 0.0509491, acc 0.984375, learning_rate 0.00010034
2017-10-10T11:47:58.470589: step 2343, loss 0.0486499, acc 1, learning_rate 0.000100339
2017-10-10T11:47:58.764888: step 2344, loss 0.116813, acc 0.953125, learning_rate 0.000100338
2017-10-10T11:47:59.000189: step 2345, loss 0.173617, acc 0.953125, learning_rate 0.000100336
2017-10-10T11:47:59.187765: step 2346, loss 0.121788, acc 0.96875, learning_rate 0.000100335
2017-10-10T11:47:59.395663: step 2347, loss 0.0928234, acc 0.96875, learning_rate 0.000100333
2017-10-10T11:47:59.600156: step 2348, loss 0.0723771, acc 0.96875, learning_rate 0.000100332
2017-10-10T11:47:59.868930: step 2349, loss 0.184986, acc 0.953125, learning_rate 0.000100331
2017-10-10T11:48:00.128899: step 2350, loss 0.0963269, acc 0.96875, learning_rate 0.000100329
2017-10-10T11:48:00.366662: step 2351, loss 0.161557, acc 0.9375, learning_rate 0.000100328
2017-10-10T11:48:00.565119: step 2352, loss 0.0473025, acc 1, learning_rate 0.000100327
2017-10-10T11:48:00.820884: step 2353, loss 0.127468, acc 0.9375, learning_rate 0.000100325
2017-10-10T11:48:01.064541: step 2354, loss 0.163148, acc 0.96875, learning_rate 0.000100324
2017-10-10T11:48:01.320625: step 2355, loss 0.121822, acc 0.96875, learning_rate 0.000100323
2017-10-10T11:48:01.578636: step 2356, loss 0.144318, acc 0.9375, learning_rate 0.000100321
2017-10-10T11:48:01.806367: step 2357, loss 0.0941958, acc 0.96875, learning_rate 0.00010032
2017-10-10T11:48:02.046433: step 2358, loss 0.166372, acc 0.953125, learning_rate 0.000100319
2017-10-10T11:48:02.307986: step 2359, loss 0.168846, acc 0.9375, learning_rate 0.000100317
2017-10-10T11:48:02.540867: step 2360, loss 0.047061, acc 1, learning_rate 0.000100316

Evaluation:
2017-10-10T11:48:03.224844: step 2360, loss 0.211216, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2360

2017-10-10T11:48:04.245856: step 2361, loss 0.0963427, acc 0.953125, learning_rate 0.000100315
2017-10-10T11:48:04.452778: step 2362, loss 0.212685, acc 0.9375, learning_rate 0.000100314
2017-10-10T11:48:04.672188: step 2363, loss 0.161886, acc 0.9375, learning_rate 0.000100312
2017-10-10T11:48:04.895139: step 2364, loss 0.218664, acc 0.921875, learning_rate 0.000100311
2017-10-10T11:48:05.127819: step 2365, loss 0.11308, acc 0.96875, learning_rate 0.00010031
2017-10-10T11:48:05.365414: step 2366, loss 0.113679, acc 0.96875, learning_rate 0.000100308
2017-10-10T11:48:05.599514: step 2367, loss 0.108624, acc 0.984375, learning_rate 0.000100307
2017-10-10T11:48:05.868415: step 2368, loss 0.10746, acc 0.96875, learning_rate 0.000100306
2017-10-10T11:48:06.122682: step 2369, loss 0.117668, acc 0.9375, learning_rate 0.000100305
2017-10-10T11:48:06.348842: step 2370, loss 0.0782722, acc 0.984375, learning_rate 0.000100303
2017-10-10T11:48:06.608690: step 2371, loss 0.0629579, acc 1, learning_rate 0.000100302
2017-10-10T11:48:06.880959: step 2372, loss 0.040033, acc 1, learning_rate 0.000100301
2017-10-10T11:48:07.096829: step 2373, loss 0.130826, acc 0.9375, learning_rate 0.0001003
2017-10-10T11:48:07.322442: step 2374, loss 0.034591, acc 1, learning_rate 0.000100299
2017-10-10T11:48:07.541135: step 2375, loss 0.212466, acc 0.9375, learning_rate 0.000100297
2017-10-10T11:48:07.733041: step 2376, loss 0.108321, acc 0.96875, learning_rate 0.000100296
2017-10-10T11:48:07.980535: step 2377, loss 0.074426, acc 0.984375, learning_rate 0.000100295
2017-10-10T11:48:08.193086: step 2378, loss 0.107434, acc 0.96875, learning_rate 0.000100294
2017-10-10T11:48:08.433358: step 2379, loss 0.18091, acc 0.90625, learning_rate 0.000100292
2017-10-10T11:48:08.671812: step 2380, loss 0.110394, acc 0.953125, learning_rate 0.000100291
2017-10-10T11:48:08.926406: step 2381, loss 0.131241, acc 0.9375, learning_rate 0.00010029
2017-10-10T11:48:09.148339: step 2382, loss 0.052517, acc 1, learning_rate 0.000100289
2017-10-10T11:48:09.408858: step 2383, loss 0.175079, acc 0.921875, learning_rate 0.000100288
2017-10-10T11:48:09.651959: step 2384, loss 0.118891, acc 0.9375, learning_rate 0.000100287
2017-10-10T11:48:09.863257: step 2385, loss 0.0502629, acc 1, learning_rate 0.000100285
2017-10-10T11:48:10.113566: step 2386, loss 0.0695282, acc 0.96875, learning_rate 0.000100284
2017-10-10T11:48:10.380878: step 2387, loss 0.174139, acc 0.9375, learning_rate 0.000100283
2017-10-10T11:48:10.617387: step 2388, loss 0.10084, acc 0.984375, learning_rate 0.000100282
2017-10-10T11:48:10.860951: step 2389, loss 0.0632023, acc 0.984375, learning_rate 0.000100281
2017-10-10T11:48:11.109871: step 2390, loss 0.222554, acc 0.921875, learning_rate 0.00010028
2017-10-10T11:48:11.384143: step 2391, loss 0.11124, acc 0.96875, learning_rate 0.000100278
2017-10-10T11:48:11.612828: step 2392, loss 0.109382, acc 0.921875, learning_rate 0.000100277
2017-10-10T11:48:11.854667: step 2393, loss 0.0860353, acc 0.96875, learning_rate 0.000100276
2017-10-10T11:48:12.115389: step 2394, loss 0.103136, acc 0.953125, learning_rate 0.000100275
2017-10-10T11:48:12.349251: step 2395, loss 0.189595, acc 0.953125, learning_rate 0.000100274
2017-10-10T11:48:12.572856: step 2396, loss 0.0631562, acc 1, learning_rate 0.000100273
2017-10-10T11:48:12.808900: step 2397, loss 0.118856, acc 0.953125, learning_rate 0.000100272
2017-10-10T11:48:13.034023: step 2398, loss 0.108467, acc 0.953125, learning_rate 0.000100271
2017-10-10T11:48:13.292846: step 2399, loss 0.102657, acc 0.96875, learning_rate 0.00010027
2017-10-10T11:48:13.499472: step 2400, loss 0.108146, acc 0.953125, learning_rate 0.000100268

Evaluation:
2017-10-10T11:48:37.235682: step 2400, loss 0.211609, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2400

2017-10-10T11:48:45.260269: step 2401, loss 0.100509, acc 0.96875, learning_rate 0.000100267
2017-10-10T11:48:45.540822: step 2402, loss 0.162461, acc 0.9375, learning_rate 0.000100266
2017-10-10T11:48:45.800807: step 2403, loss 0.0955134, acc 0.96875, learning_rate 0.000100265
2017-10-10T11:48:46.042368: step 2404, loss 0.236268, acc 0.90625, learning_rate 0.000100264
2017-10-10T11:48:46.291044: step 2405, loss 0.038483, acc 1, learning_rate 0.000100263
2017-10-10T11:48:46.531234: step 2406, loss 0.127404, acc 0.96875, learning_rate 0.000100262
2017-10-10T11:48:46.784879: step 2407, loss 0.127003, acc 0.953125, learning_rate 0.000100261
2017-10-10T11:48:47.051206: step 2408, loss 0.18946, acc 0.9375, learning_rate 0.00010026
2017-10-10T11:48:47.313798: step 2409, loss 0.152831, acc 0.984375, learning_rate 0.000100259
2017-10-10T11:48:47.582446: step 2410, loss 0.1218, acc 0.953125, learning_rate 0.000100258
2017-10-10T11:48:47.829767: step 2411, loss 0.240412, acc 0.875, learning_rate 0.000100257
2017-10-10T11:48:48.096424: step 2412, loss 0.124648, acc 0.953125, learning_rate 0.000100256
2017-10-10T11:48:48.320639: step 2413, loss 0.197143, acc 0.921875, learning_rate 0.000100255
2017-10-10T11:48:48.571417: step 2414, loss 0.137487, acc 0.953125, learning_rate 0.000100253
2017-10-10T11:48:48.880528: step 2415, loss 0.0621361, acc 0.984375, learning_rate 0.000100252
2017-10-10T11:48:49.122313: step 2416, loss 0.216209, acc 0.921875, learning_rate 0.000100251
2017-10-10T11:48:49.375195: step 2417, loss 0.104398, acc 0.953125, learning_rate 0.00010025
2017-10-10T11:48:49.638315: step 2418, loss 0.0611941, acc 0.984375, learning_rate 0.000100249
2017-10-10T11:48:49.892056: step 2419, loss 0.266848, acc 0.9375, learning_rate 0.000100248
2017-10-10T11:48:50.169403: step 2420, loss 0.10566, acc 0.953125, learning_rate 0.000100247
2017-10-10T11:48:50.410197: step 2421, loss 0.0428619, acc 0.984375, learning_rate 0.000100246
2017-10-10T11:48:50.664928: step 2422, loss 0.140976, acc 0.921875, learning_rate 0.000100245
2017-10-10T11:48:50.908858: step 2423, loss 0.130681, acc 0.953125, learning_rate 0.000100244
2017-10-10T11:48:51.152835: step 2424, loss 0.129083, acc 0.984375, learning_rate 0.000100243
2017-10-10T11:48:51.396893: step 2425, loss 0.140549, acc 0.96875, learning_rate 0.000100242
2017-10-10T11:48:51.655455: step 2426, loss 0.0588373, acc 0.96875, learning_rate 0.000100241
2017-10-10T11:48:51.887481: step 2427, loss 0.164392, acc 0.921875, learning_rate 0.00010024
2017-10-10T11:48:52.121211: step 2428, loss 0.115992, acc 0.90625, learning_rate 0.000100239
2017-10-10T11:48:52.366109: step 2429, loss 0.225285, acc 0.890625, learning_rate 0.000100238
2017-10-10T11:48:52.690968: step 2430, loss 0.0404579, acc 1, learning_rate 0.000100237
2017-10-10T11:48:52.904770: step 2431, loss 0.0676537, acc 0.984375, learning_rate 0.000100236
2017-10-10T11:48:53.112380: step 2432, loss 0.0449409, acc 1, learning_rate 0.000100235
2017-10-10T11:48:53.317191: step 2433, loss 0.0404108, acc 0.984375, learning_rate 0.000100235
2017-10-10T11:48:53.531889: step 2434, loss 0.09411, acc 0.96875, learning_rate 0.000100234
2017-10-10T11:48:53.783196: step 2435, loss 0.124277, acc 0.953125, learning_rate 0.000100233
2017-10-10T11:48:54.006236: step 2436, loss 0.0546204, acc 0.984375, learning_rate 0.000100232
2017-10-10T11:48:54.187735: step 2437, loss 0.163395, acc 0.953125, learning_rate 0.000100231
2017-10-10T11:48:54.408833: step 2438, loss 0.151141, acc 0.9375, learning_rate 0.00010023
2017-10-10T11:48:54.704953: step 2439, loss 0.0843439, acc 0.984375, learning_rate 0.000100229
2017-10-10T11:48:54.956848: step 2440, loss 0.196454, acc 0.921875, learning_rate 0.000100228

Evaluation:
2017-10-10T11:48:55.397050: step 2440, loss 0.214981, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2440

2017-10-10T11:48:56.583390: step 2441, loss 0.105141, acc 0.96875, learning_rate 0.000100227
2017-10-10T11:48:56.843597: step 2442, loss 0.203796, acc 0.90625, learning_rate 0.000100226
2017-10-10T11:48:57.098717: step 2443, loss 0.133234, acc 0.96875, learning_rate 0.000100225
2017-10-10T11:48:57.345085: step 2444, loss 0.156296, acc 0.953125, learning_rate 0.000100224
2017-10-10T11:48:57.614307: step 2445, loss 0.150544, acc 0.9375, learning_rate 0.000100223
2017-10-10T11:48:57.861979: step 2446, loss 0.0733722, acc 0.984375, learning_rate 0.000100222
2017-10-10T11:48:58.127470: step 2447, loss 0.0996849, acc 0.96875, learning_rate 0.000100221
2017-10-10T11:48:58.418999: step 2448, loss 0.0922044, acc 0.984375, learning_rate 0.000100221
2017-10-10T11:48:58.732315: step 2449, loss 0.137667, acc 0.9375, learning_rate 0.00010022
2017-10-10T11:48:58.932874: step 2450, loss 0.15569, acc 0.960784, learning_rate 0.000100219
2017-10-10T11:48:59.200842: step 2451, loss 0.0956405, acc 0.96875, learning_rate 0.000100218
2017-10-10T11:48:59.441554: step 2452, loss 0.0573894, acc 0.984375, learning_rate 0.000100217
2017-10-10T11:48:59.667581: step 2453, loss 0.152535, acc 0.9375, learning_rate 0.000100216
2017-10-10T11:48:59.900803: step 2454, loss 0.088052, acc 0.984375, learning_rate 0.000100215
2017-10-10T11:49:00.168851: step 2455, loss 0.0700201, acc 1, learning_rate 0.000100214
2017-10-10T11:49:00.430182: step 2456, loss 0.0722614, acc 1, learning_rate 0.000100213
2017-10-10T11:49:00.648575: step 2457, loss 0.206009, acc 0.96875, learning_rate 0.000100213
2017-10-10T11:49:00.885791: step 2458, loss 0.0789617, acc 0.984375, learning_rate 0.000100212
2017-10-10T11:49:01.196906: step 2459, loss 0.115172, acc 0.96875, learning_rate 0.000100211
2017-10-10T11:49:01.447270: step 2460, loss 0.119277, acc 0.96875, learning_rate 0.00010021
2017-10-10T11:49:01.654531: step 2461, loss 0.106874, acc 0.953125, learning_rate 0.000100209
2017-10-10T11:49:01.856167: step 2462, loss 0.18691, acc 0.921875, learning_rate 0.000100208
2017-10-10T11:49:02.084967: step 2463, loss 0.0591296, acc 0.984375, learning_rate 0.000100207
2017-10-10T11:49:02.277503: step 2464, loss 0.082051, acc 1, learning_rate 0.000100207
2017-10-10T11:49:02.502130: step 2465, loss 0.180106, acc 0.9375, learning_rate 0.000100206
2017-10-10T11:49:02.766852: step 2466, loss 0.0896593, acc 1, learning_rate 0.000100205
2017-10-10T11:49:02.988872: step 2467, loss 0.100488, acc 0.96875, learning_rate 0.000100204
2017-10-10T11:49:03.219479: step 2468, loss 0.0707454, acc 0.96875, learning_rate 0.000100203
2017-10-10T11:49:03.434703: step 2469, loss 0.106657, acc 0.984375, learning_rate 0.000100202
2017-10-10T11:49:03.712858: step 2470, loss 0.241534, acc 0.890625, learning_rate 0.000100202
2017-10-10T11:49:03.957130: step 2471, loss 0.0963227, acc 0.96875, learning_rate 0.000100201
2017-10-10T11:49:04.240790: step 2472, loss 0.132523, acc 0.9375, learning_rate 0.0001002
2017-10-10T11:49:04.442685: step 2473, loss 0.0806508, acc 0.96875, learning_rate 0.000100199
2017-10-10T11:49:04.695956: step 2474, loss 0.133443, acc 0.953125, learning_rate 0.000100198
2017-10-10T11:49:04.923848: step 2475, loss 0.121994, acc 0.96875, learning_rate 0.000100198
2017-10-10T11:49:05.176874: step 2476, loss 0.0767563, acc 0.96875, learning_rate 0.000100197
2017-10-10T11:49:05.513747: step 2477, loss 0.163222, acc 0.9375, learning_rate 0.000100196
2017-10-10T11:49:05.697540: step 2478, loss 0.0972937, acc 0.96875, learning_rate 0.000100195
2017-10-10T11:49:05.875063: step 2479, loss 0.0967623, acc 0.96875, learning_rate 0.000100194
2017-10-10T11:49:06.062776: step 2480, loss 0.0887273, acc 0.984375, learning_rate 0.000100194

Evaluation:
2017-10-10T11:49:06.493422: step 2480, loss 0.21181, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2480

2017-10-10T11:49:07.513672: step 2481, loss 0.224674, acc 0.921875, learning_rate 0.000100193
2017-10-10T11:49:07.774688: step 2482, loss 0.135131, acc 0.953125, learning_rate 0.000100192
2017-10-10T11:49:08.043904: step 2483, loss 0.144949, acc 0.9375, learning_rate 0.000100191
2017-10-10T11:49:08.278811: step 2484, loss 0.0411368, acc 1, learning_rate 0.00010019
2017-10-10T11:49:08.526784: step 2485, loss 0.145977, acc 0.96875, learning_rate 0.00010019
2017-10-10T11:49:08.776850: step 2486, loss 0.184433, acc 0.90625, learning_rate 0.000100189
2017-10-10T11:49:09.096911: step 2487, loss 0.168773, acc 0.90625, learning_rate 0.000100188
2017-10-10T11:49:09.344621: step 2488, loss 0.0806512, acc 0.984375, learning_rate 0.000100187
2017-10-10T11:49:09.636328: step 2489, loss 0.0875908, acc 0.96875, learning_rate 0.000100187
2017-10-10T11:49:09.882988: step 2490, loss 0.151159, acc 0.96875, learning_rate 0.000100186
2017-10-10T11:49:10.102959: step 2491, loss 0.0914766, acc 0.96875, learning_rate 0.000100185
2017-10-10T11:49:10.301283: step 2492, loss 0.127685, acc 0.953125, learning_rate 0.000100184
2017-10-10T11:49:10.511341: step 2493, loss 0.060392, acc 1, learning_rate 0.000100183
2017-10-10T11:49:10.756924: step 2494, loss 0.180546, acc 0.953125, learning_rate 0.000100183
2017-10-10T11:49:10.995025: step 2495, loss 0.148753, acc 0.921875, learning_rate 0.000100182
2017-10-10T11:49:11.221576: step 2496, loss 0.0462169, acc 0.984375, learning_rate 0.000100181
2017-10-10T11:49:11.441155: step 2497, loss 0.0477362, acc 0.984375, learning_rate 0.000100181
2017-10-10T11:49:11.684862: step 2498, loss 0.238112, acc 0.921875, learning_rate 0.00010018
2017-10-10T11:49:11.949874: step 2499, loss 0.118392, acc 0.96875, learning_rate 0.000100179
2017-10-10T11:49:12.212589: step 2500, loss 0.11369, acc 0.96875, learning_rate 0.000100178
2017-10-10T11:49:12.458218: step 2501, loss 0.14679, acc 0.96875, learning_rate 0.000100178
2017-10-10T11:49:12.700384: step 2502, loss 0.167675, acc 0.953125, learning_rate 0.000100177
2017-10-10T11:49:12.931034: step 2503, loss 0.138493, acc 0.9375, learning_rate 0.000100176
2017-10-10T11:49:13.164634: step 2504, loss 0.0930429, acc 0.953125, learning_rate 0.000100175
2017-10-10T11:49:13.440691: step 2505, loss 0.0549928, acc 0.984375, learning_rate 0.000100175
2017-10-10T11:49:13.681481: step 2506, loss 0.0822054, acc 0.96875, learning_rate 0.000100174
2017-10-10T11:49:13.921330: step 2507, loss 0.159955, acc 0.953125, learning_rate 0.000100173
2017-10-10T11:49:14.173431: step 2508, loss 0.257228, acc 0.90625, learning_rate 0.000100173
2017-10-10T11:49:14.409073: step 2509, loss 0.112864, acc 0.953125, learning_rate 0.000100172
2017-10-10T11:49:14.702799: step 2510, loss 0.0584772, acc 1, learning_rate 0.000100171
2017-10-10T11:49:14.934610: step 2511, loss 0.132973, acc 0.96875, learning_rate 0.00010017
2017-10-10T11:49:15.173899: step 2512, loss 0.102363, acc 0.953125, learning_rate 0.00010017
2017-10-10T11:49:15.432885: step 2513, loss 0.0849382, acc 0.984375, learning_rate 0.000100169
2017-10-10T11:49:15.652363: step 2514, loss 0.0777363, acc 0.984375, learning_rate 0.000100168
2017-10-10T11:49:15.956892: step 2515, loss 0.068749, acc 0.984375, learning_rate 0.000100168
2017-10-10T11:49:16.222800: step 2516, loss 0.108637, acc 0.96875, learning_rate 0.000100167
2017-10-10T11:49:16.422198: step 2517, loss 0.277196, acc 0.921875, learning_rate 0.000100166
2017-10-10T11:49:16.594899: step 2518, loss 0.0722417, acc 0.984375, learning_rate 0.000100166
2017-10-10T11:49:16.772832: step 2519, loss 0.102794, acc 0.96875, learning_rate 0.000100165
2017-10-10T11:49:16.976902: step 2520, loss 0.163615, acc 0.953125, learning_rate 0.000100164

Evaluation:
2017-10-10T11:49:17.477464: step 2520, loss 0.210512, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2520

2017-10-10T11:49:18.544983: step 2521, loss 0.0847798, acc 0.96875, learning_rate 0.000100164
2017-10-10T11:49:18.728413: step 2522, loss 0.148191, acc 0.96875, learning_rate 0.000100163
2017-10-10T11:49:18.924171: step 2523, loss 0.0698309, acc 0.96875, learning_rate 0.000100162
2017-10-10T11:49:19.141162: step 2524, loss 0.0481325, acc 1, learning_rate 0.000100162
2017-10-10T11:49:19.377055: step 2525, loss 0.120245, acc 0.953125, learning_rate 0.000100161
2017-10-10T11:49:19.628118: step 2526, loss 0.0936687, acc 0.96875, learning_rate 0.00010016
2017-10-10T11:49:19.946902: step 2527, loss 0.101579, acc 0.96875, learning_rate 0.00010016
2017-10-10T11:49:20.193119: step 2528, loss 0.04356, acc 0.984375, learning_rate 0.000100159
2017-10-10T11:49:20.463520: step 2529, loss 0.138087, acc 0.953125, learning_rate 0.000100158
2017-10-10T11:49:20.732779: step 2530, loss 0.105889, acc 0.984375, learning_rate 0.000100158
2017-10-10T11:49:20.997907: step 2531, loss 0.109355, acc 0.9375, learning_rate 0.000100157
2017-10-10T11:49:21.248675: step 2532, loss 0.0933765, acc 0.9375, learning_rate 0.000100156
2017-10-10T11:49:21.524242: step 2533, loss 0.0516641, acc 0.984375, learning_rate 0.000100156
2017-10-10T11:49:21.774679: step 2534, loss 0.0793595, acc 0.984375, learning_rate 0.000100155
2017-10-10T11:49:22.055013: step 2535, loss 0.269692, acc 0.890625, learning_rate 0.000100155
2017-10-10T11:49:22.280909: step 2536, loss 0.0990146, acc 0.953125, learning_rate 0.000100154
2017-10-10T11:49:22.536050: step 2537, loss 0.113915, acc 0.96875, learning_rate 0.000100153
2017-10-10T11:49:22.782354: step 2538, loss 0.0809276, acc 0.984375, learning_rate 0.000100153
2017-10-10T11:49:23.008834: step 2539, loss 0.0352422, acc 1, learning_rate 0.000100152
2017-10-10T11:49:23.268459: step 2540, loss 0.114907, acc 0.96875, learning_rate 0.000100151
2017-10-10T11:49:23.494027: step 2541, loss 0.0787891, acc 0.984375, learning_rate 0.000100151
2017-10-10T11:49:23.711270: step 2542, loss 0.0594024, acc 0.984375, learning_rate 0.00010015
2017-10-10T11:49:23.932076: step 2543, loss 0.162143, acc 0.9375, learning_rate 0.00010015
2017-10-10T11:49:24.183581: step 2544, loss 0.0800624, acc 0.984375, learning_rate 0.000100149
2017-10-10T11:49:24.426349: step 2545, loss 0.111997, acc 0.953125, learning_rate 0.000100148
2017-10-10T11:49:24.664905: step 2546, loss 0.218449, acc 0.921875, learning_rate 0.000100148
2017-10-10T11:49:24.916867: step 2547, loss 0.0462481, acc 1, learning_rate 0.000100147
2017-10-10T11:49:25.142403: step 2548, loss 0.0987832, acc 0.960784, learning_rate 0.000100147
2017-10-10T11:49:25.397251: step 2549, loss 0.11678, acc 0.953125, learning_rate 0.000100146
2017-10-10T11:49:25.632871: step 2550, loss 0.15292, acc 0.953125, learning_rate 0.000100145
2017-10-10T11:49:25.899858: step 2551, loss 0.149356, acc 0.953125, learning_rate 0.000100145
2017-10-10T11:49:26.220860: step 2552, loss 0.0824031, acc 0.984375, learning_rate 0.000100144
2017-10-10T11:49:26.433912: step 2553, loss 0.0682845, acc 1, learning_rate 0.000100144
2017-10-10T11:49:26.697875: step 2554, loss 0.162447, acc 0.9375, learning_rate 0.000100143
2017-10-10T11:49:26.900329: step 2555, loss 0.134504, acc 0.9375, learning_rate 0.000100142
2017-10-10T11:49:27.034989: step 2556, loss 0.10829, acc 0.96875, learning_rate 0.000100142
2017-10-10T11:49:27.209776: step 2557, loss 0.101415, acc 0.984375, learning_rate 0.000100141
2017-10-10T11:49:27.428852: step 2558, loss 0.0572839, acc 0.984375, learning_rate 0.000100141
2017-10-10T11:49:27.608431: step 2559, loss 0.0602495, acc 0.984375, learning_rate 0.00010014
2017-10-10T11:49:27.841423: step 2560, loss 0.120153, acc 0.953125, learning_rate 0.00010014

Evaluation:
2017-10-10T11:49:28.416807: step 2560, loss 0.213336, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2560

2017-10-10T11:49:29.761309: step 2561, loss 0.143777, acc 0.953125, learning_rate 0.000100139
2017-10-10T11:49:30.063241: step 2562, loss 0.0315149, acc 1, learning_rate 0.000100138
2017-10-10T11:49:30.299301: step 2563, loss 0.0652279, acc 0.984375, learning_rate 0.000100138
2017-10-10T11:49:30.553020: step 2564, loss 0.0722883, acc 0.96875, learning_rate 0.000100137
2017-10-10T11:49:30.798891: step 2565, loss 0.0275073, acc 1, learning_rate 0.000100137
2017-10-10T11:49:31.072891: step 2566, loss 0.260679, acc 0.890625, learning_rate 0.000100136
2017-10-10T11:49:31.324429: step 2567, loss 0.1431, acc 0.953125, learning_rate 0.000100136
2017-10-10T11:49:31.591472: step 2568, loss 0.133637, acc 0.96875, learning_rate 0.000100135
2017-10-10T11:49:31.839524: step 2569, loss 0.165418, acc 0.90625, learning_rate 0.000100134
2017-10-10T11:49:32.084884: step 2570, loss 0.0534889, acc 1, learning_rate 0.000100134
2017-10-10T11:49:32.358359: step 2571, loss 0.0446039, acc 1, learning_rate 0.000100133
2017-10-10T11:49:32.592909: step 2572, loss 0.094864, acc 0.953125, learning_rate 0.000100133
2017-10-10T11:49:32.819867: step 2573, loss 0.153166, acc 0.9375, learning_rate 0.000100132
2017-10-10T11:49:33.056913: step 2574, loss 0.147606, acc 0.953125, learning_rate 0.000100132
2017-10-10T11:49:33.328082: step 2575, loss 0.0685487, acc 1, learning_rate 0.000100131
2017-10-10T11:49:33.592909: step 2576, loss 0.11427, acc 0.953125, learning_rate 0.000100131
2017-10-10T11:49:33.861947: step 2577, loss 0.109574, acc 0.96875, learning_rate 0.00010013
2017-10-10T11:49:34.160939: step 2578, loss 0.291223, acc 0.921875, learning_rate 0.00010013
2017-10-10T11:49:34.378321: step 2579, loss 0.129127, acc 0.9375, learning_rate 0.000100129
2017-10-10T11:49:34.571080: step 2580, loss 0.109202, acc 0.96875, learning_rate 0.000100129
2017-10-10T11:49:34.771012: step 2581, loss 0.143405, acc 0.953125, learning_rate 0.000100128
2017-10-10T11:49:34.977885: step 2582, loss 0.0234745, acc 1, learning_rate 0.000100128
2017-10-10T11:49:35.184907: step 2583, loss 0.28567, acc 0.921875, learning_rate 0.000100127
2017-10-10T11:49:35.455403: step 2584, loss 0.104696, acc 0.96875, learning_rate 0.000100126
2017-10-10T11:49:35.712850: step 2585, loss 0.096197, acc 0.96875, learning_rate 0.000100126
2017-10-10T11:49:35.940704: step 2586, loss 0.0564491, acc 1, learning_rate 0.000100125
2017-10-10T11:49:36.180284: step 2587, loss 0.0846424, acc 0.96875, learning_rate 0.000100125
2017-10-10T11:49:36.449357: step 2588, loss 0.118465, acc 0.953125, learning_rate 0.000100124
2017-10-10T11:49:36.691789: step 2589, loss 0.18942, acc 0.9375, learning_rate 0.000100124
2017-10-10T11:49:37.044909: step 2590, loss 0.153272, acc 0.953125, learning_rate 0.000100123
2017-10-10T11:49:37.281851: step 2591, loss 0.133928, acc 0.9375, learning_rate 0.000100123
2017-10-10T11:49:37.476861: step 2592, loss 0.0299905, acc 1, learning_rate 0.000100122
2017-10-10T11:49:37.664808: step 2593, loss 0.153012, acc 0.921875, learning_rate 0.000100122
2017-10-10T11:49:37.847715: step 2594, loss 0.0943778, acc 0.953125, learning_rate 0.000100121
2017-10-10T11:49:38.043729: step 2595, loss 0.0956832, acc 0.984375, learning_rate 0.000100121
2017-10-10T11:49:38.213798: step 2596, loss 0.178945, acc 0.90625, learning_rate 0.00010012
2017-10-10T11:49:38.452564: step 2597, loss 0.0469425, acc 1, learning_rate 0.00010012
2017-10-10T11:49:38.733399: step 2598, loss 0.231844, acc 0.9375, learning_rate 0.000100119
2017-10-10T11:49:38.986121: step 2599, loss 0.090916, acc 0.984375, learning_rate 0.000100119
2017-10-10T11:49:39.264399: step 2600, loss 0.0440631, acc 1, learning_rate 0.000100118

Evaluation:
2017-10-10T11:49:39.852829: step 2600, loss 0.215023, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2600

2017-10-10T11:49:40.712295: step 2601, loss 0.11511, acc 0.9375, learning_rate 0.000100118
2017-10-10T11:49:40.944966: step 2602, loss 0.123433, acc 0.953125, learning_rate 0.000100117
2017-10-10T11:49:41.182414: step 2603, loss 0.0750571, acc 0.96875, learning_rate 0.000100117
2017-10-10T11:49:41.418442: step 2604, loss 0.130778, acc 0.953125, learning_rate 0.000100117
2017-10-10T11:49:41.708861: step 2605, loss 0.0821902, acc 0.96875, learning_rate 0.000100116
2017-10-10T11:49:41.936860: step 2606, loss 0.117821, acc 0.984375, learning_rate 0.000100116
2017-10-10T11:49:42.195064: step 2607, loss 0.114405, acc 0.96875, learning_rate 0.000100115
2017-10-10T11:49:42.432069: step 2608, loss 0.141303, acc 0.96875, learning_rate 0.000100115
2017-10-10T11:49:42.758398: step 2609, loss 0.239262, acc 0.890625, learning_rate 0.000100114
2017-10-10T11:49:42.991802: step 2610, loss 0.0907049, acc 0.96875, learning_rate 0.000100114
2017-10-10T11:49:43.204871: step 2611, loss 0.100819, acc 0.984375, learning_rate 0.000100113
2017-10-10T11:49:43.417378: step 2612, loss 0.148488, acc 0.921875, learning_rate 0.000100113
2017-10-10T11:49:43.636890: step 2613, loss 0.165858, acc 0.921875, learning_rate 0.000100112
2017-10-10T11:49:43.859599: step 2614, loss 0.228097, acc 0.890625, learning_rate 0.000100112
2017-10-10T11:49:44.092725: step 2615, loss 0.130936, acc 0.953125, learning_rate 0.000100111
2017-10-10T11:49:44.352332: step 2616, loss 0.160295, acc 0.9375, learning_rate 0.000100111
2017-10-10T11:49:44.620845: step 2617, loss 0.0919063, acc 0.96875, learning_rate 0.000100111
2017-10-10T11:49:44.882550: step 2618, loss 0.0907833, acc 0.96875, learning_rate 0.00010011
2017-10-10T11:49:45.100123: step 2619, loss 0.12761, acc 0.921875, learning_rate 0.00010011
2017-10-10T11:49:45.337477: step 2620, loss 0.0695693, acc 0.984375, learning_rate 0.000100109
2017-10-10T11:49:45.920448: step 2621, loss 0.134858, acc 0.953125, learning_rate 0.000100109
2017-10-10T11:49:46.166495: step 2622, loss 0.13974, acc 0.9375, learning_rate 0.000100108
2017-10-10T11:49:46.424449: step 2623, loss 0.145993, acc 0.953125, learning_rate 0.000100108
2017-10-10T11:49:46.679435: step 2624, loss 0.156838, acc 0.953125, learning_rate 0.000100107
2017-10-10T11:49:46.921244: step 2625, loss 0.0370679, acc 1, learning_rate 0.000100107
2017-10-10T11:49:47.195999: step 2626, loss 0.0844471, acc 0.984375, learning_rate 0.000100107
2017-10-10T11:49:47.462197: step 2627, loss 0.094155, acc 0.984375, learning_rate 0.000100106
2017-10-10T11:49:47.729082: step 2628, loss 0.0721616, acc 0.984375, learning_rate 0.000100106
2017-10-10T11:49:47.971976: step 2629, loss 0.0943753, acc 0.953125, learning_rate 0.000100105
2017-10-10T11:49:48.296850: step 2630, loss 0.217611, acc 0.9375, learning_rate 0.000100105
2017-10-10T11:49:48.557026: step 2631, loss 0.135776, acc 0.9375, learning_rate 0.000100104
2017-10-10T11:49:48.736854: step 2632, loss 0.13622, acc 0.953125, learning_rate 0.000100104
2017-10-10T11:49:48.939323: step 2633, loss 0.0753241, acc 0.984375, learning_rate 0.000100104
2017-10-10T11:49:49.121693: step 2634, loss 0.114563, acc 0.96875, learning_rate 0.000100103
2017-10-10T11:49:49.316981: step 2635, loss 0.0695923, acc 0.984375, learning_rate 0.000100103
2017-10-10T11:49:49.569097: step 2636, loss 0.102013, acc 0.96875, learning_rate 0.000100102
2017-10-10T11:49:49.823554: step 2637, loss 0.0539427, acc 0.96875, learning_rate 0.000100102
2017-10-10T11:49:50.066646: step 2638, loss 0.0559564, acc 1, learning_rate 0.000100101
2017-10-10T11:49:50.314215: step 2639, loss 0.123611, acc 0.953125, learning_rate 0.000100101
2017-10-10T11:49:50.560850: step 2640, loss 0.0600163, acc 0.984375, learning_rate 0.000100101

Evaluation:
2017-10-10T11:49:51.163619: step 2640, loss 0.210603, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2640

2017-10-10T11:49:52.151527: step 2641, loss 0.106054, acc 0.96875, learning_rate 0.0001001
2017-10-10T11:49:52.381361: step 2642, loss 0.0955197, acc 0.953125, learning_rate 0.0001001
2017-10-10T11:49:52.624358: step 2643, loss 0.0553464, acc 0.96875, learning_rate 0.000100099
2017-10-10T11:49:52.836656: step 2644, loss 0.125415, acc 0.96875, learning_rate 0.000100099
2017-10-10T11:49:53.088913: step 2645, loss 0.0957923, acc 0.96875, learning_rate 0.000100099
2017-10-10T11:49:53.314520: step 2646, loss 0.0651986, acc 1, learning_rate 0.000100098
2017-10-10T11:49:53.564845: step 2647, loss 0.0785307, acc 0.96875, learning_rate 0.000100098
2017-10-10T11:49:53.845943: step 2648, loss 0.0710527, acc 1, learning_rate 0.000100097
2017-10-10T11:49:54.084086: step 2649, loss 0.068016, acc 0.984375, learning_rate 0.000100097
2017-10-10T11:49:54.321357: step 2650, loss 0.144986, acc 0.953125, learning_rate 0.000100097
2017-10-10T11:49:54.579026: step 2651, loss 0.126875, acc 0.921875, learning_rate 0.000100096
2017-10-10T11:49:54.795089: step 2652, loss 0.116481, acc 0.96875, learning_rate 0.000100096
2017-10-10T11:49:55.034297: step 2653, loss 0.0881479, acc 0.984375, learning_rate 0.000100095
2017-10-10T11:49:55.279743: step 2654, loss 0.0924352, acc 0.984375, learning_rate 0.000100095
2017-10-10T11:49:55.520847: step 2655, loss 0.0780562, acc 0.96875, learning_rate 0.000100095
2017-10-10T11:49:55.768715: step 2656, loss 0.117051, acc 0.953125, learning_rate 0.000100094
2017-10-10T11:49:56.012907: step 2657, loss 0.0733579, acc 0.984375, learning_rate 0.000100094
2017-10-10T11:49:56.293105: step 2658, loss 0.247184, acc 0.9375, learning_rate 0.000100093
2017-10-10T11:49:56.564932: step 2659, loss 0.0439974, acc 1, learning_rate 0.000100093
2017-10-10T11:49:56.835449: step 2660, loss 0.0363899, acc 0.984375, learning_rate 0.000100093
2017-10-10T11:49:57.120307: step 2661, loss 0.141131, acc 0.96875, learning_rate 0.000100092
2017-10-10T11:49:57.370919: step 2662, loss 0.169035, acc 0.953125, learning_rate 0.000100092
2017-10-10T11:49:57.616296: step 2663, loss 0.0972618, acc 0.96875, learning_rate 0.000100092
2017-10-10T11:49:57.870527: step 2664, loss 0.095751, acc 0.953125, learning_rate 0.000100091
2017-10-10T11:49:58.100980: step 2665, loss 0.175162, acc 0.953125, learning_rate 0.000100091
2017-10-10T11:49:58.463661: step 2666, loss 0.179412, acc 0.9375, learning_rate 0.00010009
2017-10-10T11:49:58.687518: step 2667, loss 0.120047, acc 0.953125, learning_rate 0.00010009
2017-10-10T11:49:58.865318: step 2668, loss 0.121672, acc 0.96875, learning_rate 0.00010009
2017-10-10T11:49:59.076860: step 2669, loss 0.11533, acc 0.96875, learning_rate 0.000100089
2017-10-10T11:49:59.289081: step 2670, loss 0.10072, acc 0.96875, learning_rate 0.000100089
2017-10-10T11:49:59.438073: step 2671, loss 0.124467, acc 0.953125, learning_rate 0.000100089
2017-10-10T11:49:59.662367: step 2672, loss 0.133293, acc 0.953125, learning_rate 0.000100088
2017-10-10T11:49:59.876856: step 2673, loss 0.13122, acc 0.953125, learning_rate 0.000100088
2017-10-10T11:50:00.070433: step 2674, loss 0.13997, acc 0.953125, learning_rate 0.000100088
2017-10-10T11:50:00.304322: step 2675, loss 0.145336, acc 0.953125, learning_rate 0.000100087
2017-10-10T11:50:00.531009: step 2676, loss 0.102921, acc 0.96875, learning_rate 0.000100087
2017-10-10T11:50:00.795935: step 2677, loss 0.182944, acc 0.9375, learning_rate 0.000100086
2017-10-10T11:50:01.076857: step 2678, loss 0.0672065, acc 0.984375, learning_rate 0.000100086
2017-10-10T11:50:01.350525: step 2679, loss 0.0382553, acc 0.984375, learning_rate 0.000100086
2017-10-10T11:50:01.587304: step 2680, loss 0.0637259, acc 0.984375, learning_rate 0.000100085

Evaluation:
2017-10-10T11:50:02.132482: step 2680, loss 0.211346, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2680

2017-10-10T11:50:03.460839: step 2681, loss 0.0710349, acc 0.984375, learning_rate 0.000100085
2017-10-10T11:50:03.723081: step 2682, loss 0.0977201, acc 0.96875, learning_rate 0.000100085
2017-10-10T11:50:03.957368: step 2683, loss 0.0884295, acc 0.96875, learning_rate 0.000100084
2017-10-10T11:50:04.232977: step 2684, loss 0.0496782, acc 1, learning_rate 0.000100084
2017-10-10T11:50:04.472842: step 2685, loss 0.142826, acc 0.9375, learning_rate 0.000100084
2017-10-10T11:50:04.721178: step 2686, loss 0.108992, acc 0.96875, learning_rate 0.000100083
2017-10-10T11:50:04.971787: step 2687, loss 0.0665387, acc 0.984375, learning_rate 0.000100083
2017-10-10T11:50:05.219289: step 2688, loss 0.0333545, acc 1, learning_rate 0.000100083
2017-10-10T11:50:05.468728: step 2689, loss 0.0796969, acc 0.96875, learning_rate 0.000100082
2017-10-10T11:50:05.694968: step 2690, loss 0.194832, acc 0.9375, learning_rate 0.000100082
2017-10-10T11:50:05.973028: step 2691, loss 0.104272, acc 0.953125, learning_rate 0.000100082
2017-10-10T11:50:06.219858: step 2692, loss 0.089613, acc 0.96875, learning_rate 0.000100081
2017-10-10T11:50:06.443269: step 2693, loss 0.142037, acc 0.953125, learning_rate 0.000100081
2017-10-10T11:50:06.685849: step 2694, loss 0.110754, acc 0.953125, learning_rate 0.000100081
2017-10-10T11:50:06.948234: step 2695, loss 0.205845, acc 0.921875, learning_rate 0.00010008
2017-10-10T11:50:07.183668: step 2696, loss 0.0691125, acc 0.984375, learning_rate 0.00010008
2017-10-10T11:50:07.484910: step 2697, loss 0.118917, acc 0.9375, learning_rate 0.00010008
2017-10-10T11:50:07.686141: step 2698, loss 0.208524, acc 0.890625, learning_rate 0.000100079
2017-10-10T11:50:07.881810: step 2699, loss 0.0921313, acc 0.96875, learning_rate 0.000100079
2017-10-10T11:50:08.098639: step 2700, loss 0.0705994, acc 0.984375, learning_rate 0.000100079
2017-10-10T11:50:08.306495: step 2701, loss 0.129997, acc 0.96875, learning_rate 0.000100078
2017-10-10T11:50:08.568997: step 2702, loss 0.0971339, acc 0.984375, learning_rate 0.000100078
2017-10-10T11:50:08.843679: step 2703, loss 0.100103, acc 0.96875, learning_rate 0.000100078
2017-10-10T11:50:09.020854: step 2704, loss 0.104703, acc 0.953125, learning_rate 0.000100077
2017-10-10T11:50:09.224210: step 2705, loss 0.094166, acc 0.984375, learning_rate 0.000100077
2017-10-10T11:50:09.440875: step 2706, loss 0.113922, acc 0.953125, learning_rate 0.000100077
2017-10-10T11:50:09.611113: step 2707, loss 0.116306, acc 0.9375, learning_rate 0.000100076
2017-10-10T11:50:09.796373: step 2708, loss 0.131026, acc 0.96875, learning_rate 0.000100076
2017-10-10T11:50:10.043104: step 2709, loss 0.046953, acc 1, learning_rate 0.000100076
2017-10-10T11:50:10.291492: step 2710, loss 0.0320879, acc 1, learning_rate 0.000100076
2017-10-10T11:50:10.569346: step 2711, loss 0.137981, acc 0.921875, learning_rate 0.000100075
2017-10-10T11:50:10.820429: step 2712, loss 0.149466, acc 0.953125, learning_rate 0.000100075
2017-10-10T11:50:11.092812: step 2713, loss 0.141076, acc 0.921875, learning_rate 0.000100075
2017-10-10T11:50:11.332869: step 2714, loss 0.102112, acc 0.96875, learning_rate 0.000100074
2017-10-10T11:50:11.600928: step 2715, loss 0.10694, acc 0.953125, learning_rate 0.000100074
2017-10-10T11:50:11.837019: step 2716, loss 0.111771, acc 0.96875, learning_rate 0.000100074
2017-10-10T11:50:12.047225: step 2717, loss 0.0568276, acc 1, learning_rate 0.000100073
2017-10-10T11:50:12.312751: step 2718, loss 0.0664679, acc 1, learning_rate 0.000100073
2017-10-10T11:50:12.541959: step 2719, loss 0.217841, acc 0.90625, learning_rate 0.000100073
2017-10-10T11:50:12.784979: step 2720, loss 0.0762842, acc 0.96875, learning_rate 0.000100073

Evaluation:
2017-10-10T11:50:13.341529: step 2720, loss 0.212581, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2720

2017-10-10T11:50:14.375130: step 2721, loss 0.0769587, acc 0.984375, learning_rate 0.000100072
2017-10-10T11:50:14.620850: step 2722, loss 0.103964, acc 0.953125, learning_rate 0.000100072
2017-10-10T11:50:14.905006: step 2723, loss 0.0514441, acc 0.984375, learning_rate 0.000100072
2017-10-10T11:50:15.122207: step 2724, loss 0.0645181, acc 0.984375, learning_rate 0.000100071
2017-10-10T11:50:15.337420: step 2725, loss 0.058836, acc 0.984375, learning_rate 0.000100071
2017-10-10T11:50:15.561555: step 2726, loss 0.114279, acc 0.96875, learning_rate 0.000100071
2017-10-10T11:50:15.830964: step 2727, loss 0.0778945, acc 0.984375, learning_rate 0.00010007
2017-10-10T11:50:16.056822: step 2728, loss 0.0525467, acc 1, learning_rate 0.00010007
2017-10-10T11:50:16.254009: step 2729, loss 0.110241, acc 0.984375, learning_rate 0.00010007
2017-10-10T11:50:16.450188: step 2730, loss 0.206489, acc 0.9375, learning_rate 0.00010007
2017-10-10T11:50:16.649117: step 2731, loss 0.0817015, acc 0.953125, learning_rate 0.000100069
2017-10-10T11:50:16.892886: step 2732, loss 0.10049, acc 0.96875, learning_rate 0.000100069
2017-10-10T11:50:17.128859: step 2733, loss 0.0428427, acc 1, learning_rate 0.000100069
2017-10-10T11:50:17.347736: step 2734, loss 0.142554, acc 0.9375, learning_rate 0.000100068
2017-10-10T11:50:17.595821: step 2735, loss 0.100505, acc 0.953125, learning_rate 0.000100068
2017-10-10T11:50:17.861920: step 2736, loss 0.136586, acc 0.953125, learning_rate 0.000100068
2017-10-10T11:50:18.110648: step 2737, loss 0.0757885, acc 0.984375, learning_rate 0.000100068
2017-10-10T11:50:18.383805: step 2738, loss 0.112685, acc 0.96875, learning_rate 0.000100067
2017-10-10T11:50:18.640496: step 2739, loss 0.152844, acc 0.953125, learning_rate 0.000100067
2017-10-10T11:50:18.902127: step 2740, loss 0.144211, acc 0.9375, learning_rate 0.000100067
2017-10-10T11:50:19.220926: step 2741, loss 0.0725558, acc 0.984375, learning_rate 0.000100067
2017-10-10T11:50:19.505663: step 2742, loss 0.121951, acc 0.953125, learning_rate 0.000100066
2017-10-10T11:50:19.686161: step 2743, loss 0.0563902, acc 0.984375, learning_rate 0.000100066
2017-10-10T11:50:19.844428: step 2744, loss 0.148509, acc 0.941176, learning_rate 0.000100066
2017-10-10T11:50:20.042526: step 2745, loss 0.0910571, acc 0.953125, learning_rate 0.000100065
2017-10-10T11:50:20.232893: step 2746, loss 0.192258, acc 0.90625, learning_rate 0.000100065
2017-10-10T11:50:20.488894: step 2747, loss 0.137682, acc 0.9375, learning_rate 0.000100065
2017-10-10T11:50:20.716855: step 2748, loss 0.135495, acc 0.96875, learning_rate 0.000100065
2017-10-10T11:50:20.948747: step 2749, loss 0.104373, acc 0.96875, learning_rate 0.000100064
2017-10-10T11:50:21.240864: step 2750, loss 0.0404512, acc 1, learning_rate 0.000100064
2017-10-10T11:50:21.488783: step 2751, loss 0.110552, acc 0.9375, learning_rate 0.000100064
2017-10-10T11:50:21.720858: step 2752, loss 0.175997, acc 0.953125, learning_rate 0.000100064
2017-10-10T11:50:21.988932: step 2753, loss 0.0747956, acc 0.984375, learning_rate 0.000100063
2017-10-10T11:50:22.215922: step 2754, loss 0.093369, acc 0.953125, learning_rate 0.000100063
2017-10-10T11:50:22.445378: step 2755, loss 0.0906044, acc 0.984375, learning_rate 0.000100063
2017-10-10T11:50:22.697676: step 2756, loss 0.193573, acc 0.921875, learning_rate 0.000100063
2017-10-10T11:50:22.932944: step 2757, loss 0.058892, acc 0.96875, learning_rate 0.000100062
2017-10-10T11:50:23.177336: step 2758, loss 0.146199, acc 0.96875, learning_rate 0.000100062
2017-10-10T11:50:23.425004: step 2759, loss 0.17585, acc 0.921875, learning_rate 0.000100062
2017-10-10T11:50:23.705682: step 2760, loss 0.0360741, acc 1, learning_rate 0.000100062

Evaluation:
2017-10-10T11:50:24.262991: step 2760, loss 0.211853, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2760

2017-10-10T11:50:25.517552: step 2761, loss 0.0391294, acc 0.984375, learning_rate 0.000100061
2017-10-10T11:50:25.767290: step 2762, loss 0.0900212, acc 0.96875, learning_rate 0.000100061
2017-10-10T11:50:26.014614: step 2763, loss 0.113326, acc 0.96875, learning_rate 0.000100061
2017-10-10T11:50:26.278836: step 2764, loss 0.188184, acc 0.9375, learning_rate 0.000100061
2017-10-10T11:50:26.555980: step 2765, loss 0.117118, acc 0.9375, learning_rate 0.00010006
2017-10-10T11:50:26.797059: step 2766, loss 0.10025, acc 0.984375, learning_rate 0.00010006
2017-10-10T11:50:27.048866: step 2767, loss 0.125132, acc 0.953125, learning_rate 0.00010006
2017-10-10T11:50:27.290622: step 2768, loss 0.188263, acc 0.9375, learning_rate 0.00010006
2017-10-10T11:50:27.560173: step 2769, loss 0.188536, acc 0.9375, learning_rate 0.000100059
2017-10-10T11:50:27.796850: step 2770, loss 0.106846, acc 0.984375, learning_rate 0.000100059
2017-10-10T11:50:28.056853: step 2771, loss 0.150078, acc 0.921875, learning_rate 0.000100059
2017-10-10T11:50:28.327628: step 2772, loss 0.0518457, acc 0.984375, learning_rate 0.000100059
2017-10-10T11:50:28.576123: step 2773, loss 0.174478, acc 0.9375, learning_rate 0.000100058
2017-10-10T11:50:28.836896: step 2774, loss 0.160508, acc 0.9375, learning_rate 0.000100058
2017-10-10T11:50:29.079827: step 2775, loss 0.060943, acc 1, learning_rate 0.000100058
2017-10-10T11:50:29.353161: step 2776, loss 0.160019, acc 0.9375, learning_rate 0.000100058
2017-10-10T11:50:29.658796: step 2777, loss 0.0510741, acc 0.984375, learning_rate 0.000100057
2017-10-10T11:50:29.840846: step 2778, loss 0.0732306, acc 0.96875, learning_rate 0.000100057
2017-10-10T11:50:30.047933: step 2779, loss 0.0584569, acc 0.984375, learning_rate 0.000100057
2017-10-10T11:50:30.229197: step 2780, loss 0.191183, acc 0.9375, learning_rate 0.000100057
2017-10-10T11:50:30.404573: step 2781, loss 0.18144, acc 0.921875, learning_rate 0.000100056
2017-10-10T11:50:30.656907: step 2782, loss 0.0719243, acc 0.984375, learning_rate 0.000100056
2017-10-10T11:50:30.915927: step 2783, loss 0.142824, acc 0.96875, learning_rate 0.000100056
2017-10-10T11:50:31.159203: step 2784, loss 0.103715, acc 0.953125, learning_rate 0.000100056
2017-10-10T11:50:31.392538: step 2785, loss 0.062722, acc 0.96875, learning_rate 0.000100056
2017-10-10T11:50:31.633910: step 2786, loss 0.137027, acc 0.9375, learning_rate 0.000100055
2017-10-10T11:50:31.870363: step 2787, loss 0.0802035, acc 0.984375, learning_rate 0.000100055
2017-10-10T11:50:32.112685: step 2788, loss 0.182836, acc 0.9375, learning_rate 0.000100055
2017-10-10T11:50:32.363493: step 2789, loss 0.192012, acc 0.953125, learning_rate 0.000100055
2017-10-10T11:50:32.651602: step 2790, loss 0.15755, acc 0.921875, learning_rate 0.000100054
2017-10-10T11:50:32.838562: step 2791, loss 0.117135, acc 0.984375, learning_rate 0.000100054
2017-10-10T11:50:33.019384: step 2792, loss 0.044473, acc 0.984375, learning_rate 0.000100054
2017-10-10T11:50:33.241954: step 2793, loss 0.2269, acc 0.921875, learning_rate 0.000100054
2017-10-10T11:50:33.410692: step 2794, loss 0.0705254, acc 0.984375, learning_rate 0.000100054
2017-10-10T11:50:33.628027: step 2795, loss 0.13369, acc 0.953125, learning_rate 0.000100053
2017-10-10T11:50:33.805076: step 2796, loss 0.0818309, acc 0.984375, learning_rate 0.000100053
2017-10-10T11:50:33.987951: step 2797, loss 0.0315344, acc 1, learning_rate 0.000100053
2017-10-10T11:50:34.212979: step 2798, loss 0.12069, acc 0.96875, learning_rate 0.000100053
2017-10-10T11:50:34.465348: step 2799, loss 0.129732, acc 0.96875, learning_rate 0.000100052
2017-10-10T11:50:34.704759: step 2800, loss 0.0688071, acc 0.984375, learning_rate 0.000100052

Evaluation:
2017-10-10T11:50:35.268865: step 2800, loss 0.211333, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2800

2017-10-10T11:50:36.548853: step 2801, loss 0.0466856, acc 1, learning_rate 0.000100052
2017-10-10T11:50:36.790431: step 2802, loss 0.0843444, acc 0.96875, learning_rate 0.000100052
2017-10-10T11:50:37.036904: step 2803, loss 0.0484505, acc 0.984375, learning_rate 0.000100052
2017-10-10T11:50:37.259506: step 2804, loss 0.0754267, acc 0.984375, learning_rate 0.000100051
2017-10-10T11:50:37.517251: step 2805, loss 0.175475, acc 0.953125, learning_rate 0.000100051
2017-10-10T11:50:37.762980: step 2806, loss 0.138187, acc 0.96875, learning_rate 0.000100051
2017-10-10T11:50:37.991374: step 2807, loss 0.131586, acc 0.953125, learning_rate 0.000100051
2017-10-10T11:50:38.229074: step 2808, loss 0.153744, acc 0.953125, learning_rate 0.000100051
2017-10-10T11:50:38.472121: step 2809, loss 0.101051, acc 0.984375, learning_rate 0.00010005
2017-10-10T11:50:38.699717: step 2810, loss 0.0827597, acc 0.984375, learning_rate 0.00010005
2017-10-10T11:50:38.951512: step 2811, loss 0.0663596, acc 0.984375, learning_rate 0.00010005
2017-10-10T11:50:39.188874: step 2812, loss 0.123123, acc 0.96875, learning_rate 0.00010005
2017-10-10T11:50:39.476869: step 2813, loss 0.0822392, acc 0.96875, learning_rate 0.00010005
2017-10-10T11:50:39.758872: step 2814, loss 0.205439, acc 0.90625, learning_rate 0.000100049
2017-10-10T11:50:39.953062: step 2815, loss 0.138379, acc 0.9375, learning_rate 0.000100049
2017-10-10T11:50:40.144042: step 2816, loss 0.0958384, acc 0.953125, learning_rate 0.000100049
2017-10-10T11:50:40.328834: step 2817, loss 0.135501, acc 0.96875, learning_rate 0.000100049
2017-10-10T11:50:40.504830: step 2818, loss 0.0862572, acc 0.953125, learning_rate 0.000100049
2017-10-10T11:50:40.671237: step 2819, loss 0.119328, acc 0.96875, learning_rate 0.000100048
2017-10-10T11:50:40.902312: step 2820, loss 0.0464521, acc 0.984375, learning_rate 0.000100048
2017-10-10T11:50:41.120420: step 2821, loss 0.172913, acc 0.953125, learning_rate 0.000100048
2017-10-10T11:50:41.385821: step 2822, loss 0.179327, acc 0.921875, learning_rate 0.000100048
2017-10-10T11:50:41.570254: step 2823, loss 0.0381627, acc 0.984375, learning_rate 0.000100048
2017-10-10T11:50:41.753023: step 2824, loss 0.163707, acc 0.953125, learning_rate 0.000100047
2017-10-10T11:50:41.969127: step 2825, loss 0.0950965, acc 0.96875, learning_rate 0.000100047
2017-10-10T11:50:42.185410: step 2826, loss 0.0925751, acc 0.96875, learning_rate 0.000100047
2017-10-10T11:50:42.395231: step 2827, loss 0.0978156, acc 0.984375, learning_rate 0.000100047
2017-10-10T11:50:42.638597: step 2828, loss 0.158684, acc 0.921875, learning_rate 0.000100047
2017-10-10T11:50:42.908924: step 2829, loss 0.113269, acc 0.96875, learning_rate 0.000100046
2017-10-10T11:50:43.149174: step 2830, loss 0.250472, acc 0.9375, learning_rate 0.000100046
2017-10-10T11:50:43.380901: step 2831, loss 0.156409, acc 0.953125, learning_rate 0.000100046
2017-10-10T11:50:43.633178: step 2832, loss 0.202045, acc 0.953125, learning_rate 0.000100046
2017-10-10T11:50:43.892151: step 2833, loss 0.070455, acc 0.984375, learning_rate 0.000100046
2017-10-10T11:50:44.144768: step 2834, loss 0.0998019, acc 0.96875, learning_rate 0.000100045
2017-10-10T11:50:44.359029: step 2835, loss 0.130525, acc 0.984375, learning_rate 0.000100045
2017-10-10T11:50:44.583875: step 2836, loss 0.138382, acc 0.953125, learning_rate 0.000100045
2017-10-10T11:50:44.814366: step 2837, loss 0.165174, acc 0.953125, learning_rate 0.000100045
2017-10-10T11:50:45.068304: step 2838, loss 0.149206, acc 0.984375, learning_rate 0.000100045
2017-10-10T11:50:45.311480: step 2839, loss 0.163968, acc 0.953125, learning_rate 0.000100045
2017-10-10T11:50:45.560603: step 2840, loss 0.0409577, acc 1, learning_rate 0.000100044

Evaluation:
2017-10-10T11:50:46.128839: step 2840, loss 0.21102, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2840

2017-10-10T11:50:47.176852: step 2841, loss 0.107433, acc 0.953125, learning_rate 0.000100044
2017-10-10T11:50:47.401085: step 2842, loss 0.106531, acc 0.960784, learning_rate 0.000100044
2017-10-10T11:50:47.659673: step 2843, loss 0.0966501, acc 0.96875, learning_rate 0.000100044
2017-10-10T11:50:47.928263: step 2844, loss 0.149175, acc 0.96875, learning_rate 0.000100044
2017-10-10T11:50:48.174859: step 2845, loss 0.123642, acc 0.953125, learning_rate 0.000100043
2017-10-10T11:50:48.428854: step 2846, loss 0.0643795, acc 0.984375, learning_rate 0.000100043
2017-10-10T11:50:48.672728: step 2847, loss 0.133262, acc 0.953125, learning_rate 0.000100043
2017-10-10T11:50:48.954657: step 2848, loss 0.140092, acc 0.953125, learning_rate 0.000100043
2017-10-10T11:50:49.191355: step 2849, loss 0.160414, acc 0.953125, learning_rate 0.000100043
2017-10-10T11:50:49.444015: step 2850, loss 0.0716432, acc 0.984375, learning_rate 0.000100043
2017-10-10T11:50:49.700952: step 2851, loss 0.0503367, acc 0.984375, learning_rate 0.000100042
2017-10-10T11:50:49.998738: step 2852, loss 0.112341, acc 0.96875, learning_rate 0.000100042
2017-10-10T11:50:50.249320: step 2853, loss 0.208552, acc 0.9375, learning_rate 0.000100042
2017-10-10T11:50:50.388222: step 2854, loss 0.0569537, acc 0.984375, learning_rate 0.000100042
2017-10-10T11:50:50.522335: step 2855, loss 0.10787, acc 0.96875, learning_rate 0.000100042
2017-10-10T11:50:50.664050: step 2856, loss 0.117668, acc 0.9375, learning_rate 0.000100042
2017-10-10T11:50:50.849253: step 2857, loss 0.0835341, acc 0.96875, learning_rate 0.000100041
2017-10-10T11:50:51.087136: step 2858, loss 0.0735163, acc 0.984375, learning_rate 0.000100041
2017-10-10T11:50:51.348091: step 2859, loss 0.0796556, acc 0.96875, learning_rate 0.000100041
2017-10-10T11:50:51.606650: step 2860, loss 0.113607, acc 0.96875, learning_rate 0.000100041
2017-10-10T11:50:51.848901: step 2861, loss 0.115703, acc 0.953125, learning_rate 0.000100041
2017-10-10T11:50:52.108822: step 2862, loss 0.102149, acc 0.96875, learning_rate 0.000100041
2017-10-10T11:50:52.331101: step 2863, loss 0.124383, acc 0.96875, learning_rate 0.00010004
2017-10-10T11:50:52.603976: step 2864, loss 0.175859, acc 0.953125, learning_rate 0.00010004
2017-10-10T11:50:52.857539: step 2865, loss 0.0551643, acc 0.984375, learning_rate 0.00010004
2017-10-10T11:50:53.087747: step 2866, loss 0.0958332, acc 0.953125, learning_rate 0.00010004
2017-10-10T11:50:53.352247: step 2867, loss 0.113211, acc 0.96875, learning_rate 0.00010004
2017-10-10T11:50:53.594921: step 2868, loss 0.128451, acc 0.96875, learning_rate 0.00010004
2017-10-10T11:50:53.864301: step 2869, loss 0.0821625, acc 0.984375, learning_rate 0.000100039
2017-10-10T11:50:54.097460: step 2870, loss 0.0795633, acc 0.96875, learning_rate 0.000100039
2017-10-10T11:50:54.299187: step 2871, loss 0.060232, acc 0.984375, learning_rate 0.000100039
2017-10-10T11:50:54.581398: step 2872, loss 0.0624544, acc 1, learning_rate 0.000100039
2017-10-10T11:50:54.820888: step 2873, loss 0.0940072, acc 0.96875, learning_rate 0.000100039
2017-10-10T11:50:55.076112: step 2874, loss 0.0682915, acc 0.984375, learning_rate 0.000100039
2017-10-10T11:50:55.327437: step 2875, loss 0.156664, acc 0.921875, learning_rate 0.000100038
2017-10-10T11:50:55.572850: step 2876, loss 0.0566754, acc 0.96875, learning_rate 0.000100038
2017-10-10T11:50:55.840977: step 2877, loss 0.0700252, acc 0.984375, learning_rate 0.000100038
2017-10-10T11:50:56.071646: step 2878, loss 0.0723534, acc 0.96875, learning_rate 0.000100038
2017-10-10T11:50:56.321419: step 2879, loss 0.0932705, acc 0.96875, learning_rate 0.000100038
2017-10-10T11:50:56.559799: step 2880, loss 0.102074, acc 0.953125, learning_rate 0.000100038

Evaluation:
2017-10-10T11:50:57.131582: step 2880, loss 0.212019, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2880

2017-10-10T11:50:58.282714: step 2881, loss 0.107748, acc 0.96875, learning_rate 0.000100038
2017-10-10T11:50:58.498224: step 2882, loss 0.0724841, acc 0.984375, learning_rate 0.000100037
2017-10-10T11:50:58.717479: step 2883, loss 0.130189, acc 0.9375, learning_rate 0.000100037
2017-10-10T11:50:58.905450: step 2884, loss 0.0615618, acc 0.984375, learning_rate 0.000100037
2017-10-10T11:50:59.092840: step 2885, loss 0.102927, acc 0.953125, learning_rate 0.000100037
2017-10-10T11:50:59.333625: step 2886, loss 0.0496632, acc 1, learning_rate 0.000100037
2017-10-10T11:50:59.583045: step 2887, loss 0.112533, acc 0.96875, learning_rate 0.000100037
2017-10-10T11:50:59.866036: step 2888, loss 0.159621, acc 0.96875, learning_rate 0.000100036
2017-10-10T11:51:00.144878: step 2889, loss 0.102012, acc 0.9375, learning_rate 0.000100036
2017-10-10T11:51:00.407873: step 2890, loss 0.147496, acc 0.953125, learning_rate 0.000100036
2017-10-10T11:51:00.634438: step 2891, loss 0.149855, acc 0.9375, learning_rate 0.000100036
2017-10-10T11:51:00.823145: step 2892, loss 0.0724395, acc 0.96875, learning_rate 0.000100036
2017-10-10T11:51:01.048719: step 2893, loss 0.0954703, acc 0.984375, learning_rate 0.000100036
2017-10-10T11:51:01.242490: step 2894, loss 0.209154, acc 0.9375, learning_rate 0.000100036
2017-10-10T11:51:01.504653: step 2895, loss 0.0531969, acc 0.984375, learning_rate 0.000100035
2017-10-10T11:51:01.737042: step 2896, loss 0.0749523, acc 1, learning_rate 0.000100035
2017-10-10T11:51:01.975997: step 2897, loss 0.0675192, acc 0.984375, learning_rate 0.000100035
2017-10-10T11:51:02.215027: step 2898, loss 0.12433, acc 0.953125, learning_rate 0.000100035
2017-10-10T11:51:02.475235: step 2899, loss 0.0893021, acc 0.96875, learning_rate 0.000100035
2017-10-10T11:51:02.731719: step 2900, loss 0.204276, acc 0.921875, learning_rate 0.000100035
2017-10-10T11:51:02.969171: step 2901, loss 0.0859635, acc 0.984375, learning_rate 0.000100035
2017-10-10T11:51:03.216916: step 2902, loss 0.0802119, acc 0.984375, learning_rate 0.000100034
2017-10-10T11:51:03.489204: step 2903, loss 0.118574, acc 0.953125, learning_rate 0.000100034
2017-10-10T11:51:03.782062: step 2904, loss 0.137859, acc 0.953125, learning_rate 0.000100034
2017-10-10T11:51:04.046906: step 2905, loss 0.0561746, acc 0.984375, learning_rate 0.000100034
2017-10-10T11:51:04.289692: step 2906, loss 0.200139, acc 0.921875, learning_rate 0.000100034
2017-10-10T11:51:04.568869: step 2907, loss 0.147005, acc 0.9375, learning_rate 0.000100034
2017-10-10T11:51:04.830566: step 2908, loss 0.0572738, acc 1, learning_rate 0.000100034
2017-10-10T11:51:05.077064: step 2909, loss 0.167136, acc 0.953125, learning_rate 0.000100033
2017-10-10T11:51:05.324976: step 2910, loss 0.108367, acc 0.953125, learning_rate 0.000100033
2017-10-10T11:51:05.579405: step 2911, loss 0.0921667, acc 0.96875, learning_rate 0.000100033
2017-10-10T11:51:05.821509: step 2912, loss 0.0747107, acc 0.984375, learning_rate 0.000100033
2017-10-10T11:51:05.935831: step 2913, loss 0.205315, acc 0.9375, learning_rate 0.000100033
2017-10-10T11:51:06.053314: step 2914, loss 0.0575415, acc 0.984375, learning_rate 0.000100033
2017-10-10T11:51:06.170576: step 2915, loss 0.163669, acc 0.9375, learning_rate 0.000100033
2017-10-10T11:51:06.288129: step 2916, loss 0.0964874, acc 0.96875, learning_rate 0.000100033
2017-10-10T11:51:06.506347: step 2917, loss 0.13852, acc 0.953125, learning_rate 0.000100032
2017-10-10T11:51:06.740841: step 2918, loss 0.0656738, acc 0.96875, learning_rate 0.000100032
2017-10-10T11:51:07.047173: step 2919, loss 0.0834116, acc 0.96875, learning_rate 0.000100032
2017-10-10T11:51:07.308054: step 2920, loss 0.166615, acc 0.9375, learning_rate 0.000100032

Evaluation:
2017-10-10T11:51:07.760830: step 2920, loss 0.211149, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2920

2017-10-10T11:51:08.967914: step 2921, loss 0.0652608, acc 0.984375, learning_rate 0.000100032
2017-10-10T11:51:09.196914: step 2922, loss 0.0976057, acc 0.953125, learning_rate 0.000100032
2017-10-10T11:51:09.488858: step 2923, loss 0.0858861, acc 0.953125, learning_rate 0.000100032
2017-10-10T11:51:09.726611: step 2924, loss 0.0223515, acc 1, learning_rate 0.000100031
2017-10-10T11:51:09.972364: step 2925, loss 0.0614216, acc 0.96875, learning_rate 0.000100031
2017-10-10T11:51:10.240389: step 2926, loss 0.110031, acc 0.96875, learning_rate 0.000100031
2017-10-10T11:51:10.484925: step 2927, loss 0.109991, acc 0.96875, learning_rate 0.000100031
2017-10-10T11:51:10.828907: step 2928, loss 0.203656, acc 0.921875, learning_rate 0.000100031
2017-10-10T11:51:11.078650: step 2929, loss 0.0832154, acc 0.96875, learning_rate 0.000100031
2017-10-10T11:51:11.271292: step 2930, loss 0.205553, acc 0.9375, learning_rate 0.000100031
2017-10-10T11:51:11.472403: step 2931, loss 0.142801, acc 0.921875, learning_rate 0.000100031
2017-10-10T11:51:11.665509: step 2932, loss 0.189672, acc 0.9375, learning_rate 0.00010003
2017-10-10T11:51:11.880718: step 2933, loss 0.0759531, acc 0.984375, learning_rate 0.00010003
2017-10-10T11:51:12.127372: step 2934, loss 0.103282, acc 0.96875, learning_rate 0.00010003
2017-10-10T11:51:12.404361: step 2935, loss 0.164444, acc 0.9375, learning_rate 0.00010003
2017-10-10T11:51:12.637922: step 2936, loss 0.13205, acc 0.9375, learning_rate 0.00010003
2017-10-10T11:51:12.877481: step 2937, loss 0.17295, acc 0.90625, learning_rate 0.00010003
2017-10-10T11:51:13.141591: step 2938, loss 0.0352672, acc 0.984375, learning_rate 0.00010003
2017-10-10T11:51:13.441158: step 2939, loss 0.113995, acc 0.953125, learning_rate 0.00010003
2017-10-10T11:51:13.672127: step 2940, loss 0.0519605, acc 0.980392, learning_rate 0.000100029
2017-10-10T11:51:13.937285: step 2941, loss 0.086435, acc 0.96875, learning_rate 0.000100029
2017-10-10T11:51:14.179063: step 2942, loss 0.203043, acc 0.953125, learning_rate 0.000100029
2017-10-10T11:51:14.424145: step 2943, loss 0.1221, acc 0.96875, learning_rate 0.000100029
2017-10-10T11:51:14.685283: step 2944, loss 0.182631, acc 0.953125, learning_rate 0.000100029
2017-10-10T11:51:14.943427: step 2945, loss 0.0506988, acc 1, learning_rate 0.000100029
2017-10-10T11:51:15.205021: step 2946, loss 0.0712226, acc 0.96875, learning_rate 0.000100029
2017-10-10T11:51:15.449380: step 2947, loss 0.0947158, acc 0.984375, learning_rate 0.000100029
2017-10-10T11:51:15.750188: step 2948, loss 0.150981, acc 0.9375, learning_rate 0.000100029
2017-10-10T11:51:15.975480: step 2949, loss 0.183773, acc 0.9375, learning_rate 0.000100028
2017-10-10T11:51:16.177151: step 2950, loss 0.0996854, acc 0.984375, learning_rate 0.000100028
2017-10-10T11:51:16.368715: step 2951, loss 0.125286, acc 0.953125, learning_rate 0.000100028
2017-10-10T11:51:16.570714: step 2952, loss 0.130487, acc 0.9375, learning_rate 0.000100028
2017-10-10T11:51:16.791542: step 2953, loss 0.122258, acc 0.96875, learning_rate 0.000100028
2017-10-10T11:51:17.042510: step 2954, loss 0.0939714, acc 0.96875, learning_rate 0.000100028
2017-10-10T11:51:17.296892: step 2955, loss 0.0512086, acc 0.984375, learning_rate 0.000100028
2017-10-10T11:51:17.511099: step 2956, loss 0.0933206, acc 0.96875, learning_rate 0.000100028
2017-10-10T11:51:17.771503: step 2957, loss 0.0744112, acc 0.96875, learning_rate 0.000100028
2017-10-10T11:51:18.006641: step 2958, loss 0.1358, acc 0.9375, learning_rate 0.000100027
2017-10-10T11:51:18.294476: step 2959, loss 0.0854669, acc 0.953125, learning_rate 0.000100027
2017-10-10T11:51:18.530064: step 2960, loss 0.0924004, acc 0.96875, learning_rate 0.000100027

Evaluation:
2017-10-10T11:51:19.106250: step 2960, loss 0.212944, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-2960

2017-10-10T11:51:20.197218: step 2961, loss 0.239662, acc 0.96875, learning_rate 0.000100027
2017-10-10T11:51:20.437735: step 2962, loss 0.0899041, acc 0.984375, learning_rate 0.000100027
2017-10-10T11:51:20.685315: step 2963, loss 0.0799004, acc 0.953125, learning_rate 0.000100027
2017-10-10T11:51:20.944939: step 2964, loss 0.0645685, acc 0.984375, learning_rate 0.000100027
2017-10-10T11:51:21.231650: step 2965, loss 0.0512004, acc 0.984375, learning_rate 0.000100027
2017-10-10T11:51:21.548362: step 2966, loss 0.141435, acc 0.9375, learning_rate 0.000100027
2017-10-10T11:51:21.718179: step 2967, loss 0.162342, acc 0.9375, learning_rate 0.000100026
2017-10-10T11:51:21.918860: step 2968, loss 0.0414029, acc 0.984375, learning_rate 0.000100026
2017-10-10T11:51:22.106206: step 2969, loss 0.125403, acc 0.953125, learning_rate 0.000100026
2017-10-10T11:51:22.281557: step 2970, loss 0.130823, acc 0.96875, learning_rate 0.000100026
2017-10-10T11:51:22.478858: step 2971, loss 0.0626148, acc 1, learning_rate 0.000100026
2017-10-10T11:51:22.729731: step 2972, loss 0.140779, acc 0.9375, learning_rate 0.000100026
2017-10-10T11:51:23.005976: step 2973, loss 0.0500138, acc 0.984375, learning_rate 0.000100026
2017-10-10T11:51:23.236835: step 2974, loss 0.0632375, acc 0.984375, learning_rate 0.000100026
2017-10-10T11:51:23.459339: step 2975, loss 0.105956, acc 0.96875, learning_rate 0.000100026
2017-10-10T11:51:23.755711: step 2976, loss 0.0300515, acc 1, learning_rate 0.000100025
2017-10-10T11:51:23.954273: step 2977, loss 0.181006, acc 0.9375, learning_rate 0.000100025
2017-10-10T11:51:24.136882: step 2978, loss 0.137948, acc 0.953125, learning_rate 0.000100025
2017-10-10T11:51:24.329815: step 2979, loss 0.0648588, acc 1, learning_rate 0.000100025
2017-10-10T11:51:24.514253: step 2980, loss 0.106268, acc 0.96875, learning_rate 0.000100025
2017-10-10T11:51:24.720830: step 2981, loss 0.0783896, acc 0.984375, learning_rate 0.000100025
2017-10-10T11:51:24.948387: step 2982, loss 0.0828535, acc 0.984375, learning_rate 0.000100025
2017-10-10T11:51:25.162276: step 2983, loss 0.156064, acc 0.96875, learning_rate 0.000100025
2017-10-10T11:51:25.400831: step 2984, loss 0.0650342, acc 0.984375, learning_rate 0.000100025
2017-10-10T11:51:25.627604: step 2985, loss 0.197173, acc 0.953125, learning_rate 0.000100025
2017-10-10T11:51:25.896075: step 2986, loss 0.103488, acc 0.96875, learning_rate 0.000100024
2017-10-10T11:51:26.128666: step 2987, loss 0.116731, acc 0.9375, learning_rate 0.000100024
2017-10-10T11:51:26.361167: step 2988, loss 0.113235, acc 0.921875, learning_rate 0.000100024
2017-10-10T11:51:26.622815: step 2989, loss 0.222741, acc 0.9375, learning_rate 0.000100024
2017-10-10T11:51:26.848850: step 2990, loss 0.0887258, acc 0.953125, learning_rate 0.000100024
2017-10-10T11:51:27.112902: step 2991, loss 0.0715837, acc 0.9375, learning_rate 0.000100024
2017-10-10T11:51:27.352891: step 2992, loss 0.206053, acc 0.875, learning_rate 0.000100024
2017-10-10T11:51:27.588928: step 2993, loss 0.0867756, acc 0.984375, learning_rate 0.000100024
2017-10-10T11:51:27.847415: step 2994, loss 0.0543203, acc 0.984375, learning_rate 0.000100024
2017-10-10T11:51:28.064184: step 2995, loss 0.0367781, acc 1, learning_rate 0.000100024
2017-10-10T11:51:28.355219: step 2996, loss 0.083147, acc 0.984375, learning_rate 0.000100023
2017-10-10T11:51:28.587891: step 2997, loss 0.0293404, acc 1, learning_rate 0.000100023
2017-10-10T11:51:28.841368: step 2998, loss 0.108283, acc 0.96875, learning_rate 0.000100023
2017-10-10T11:51:29.104489: step 2999, loss 0.0942297, acc 0.953125, learning_rate 0.000100023
2017-10-10T11:51:29.365237: step 3000, loss 0.106623, acc 0.96875, learning_rate 0.000100023

Evaluation:
2017-10-10T11:51:29.904311: step 3000, loss 0.209797, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3000

2017-10-10T11:51:31.054861: step 3001, loss 0.141993, acc 0.9375, learning_rate 0.000100023
2017-10-10T11:51:31.297830: step 3002, loss 0.148311, acc 0.9375, learning_rate 0.000100023
2017-10-10T11:51:31.521715: step 3003, loss 0.197113, acc 0.921875, learning_rate 0.000100023
2017-10-10T11:51:31.866766: step 3004, loss 0.0526417, acc 0.984375, learning_rate 0.000100023
2017-10-10T11:51:32.084841: step 3005, loss 0.0941948, acc 0.96875, learning_rate 0.000100023
2017-10-10T11:51:32.335817: step 3006, loss 0.194335, acc 0.9375, learning_rate 0.000100023
2017-10-10T11:51:32.469433: step 3007, loss 0.180424, acc 0.921875, learning_rate 0.000100022
2017-10-10T11:51:32.603156: step 3008, loss 0.164786, acc 0.90625, learning_rate 0.000100022
2017-10-10T11:51:32.747496: step 3009, loss 0.0911018, acc 0.953125, learning_rate 0.000100022
2017-10-10T11:51:32.879785: step 3010, loss 0.168743, acc 0.9375, learning_rate 0.000100022
2017-10-10T11:51:33.090872: step 3011, loss 0.0961948, acc 0.984375, learning_rate 0.000100022
2017-10-10T11:51:33.267011: step 3012, loss 0.107582, acc 0.96875, learning_rate 0.000100022
2017-10-10T11:51:33.504994: step 3013, loss 0.0814261, acc 0.984375, learning_rate 0.000100022
2017-10-10T11:51:33.742794: step 3014, loss 0.0977224, acc 0.96875, learning_rate 0.000100022
2017-10-10T11:51:33.993717: step 3015, loss 0.122561, acc 0.9375, learning_rate 0.000100022
2017-10-10T11:51:34.239553: step 3016, loss 0.168111, acc 0.9375, learning_rate 0.000100022
2017-10-10T11:51:34.480839: step 3017, loss 0.0852114, acc 0.96875, learning_rate 0.000100022
2017-10-10T11:51:34.751970: step 3018, loss 0.110144, acc 0.9375, learning_rate 0.000100021
2017-10-10T11:51:34.964213: step 3019, loss 0.16959, acc 0.921875, learning_rate 0.000100021
2017-10-10T11:51:35.197685: step 3020, loss 0.0451249, acc 1, learning_rate 0.000100021
2017-10-10T11:51:35.468264: step 3021, loss 0.0624114, acc 0.96875, learning_rate 0.000100021
2017-10-10T11:51:35.689103: step 3022, loss 0.204441, acc 0.921875, learning_rate 0.000100021
2017-10-10T11:51:35.925223: step 3023, loss 0.104192, acc 0.984375, learning_rate 0.000100021
2017-10-10T11:51:36.159878: step 3024, loss 0.0624481, acc 0.984375, learning_rate 0.000100021
2017-10-10T11:51:36.398159: step 3025, loss 0.159944, acc 0.9375, learning_rate 0.000100021
2017-10-10T11:51:36.612556: step 3026, loss 0.0570685, acc 1, learning_rate 0.000100021
2017-10-10T11:51:36.881251: step 3027, loss 0.0355709, acc 1, learning_rate 0.000100021
2017-10-10T11:51:37.097589: step 3028, loss 0.129619, acc 0.96875, learning_rate 0.000100021
2017-10-10T11:51:37.350578: step 3029, loss 0.0401997, acc 1, learning_rate 0.00010002
2017-10-10T11:51:37.593962: step 3030, loss 0.127426, acc 0.96875, learning_rate 0.00010002
2017-10-10T11:51:37.808922: step 3031, loss 0.158878, acc 0.953125, learning_rate 0.00010002
2017-10-10T11:51:38.064867: step 3032, loss 0.0660562, acc 0.984375, learning_rate 0.00010002
2017-10-10T11:51:38.307879: step 3033, loss 0.0975073, acc 0.96875, learning_rate 0.00010002
2017-10-10T11:51:38.546491: step 3034, loss 0.17496, acc 0.96875, learning_rate 0.00010002
2017-10-10T11:51:38.796964: step 3035, loss 0.168276, acc 0.9375, learning_rate 0.00010002
2017-10-10T11:51:39.033155: step 3036, loss 0.216359, acc 0.921875, learning_rate 0.00010002
2017-10-10T11:51:39.271903: step 3037, loss 0.0358839, acc 0.984375, learning_rate 0.00010002
2017-10-10T11:51:39.428238: step 3038, loss 0.0739206, acc 0.960784, learning_rate 0.00010002
2017-10-10T11:51:39.639621: step 3039, loss 0.147016, acc 0.953125, learning_rate 0.00010002
2017-10-10T11:51:39.931342: step 3040, loss 0.151432, acc 0.9375, learning_rate 0.00010002

Evaluation:
2017-10-10T11:51:40.433534: step 3040, loss 0.208208, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3040

2017-10-10T11:51:41.475912: step 3041, loss 0.20725, acc 0.921875, learning_rate 0.00010002
2017-10-10T11:51:41.694233: step 3042, loss 0.125544, acc 0.96875, learning_rate 0.000100019
2017-10-10T11:51:41.940664: step 3043, loss 0.112835, acc 0.9375, learning_rate 0.000100019
2017-10-10T11:51:42.256856: step 3044, loss 0.146761, acc 0.9375, learning_rate 0.000100019
2017-10-10T11:51:42.520831: step 3045, loss 0.110738, acc 0.9375, learning_rate 0.000100019
2017-10-10T11:51:42.694503: step 3046, loss 0.0371519, acc 1, learning_rate 0.000100019
2017-10-10T11:51:42.894246: step 3047, loss 0.175389, acc 0.953125, learning_rate 0.000100019
2017-10-10T11:51:43.069030: step 3048, loss 0.0742013, acc 0.96875, learning_rate 0.000100019
2017-10-10T11:51:43.252965: step 3049, loss 0.136817, acc 0.9375, learning_rate 0.000100019
2017-10-10T11:51:43.433221: step 3050, loss 0.0798139, acc 0.984375, learning_rate 0.000100019
2017-10-10T11:51:43.624709: step 3051, loss 0.0561206, acc 1, learning_rate 0.000100019
2017-10-10T11:51:43.870001: step 3052, loss 0.0504404, acc 0.984375, learning_rate 0.000100019
2017-10-10T11:51:44.097686: step 3053, loss 0.0590673, acc 0.96875, learning_rate 0.000100019
2017-10-10T11:51:44.324620: step 3054, loss 0.1425, acc 0.96875, learning_rate 0.000100018
2017-10-10T11:51:44.565739: step 3055, loss 0.037464, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:51:44.802949: step 3056, loss 0.0963534, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:51:45.063600: step 3057, loss 0.0655195, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:51:45.288913: step 3058, loss 0.0995489, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:51:45.506686: step 3059, loss 0.0802771, acc 0.96875, learning_rate 0.000100018
2017-10-10T11:51:45.762654: step 3060, loss 0.0995033, acc 0.96875, learning_rate 0.000100018
2017-10-10T11:51:46.006767: step 3061, loss 0.0741624, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:51:46.243977: step 3062, loss 0.131629, acc 0.953125, learning_rate 0.000100018
2017-10-10T11:51:46.489610: step 3063, loss 0.072513, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:51:46.718699: step 3064, loss 0.136676, acc 0.9375, learning_rate 0.000100018
2017-10-10T11:51:46.963336: step 3065, loss 0.081724, acc 0.984375, learning_rate 0.000100018
2017-10-10T11:51:47.240257: step 3066, loss 0.114004, acc 0.96875, learning_rate 0.000100018
2017-10-10T11:51:47.490442: step 3067, loss 0.137877, acc 0.9375, learning_rate 0.000100018
2017-10-10T11:51:47.732937: step 3068, loss 0.0765712, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:51:48.000291: step 3069, loss 0.136309, acc 0.953125, learning_rate 0.000100017
2017-10-10T11:51:48.246636: step 3070, loss 0.0720214, acc 0.984375, learning_rate 0.000100017
2017-10-10T11:51:48.468981: step 3071, loss 0.0757663, acc 0.984375, learning_rate 0.000100017
2017-10-10T11:51:48.712900: step 3072, loss 0.0898269, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:51:48.928433: step 3073, loss 0.0936139, acc 0.984375, learning_rate 0.000100017
2017-10-10T11:51:49.268907: step 3074, loss 0.120593, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:51:49.466264: step 3075, loss 0.101504, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:51:49.647977: step 3076, loss 0.198021, acc 0.953125, learning_rate 0.000100017
2017-10-10T11:51:49.824352: step 3077, loss 0.194058, acc 0.921875, learning_rate 0.000100017
2017-10-10T11:51:50.045058: step 3078, loss 0.112213, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:51:50.248832: step 3079, loss 0.136914, acc 0.96875, learning_rate 0.000100017
2017-10-10T11:51:50.508871: step 3080, loss 0.0764171, acc 0.984375, learning_rate 0.000100017

Evaluation:
2017-10-10T11:51:51.083967: step 3080, loss 0.209391, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3080

2017-10-10T11:51:52.387539: step 3081, loss 0.162537, acc 0.9375, learning_rate 0.000100017
2017-10-10T11:51:52.766960: step 3082, loss 0.081181, acc 0.96875, learning_rate 0.000100016
2017-10-10T11:51:52.939334: step 3083, loss 0.0313529, acc 1, learning_rate 0.000100016
2017-10-10T11:51:53.113138: step 3084, loss 0.143056, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:51:53.284700: step 3085, loss 0.0917813, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:51:53.473467: step 3086, loss 0.115187, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:51:53.663374: step 3087, loss 0.127653, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:51:53.859498: step 3088, loss 0.176147, acc 0.96875, learning_rate 0.000100016
2017-10-10T11:51:54.102122: step 3089, loss 0.0689988, acc 1, learning_rate 0.000100016
2017-10-10T11:51:54.354009: step 3090, loss 0.0589277, acc 0.984375, learning_rate 0.000100016
2017-10-10T11:51:54.591653: step 3091, loss 0.140331, acc 0.921875, learning_rate 0.000100016
2017-10-10T11:51:54.836333: step 3092, loss 0.132362, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:51:55.129733: step 3093, loss 0.0689898, acc 0.984375, learning_rate 0.000100016
2017-10-10T11:51:55.384861: step 3094, loss 0.119309, acc 0.953125, learning_rate 0.000100016
2017-10-10T11:51:55.623936: step 3095, loss 0.199877, acc 0.9375, learning_rate 0.000100016
2017-10-10T11:51:55.862633: step 3096, loss 0.0920553, acc 0.96875, learning_rate 0.000100016
2017-10-10T11:51:56.115708: step 3097, loss 0.0728772, acc 0.984375, learning_rate 0.000100016
2017-10-10T11:51:56.358760: step 3098, loss 0.11338, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:51:56.581212: step 3099, loss 0.188195, acc 0.921875, learning_rate 0.000100015
2017-10-10T11:51:56.858991: step 3100, loss 0.0687683, acc 1, learning_rate 0.000100015
2017-10-10T11:51:57.076967: step 3101, loss 0.143726, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:51:57.404244: step 3102, loss 0.0639606, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:51:57.584247: step 3103, loss 0.179947, acc 0.9375, learning_rate 0.000100015
2017-10-10T11:51:57.796148: step 3104, loss 0.0594984, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:51:57.984375: step 3105, loss 0.112149, acc 0.921875, learning_rate 0.000100015
2017-10-10T11:51:58.185011: step 3106, loss 0.0825521, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:51:58.389919: step 3107, loss 0.0761169, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:51:58.548718: step 3108, loss 0.121411, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:51:58.771375: step 3109, loss 0.13714, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:51:59.032428: step 3110, loss 0.0535123, acc 0.984375, learning_rate 0.000100015
2017-10-10T11:51:59.251814: step 3111, loss 0.150737, acc 0.953125, learning_rate 0.000100015
2017-10-10T11:51:59.518736: step 3112, loss 0.130636, acc 0.9375, learning_rate 0.000100015
2017-10-10T11:51:59.764960: step 3113, loss 0.0454151, acc 1, learning_rate 0.000100015
2017-10-10T11:52:00.039894: step 3114, loss 0.0746392, acc 0.984375, learning_rate 0.000100014
2017-10-10T11:52:00.289064: step 3115, loss 0.179405, acc 0.9375, learning_rate 0.000100014
2017-10-10T11:52:00.523172: step 3116, loss 0.105729, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:52:00.761339: step 3117, loss 0.123908, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:52:01.015465: step 3118, loss 0.128258, acc 0.953125, learning_rate 0.000100014
2017-10-10T11:52:01.277041: step 3119, loss 0.0375029, acc 1, learning_rate 0.000100014
2017-10-10T11:52:01.543929: step 3120, loss 0.139232, acc 0.984375, learning_rate 0.000100014

Evaluation:
2017-10-10T11:52:02.108856: step 3120, loss 0.209734, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3120

2017-10-10T11:52:03.264837: step 3121, loss 0.0813209, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:52:03.520788: step 3122, loss 0.0649382, acc 0.984375, learning_rate 0.000100014
2017-10-10T11:52:03.699400: step 3123, loss 0.223193, acc 0.921875, learning_rate 0.000100014
2017-10-10T11:52:03.875364: step 3124, loss 0.138626, acc 0.953125, learning_rate 0.000100014
2017-10-10T11:52:04.066124: step 3125, loss 0.117777, acc 0.953125, learning_rate 0.000100014
2017-10-10T11:52:04.273496: step 3126, loss 0.0886286, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:52:04.521851: step 3127, loss 0.0674228, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:52:04.768878: step 3128, loss 0.0774764, acc 0.984375, learning_rate 0.000100014
2017-10-10T11:52:05.036006: step 3129, loss 0.0905544, acc 0.984375, learning_rate 0.000100014
2017-10-10T11:52:05.308884: step 3130, loss 0.0855992, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:52:05.588792: step 3131, loss 0.101746, acc 0.96875, learning_rate 0.000100014
2017-10-10T11:52:05.834640: step 3132, loss 0.0705873, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:52:06.040889: step 3133, loss 0.121917, acc 0.9375, learning_rate 0.000100013
2017-10-10T11:52:06.235660: step 3134, loss 0.0625457, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:52:06.423333: step 3135, loss 0.0882441, acc 0.953125, learning_rate 0.000100013
2017-10-10T11:52:06.650054: step 3136, loss 0.129648, acc 0.921569, learning_rate 0.000100013
2017-10-10T11:52:06.900898: step 3137, loss 0.109514, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:52:07.126247: step 3138, loss 0.0806094, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:52:07.345920: step 3139, loss 0.0933203, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:52:07.580216: step 3140, loss 0.133275, acc 0.9375, learning_rate 0.000100013
2017-10-10T11:52:07.829231: step 3141, loss 0.076423, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:52:08.067042: step 3142, loss 0.0679215, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:52:08.305302: step 3143, loss 0.0608431, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:52:08.582468: step 3144, loss 0.079186, acc 0.984375, learning_rate 0.000100013
2017-10-10T11:52:08.832959: step 3145, loss 0.0279753, acc 1, learning_rate 0.000100013
2017-10-10T11:52:09.093376: step 3146, loss 0.038111, acc 1, learning_rate 0.000100013
2017-10-10T11:52:09.361621: step 3147, loss 0.101554, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:52:09.593327: step 3148, loss 0.134716, acc 0.9375, learning_rate 0.000100013
2017-10-10T11:52:09.845872: step 3149, loss 0.11426, acc 0.96875, learning_rate 0.000100013
2017-10-10T11:52:10.117363: step 3150, loss 0.0796366, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:52:10.361513: step 3151, loss 0.0492322, acc 1, learning_rate 0.000100012
2017-10-10T11:52:10.611130: step 3152, loss 0.0717491, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:52:10.873858: step 3153, loss 0.0615798, acc 1, learning_rate 0.000100012
2017-10-10T11:52:11.137706: step 3154, loss 0.113886, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:52:11.377762: step 3155, loss 0.128773, acc 0.9375, learning_rate 0.000100012
2017-10-10T11:52:11.576745: step 3156, loss 0.126309, acc 0.9375, learning_rate 0.000100012
2017-10-10T11:52:12.295029: step 3157, loss 0.0938957, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:52:12.547988: step 3158, loss 0.111108, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:52:12.789352: step 3159, loss 0.08991, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:52:13.036484: step 3160, loss 0.09544, acc 0.953125, learning_rate 0.000100012

Evaluation:
2017-10-10T11:52:13.596034: step 3160, loss 0.211503, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3160

2017-10-10T11:52:14.662536: step 3161, loss 0.150287, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:52:14.796510: step 3162, loss 0.1575, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:52:14.931173: step 3163, loss 0.210747, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:52:15.070255: step 3164, loss 0.096833, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:52:15.200577: step 3165, loss 0.110736, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:52:15.468882: step 3166, loss 0.171814, acc 0.953125, learning_rate 0.000100012
2017-10-10T11:52:15.700954: step 3167, loss 0.158, acc 0.921875, learning_rate 0.000100012
2017-10-10T11:52:15.952701: step 3168, loss 0.0808801, acc 1, learning_rate 0.000100012
2017-10-10T11:52:16.197095: step 3169, loss 0.105164, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:52:16.461000: step 3170, loss 0.0801011, acc 0.96875, learning_rate 0.000100012
2017-10-10T11:52:16.690702: step 3171, loss 0.0747182, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:52:16.937015: step 3172, loss 0.0389192, acc 1, learning_rate 0.000100011
2017-10-10T11:52:17.217813: step 3173, loss 0.12494, acc 0.921875, learning_rate 0.000100011
2017-10-10T11:52:17.444836: step 3174, loss 0.0987176, acc 0.96875, learning_rate 0.000100011
2017-10-10T11:52:17.716888: step 3175, loss 0.0617302, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:52:17.968718: step 3176, loss 0.0860131, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:52:18.205644: step 3177, loss 0.0316332, acc 1, learning_rate 0.000100011
2017-10-10T11:52:18.471092: step 3178, loss 0.136373, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:52:18.720687: step 3179, loss 0.135279, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:52:18.992807: step 3180, loss 0.159821, acc 0.90625, learning_rate 0.000100011
2017-10-10T11:52:19.215414: step 3181, loss 0.0499783, acc 1, learning_rate 0.000100011
2017-10-10T11:52:19.491584: step 3182, loss 0.119466, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:52:19.723210: step 3183, loss 0.0665699, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:52:19.974790: step 3184, loss 0.150145, acc 0.921875, learning_rate 0.000100011
2017-10-10T11:52:20.231876: step 3185, loss 0.0637585, acc 1, learning_rate 0.000100011
2017-10-10T11:52:20.465314: step 3186, loss 0.102877, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:52:20.738154: step 3187, loss 0.091944, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:52:20.985578: step 3188, loss 0.178998, acc 0.9375, learning_rate 0.000100011
2017-10-10T11:52:21.225912: step 3189, loss 0.0661633, acc 0.984375, learning_rate 0.000100011
2017-10-10T11:52:21.481679: step 3190, loss 0.100168, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:52:21.721023: step 3191, loss 0.222416, acc 0.9375, learning_rate 0.000100011
2017-10-10T11:52:22.008621: step 3192, loss 0.0928539, acc 0.953125, learning_rate 0.000100011
2017-10-10T11:52:22.239126: step 3193, loss 0.0955371, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:52:22.507630: step 3194, loss 0.0526055, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:52:22.717039: step 3195, loss 0.0806831, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:52:23.038861: step 3196, loss 0.13336, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:52:23.271883: step 3197, loss 0.0577657, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:52:23.474415: step 3198, loss 0.115572, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:52:23.667456: step 3199, loss 0.0570077, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:52:23.854286: step 3200, loss 0.0690989, acc 0.984375, learning_rate 0.00010001

Evaluation:
2017-10-10T11:52:24.410884: step 3200, loss 0.210722, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3200

2017-10-10T11:52:25.601475: step 3201, loss 0.152595, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:52:25.860907: step 3202, loss 0.0517901, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:52:26.096623: step 3203, loss 0.0804652, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:52:26.372903: step 3204, loss 0.108399, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:52:26.635448: step 3205, loss 0.122036, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:52:26.859902: step 3206, loss 0.198013, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:52:27.097530: step 3207, loss 0.154557, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:52:27.360325: step 3208, loss 0.0774871, acc 0.984375, learning_rate 0.00010001
2017-10-10T11:52:27.600227: step 3209, loss 0.212932, acc 0.90625, learning_rate 0.00010001
2017-10-10T11:52:27.869666: step 3210, loss 0.0845534, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:52:28.099776: step 3211, loss 0.18471, acc 0.9375, learning_rate 0.00010001
2017-10-10T11:52:28.323002: step 3212, loss 0.0998419, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:52:28.575470: step 3213, loss 0.100498, acc 0.96875, learning_rate 0.00010001
2017-10-10T11:52:28.823801: step 3214, loss 0.142967, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:52:29.107937: step 3215, loss 0.129648, acc 0.953125, learning_rate 0.00010001
2017-10-10T11:52:29.401518: step 3216, loss 0.0314057, acc 1, learning_rate 0.00010001
2017-10-10T11:52:29.636175: step 3217, loss 0.158238, acc 0.9375, learning_rate 0.000100009
2017-10-10T11:52:29.897021: step 3218, loss 0.107359, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:52:30.115106: step 3219, loss 0.138779, acc 0.9375, learning_rate 0.000100009
2017-10-10T11:52:30.357768: step 3220, loss 0.169512, acc 0.921875, learning_rate 0.000100009
2017-10-10T11:52:30.594759: step 3221, loss 0.135194, acc 0.9375, learning_rate 0.000100009
2017-10-10T11:52:30.918513: step 3222, loss 0.0815267, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:52:31.134551: step 3223, loss 0.0836465, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:52:31.347975: step 3224, loss 0.0411945, acc 1, learning_rate 0.000100009
2017-10-10T11:52:31.551434: step 3225, loss 0.134577, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:52:31.811522: step 3226, loss 0.0703429, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:52:32.107872: step 3227, loss 0.276335, acc 0.90625, learning_rate 0.000100009
2017-10-10T11:52:32.361379: step 3228, loss 0.0939535, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:52:32.598602: step 3229, loss 0.0781716, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:52:32.834367: step 3230, loss 0.135724, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:52:33.064890: step 3231, loss 0.105336, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:52:33.312402: step 3232, loss 0.119904, acc 0.9375, learning_rate 0.000100009
2017-10-10T11:52:33.576652: step 3233, loss 0.120574, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:52:33.816141: step 3234, loss 0.0660708, acc 0.980392, learning_rate 0.000100009
2017-10-10T11:52:34.064833: step 3235, loss 0.0626111, acc 1, learning_rate 0.000100009
2017-10-10T11:52:34.329015: step 3236, loss 0.0709299, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:52:34.578106: step 3237, loss 0.0527217, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:52:34.812921: step 3238, loss 0.0387165, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:52:35.048088: step 3239, loss 0.0925332, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:52:35.382859: step 3240, loss 0.102901, acc 0.96875, learning_rate 0.000100009

Evaluation:
2017-10-10T11:52:35.829929: step 3240, loss 0.206807, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3240

2017-10-10T11:52:36.688449: step 3241, loss 0.125097, acc 0.953125, learning_rate 0.000100009
2017-10-10T11:52:36.967179: step 3242, loss 0.108844, acc 0.984375, learning_rate 0.000100009
2017-10-10T11:52:37.220866: step 3243, loss 0.116673, acc 0.96875, learning_rate 0.000100009
2017-10-10T11:52:37.432299: step 3244, loss 0.0590197, acc 1, learning_rate 0.000100009
2017-10-10T11:52:37.684961: step 3245, loss 0.166672, acc 0.921875, learning_rate 0.000100008
2017-10-10T11:52:37.914861: step 3246, loss 0.142988, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:52:38.156348: step 3247, loss 0.118317, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:52:38.415260: step 3248, loss 0.0906942, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:52:38.692947: step 3249, loss 0.0860853, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:52:38.939263: step 3250, loss 0.0374576, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:52:39.160875: step 3251, loss 0.0673943, acc 1, learning_rate 0.000100008
2017-10-10T11:52:39.352878: step 3252, loss 0.200875, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:52:39.553125: step 3253, loss 0.105114, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:52:39.768967: step 3254, loss 0.222442, acc 0.921875, learning_rate 0.000100008
2017-10-10T11:52:40.011261: step 3255, loss 0.106692, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:52:40.281567: step 3256, loss 0.0692548, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:52:40.558121: step 3257, loss 0.0988461, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:52:40.804994: step 3258, loss 0.168078, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:52:41.054366: step 3259, loss 0.191964, acc 0.90625, learning_rate 0.000100008
2017-10-10T11:52:41.300903: step 3260, loss 0.186261, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:52:41.522378: step 3261, loss 0.0472225, acc 1, learning_rate 0.000100008
2017-10-10T11:52:41.772472: step 3262, loss 0.108262, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:52:42.019708: step 3263, loss 0.0626298, acc 1, learning_rate 0.000100008
2017-10-10T11:52:42.271659: step 3264, loss 0.0772503, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:52:42.525962: step 3265, loss 0.248081, acc 0.921875, learning_rate 0.000100008
2017-10-10T11:52:42.756835: step 3266, loss 0.108285, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:52:43.035840: step 3267, loss 0.142124, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:52:43.255593: step 3268, loss 0.0627315, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:52:43.524816: step 3269, loss 0.112395, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:52:43.779974: step 3270, loss 0.162658, acc 0.9375, learning_rate 0.000100008
2017-10-10T11:52:44.046082: step 3271, loss 0.106001, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:52:44.289180: step 3272, loss 0.0820093, acc 0.96875, learning_rate 0.000100008
2017-10-10T11:52:44.536944: step 3273, loss 0.0913164, acc 0.953125, learning_rate 0.000100008
2017-10-10T11:52:44.790870: step 3274, loss 0.0614494, acc 0.984375, learning_rate 0.000100008
2017-10-10T11:52:45.001679: step 3275, loss 0.120213, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:52:45.256946: step 3276, loss 0.0877798, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:52:45.513172: step 3277, loss 0.119, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:52:45.772907: step 3278, loss 0.108377, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:52:46.120816: step 3279, loss 0.13519, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:52:46.328874: step 3280, loss 0.0538118, acc 1, learning_rate 0.000100007

Evaluation:
2017-10-10T11:52:46.800001: step 3280, loss 0.210062, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3280

2017-10-10T11:52:47.864908: step 3281, loss 0.0597416, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:52:48.053023: step 3282, loss 0.0836199, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:52:48.304881: step 3283, loss 0.228224, acc 0.921875, learning_rate 0.000100007
2017-10-10T11:52:48.560868: step 3284, loss 0.0866461, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:52:48.820198: step 3285, loss 0.126667, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:52:49.098820: step 3286, loss 0.0646184, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:52:49.338550: step 3287, loss 0.0793697, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:52:49.604032: step 3288, loss 0.0571295, acc 1, learning_rate 0.000100007
2017-10-10T11:52:49.854144: step 3289, loss 0.146779, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:52:50.068958: step 3290, loss 0.0596516, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:52:50.299769: step 3291, loss 0.152485, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:52:50.526148: step 3292, loss 0.101025, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:52:50.787484: step 3293, loss 0.110829, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:52:51.017956: step 3294, loss 0.0874684, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:52:51.214490: step 3295, loss 0.0778078, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:52:51.429101: step 3296, loss 0.0406308, acc 1, learning_rate 0.000100007
2017-10-10T11:52:51.684482: step 3297, loss 0.258215, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:52:51.944983: step 3298, loss 0.214305, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:52:52.164884: step 3299, loss 0.165193, acc 0.953125, learning_rate 0.000100007
2017-10-10T11:52:52.415160: step 3300, loss 0.114857, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:52:52.660830: step 3301, loss 0.0836793, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:52:52.933582: step 3302, loss 0.129817, acc 0.921875, learning_rate 0.000100007
2017-10-10T11:52:53.180899: step 3303, loss 0.0850232, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:52:53.404594: step 3304, loss 0.0900158, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:52:53.670867: step 3305, loss 0.0939171, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:52:53.940880: step 3306, loss 0.0886221, acc 0.984375, learning_rate 0.000100007
2017-10-10T11:52:54.135050: step 3307, loss 0.0330727, acc 1, learning_rate 0.000100007
2017-10-10T11:52:54.364460: step 3308, loss 0.0839713, acc 0.96875, learning_rate 0.000100007
2017-10-10T11:52:54.598076: step 3309, loss 0.209878, acc 0.9375, learning_rate 0.000100007
2017-10-10T11:52:54.825118: step 3310, loss 0.0974303, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:52:55.049047: step 3311, loss 0.111525, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:52:55.292565: step 3312, loss 0.0775335, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:52:55.540869: step 3313, loss 0.142106, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:52:55.792914: step 3314, loss 0.104317, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:52:56.109057: step 3315, loss 0.131516, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:52:56.355812: step 3316, loss 0.101888, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:52:56.490630: step 3317, loss 0.116658, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:52:56.628969: step 3318, loss 0.14085, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:52:56.765145: step 3319, loss 0.0525948, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:52:56.898859: step 3320, loss 0.108811, acc 0.984375, learning_rate 0.000100006

Evaluation:
2017-10-10T11:52:57.384982: step 3320, loss 0.208166, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3320

2017-10-10T11:52:58.770298: step 3321, loss 0.0625543, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:52:59.036827: step 3322, loss 0.148633, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:52:59.295804: step 3323, loss 0.082216, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:52:59.555335: step 3324, loss 0.107035, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:52:59.856209: step 3325, loss 0.0422332, acc 1, learning_rate 0.000100006
2017-10-10T11:53:00.095455: step 3326, loss 0.133126, acc 0.9375, learning_rate 0.000100006
2017-10-10T11:53:00.383648: step 3327, loss 0.0950532, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:53:00.624977: step 3328, loss 0.0805295, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:53:00.877217: step 3329, loss 0.0928432, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:53:01.129310: step 3330, loss 0.20236, acc 0.921875, learning_rate 0.000100006
2017-10-10T11:53:01.381169: step 3331, loss 0.0968442, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:53:01.604238: step 3332, loss 0.202858, acc 0.921569, learning_rate 0.000100006
2017-10-10T11:53:01.834405: step 3333, loss 0.0547224, acc 1, learning_rate 0.000100006
2017-10-10T11:53:02.073101: step 3334, loss 0.113046, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:53:02.315017: step 3335, loss 0.103354, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:53:02.538199: step 3336, loss 0.0860992, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:53:02.784879: step 3337, loss 0.0796493, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:53:03.030064: step 3338, loss 0.0442248, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:53:03.260872: step 3339, loss 0.0494851, acc 1, learning_rate 0.000100006
2017-10-10T11:53:03.526527: step 3340, loss 0.0831293, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:53:03.741999: step 3341, loss 0.047309, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:53:03.980054: step 3342, loss 0.14509, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:53:04.272099: step 3343, loss 0.116751, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:53:04.513497: step 3344, loss 0.12536, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:53:04.719874: step 3345, loss 0.0843665, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:53:04.921152: step 3346, loss 0.310631, acc 0.921875, learning_rate 0.000100006
2017-10-10T11:53:05.112553: step 3347, loss 0.0955986, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:53:05.351302: step 3348, loss 0.120804, acc 0.953125, learning_rate 0.000100006
2017-10-10T11:53:05.612886: step 3349, loss 0.0986449, acc 0.96875, learning_rate 0.000100006
2017-10-10T11:53:05.884799: step 3350, loss 0.0374557, acc 0.984375, learning_rate 0.000100006
2017-10-10T11:53:06.112261: step 3351, loss 0.105129, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:53:06.408879: step 3352, loss 0.16248, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:53:06.672938: step 3353, loss 0.0522768, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:06.846394: step 3354, loss 0.053489, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:53:07.028836: step 3355, loss 0.127921, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:53:07.229837: step 3356, loss 0.114048, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:07.422028: step 3357, loss 0.0541682, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:07.675226: step 3358, loss 0.132546, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:53:07.921370: step 3359, loss 0.0643509, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:53:08.156742: step 3360, loss 0.129109, acc 0.921875, learning_rate 0.000100005

Evaluation:
2017-10-10T11:53:08.752866: step 3360, loss 0.207848, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3360

2017-10-10T11:53:09.816836: step 3361, loss 0.0784328, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:10.037093: step 3362, loss 0.09615, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:53:10.285077: step 3363, loss 0.0653126, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:53:10.534697: step 3364, loss 0.143516, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:53:10.799924: step 3365, loss 0.0832358, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:53:11.076872: step 3366, loss 0.133253, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:53:11.313029: step 3367, loss 0.103143, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:11.583328: step 3368, loss 0.0586817, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:11.802114: step 3369, loss 0.0399349, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:12.016937: step 3370, loss 0.0412531, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:12.328996: step 3371, loss 0.260086, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:53:12.562678: step 3372, loss 0.0776709, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:12.772411: step 3373, loss 0.149906, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:53:12.952934: step 3374, loss 0.0931893, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:53:13.212361: step 3375, loss 0.118651, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:53:13.422141: step 3376, loss 0.0986045, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:53:13.671543: step 3377, loss 0.072926, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:13.924731: step 3378, loss 0.175131, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:53:14.160749: step 3379, loss 0.0884429, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:53:14.410475: step 3380, loss 0.108509, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:14.656737: step 3381, loss 0.208224, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:53:14.886170: step 3382, loss 0.0957504, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:15.117320: step 3383, loss 0.128461, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:53:15.389473: step 3384, loss 0.197746, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:53:15.603611: step 3385, loss 0.051024, acc 1, learning_rate 0.000100005
2017-10-10T11:53:15.849231: step 3386, loss 0.0658479, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:16.089513: step 3387, loss 0.0581939, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:53:16.340756: step 3388, loss 0.0803256, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:53:16.581853: step 3389, loss 0.0863551, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:16.891412: step 3390, loss 0.128994, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:53:17.161620: step 3391, loss 0.108344, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:53:17.348520: step 3392, loss 0.170079, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:53:17.544787: step 3393, loss 0.168241, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:53:17.733808: step 3394, loss 0.132649, acc 0.9375, learning_rate 0.000100005
2017-10-10T11:53:17.922093: step 3395, loss 0.0690493, acc 0.984375, learning_rate 0.000100005
2017-10-10T11:53:18.140978: step 3396, loss 0.096907, acc 0.96875, learning_rate 0.000100005
2017-10-10T11:53:18.392278: step 3397, loss 0.24895, acc 0.890625, learning_rate 0.000100005
2017-10-10T11:53:18.707467: step 3398, loss 0.165092, acc 0.921875, learning_rate 0.000100005
2017-10-10T11:53:18.923532: step 3399, loss 0.101713, acc 0.953125, learning_rate 0.000100005
2017-10-10T11:53:19.140565: step 3400, loss 0.20703, acc 0.921875, learning_rate 0.000100004

Evaluation:
2017-10-10T11:53:19.657831: step 3400, loss 0.207615, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3400

2017-10-10T11:53:20.827020: step 3401, loss 0.0473186, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:21.031469: step 3402, loss 0.120615, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:53:21.255881: step 3403, loss 0.0559376, acc 1, learning_rate 0.000100004
2017-10-10T11:53:21.467315: step 3404, loss 0.0839062, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:53:21.673189: step 3405, loss 0.113224, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:53:21.878029: step 3406, loss 0.202351, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:53:22.164044: step 3407, loss 0.171508, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:53:22.419544: step 3408, loss 0.0812432, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:22.672844: step 3409, loss 0.0633406, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:22.939629: step 3410, loss 0.152744, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:53:23.185564: step 3411, loss 0.13461, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:53:23.451669: step 3412, loss 0.0864677, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:23.679826: step 3413, loss 0.0254783, acc 1, learning_rate 0.000100004
2017-10-10T11:53:23.923903: step 3414, loss 0.0775772, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:53:24.192318: step 3415, loss 0.048709, acc 1, learning_rate 0.000100004
2017-10-10T11:53:24.454222: step 3416, loss 0.115609, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:53:24.704911: step 3417, loss 0.0843885, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:53:24.942431: step 3418, loss 0.118652, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:53:25.174067: step 3419, loss 0.131603, acc 0.921875, learning_rate 0.000100004
2017-10-10T11:53:25.416917: step 3420, loss 0.123367, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:53:25.680059: step 3421, loss 0.0551057, acc 1, learning_rate 0.000100004
2017-10-10T11:53:25.937000: step 3422, loss 0.0868747, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:26.204861: step 3423, loss 0.0933959, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:26.460691: step 3424, loss 0.0471678, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:26.758257: step 3425, loss 0.0729642, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:26.984868: step 3426, loss 0.257795, acc 0.90625, learning_rate 0.000100004
2017-10-10T11:53:27.319322: step 3427, loss 0.132242, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:53:27.548578: step 3428, loss 0.124258, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:53:27.743713: step 3429, loss 0.123196, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:53:27.902341: step 3430, loss 0.14165, acc 0.960784, learning_rate 0.000100004
2017-10-10T11:53:28.072047: step 3431, loss 0.0565734, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:53:28.285940: step 3432, loss 0.0826305, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:28.547749: step 3433, loss 0.0550849, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:28.792929: step 3434, loss 0.0731143, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:29.056881: step 3435, loss 0.115407, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:53:29.397888: step 3436, loss 0.0490862, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:29.577973: step 3437, loss 0.0776069, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:53:29.762291: step 3438, loss 0.12865, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:53:29.967319: step 3439, loss 0.0860586, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:30.165485: step 3440, loss 0.0683293, acc 0.984375, learning_rate 0.000100004

Evaluation:
2017-10-10T11:53:30.724259: step 3440, loss 0.208297, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3440

2017-10-10T11:53:31.887923: step 3441, loss 0.168808, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:53:32.136728: step 3442, loss 0.0815429, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:53:32.356850: step 3443, loss 0.0688566, acc 0.96875, learning_rate 0.000100004
2017-10-10T11:53:32.612517: step 3444, loss 0.108103, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:53:32.852834: step 3445, loss 0.0444447, acc 1, learning_rate 0.000100004
2017-10-10T11:53:33.080861: step 3446, loss 0.131517, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:53:33.340254: step 3447, loss 0.11575, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:53:33.594416: step 3448, loss 0.0988216, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:33.842872: step 3449, loss 0.0867422, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:34.042920: step 3450, loss 0.0669448, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:34.276955: step 3451, loss 0.04475, acc 1, learning_rate 0.000100004
2017-10-10T11:53:34.529020: step 3452, loss 0.0747003, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:34.789192: step 3453, loss 0.027118, acc 1, learning_rate 0.000100004
2017-10-10T11:53:35.039400: step 3454, loss 0.136591, acc 0.9375, learning_rate 0.000100004
2017-10-10T11:53:35.279755: step 3455, loss 0.0671716, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:35.498656: step 3456, loss 0.0898266, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:53:35.744353: step 3457, loss 0.056765, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:35.960894: step 3458, loss 0.128744, acc 0.953125, learning_rate 0.000100004
2017-10-10T11:53:36.231677: step 3459, loss 0.0688196, acc 0.984375, learning_rate 0.000100004
2017-10-10T11:53:36.486118: step 3460, loss 0.251989, acc 0.90625, learning_rate 0.000100004
2017-10-10T11:53:36.744052: step 3461, loss 0.0669856, acc 1, learning_rate 0.000100004
2017-10-10T11:53:37.005897: step 3462, loss 0.105971, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:37.293915: step 3463, loss 0.136002, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:37.601018: step 3464, loss 0.127038, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:37.905127: step 3465, loss 0.0929518, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:38.069841: step 3466, loss 0.123008, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:38.205089: step 3467, loss 0.150768, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:53:38.341830: step 3468, loss 0.0886805, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:38.493103: step 3469, loss 0.119635, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:38.678215: step 3470, loss 0.103406, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:38.863923: step 3471, loss 0.0998658, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:39.076854: step 3472, loss 0.101882, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:39.329087: step 3473, loss 0.0552109, acc 1, learning_rate 0.000100003
2017-10-10T11:53:39.589812: step 3474, loss 0.208866, acc 0.90625, learning_rate 0.000100003
2017-10-10T11:53:39.838994: step 3475, loss 0.0355297, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:40.106986: step 3476, loss 0.0575405, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:40.362714: step 3477, loss 0.0546161, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:40.642174: step 3478, loss 0.129927, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:53:40.888841: step 3479, loss 0.0522572, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:41.138359: step 3480, loss 0.152617, acc 0.953125, learning_rate 0.000100003

Evaluation:
2017-10-10T11:53:41.691413: step 3480, loss 0.206233, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3480

2017-10-10T11:53:42.997085: step 3481, loss 0.052022, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:43.244372: step 3482, loss 0.0694159, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:43.503739: step 3483, loss 0.0851502, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:43.744823: step 3484, loss 0.169494, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:53:43.990507: step 3485, loss 0.10114, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:44.259089: step 3486, loss 0.0323675, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:44.546179: step 3487, loss 0.061496, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:44.783427: step 3488, loss 0.0819168, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:45.012578: step 3489, loss 0.051526, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:45.276870: step 3490, loss 0.0997415, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:45.524871: step 3491, loss 0.104542, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:45.796883: step 3492, loss 0.0740323, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:46.032037: step 3493, loss 0.090496, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:46.247489: step 3494, loss 0.12128, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:46.455711: step 3495, loss 0.155726, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:46.670475: step 3496, loss 0.0687787, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:46.849209: step 3497, loss 0.0567262, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:47.044387: step 3498, loss 0.198683, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:53:47.301103: step 3499, loss 0.115457, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:47.536894: step 3500, loss 0.138193, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:47.748933: step 3501, loss 0.131743, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:48.124495: step 3502, loss 0.149245, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:53:48.316827: step 3503, loss 0.0817246, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:48.501900: step 3504, loss 0.134881, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:48.696839: step 3505, loss 0.124302, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:48.897755: step 3506, loss 0.110982, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:49.151072: step 3507, loss 0.0703382, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:49.387295: step 3508, loss 0.0913348, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:49.631797: step 3509, loss 0.0964065, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:49.910743: step 3510, loss 0.135243, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:53:50.161091: step 3511, loss 0.146386, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:50.432892: step 3512, loss 0.211446, acc 0.921875, learning_rate 0.000100003
2017-10-10T11:53:50.699332: step 3513, loss 0.044876, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:50.976862: step 3514, loss 0.11072, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:51.208124: step 3515, loss 0.139523, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:51.454749: step 3516, loss 0.036584, acc 1, learning_rate 0.000100003
2017-10-10T11:53:51.693259: step 3517, loss 0.0633445, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:51.941247: step 3518, loss 0.0823998, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:52.188051: step 3519, loss 0.08802, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:52.431450: step 3520, loss 0.0900328, acc 0.984375, learning_rate 0.000100003

Evaluation:
2017-10-10T11:53:52.971984: step 3520, loss 0.207118, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3520

2017-10-10T11:53:54.002756: step 3521, loss 0.12019, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:54.282825: step 3522, loss 0.103928, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:54.568791: step 3523, loss 0.112336, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:54.790910: step 3524, loss 0.112814, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:54.987571: step 3525, loss 0.125023, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:55.179449: step 3526, loss 0.0610676, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:55.416982: step 3527, loss 0.0850912, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:55.635458: step 3528, loss 0.0380357, acc 1, learning_rate 0.000100003
2017-10-10T11:53:55.854711: step 3529, loss 0.182687, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:53:56.113271: step 3530, loss 0.056549, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:56.353511: step 3531, loss 0.185511, acc 0.9375, learning_rate 0.000100003
2017-10-10T11:53:56.605467: step 3532, loss 0.0599295, acc 1, learning_rate 0.000100003
2017-10-10T11:53:56.873140: step 3533, loss 0.163477, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:57.100513: step 3534, loss 0.120289, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:57.346249: step 3535, loss 0.0852851, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:57.589514: step 3536, loss 0.074376, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:57.855261: step 3537, loss 0.0768799, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:58.085654: step 3538, loss 0.111997, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:58.400855: step 3539, loss 0.128525, acc 0.96875, learning_rate 0.000100003
2017-10-10T11:53:58.664941: step 3540, loss 0.138461, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:58.862974: step 3541, loss 0.143263, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:59.051434: step 3542, loss 0.187467, acc 0.953125, learning_rate 0.000100003
2017-10-10T11:53:59.232853: step 3543, loss 0.0645173, acc 0.984375, learning_rate 0.000100003
2017-10-10T11:53:59.432106: step 3544, loss 0.124908, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:53:59.674466: step 3545, loss 0.110636, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:53:59.914311: step 3546, loss 0.0879587, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:00.186389: step 3547, loss 0.112075, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:00.436879: step 3548, loss 0.0745877, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:00.744068: step 3549, loss 0.0952701, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:00.971982: step 3550, loss 0.0602918, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:01.247712: step 3551, loss 0.0696626, acc 1, learning_rate 0.000100002
2017-10-10T11:54:01.480899: step 3552, loss 0.059997, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:01.713976: step 3553, loss 0.081904, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:01.980945: step 3554, loss 0.0611395, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:02.246832: step 3555, loss 0.122693, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:02.536885: step 3556, loss 0.0542185, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:02.804968: step 3557, loss 0.112896, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:03.044626: step 3558, loss 0.037621, acc 1, learning_rate 0.000100002
2017-10-10T11:54:03.259352: step 3559, loss 0.0645668, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:03.460100: step 3560, loss 0.0963882, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-10-10T11:54:03.990195: step 3560, loss 0.209572, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3560

2017-10-10T11:54:05.402312: step 3561, loss 0.0356129, acc 1, learning_rate 0.000100002
2017-10-10T11:54:05.656060: step 3562, loss 0.0763686, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:05.872963: step 3563, loss 0.156666, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:06.155821: step 3564, loss 0.0511487, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:06.399337: step 3565, loss 0.0523141, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:06.695046: step 3566, loss 0.0691401, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:06.966978: step 3567, loss 0.0654462, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:07.192805: step 3568, loss 0.097479, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:07.433433: step 3569, loss 0.0797041, acc 1, learning_rate 0.000100002
2017-10-10T11:54:07.632981: step 3570, loss 0.146857, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:54:07.879371: step 3571, loss 0.15307, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:54:08.110053: step 3572, loss 0.0575681, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:08.342656: step 3573, loss 0.127096, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:08.612851: step 3574, loss 0.185192, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:54:08.888439: step 3575, loss 0.12576, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:09.066523: step 3576, loss 0.0636886, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:09.251216: step 3577, loss 0.1022, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:09.436944: step 3578, loss 0.100678, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:09.629646: step 3579, loss 0.0881116, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:09.813833: step 3580, loss 0.0377806, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:10.059385: step 3581, loss 0.111637, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:10.319415: step 3582, loss 0.0564367, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:10.564902: step 3583, loss 0.177119, acc 0.90625, learning_rate 0.000100002
2017-10-10T11:54:10.897494: step 3584, loss 0.131245, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:54:11.112936: step 3585, loss 0.054881, acc 1, learning_rate 0.000100002
2017-10-10T11:54:11.300888: step 3586, loss 0.138648, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:11.538066: step 3587, loss 0.0428985, acc 1, learning_rate 0.000100002
2017-10-10T11:54:11.775457: step 3588, loss 0.126675, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:12.021757: step 3589, loss 0.126969, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:12.244743: step 3590, loss 0.131503, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:12.480700: step 3591, loss 0.0647608, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:12.739370: step 3592, loss 0.0727335, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:13.012887: step 3593, loss 0.130119, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:13.273268: step 3594, loss 0.0703437, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:13.580909: step 3595, loss 0.0715917, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:13.841961: step 3596, loss 0.097423, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:14.104855: step 3597, loss 0.146757, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:14.349055: step 3598, loss 0.0618214, acc 1, learning_rate 0.000100002
2017-10-10T11:54:14.609844: step 3599, loss 0.1121, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:14.875384: step 3600, loss 0.0791587, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-10-10T11:54:15.453070: step 3600, loss 0.207629, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3600

2017-10-10T11:54:16.801381: step 3601, loss 0.0721509, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:17.072923: step 3602, loss 0.0862444, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:17.335223: step 3603, loss 0.0642203, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:17.616571: step 3604, loss 0.0373177, acc 1, learning_rate 0.000100002
2017-10-10T11:54:17.889248: step 3605, loss 0.084925, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:18.130019: step 3606, loss 0.0832279, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:18.398778: step 3607, loss 0.104312, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:18.637103: step 3608, loss 0.0442824, acc 1, learning_rate 0.000100002
2017-10-10T11:54:18.885810: step 3609, loss 0.0851945, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:19.239217: step 3610, loss 0.0590806, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:19.472755: step 3611, loss 0.106343, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:19.628799: step 3612, loss 0.0502921, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:19.797426: step 3613, loss 0.0478795, acc 1, learning_rate 0.000100002
2017-10-10T11:54:19.944189: step 3614, loss 0.0760634, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:20.086356: step 3615, loss 0.179148, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:54:20.355208: step 3616, loss 0.0897398, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:20.592783: step 3617, loss 0.0397088, acc 1, learning_rate 0.000100002
2017-10-10T11:54:20.812924: step 3618, loss 0.0839039, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:21.080859: step 3619, loss 0.0790247, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:21.307841: step 3620, loss 0.13386, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:54:21.586534: step 3621, loss 0.0703395, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:21.815512: step 3622, loss 0.156699, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:54:22.039901: step 3623, loss 0.0564205, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:22.283061: step 3624, loss 0.0882326, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:22.516687: step 3625, loss 0.116548, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:22.738642: step 3626, loss 0.165054, acc 0.921569, learning_rate 0.000100002
2017-10-10T11:54:22.946965: step 3627, loss 0.133897, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:23.180864: step 3628, loss 0.120692, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:23.431245: step 3629, loss 0.0702829, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:23.715698: step 3630, loss 0.115385, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:23.956978: step 3631, loss 0.0912086, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:24.229297: step 3632, loss 0.109613, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:24.444840: step 3633, loss 0.0491252, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:24.728932: step 3634, loss 0.0884421, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:24.945506: step 3635, loss 0.0894492, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:25.168833: step 3636, loss 0.087725, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:25.384278: step 3637, loss 0.0617246, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:25.636391: step 3638, loss 0.148664, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:54:25.880586: step 3639, loss 0.145082, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:54:26.102997: step 3640, loss 0.0485262, acc 1, learning_rate 0.000100002

Evaluation:
2017-10-10T11:54:26.642414: step 3640, loss 0.208937, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3640

2017-10-10T11:54:27.664244: step 3641, loss 0.154151, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:27.906862: step 3642, loss 0.18489, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:54:28.099011: step 3643, loss 0.103459, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:28.307942: step 3644, loss 0.147255, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:54:28.529423: step 3645, loss 0.0770901, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:28.748840: step 3646, loss 0.0537015, acc 1, learning_rate 0.000100002
2017-10-10T11:54:29.045604: step 3647, loss 0.0770362, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:29.284122: step 3648, loss 0.0601349, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:29.503012: step 3649, loss 0.0981238, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:29.840501: step 3650, loss 0.0625676, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:30.017921: step 3651, loss 0.085377, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:30.197543: step 3652, loss 0.121435, acc 0.953125, learning_rate 0.000100002
2017-10-10T11:54:30.406406: step 3653, loss 0.0560072, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:30.592454: step 3654, loss 0.098058, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:30.783778: step 3655, loss 0.0735559, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:31.067832: step 3656, loss 0.0782755, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:31.331054: step 3657, loss 0.0700224, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:31.535357: step 3658, loss 0.124349, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:31.799988: step 3659, loss 0.0685396, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:32.044664: step 3660, loss 0.0581133, acc 1, learning_rate 0.000100002
2017-10-10T11:54:32.314751: step 3661, loss 0.0857836, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:32.555675: step 3662, loss 0.0996787, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:32.798149: step 3663, loss 0.0895691, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:33.028885: step 3664, loss 0.0881768, acc 0.96875, learning_rate 0.000100002
2017-10-10T11:54:33.273221: step 3665, loss 0.127922, acc 0.9375, learning_rate 0.000100002
2017-10-10T11:54:33.519413: step 3666, loss 0.139849, acc 0.921875, learning_rate 0.000100002
2017-10-10T11:54:33.772886: step 3667, loss 0.058665, acc 0.984375, learning_rate 0.000100002
2017-10-10T11:54:34.009836: step 3668, loss 0.0499692, acc 1, learning_rate 0.000100002
2017-10-10T11:54:34.284862: step 3669, loss 0.0231101, acc 1, learning_rate 0.000100001
2017-10-10T11:54:34.546701: step 3670, loss 0.0699037, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:34.784845: step 3671, loss 0.0957579, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:35.038791: step 3672, loss 0.0584127, acc 1, learning_rate 0.000100001
2017-10-10T11:54:35.288526: step 3673, loss 0.154104, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:35.492833: step 3674, loss 0.125817, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:35.786435: step 3675, loss 0.0323347, acc 1, learning_rate 0.000100001
2017-10-10T11:54:35.996175: step 3676, loss 0.0582499, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:36.209590: step 3677, loss 0.0820134, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:36.405844: step 3678, loss 0.14052, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:36.616836: step 3679, loss 0.079313, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:36.857676: step 3680, loss 0.059256, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T11:54:37.392816: step 3680, loss 0.207431, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3680

2017-10-10T11:54:38.580962: step 3681, loss 0.0622347, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:38.878701: step 3682, loss 0.0718822, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:39.119446: step 3683, loss 0.0492144, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:39.362484: step 3684, loss 0.097304, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:39.608094: step 3685, loss 0.109228, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:39.862235: step 3686, loss 0.0777524, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:40.156898: step 3687, loss 0.248859, acc 0.890625, learning_rate 0.000100001
2017-10-10T11:54:40.447252: step 3688, loss 0.144997, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:40.636922: step 3689, loss 0.0907886, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:40.828714: step 3690, loss 0.0488045, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:41.028114: step 3691, loss 0.125226, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:41.211846: step 3692, loss 0.0330949, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:41.376845: step 3693, loss 0.0885569, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:41.632811: step 3694, loss 0.149169, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:54:41.907748: step 3695, loss 0.0648564, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:42.140855: step 3696, loss 0.103238, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:42.377474: step 3697, loss 0.101619, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:42.658814: step 3698, loss 0.080209, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:42.876911: step 3699, loss 0.16728, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:54:43.153869: step 3700, loss 0.0795598, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:43.401115: step 3701, loss 0.095185, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:43.642275: step 3702, loss 0.211224, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:54:43.959939: step 3703, loss 0.0889253, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:44.175156: step 3704, loss 0.0621669, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:44.368805: step 3705, loss 0.0535026, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:44.585033: step 3706, loss 0.0360499, acc 1, learning_rate 0.000100001
2017-10-10T11:54:44.780953: step 3707, loss 0.0790254, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:44.998039: step 3708, loss 0.0751103, acc 1, learning_rate 0.000100001
2017-10-10T11:54:45.244614: step 3709, loss 0.0724857, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:45.489690: step 3710, loss 0.0910004, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:45.769431: step 3711, loss 0.157998, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:46.024877: step 3712, loss 0.173756, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:54:46.284890: step 3713, loss 0.0671741, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:46.511536: step 3714, loss 0.0617359, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:46.776898: step 3715, loss 0.13015, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:54:47.012604: step 3716, loss 0.142552, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:47.254942: step 3717, loss 0.117726, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:54:47.508226: step 3718, loss 0.0389058, acc 1, learning_rate 0.000100001
2017-10-10T11:54:47.747697: step 3719, loss 0.126697, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:47.975826: step 3720, loss 0.153947, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-10-10T11:54:48.544883: step 3720, loss 0.207129, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3720

2017-10-10T11:54:50.020843: step 3721, loss 0.0807632, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:50.287950: step 3722, loss 0.0902087, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:50.533914: step 3723, loss 0.0549533, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:50.793101: step 3724, loss 0.0906369, acc 0.980392, learning_rate 0.000100001
2017-10-10T11:54:51.066299: step 3725, loss 0.0908747, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:51.240865: step 3726, loss 0.102613, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:51.455279: step 3727, loss 0.120832, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:54:51.636170: step 3728, loss 0.0952992, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:51.815280: step 3729, loss 0.0978117, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:52.140414: step 3730, loss 0.104103, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:52.323249: step 3731, loss 0.137536, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:54:52.523448: step 3732, loss 0.120692, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:52.728846: step 3733, loss 0.0940572, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:52.922284: step 3734, loss 0.105173, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:53.157333: step 3735, loss 0.0837836, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:53.384513: step 3736, loss 0.0509378, acc 1, learning_rate 0.000100001
2017-10-10T11:54:53.647463: step 3737, loss 0.0569303, acc 1, learning_rate 0.000100001
2017-10-10T11:54:53.919202: step 3738, loss 0.0666079, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:54.144874: step 3739, loss 0.059221, acc 1, learning_rate 0.000100001
2017-10-10T11:54:54.400850: step 3740, loss 0.12038, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:54.660357: step 3741, loss 0.0964867, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:54.947709: step 3742, loss 0.109192, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:55.205053: step 3743, loss 0.0900359, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:55.482883: step 3744, loss 0.141308, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:55.715061: step 3745, loss 0.117677, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:55.915209: step 3746, loss 0.0542856, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:56.136864: step 3747, loss 0.0437256, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:56.354033: step 3748, loss 0.100026, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:56.593757: step 3749, loss 0.0810846, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:56.848437: step 3750, loss 0.0976049, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:57.097472: step 3751, loss 0.0858257, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:57.357439: step 3752, loss 0.0839175, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:57.629163: step 3753, loss 0.0820057, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:57.859913: step 3754, loss 0.144652, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:54:58.105082: step 3755, loss 0.0669069, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:54:58.364124: step 3756, loss 0.114455, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:54:58.598857: step 3757, loss 0.119911, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:54:58.816880: step 3758, loss 0.138873, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:54:59.074049: step 3759, loss 0.124713, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:54:59.338913: step 3760, loss 0.116786, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T11:54:59.937153: step 3760, loss 0.209604, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3760

2017-10-10T11:55:00.870513: step 3761, loss 0.204676, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:55:01.117284: step 3762, loss 0.073852, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:01.336547: step 3763, loss 0.0979691, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:01.469439: step 3764, loss 0.161172, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:01.653704: step 3765, loss 0.071494, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:01.853052: step 3766, loss 0.106977, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:02.028602: step 3767, loss 0.0646806, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:02.203545: step 3768, loss 0.129237, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:02.420109: step 3769, loss 0.0818668, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:02.668091: step 3770, loss 0.136542, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:02.905196: step 3771, loss 0.0913307, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:03.156787: step 3772, loss 0.127613, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:03.379989: step 3773, loss 0.075487, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:03.609539: step 3774, loss 0.0945046, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:03.852887: step 3775, loss 0.0709874, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:04.119338: step 3776, loss 0.0958551, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:04.369547: step 3777, loss 0.047901, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:04.620865: step 3778, loss 0.0967154, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:04.885461: step 3779, loss 0.0719152, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:05.122668: step 3780, loss 0.0967937, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:05.381672: step 3781, loss 0.0784914, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:05.610524: step 3782, loss 0.165849, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:05.821016: step 3783, loss 0.0891831, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:06.056890: step 3784, loss 0.0416922, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:06.300606: step 3785, loss 0.0604778, acc 1, learning_rate 0.000100001
2017-10-10T11:55:06.524389: step 3786, loss 0.0579657, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:06.764845: step 3787, loss 0.140103, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:07.008072: step 3788, loss 0.0589623, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:07.248166: step 3789, loss 0.140367, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:55:07.504534: step 3790, loss 0.100682, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:07.800344: step 3791, loss 0.118778, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:08.024971: step 3792, loss 0.12898, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:08.272140: step 3793, loss 0.0929924, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:08.489213: step 3794, loss 0.0807322, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:08.736878: step 3795, loss 0.0749858, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:08.998001: step 3796, loss 0.191121, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:09.199437: step 3797, loss 0.276095, acc 0.890625, learning_rate 0.000100001
2017-10-10T11:55:09.399980: step 3798, loss 0.0723474, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:09.608736: step 3799, loss 0.0871144, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:09.840419: step 3800, loss 0.0604247, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T11:55:10.392472: step 3800, loss 0.206155, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3800

2017-10-10T11:55:11.625101: step 3801, loss 0.0789189, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:11.818705: step 3802, loss 0.107682, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:12.029039: step 3803, loss 0.0724958, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:12.224430: step 3804, loss 0.148121, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:12.426395: step 3805, loss 0.0455362, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:12.675429: step 3806, loss 0.0869993, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:12.956869: step 3807, loss 0.0705845, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:13.197465: step 3808, loss 0.0659906, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:13.429424: step 3809, loss 0.116147, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:55:13.664478: step 3810, loss 0.131763, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:13.920854: step 3811, loss 0.0557475, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:14.173033: step 3812, loss 0.108043, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:14.400971: step 3813, loss 0.154107, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:55:14.628216: step 3814, loss 0.112345, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:14.880890: step 3815, loss 0.0482453, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:15.132838: step 3816, loss 0.129887, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:15.387308: step 3817, loss 0.0782137, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:15.655712: step 3818, loss 0.119311, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:15.925373: step 3819, loss 0.153931, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:16.185783: step 3820, loss 0.0680435, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:16.451678: step 3821, loss 0.155531, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:55:16.666998: step 3822, loss 0.0731955, acc 0.960784, learning_rate 0.000100001
2017-10-10T11:55:16.928862: step 3823, loss 0.0998775, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:17.216478: step 3824, loss 0.0925099, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:17.438316: step 3825, loss 0.0887912, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:17.634334: step 3826, loss 0.0532952, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:17.848667: step 3827, loss 0.0880879, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:18.094989: step 3828, loss 0.118888, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:18.337795: step 3829, loss 0.0567918, acc 1, learning_rate 0.000100001
2017-10-10T11:55:18.587465: step 3830, loss 0.101919, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:18.837206: step 3831, loss 0.0298046, acc 1, learning_rate 0.000100001
2017-10-10T11:55:19.056877: step 3832, loss 0.0471399, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:19.308857: step 3833, loss 0.0455272, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:19.546030: step 3834, loss 0.0923008, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:19.800011: step 3835, loss 0.0477694, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:20.034114: step 3836, loss 0.171117, acc 0.890625, learning_rate 0.000100001
2017-10-10T11:55:20.288947: step 3837, loss 0.200963, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:20.561103: step 3838, loss 0.0740379, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:20.836871: step 3839, loss 0.0418651, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:21.097620: step 3840, loss 0.0382693, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T11:55:21.700826: step 3840, loss 0.205938, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3840

2017-10-10T11:55:22.993235: step 3841, loss 0.0273619, acc 1, learning_rate 0.000100001
2017-10-10T11:55:23.224813: step 3842, loss 0.168534, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:55:23.488148: step 3843, loss 0.0620597, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:23.718628: step 3844, loss 0.0739228, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:23.966175: step 3845, loss 0.0545148, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:24.238286: step 3846, loss 0.0500415, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:24.484101: step 3847, loss 0.200699, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:55:24.740887: step 3848, loss 0.0921771, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:25.055946: step 3849, loss 0.0741852, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:25.265964: step 3850, loss 0.111458, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:55:25.464814: step 3851, loss 0.0902805, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:25.672877: step 3852, loss 0.113357, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:25.869180: step 3853, loss 0.126306, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:26.071201: step 3854, loss 0.0927947, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:26.350501: step 3855, loss 0.0879437, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:26.598692: step 3856, loss 0.112009, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:55:26.872394: step 3857, loss 0.0913119, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:27.117274: step 3858, loss 0.0705203, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:27.364137: step 3859, loss 0.0483658, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:27.621820: step 3860, loss 0.0189959, acc 1, learning_rate 0.000100001
2017-10-10T11:55:27.872892: step 3861, loss 0.13845, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:28.124859: step 3862, loss 0.0784805, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:28.424890: step 3863, loss 0.0495883, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:28.677904: step 3864, loss 0.195422, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:28.936870: step 3865, loss 0.100735, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:29.221508: step 3866, loss 0.0964371, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:29.474370: step 3867, loss 0.119267, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:29.732849: step 3868, loss 0.0842807, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:29.973783: step 3869, loss 0.081813, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:30.232077: step 3870, loss 0.0435547, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:30.473077: step 3871, loss 0.0651553, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:30.704967: step 3872, loss 0.176433, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:30.983703: step 3873, loss 0.0406297, acc 1, learning_rate 0.000100001
2017-10-10T11:55:31.239167: step 3874, loss 0.0728184, acc 1, learning_rate 0.000100001
2017-10-10T11:55:31.493929: step 3875, loss 0.0659796, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:31.774629: step 3876, loss 0.0947314, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:31.990698: step 3877, loss 0.0801312, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:32.245089: step 3878, loss 0.0483815, acc 1, learning_rate 0.000100001
2017-10-10T11:55:32.513777: step 3879, loss 0.205107, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:32.705428: step 3880, loss 0.133612, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-10-10T11:55:33.131924: step 3880, loss 0.207067, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3880

2017-10-10T11:55:34.302296: step 3881, loss 0.0636643, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:34.512865: step 3882, loss 0.144025, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:34.728066: step 3883, loss 0.0884332, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:34.973758: step 3884, loss 0.1055, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:35.209275: step 3885, loss 0.143635, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:55:35.477273: step 3886, loss 0.0479019, acc 1, learning_rate 0.000100001
2017-10-10T11:55:35.742096: step 3887, loss 0.10502, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:35.992869: step 3888, loss 0.0299108, acc 1, learning_rate 0.000100001
2017-10-10T11:55:36.233323: step 3889, loss 0.0714267, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:36.472759: step 3890, loss 0.0337653, acc 1, learning_rate 0.000100001
2017-10-10T11:55:36.694620: step 3891, loss 0.136215, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:55:36.972539: step 3892, loss 0.121235, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:37.209616: step 3893, loss 0.0405887, acc 1, learning_rate 0.000100001
2017-10-10T11:55:37.434541: step 3894, loss 0.0964644, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:37.687217: step 3895, loss 0.113916, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:37.939836: step 3896, loss 0.159384, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:55:38.189711: step 3897, loss 0.0456912, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:38.444006: step 3898, loss 0.120215, acc 0.921875, learning_rate 0.000100001
2017-10-10T11:55:38.676870: step 3899, loss 0.0870076, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:38.951765: step 3900, loss 0.111983, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:39.254120: step 3901, loss 0.154753, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:55:39.516115: step 3902, loss 0.110127, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:39.744938: step 3903, loss 0.101681, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:40.025853: step 3904, loss 0.0966097, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:40.289247: step 3905, loss 0.0690164, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:40.559460: step 3906, loss 0.0717298, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:40.790069: step 3907, loss 0.108141, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:41.055135: step 3908, loss 0.117866, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:41.319182: step 3909, loss 0.0580156, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:41.574713: step 3910, loss 0.047012, acc 1, learning_rate 0.000100001
2017-10-10T11:55:41.888064: step 3911, loss 0.156338, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:42.144838: step 3912, loss 0.112512, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:55:42.331639: step 3913, loss 0.128614, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:42.624860: step 3914, loss 0.105491, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:55:42.814671: step 3915, loss 0.0983382, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:42.956839: step 3916, loss 0.0594164, acc 1, learning_rate 0.000100001
2017-10-10T11:55:43.152802: step 3917, loss 0.0944195, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:43.340430: step 3918, loss 0.0636739, acc 1, learning_rate 0.000100001
2017-10-10T11:55:43.536762: step 3919, loss 0.0599596, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:43.695070: step 3920, loss 0.0869237, acc 0.960784, learning_rate 0.000100001

Evaluation:
2017-10-10T11:55:44.261285: step 3920, loss 0.206179, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3920

2017-10-10T11:55:45.469117: step 3921, loss 0.0883651, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:45.699189: step 3922, loss 0.0976461, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:45.927780: step 3923, loss 0.0362465, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:46.190675: step 3924, loss 0.121242, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:46.426801: step 3925, loss 0.10863, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:46.662394: step 3926, loss 0.0618928, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:46.914839: step 3927, loss 0.0486334, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:47.145665: step 3928, loss 0.144299, acc 0.9375, learning_rate 0.000100001
2017-10-10T11:55:47.382755: step 3929, loss 0.178064, acc 0.90625, learning_rate 0.000100001
2017-10-10T11:55:47.660038: step 3930, loss 0.0963024, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:47.905012: step 3931, loss 0.0607538, acc 1, learning_rate 0.000100001
2017-10-10T11:55:48.160970: step 3932, loss 0.0678796, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:48.424820: step 3933, loss 0.0728301, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:48.681187: step 3934, loss 0.0818291, acc 0.96875, learning_rate 0.000100001
2017-10-10T11:55:48.913464: step 3935, loss 0.0719197, acc 0.984375, learning_rate 0.000100001
2017-10-10T11:55:49.159078: step 3936, loss 0.171987, acc 0.953125, learning_rate 0.000100001
2017-10-10T11:55:49.392903: step 3937, loss 0.121939, acc 0.96875, learning_rate 0.0001
2017-10-10T11:55:49.673109: step 3938, loss 0.121179, acc 0.953125, learning_rate 0.0001
2017-10-10T11:55:50.006445: step 3939, loss 0.0381395, acc 1, learning_rate 0.0001
2017-10-10T11:55:50.236871: step 3940, loss 0.0624444, acc 0.96875, learning_rate 0.0001
2017-10-10T11:55:50.452893: step 3941, loss 0.0553086, acc 0.984375, learning_rate 0.0001
2017-10-10T11:55:50.662517: step 3942, loss 0.0987997, acc 0.96875, learning_rate 0.0001
2017-10-10T11:55:50.855399: step 3943, loss 0.0670331, acc 0.984375, learning_rate 0.0001
2017-10-10T11:55:51.052949: step 3944, loss 0.031403, acc 1, learning_rate 0.0001
2017-10-10T11:55:51.319155: step 3945, loss 0.0574407, acc 1, learning_rate 0.0001
2017-10-10T11:55:51.549863: step 3946, loss 0.0713522, acc 0.953125, learning_rate 0.0001
2017-10-10T11:55:51.818069: step 3947, loss 0.0821982, acc 0.96875, learning_rate 0.0001
2017-10-10T11:55:52.068226: step 3948, loss 0.09695, acc 0.953125, learning_rate 0.0001
2017-10-10T11:55:52.295254: step 3949, loss 0.0480506, acc 0.984375, learning_rate 0.0001
2017-10-10T11:55:52.569106: step 3950, loss 0.0795383, acc 0.96875, learning_rate 0.0001
2017-10-10T11:55:52.803852: step 3951, loss 0.203196, acc 0.9375, learning_rate 0.0001
2017-10-10T11:55:53.156405: step 3952, loss 0.176455, acc 0.96875, learning_rate 0.0001
2017-10-10T11:55:53.373945: step 3953, loss 0.0874103, acc 0.953125, learning_rate 0.0001
2017-10-10T11:55:53.554329: step 3954, loss 0.0497448, acc 1, learning_rate 0.0001
2017-10-10T11:55:53.727665: step 3955, loss 0.105155, acc 0.953125, learning_rate 0.0001
2017-10-10T11:55:53.937680: step 3956, loss 0.0524343, acc 0.984375, learning_rate 0.0001
2017-10-10T11:55:54.128287: step 3957, loss 0.071095, acc 0.96875, learning_rate 0.0001
2017-10-10T11:55:54.393453: step 3958, loss 0.114854, acc 0.953125, learning_rate 0.0001
2017-10-10T11:55:54.661846: step 3959, loss 0.0408137, acc 1, learning_rate 0.0001
2017-10-10T11:55:54.902166: step 3960, loss 0.0398549, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:55:55.501212: step 3960, loss 0.204868, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-3960

2017-10-10T11:55:56.632638: step 3961, loss 0.108793, acc 0.96875, learning_rate 0.0001
2017-10-10T11:55:56.846261: step 3962, loss 0.0573793, acc 0.96875, learning_rate 0.0001
2017-10-10T11:55:57.076834: step 3963, loss 0.0414893, acc 0.984375, learning_rate 0.0001
2017-10-10T11:55:57.341729: step 3964, loss 0.164942, acc 0.921875, learning_rate 0.0001
2017-10-10T11:55:57.561023: step 3965, loss 0.0702211, acc 0.984375, learning_rate 0.0001
2017-10-10T11:55:57.775508: step 3966, loss 0.101188, acc 0.953125, learning_rate 0.0001
2017-10-10T11:55:58.001242: step 3967, loss 0.0656057, acc 0.984375, learning_rate 0.0001
2017-10-10T11:55:58.263387: step 3968, loss 0.059522, acc 0.96875, learning_rate 0.0001
2017-10-10T11:55:58.623161: step 3969, loss 0.0682415, acc 1, learning_rate 0.0001
2017-10-10T11:55:58.820845: step 3970, loss 0.0930314, acc 0.96875, learning_rate 0.0001
2017-10-10T11:55:59.033466: step 3971, loss 0.126644, acc 0.96875, learning_rate 0.0001
2017-10-10T11:55:59.237543: step 3972, loss 0.0681213, acc 0.984375, learning_rate 0.0001
2017-10-10T11:55:59.433312: step 3973, loss 0.0621023, acc 0.96875, learning_rate 0.0001
2017-10-10T11:55:59.633126: step 3974, loss 0.084091, acc 0.984375, learning_rate 0.0001
2017-10-10T11:55:59.864587: step 3975, loss 0.0465685, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:00.107823: step 3976, loss 0.0478801, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:00.344749: step 3977, loss 0.034722, acc 1, learning_rate 0.0001
2017-10-10T11:56:00.607468: step 3978, loss 0.0430743, acc 1, learning_rate 0.0001
2017-10-10T11:56:00.855155: step 3979, loss 0.0424054, acc 1, learning_rate 0.0001
2017-10-10T11:56:01.140880: step 3980, loss 0.133381, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:01.392842: step 3981, loss 0.0251341, acc 1, learning_rate 0.0001
2017-10-10T11:56:01.664879: step 3982, loss 0.0764805, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:01.888732: step 3983, loss 0.12374, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:02.140917: step 3984, loss 0.125697, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:02.369015: step 3985, loss 0.0760247, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:02.609568: step 3986, loss 0.0714969, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:02.852926: step 3987, loss 0.048237, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:03.072836: step 3988, loss 0.0706701, acc 1, learning_rate 0.0001
2017-10-10T11:56:03.313611: step 3989, loss 0.174475, acc 0.921875, learning_rate 0.0001
2017-10-10T11:56:03.600937: step 3990, loss 0.0816714, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:03.885833: step 3991, loss 0.0476786, acc 1, learning_rate 0.0001
2017-10-10T11:56:04.063522: step 3992, loss 0.0551471, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:04.249115: step 3993, loss 0.12291, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:04.428934: step 3994, loss 0.0915899, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:04.604860: step 3995, loss 0.0824494, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:04.797249: step 3996, loss 0.148791, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:05.038581: step 3997, loss 0.142264, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:05.259175: step 3998, loss 0.0784182, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:05.538846: step 3999, loss 0.127548, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:05.796889: step 4000, loss 0.0664725, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:56:06.385442: step 4000, loss 0.205709, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4000

2017-10-10T11:56:07.391689: step 4001, loss 0.119989, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:07.587409: step 4002, loss 0.100236, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:07.798043: step 4003, loss 0.111794, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:07.989130: step 4004, loss 0.0771556, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:08.190466: step 4005, loss 0.0880401, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:08.417000: step 4006, loss 0.0925674, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:08.670209: step 4007, loss 0.170966, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:08.922408: step 4008, loss 0.151982, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:09.146147: step 4009, loss 0.066754, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:09.404850: step 4010, loss 0.165985, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:09.630483: step 4011, loss 0.168244, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:09.891249: step 4012, loss 0.121204, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:10.133501: step 4013, loss 0.179853, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:10.388926: step 4014, loss 0.145437, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:10.615826: step 4015, loss 0.152086, acc 0.921875, learning_rate 0.0001
2017-10-10T11:56:10.849735: step 4016, loss 0.0737395, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:11.128749: step 4017, loss 0.118081, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:11.331070: step 4018, loss 0.106883, acc 0.960784, learning_rate 0.0001
2017-10-10T11:56:11.586630: step 4019, loss 0.0380494, acc 1, learning_rate 0.0001
2017-10-10T11:56:11.853975: step 4020, loss 0.101118, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:12.092234: step 4021, loss 0.0613193, acc 1, learning_rate 0.0001
2017-10-10T11:56:12.324586: step 4022, loss 0.155484, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:12.601087: step 4023, loss 0.0132944, acc 1, learning_rate 0.0001
2017-10-10T11:56:12.840179: step 4024, loss 0.0447368, acc 1, learning_rate 0.0001
2017-10-10T11:56:13.095538: step 4025, loss 0.0981548, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:13.349744: step 4026, loss 0.133884, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:13.566279: step 4027, loss 0.120295, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:13.789096: step 4028, loss 0.108982, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:14.030362: step 4029, loss 0.0449614, acc 1, learning_rate 0.0001
2017-10-10T11:56:14.337051: step 4030, loss 0.129122, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:14.624752: step 4031, loss 0.0623862, acc 1, learning_rate 0.0001
2017-10-10T11:56:14.801399: step 4032, loss 0.125889, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:14.978518: step 4033, loss 0.0422383, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:15.160830: step 4034, loss 0.0557875, acc 1, learning_rate 0.0001
2017-10-10T11:56:15.437955: step 4035, loss 0.177669, acc 0.921875, learning_rate 0.0001
2017-10-10T11:56:15.653670: step 4036, loss 0.164978, acc 0.921875, learning_rate 0.0001
2017-10-10T11:56:15.882302: step 4037, loss 0.0862812, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:16.086186: step 4038, loss 0.0384058, acc 1, learning_rate 0.0001
2017-10-10T11:56:16.298513: step 4039, loss 0.123749, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:16.542448: step 4040, loss 0.0650938, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:56:17.220870: step 4040, loss 0.204897, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4040

2017-10-10T11:56:18.352967: step 4041, loss 0.0402149, acc 1, learning_rate 0.0001
2017-10-10T11:56:18.634170: step 4042, loss 0.0391237, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:18.856867: step 4043, loss 0.0638017, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:19.090136: step 4044, loss 0.0345156, acc 1, learning_rate 0.0001
2017-10-10T11:56:19.321254: step 4045, loss 0.0862823, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:19.598208: step 4046, loss 0.215466, acc 0.890625, learning_rate 0.0001
2017-10-10T11:56:19.838801: step 4047, loss 0.159602, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:20.084937: step 4048, loss 0.0916679, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:20.345479: step 4049, loss 0.0288419, acc 1, learning_rate 0.0001
2017-10-10T11:56:20.594126: step 4050, loss 0.0866829, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:20.852429: step 4051, loss 0.0705309, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:21.098957: step 4052, loss 0.0690806, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:21.344862: step 4053, loss 0.127964, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:21.597622: step 4054, loss 0.0904478, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:21.819700: step 4055, loss 0.197393, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:22.065336: step 4056, loss 0.03998, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:22.360972: step 4057, loss 0.0906552, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:22.604866: step 4058, loss 0.0987671, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:22.863234: step 4059, loss 0.0820496, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:23.097536: step 4060, loss 0.0732481, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:23.351840: step 4061, loss 0.103396, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:23.684966: step 4062, loss 0.0846688, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:23.886044: step 4063, loss 0.0808143, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:24.103786: step 4064, loss 0.0578502, acc 1, learning_rate 0.0001
2017-10-10T11:56:24.313351: step 4065, loss 0.0888536, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:24.525358: step 4066, loss 0.0707466, acc 1, learning_rate 0.0001
2017-10-10T11:56:24.808938: step 4067, loss 0.100167, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:25.089896: step 4068, loss 0.0264109, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:25.285744: step 4069, loss 0.09468, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:25.464844: step 4070, loss 0.0546672, acc 1, learning_rate 0.0001
2017-10-10T11:56:25.648921: step 4071, loss 0.155196, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:25.841853: step 4072, loss 0.0791836, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:26.029205: step 4073, loss 0.0986639, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:26.222972: step 4074, loss 0.149859, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:26.477219: step 4075, loss 0.224603, acc 0.90625, learning_rate 0.0001
2017-10-10T11:56:26.764842: step 4076, loss 0.117909, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:27.022679: step 4077, loss 0.0586603, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:27.263511: step 4078, loss 0.114581, acc 0.921875, learning_rate 0.0001
2017-10-10T11:56:27.507426: step 4079, loss 0.165432, acc 0.921875, learning_rate 0.0001
2017-10-10T11:56:27.788523: step 4080, loss 0.106938, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:56:28.417054: step 4080, loss 0.205728, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4080

2017-10-10T11:56:29.524859: step 4081, loss 0.0841798, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:29.802931: step 4082, loss 0.0826204, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:30.048936: step 4083, loss 0.0632454, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:30.287392: step 4084, loss 0.055387, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:30.518016: step 4085, loss 0.109389, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:30.747856: step 4086, loss 0.0470997, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:31.006822: step 4087, loss 0.0724124, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:31.208861: step 4088, loss 0.164515, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:31.439482: step 4089, loss 0.0857728, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:31.729261: step 4090, loss 0.117584, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:31.997763: step 4091, loss 0.0948204, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:32.216993: step 4092, loss 0.0775673, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:32.434698: step 4093, loss 0.114636, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:32.639066: step 4094, loss 0.0571847, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:32.830595: step 4095, loss 0.120612, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:33.024890: step 4096, loss 0.0515466, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:33.244864: step 4097, loss 0.0396667, acc 1, learning_rate 0.0001
2017-10-10T11:56:33.492945: step 4098, loss 0.137216, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:33.715252: step 4099, loss 0.0781828, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:33.965289: step 4100, loss 0.024167, acc 1, learning_rate 0.0001
2017-10-10T11:56:34.176785: step 4101, loss 0.0789186, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:34.422269: step 4102, loss 0.0408027, acc 1, learning_rate 0.0001
2017-10-10T11:56:34.682513: step 4103, loss 0.0502341, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:34.940887: step 4104, loss 0.0662683, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:35.200989: step 4105, loss 0.103948, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:35.524993: step 4106, loss 0.0866968, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:35.821259: step 4107, loss 0.0683611, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:36.003002: step 4108, loss 0.0656753, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:36.183288: step 4109, loss 0.163743, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:36.378054: step 4110, loss 0.0700924, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:36.565025: step 4111, loss 0.0568651, acc 1, learning_rate 0.0001
2017-10-10T11:56:36.828874: step 4112, loss 0.0936258, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:37.092944: step 4113, loss 0.13638, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:37.372870: step 4114, loss 0.0800635, acc 1, learning_rate 0.0001
2017-10-10T11:56:37.616998: step 4115, loss 0.111358, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:37.850927: step 4116, loss 0.0600814, acc 0.960784, learning_rate 0.0001
2017-10-10T11:56:38.115396: step 4117, loss 0.148711, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:38.335975: step 4118, loss 0.0403478, acc 1, learning_rate 0.0001
2017-10-10T11:56:38.567323: step 4119, loss 0.054484, acc 1, learning_rate 0.0001
2017-10-10T11:56:38.815313: step 4120, loss 0.0772532, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:56:39.387515: step 4120, loss 0.203804, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4120

2017-10-10T11:56:40.668241: step 4121, loss 0.068498, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:40.879619: step 4122, loss 0.0428589, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:41.068281: step 4123, loss 0.187056, acc 0.890625, learning_rate 0.0001
2017-10-10T11:56:41.307460: step 4124, loss 0.14723, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:41.513786: step 4125, loss 0.080487, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:41.786354: step 4126, loss 0.191683, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:42.020852: step 4127, loss 0.10406, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:42.254146: step 4128, loss 0.162181, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:42.484628: step 4129, loss 0.107375, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:42.738597: step 4130, loss 0.0534296, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:42.972867: step 4131, loss 0.0629811, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:43.204852: step 4132, loss 0.0905926, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:43.465096: step 4133, loss 0.166803, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:43.725895: step 4134, loss 0.104147, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:43.976773: step 4135, loss 0.063234, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:44.208911: step 4136, loss 0.208063, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:44.450624: step 4137, loss 0.10621, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:44.700026: step 4138, loss 0.0569608, acc 1, learning_rate 0.0001
2017-10-10T11:56:44.952895: step 4139, loss 0.118093, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:45.208887: step 4140, loss 0.0708725, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:45.452816: step 4141, loss 0.0584499, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:45.680341: step 4142, loss 0.100495, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:46.016987: step 4143, loss 0.0413146, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:46.280435: step 4144, loss 0.0682037, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:46.435659: step 4145, loss 0.206355, acc 0.890625, learning_rate 0.0001
2017-10-10T11:56:46.557020: step 4146, loss 0.102739, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:46.670597: step 4147, loss 0.0972828, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:46.860810: step 4148, loss 0.136827, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:47.068493: step 4149, loss 0.0735129, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:47.272619: step 4150, loss 0.0697802, acc 1, learning_rate 0.0001
2017-10-10T11:56:47.469548: step 4151, loss 0.123356, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:47.715664: step 4152, loss 0.0946876, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:48.012859: step 4153, loss 0.100868, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:48.239575: step 4154, loss 0.134396, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:48.573173: step 4155, loss 0.0777032, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:48.857472: step 4156, loss 0.0470282, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:49.082959: step 4157, loss 0.119459, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:49.288876: step 4158, loss 0.0990444, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:49.508822: step 4159, loss 0.113885, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:49.700838: step 4160, loss 0.103363, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:56:50.292878: step 4160, loss 0.205639, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4160

2017-10-10T11:56:51.313289: step 4161, loss 0.0526334, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:51.576936: step 4162, loss 0.104517, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:51.864518: step 4163, loss 0.0576589, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:52.128118: step 4164, loss 0.0962988, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:52.352936: step 4165, loss 0.0422781, acc 1, learning_rate 0.0001
2017-10-10T11:56:52.603635: step 4166, loss 0.0895241, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:52.860021: step 4167, loss 0.0679277, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:53.099577: step 4168, loss 0.102984, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:53.351567: step 4169, loss 0.17547, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:53.607281: step 4170, loss 0.104007, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:53.871539: step 4171, loss 0.114329, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:54.120828: step 4172, loss 0.192518, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:54.368956: step 4173, loss 0.114825, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:54.593092: step 4174, loss 0.059465, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:54.826166: step 4175, loss 0.0786144, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:55.052002: step 4176, loss 0.182496, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:55.305031: step 4177, loss 0.105985, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:55.553054: step 4178, loss 0.0347057, acc 1, learning_rate 0.0001
2017-10-10T11:56:55.813167: step 4179, loss 0.12918, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:56.076851: step 4180, loss 0.150794, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:56.328450: step 4181, loss 0.116479, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:56.588329: step 4182, loss 0.135151, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:56.912942: step 4183, loss 0.0836357, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:57.239603: step 4184, loss 0.0371528, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:57.393376: step 4185, loss 0.111895, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:57.536718: step 4186, loss 0.114388, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:57.681909: step 4187, loss 0.106953, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:57.833152: step 4188, loss 0.0985094, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:57.976055: step 4189, loss 0.0663054, acc 0.984375, learning_rate 0.0001
2017-10-10T11:56:58.108532: step 4190, loss 0.139965, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:58.312855: step 4191, loss 0.042066, acc 1, learning_rate 0.0001
2017-10-10T11:56:58.562961: step 4192, loss 0.114175, acc 0.953125, learning_rate 0.0001
2017-10-10T11:56:58.813602: step 4193, loss 0.12793, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:59.043303: step 4194, loss 0.138104, acc 0.9375, learning_rate 0.0001
2017-10-10T11:56:59.264871: step 4195, loss 0.0914315, acc 1, learning_rate 0.0001
2017-10-10T11:56:59.502366: step 4196, loss 0.0812332, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:59.722184: step 4197, loss 0.104004, acc 0.96875, learning_rate 0.0001
2017-10-10T11:56:59.955253: step 4198, loss 0.0613939, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:00.183662: step 4199, loss 0.179212, acc 0.921875, learning_rate 0.0001
2017-10-10T11:57:00.420977: step 4200, loss 0.146135, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:57:01.102201: step 4200, loss 0.203709, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4200

2017-10-10T11:57:02.188602: step 4201, loss 0.0727777, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:02.435136: step 4202, loss 0.165193, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:02.710682: step 4203, loss 0.123328, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:02.967901: step 4204, loss 0.0473889, acc 1, learning_rate 0.0001
2017-10-10T11:57:03.196169: step 4205, loss 0.180845, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:03.427333: step 4206, loss 0.133477, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:03.665401: step 4207, loss 0.0641946, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:03.920840: step 4208, loss 0.0415791, acc 1, learning_rate 0.0001
2017-10-10T11:57:04.151166: step 4209, loss 0.086486, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:04.360979: step 4210, loss 0.0344547, acc 1, learning_rate 0.0001
2017-10-10T11:57:04.621208: step 4211, loss 0.048402, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:04.851634: step 4212, loss 0.0569079, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:05.092363: step 4213, loss 0.0496238, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:05.328042: step 4214, loss 0.0681475, acc 0.980392, learning_rate 0.0001
2017-10-10T11:57:05.577635: step 4215, loss 0.0689051, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:05.881542: step 4216, loss 0.0559792, acc 1, learning_rate 0.0001
2017-10-10T11:57:06.075281: step 4217, loss 0.0969882, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:06.274923: step 4218, loss 0.147516, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:06.469602: step 4219, loss 0.105483, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:06.701464: step 4220, loss 0.0382478, acc 1, learning_rate 0.0001
2017-10-10T11:57:06.936933: step 4221, loss 0.0809098, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:07.217088: step 4222, loss 0.0725755, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:07.510864: step 4223, loss 0.0460867, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:07.706059: step 4224, loss 0.116948, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:07.895133: step 4225, loss 0.102638, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:08.092589: step 4226, loss 0.0863046, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:08.280074: step 4227, loss 0.128373, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:08.525750: step 4228, loss 0.122662, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:08.791120: step 4229, loss 0.0344907, acc 1, learning_rate 0.0001
2017-10-10T11:57:09.124488: step 4230, loss 0.101327, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:09.372579: step 4231, loss 0.0304595, acc 1, learning_rate 0.0001
2017-10-10T11:57:09.610405: step 4232, loss 0.10299, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:09.844817: step 4233, loss 0.0543538, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:10.087159: step 4234, loss 0.0700848, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:10.342887: step 4235, loss 0.0774944, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:10.586378: step 4236, loss 0.0628277, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:10.821055: step 4237, loss 0.0841204, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:11.066282: step 4238, loss 0.0796413, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:11.308847: step 4239, loss 0.0856607, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:11.543372: step 4240, loss 0.169992, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T11:57:12.171628: step 4240, loss 0.201914, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4240

2017-10-10T11:57:13.477422: step 4241, loss 0.077377, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:13.688814: step 4242, loss 0.109738, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:14.040902: step 4243, loss 0.0537221, acc 1, learning_rate 0.0001
2017-10-10T11:57:14.256871: step 4244, loss 0.0759223, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:14.429655: step 4245, loss 0.103945, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:14.591946: step 4246, loss 0.200863, acc 0.921875, learning_rate 0.0001
2017-10-10T11:57:14.768847: step 4247, loss 0.0590308, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:14.971744: step 4248, loss 0.0989811, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:15.214560: step 4249, loss 0.095404, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:15.509158: step 4250, loss 0.138375, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:15.748844: step 4251, loss 0.120249, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:15.994243: step 4252, loss 0.0273426, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:16.276556: step 4253, loss 0.0944646, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:16.532933: step 4254, loss 0.192395, acc 0.921875, learning_rate 0.0001
2017-10-10T11:57:16.763578: step 4255, loss 0.0866021, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:17.040172: step 4256, loss 0.124663, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:17.272144: step 4257, loss 0.0322615, acc 1, learning_rate 0.0001
2017-10-10T11:57:17.596972: step 4258, loss 0.0775126, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:17.836212: step 4259, loss 0.096246, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:18.028606: step 4260, loss 0.176604, acc 0.921875, learning_rate 0.0001
2017-10-10T11:57:18.192176: step 4261, loss 0.0473271, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:18.380771: step 4262, loss 0.0952055, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:18.561095: step 4263, loss 0.0778602, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:18.841146: step 4264, loss 0.152306, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:19.069800: step 4265, loss 0.0905148, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:19.346786: step 4266, loss 0.0663394, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:19.609004: step 4267, loss 0.106192, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:19.876950: step 4268, loss 0.126632, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:20.104901: step 4269, loss 0.109401, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:20.353721: step 4270, loss 0.0884499, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:20.582040: step 4271, loss 0.078053, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:20.814376: step 4272, loss 0.0897625, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:21.086161: step 4273, loss 0.0995663, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:21.309112: step 4274, loss 0.0490061, acc 1, learning_rate 0.0001
2017-10-10T11:57:21.560115: step 4275, loss 0.117896, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:21.824865: step 4276, loss 0.0679556, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:22.066930: step 4277, loss 0.12954, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:22.332826: step 4278, loss 0.0736653, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:22.604044: step 4279, loss 0.0946407, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:22.824581: step 4280, loss 0.168032, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:57:23.343399: step 4280, loss 0.205233, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4280

2017-10-10T11:57:24.324736: step 4281, loss 0.0616573, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:24.588880: step 4282, loss 0.0888791, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:24.844853: step 4283, loss 0.126215, acc 0.921875, learning_rate 0.0001
2017-10-10T11:57:25.107166: step 4284, loss 0.074588, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:25.369704: step 4285, loss 0.201619, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:25.620338: step 4286, loss 0.0716962, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:25.864891: step 4287, loss 0.117738, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:26.138523: step 4288, loss 0.051985, acc 1, learning_rate 0.0001
2017-10-10T11:57:26.361628: step 4289, loss 0.071513, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:26.598181: step 4290, loss 0.0833929, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:26.883758: step 4291, loss 0.0912652, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:27.133665: step 4292, loss 0.0687604, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:27.376874: step 4293, loss 0.05124, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:27.604827: step 4294, loss 0.099447, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:27.904845: step 4295, loss 0.132952, acc 0.921875, learning_rate 0.0001
2017-10-10T11:57:28.142397: step 4296, loss 0.0817318, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:28.351507: step 4297, loss 0.0547367, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:28.553933: step 4298, loss 0.0680684, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:28.725008: step 4299, loss 0.179764, acc 0.921875, learning_rate 0.0001
2017-10-10T11:57:28.912578: step 4300, loss 0.0651564, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:29.085286: step 4301, loss 0.0545493, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:29.284562: step 4302, loss 0.0993488, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:29.518433: step 4303, loss 0.0165981, acc 1, learning_rate 0.0001
2017-10-10T11:57:29.730207: step 4304, loss 0.105561, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:29.985226: step 4305, loss 0.0405644, acc 1, learning_rate 0.0001
2017-10-10T11:57:30.206676: step 4306, loss 0.0616589, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:30.513283: step 4307, loss 0.141673, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:30.833083: step 4308, loss 0.0876914, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:31.058503: step 4309, loss 0.0917661, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:31.268539: step 4310, loss 0.0765061, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:31.482612: step 4311, loss 0.0626933, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:31.685858: step 4312, loss 0.100959, acc 0.941176, learning_rate 0.0001
2017-10-10T11:57:31.932428: step 4313, loss 0.0335485, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:32.164861: step 4314, loss 0.1209, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:32.417604: step 4315, loss 0.0888066, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:32.680891: step 4316, loss 0.0155088, acc 1, learning_rate 0.0001
2017-10-10T11:57:32.951381: step 4317, loss 0.0569715, acc 1, learning_rate 0.0001
2017-10-10T11:57:33.186104: step 4318, loss 0.0958066, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:33.471084: step 4319, loss 0.0542639, acc 1, learning_rate 0.0001
2017-10-10T11:57:33.702221: step 4320, loss 0.0797569, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:57:34.296602: step 4320, loss 0.205822, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4320

2017-10-10T11:57:35.633151: step 4321, loss 0.20223, acc 0.90625, learning_rate 0.0001
2017-10-10T11:57:35.888919: step 4322, loss 0.0839877, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:36.121896: step 4323, loss 0.0869658, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:36.361063: step 4324, loss 0.156425, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:36.626834: step 4325, loss 0.140306, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:36.858067: step 4326, loss 0.0681941, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:37.105658: step 4327, loss 0.0292728, acc 1, learning_rate 0.0001
2017-10-10T11:57:37.360854: step 4328, loss 0.0403291, acc 1, learning_rate 0.0001
2017-10-10T11:57:37.622295: step 4329, loss 0.0638549, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:37.868336: step 4330, loss 0.0924439, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:38.121387: step 4331, loss 0.169755, acc 0.921875, learning_rate 0.0001
2017-10-10T11:57:38.384158: step 4332, loss 0.145807, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:38.628839: step 4333, loss 0.10237, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:38.976732: step 4334, loss 0.0944761, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:39.186944: step 4335, loss 0.142135, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:39.329634: step 4336, loss 0.0950106, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:39.471181: step 4337, loss 0.0451127, acc 1, learning_rate 0.0001
2017-10-10T11:57:39.624555: step 4338, loss 0.171924, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:39.760358: step 4339, loss 0.0345326, acc 1, learning_rate 0.0001
2017-10-10T11:57:39.957856: step 4340, loss 0.0367846, acc 1, learning_rate 0.0001
2017-10-10T11:57:40.225595: step 4341, loss 0.148394, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:40.467737: step 4342, loss 0.0271616, acc 1, learning_rate 0.0001
2017-10-10T11:57:40.719268: step 4343, loss 0.0434135, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:40.969214: step 4344, loss 0.103913, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:41.241908: step 4345, loss 0.0397419, acc 1, learning_rate 0.0001
2017-10-10T11:57:41.530777: step 4346, loss 0.0759907, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:41.754151: step 4347, loss 0.08108, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:41.949834: step 4348, loss 0.1114, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:42.165033: step 4349, loss 0.155236, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:42.405566: step 4350, loss 0.0484691, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:42.614169: step 4351, loss 0.0882614, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:42.888810: step 4352, loss 0.0943235, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:43.152854: step 4353, loss 0.128165, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:43.387828: step 4354, loss 0.0501104, acc 1, learning_rate 0.0001
2017-10-10T11:57:43.613791: step 4355, loss 0.129768, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:43.853741: step 4356, loss 0.151484, acc 0.921875, learning_rate 0.0001
2017-10-10T11:57:44.101121: step 4357, loss 0.14699, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:44.353477: step 4358, loss 0.045541, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:44.587175: step 4359, loss 0.161051, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:44.834850: step 4360, loss 0.0903964, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:57:45.429777: step 4360, loss 0.201496, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4360

2017-10-10T11:57:46.758299: step 4361, loss 0.0752526, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:47.013388: step 4362, loss 0.0582332, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:47.289977: step 4363, loss 0.129877, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:47.589294: step 4364, loss 0.0187469, acc 1, learning_rate 0.0001
2017-10-10T11:57:47.797360: step 4365, loss 0.140115, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:48.007739: step 4366, loss 0.0590754, acc 1, learning_rate 0.0001
2017-10-10T11:57:48.206341: step 4367, loss 0.109616, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:48.403502: step 4368, loss 0.0413171, acc 1, learning_rate 0.0001
2017-10-10T11:57:48.658512: step 4369, loss 0.0299328, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:48.916396: step 4370, loss 0.144334, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:49.225019: step 4371, loss 0.0888654, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:49.485842: step 4372, loss 0.0982098, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:49.661651: step 4373, loss 0.0866207, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:49.861407: step 4374, loss 0.0594843, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:50.060954: step 4375, loss 0.0611873, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:50.263495: step 4376, loss 0.105945, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:50.452996: step 4377, loss 0.063089, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:50.713616: step 4378, loss 0.0331972, acc 1, learning_rate 0.0001
2017-10-10T11:57:50.959804: step 4379, loss 0.133953, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:51.239928: step 4380, loss 0.0749108, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:51.460418: step 4381, loss 0.122037, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:51.689009: step 4382, loss 0.167593, acc 0.921875, learning_rate 0.0001
2017-10-10T11:57:51.941220: step 4383, loss 0.0514967, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:52.165695: step 4384, loss 0.0181616, acc 1, learning_rate 0.0001
2017-10-10T11:57:52.393923: step 4385, loss 0.0829454, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:52.678477: step 4386, loss 0.065126, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:52.902874: step 4387, loss 0.12859, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:53.161295: step 4388, loss 0.0794246, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:53.431480: step 4389, loss 0.0401409, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:53.653027: step 4390, loss 0.0751302, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:53.888854: step 4391, loss 0.125144, acc 0.953125, learning_rate 0.0001
2017-10-10T11:57:54.141001: step 4392, loss 0.0622455, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:54.361005: step 4393, loss 0.0719171, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:54.655385: step 4394, loss 0.0478962, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:54.919158: step 4395, loss 0.11023, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:55.160239: step 4396, loss 0.0640683, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:55.400976: step 4397, loss 0.0627503, acc 1, learning_rate 0.0001
2017-10-10T11:57:55.729011: step 4398, loss 0.0903258, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:55.983638: step 4399, loss 0.0941018, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:56.180831: step 4400, loss 0.182813, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:57:56.749732: step 4400, loss 0.203087, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4400

2017-10-10T11:57:57.854571: step 4401, loss 0.0614357, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:58.065054: step 4402, loss 0.0338441, acc 1, learning_rate 0.0001
2017-10-10T11:57:58.382026: step 4403, loss 0.142631, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:58.611986: step 4404, loss 0.0729429, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:58.838946: step 4405, loss 0.0474503, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:59.062832: step 4406, loss 0.0544316, acc 0.984375, learning_rate 0.0001
2017-10-10T11:57:59.288891: step 4407, loss 0.123805, acc 0.9375, learning_rate 0.0001
2017-10-10T11:57:59.544873: step 4408, loss 0.0780752, acc 0.96875, learning_rate 0.0001
2017-10-10T11:57:59.817524: step 4409, loss 0.085713, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:00.020379: step 4410, loss 0.116991, acc 0.980392, learning_rate 0.0001
2017-10-10T11:58:00.235401: step 4411, loss 0.0970708, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:00.423999: step 4412, loss 0.0388804, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:00.644044: step 4413, loss 0.0987278, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:00.819857: step 4414, loss 0.142924, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:01.059904: step 4415, loss 0.0492579, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:01.344797: step 4416, loss 0.143654, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:01.580173: step 4417, loss 0.0456552, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:01.823856: step 4418, loss 0.107115, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:02.083841: step 4419, loss 0.113122, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:02.347303: step 4420, loss 0.0475031, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:02.593553: step 4421, loss 0.0721731, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:02.862570: step 4422, loss 0.0513428, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:03.113596: step 4423, loss 0.130614, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:03.380672: step 4424, loss 0.075981, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:03.612854: step 4425, loss 0.0319461, acc 1, learning_rate 0.0001
2017-10-10T11:58:03.852897: step 4426, loss 0.0275038, acc 1, learning_rate 0.0001
2017-10-10T11:58:04.140559: step 4427, loss 0.0340942, acc 1, learning_rate 0.0001
2017-10-10T11:58:04.404155: step 4428, loss 0.0975024, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:04.595187: step 4429, loss 0.138396, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:04.812038: step 4430, loss 0.144048, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:05.004037: step 4431, loss 0.0960042, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:05.221059: step 4432, loss 0.124044, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:05.453238: step 4433, loss 0.0529612, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:05.701022: step 4434, loss 0.0915812, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:05.957009: step 4435, loss 0.136897, acc 0.921875, learning_rate 0.0001
2017-10-10T11:58:06.213107: step 4436, loss 0.0748644, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:06.500505: step 4437, loss 0.111411, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:06.742961: step 4438, loss 0.026769, acc 1, learning_rate 0.0001
2017-10-10T11:58:06.988913: step 4439, loss 0.15042, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:07.261660: step 4440, loss 0.0523915, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:58:07.952851: step 4440, loss 0.204143, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4440

2017-10-10T11:58:09.090518: step 4441, loss 0.0639853, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:09.334352: step 4442, loss 0.0521466, acc 1, learning_rate 0.0001
2017-10-10T11:58:09.591591: step 4443, loss 0.0475878, acc 1, learning_rate 0.0001
2017-10-10T11:58:09.843793: step 4444, loss 0.138138, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:10.077529: step 4445, loss 0.113929, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:10.373091: step 4446, loss 0.136742, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:10.656762: step 4447, loss 0.0715312, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:10.844832: step 4448, loss 0.12373, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:11.028291: step 4449, loss 0.089064, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:11.205172: step 4450, loss 0.0552543, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:11.383096: step 4451, loss 0.0832617, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:11.552891: step 4452, loss 0.100279, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:11.815958: step 4453, loss 0.056177, acc 1, learning_rate 0.0001
2017-10-10T11:58:12.074161: step 4454, loss 0.0975967, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:12.341419: step 4455, loss 0.0870592, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:12.596923: step 4456, loss 0.064685, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:12.798166: step 4457, loss 0.175285, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:13.025074: step 4458, loss 0.116717, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:13.218660: step 4459, loss 0.100906, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:13.414088: step 4460, loss 0.03826, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:13.666290: step 4461, loss 0.113367, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:13.923113: step 4462, loss 0.0456841, acc 1, learning_rate 0.0001
2017-10-10T11:58:14.176839: step 4463, loss 0.0457574, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:14.416849: step 4464, loss 0.0544023, acc 1, learning_rate 0.0001
2017-10-10T11:58:14.656905: step 4465, loss 0.0833559, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:14.918488: step 4466, loss 0.0556144, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:15.145153: step 4467, loss 0.0295045, acc 1, learning_rate 0.0001
2017-10-10T11:58:15.436918: step 4468, loss 0.0713704, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:15.675720: step 4469, loss 0.059145, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:15.947715: step 4470, loss 0.0653098, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:16.181653: step 4471, loss 0.0712595, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:16.406240: step 4472, loss 0.0915072, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:16.683353: step 4473, loss 0.0440446, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:16.915242: step 4474, loss 0.0867433, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:17.151212: step 4475, loss 0.0726391, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:17.405934: step 4476, loss 0.0592887, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:17.645186: step 4477, loss 0.122246, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:17.884598: step 4478, loss 0.145159, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:18.118554: step 4479, loss 0.0684096, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:18.358101: step 4480, loss 0.0545223, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T11:58:19.048925: step 4480, loss 0.204559, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4480

2017-10-10T11:58:20.203705: step 4481, loss 0.0844247, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:20.458202: step 4482, loss 0.11223, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:20.690625: step 4483, loss 0.0940496, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:20.993061: step 4484, loss 0.0964357, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:21.228397: step 4485, loss 0.0862573, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:21.383567: step 4486, loss 0.0730919, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:21.517529: step 4487, loss 0.177192, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:21.652216: step 4488, loss 0.111765, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:21.786687: step 4489, loss 0.131855, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:21.915829: step 4490, loss 0.0651932, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:22.097840: step 4491, loss 0.132614, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:22.373069: step 4492, loss 0.12994, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:22.632834: step 4493, loss 0.116115, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:22.880837: step 4494, loss 0.102849, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:23.116624: step 4495, loss 0.102267, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:23.332848: step 4496, loss 0.0899712, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:23.576778: step 4497, loss 0.118985, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:23.804416: step 4498, loss 0.10122, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:24.067275: step 4499, loss 0.0614892, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:24.314291: step 4500, loss 0.0813692, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:24.544263: step 4501, loss 0.148429, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:24.767918: step 4502, loss 0.110487, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:25.025641: step 4503, loss 0.129551, acc 0.921875, learning_rate 0.0001
2017-10-10T11:58:25.266694: step 4504, loss 0.113268, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:25.524586: step 4505, loss 0.0273603, acc 1, learning_rate 0.0001
2017-10-10T11:58:25.750319: step 4506, loss 0.111635, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:26.009062: step 4507, loss 0.0738338, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:26.192353: step 4508, loss 0.0663683, acc 0.960784, learning_rate 0.0001
2017-10-10T11:58:26.415303: step 4509, loss 0.0764977, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:26.670767: step 4510, loss 0.0700375, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:26.918251: step 4511, loss 0.0691059, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:27.146632: step 4512, loss 0.0182616, acc 1, learning_rate 0.0001
2017-10-10T11:58:27.399688: step 4513, loss 0.105589, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:27.635464: step 4514, loss 0.10348, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:27.872364: step 4515, loss 0.031832, acc 1, learning_rate 0.0001
2017-10-10T11:58:28.138863: step 4516, loss 0.0393132, acc 1, learning_rate 0.0001
2017-10-10T11:58:28.406735: step 4517, loss 0.156901, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:28.644176: step 4518, loss 0.0596513, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:28.883188: step 4519, loss 0.12196, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:29.088021: step 4520, loss 0.045691, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:58:29.880107: step 4520, loss 0.2031, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4520

2017-10-10T11:58:31.216281: step 4521, loss 0.0316008, acc 1, learning_rate 0.0001
2017-10-10T11:58:31.459184: step 4522, loss 0.128353, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:31.632763: step 4523, loss 0.163429, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:31.812256: step 4524, loss 0.022837, acc 1, learning_rate 0.0001
2017-10-10T11:58:31.995106: step 4525, loss 0.0894371, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:32.200768: step 4526, loss 0.163966, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:32.407247: step 4527, loss 0.0843702, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:32.635952: step 4528, loss 0.0888699, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:32.856881: step 4529, loss 0.0889384, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:33.133454: step 4530, loss 0.0508167, acc 1, learning_rate 0.0001
2017-10-10T11:58:33.368330: step 4531, loss 0.069827, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:33.605982: step 4532, loss 0.0619056, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:33.884623: step 4533, loss 0.138831, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:34.125145: step 4534, loss 0.125277, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:34.388865: step 4535, loss 0.0691216, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:34.619143: step 4536, loss 0.0912404, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:34.846362: step 4537, loss 0.0295518, acc 1, learning_rate 0.0001
2017-10-10T11:58:35.072946: step 4538, loss 0.0719323, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:35.324860: step 4539, loss 0.0455991, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:35.566211: step 4540, loss 0.0991242, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:35.815724: step 4541, loss 0.0313639, acc 1, learning_rate 0.0001
2017-10-10T11:58:36.092844: step 4542, loss 0.0644027, acc 1, learning_rate 0.0001
2017-10-10T11:58:36.350974: step 4543, loss 0.0565087, acc 1, learning_rate 0.0001
2017-10-10T11:58:36.548244: step 4544, loss 0.0605083, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:36.811173: step 4545, loss 0.0961577, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:37.065259: step 4546, loss 0.0421108, acc 1, learning_rate 0.0001
2017-10-10T11:58:37.291020: step 4547, loss 0.115794, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:37.509330: step 4548, loss 0.0636344, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:37.793071: step 4549, loss 0.050909, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:38.041189: step 4550, loss 0.063612, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:38.280230: step 4551, loss 0.0503033, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:38.486191: step 4552, loss 0.226187, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:38.681198: step 4553, loss 0.0509463, acc 1, learning_rate 0.0001
2017-10-10T11:58:38.898173: step 4554, loss 0.0384927, acc 1, learning_rate 0.0001
2017-10-10T11:58:39.129463: step 4555, loss 0.0816591, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:39.369538: step 4556, loss 0.0630576, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:39.614617: step 4557, loss 0.157494, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:39.871409: step 4558, loss 0.0736595, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:40.165953: step 4559, loss 0.112503, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:40.428869: step 4560, loss 0.133409, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:58:41.093424: step 4560, loss 0.201794, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4560

2017-10-10T11:58:42.092703: step 4561, loss 0.0407332, acc 1, learning_rate 0.0001
2017-10-10T11:58:42.271046: step 4562, loss 0.0602733, acc 1, learning_rate 0.0001
2017-10-10T11:58:42.457001: step 4563, loss 0.100369, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:42.653256: step 4564, loss 0.057411, acc 1, learning_rate 0.0001
2017-10-10T11:58:42.832166: step 4565, loss 0.0820925, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:43.025294: step 4566, loss 0.0927237, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:43.281925: step 4567, loss 0.0461015, acc 1, learning_rate 0.0001
2017-10-10T11:58:43.517567: step 4568, loss 0.121036, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:43.772131: step 4569, loss 0.121368, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:44.014738: step 4570, loss 0.11911, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:44.243994: step 4571, loss 0.169137, acc 0.921875, learning_rate 0.0001
2017-10-10T11:58:44.469075: step 4572, loss 0.0804409, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:44.731866: step 4573, loss 0.10191, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:44.960796: step 4574, loss 0.135815, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:45.204829: step 4575, loss 0.111168, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:45.454081: step 4576, loss 0.104173, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:45.705126: step 4577, loss 0.174341, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:45.976861: step 4578, loss 0.12394, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:46.287862: step 4579, loss 0.0843479, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:46.485321: step 4580, loss 0.0639687, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:46.684776: step 4581, loss 0.0628847, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:46.902954: step 4582, loss 0.09783, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:47.115720: step 4583, loss 0.0815662, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:47.332531: step 4584, loss 0.0595739, acc 1, learning_rate 0.0001
2017-10-10T11:58:47.576142: step 4585, loss 0.10285, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:47.821974: step 4586, loss 0.0716163, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:48.060873: step 4587, loss 0.135499, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:48.325875: step 4588, loss 0.139318, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:48.571920: step 4589, loss 0.123455, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:48.841109: step 4590, loss 0.0643668, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:49.085170: step 4591, loss 0.0389704, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:49.297315: step 4592, loss 0.0388221, acc 1, learning_rate 0.0001
2017-10-10T11:58:49.546966: step 4593, loss 0.0647793, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:49.767529: step 4594, loss 0.0948824, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:49.977390: step 4595, loss 0.161196, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:50.220872: step 4596, loss 0.143608, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:50.454199: step 4597, loss 0.0648956, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:50.704786: step 4598, loss 0.0941452, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:50.937548: step 4599, loss 0.109138, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:51.180620: step 4600, loss 0.12846, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T11:58:51.772541: step 4600, loss 0.201264, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4600

2017-10-10T11:58:52.931108: step 4601, loss 0.108034, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:53.132151: step 4602, loss 0.0528157, acc 1, learning_rate 0.0001
2017-10-10T11:58:53.317168: step 4603, loss 0.0499245, acc 1, learning_rate 0.0001
2017-10-10T11:58:53.499266: step 4604, loss 0.0858795, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:53.756921: step 4605, loss 0.114088, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:53.986997: step 4606, loss 0.0802985, acc 0.980392, learning_rate 0.0001
2017-10-10T11:58:54.249421: step 4607, loss 0.0399414, acc 1, learning_rate 0.0001
2017-10-10T11:58:54.560337: step 4608, loss 0.0433879, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:54.823419: step 4609, loss 0.100669, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:55.023642: step 4610, loss 0.105497, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:55.227935: step 4611, loss 0.114856, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:55.453209: step 4612, loss 0.0703331, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:55.687086: step 4613, loss 0.0219719, acc 1, learning_rate 0.0001
2017-10-10T11:58:55.957527: step 4614, loss 0.171717, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:56.204655: step 4615, loss 0.0888708, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:56.448233: step 4616, loss 0.142107, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:56.716269: step 4617, loss 0.0789626, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:56.933421: step 4618, loss 0.14943, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:57.181966: step 4619, loss 0.133685, acc 0.9375, learning_rate 0.0001
2017-10-10T11:58:57.411534: step 4620, loss 0.115027, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:57.646739: step 4621, loss 0.0690489, acc 1, learning_rate 0.0001
2017-10-10T11:58:57.908927: step 4622, loss 0.097961, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:58.172843: step 4623, loss 0.0769589, acc 0.96875, learning_rate 0.0001
2017-10-10T11:58:58.414915: step 4624, loss 0.178381, acc 0.921875, learning_rate 0.0001
2017-10-10T11:58:58.662177: step 4625, loss 0.0559282, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:58.922969: step 4626, loss 0.212544, acc 0.953125, learning_rate 0.0001
2017-10-10T11:58:59.171506: step 4627, loss 0.056761, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:59.464603: step 4628, loss 0.0533183, acc 1, learning_rate 0.0001
2017-10-10T11:58:59.725002: step 4629, loss 0.0742136, acc 0.984375, learning_rate 0.0001
2017-10-10T11:58:59.985234: step 4630, loss 0.0674072, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:00.241877: step 4631, loss 0.123126, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:00.474114: step 4632, loss 0.0853962, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:00.744752: step 4633, loss 0.125484, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:00.994597: step 4634, loss 0.112186, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:01.257377: step 4635, loss 0.0649887, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:01.503904: step 4636, loss 0.0702285, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:01.738302: step 4637, loss 0.0665392, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:02.000348: step 4638, loss 0.117454, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:02.246949: step 4639, loss 0.0607459, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:02.539698: step 4640, loss 0.111629, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:59:03.212885: step 4640, loss 0.198741, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4640

2017-10-10T11:59:04.427482: step 4641, loss 0.0593658, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:04.648829: step 4642, loss 0.0970861, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:04.902746: step 4643, loss 0.104409, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:05.117181: step 4644, loss 0.0498886, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:05.352167: step 4645, loss 0.143617, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:05.607523: step 4646, loss 0.0802098, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:05.858524: step 4647, loss 0.0704957, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:06.147032: step 4648, loss 0.047194, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:06.361960: step 4649, loss 0.0396908, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:06.597724: step 4650, loss 0.0735516, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:06.835818: step 4651, loss 0.137857, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:07.061002: step 4652, loss 0.0790456, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:07.327512: step 4653, loss 0.0411079, acc 1, learning_rate 0.0001
2017-10-10T11:59:07.554162: step 4654, loss 0.0701221, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:07.808875: step 4655, loss 0.100989, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:08.055652: step 4656, loss 0.0280396, acc 1, learning_rate 0.0001
2017-10-10T11:59:08.290963: step 4657, loss 0.14516, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:08.572415: step 4658, loss 0.0875421, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:08.809455: step 4659, loss 0.0781733, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:09.046018: step 4660, loss 0.0247597, acc 1, learning_rate 0.0001
2017-10-10T11:59:09.283050: step 4661, loss 0.105858, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:09.505212: step 4662, loss 0.039382, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:09.744431: step 4663, loss 0.0978965, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:10.009023: step 4664, loss 0.0863939, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:10.265194: step 4665, loss 0.0818951, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:10.521589: step 4666, loss 0.106734, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:10.778741: step 4667, loss 0.0879152, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:11.006396: step 4668, loss 0.182816, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:11.284914: step 4669, loss 0.0798914, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:11.521844: step 4670, loss 0.0649069, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:11.837013: step 4671, loss 0.134202, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:12.105903: step 4672, loss 0.054602, acc 1, learning_rate 0.0001
2017-10-10T11:59:12.309883: step 4673, loss 0.133958, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:12.534935: step 4674, loss 0.0866044, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:12.728963: step 4675, loss 0.0870457, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:12.929726: step 4676, loss 0.0512362, acc 1, learning_rate 0.0001
2017-10-10T11:59:13.142504: step 4677, loss 0.0478243, acc 1, learning_rate 0.0001
2017-10-10T11:59:13.370220: step 4678, loss 0.0987994, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:13.702909: step 4679, loss 0.0989831, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:13.939624: step 4680, loss 0.0551811, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:59:14.483999: step 4680, loss 0.203464, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4680

2017-10-10T11:59:15.349109: step 4681, loss 0.0791711, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:15.586264: step 4682, loss 0.1084, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:15.828111: step 4683, loss 0.0576777, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:16.071341: step 4684, loss 0.0416057, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:16.304696: step 4685, loss 0.055075, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:16.576592: step 4686, loss 0.0639548, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:16.820933: step 4687, loss 0.0588199, acc 1, learning_rate 0.0001
2017-10-10T11:59:17.077196: step 4688, loss 0.0617764, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:17.333090: step 4689, loss 0.0863333, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:17.537889: step 4690, loss 0.054102, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:17.747629: step 4691, loss 0.0645099, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:17.983249: step 4692, loss 0.0483404, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:18.237042: step 4693, loss 0.0861049, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:18.486250: step 4694, loss 0.0560032, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:18.761369: step 4695, loss 0.0644261, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:19.006953: step 4696, loss 0.034233, acc 1, learning_rate 0.0001
2017-10-10T11:59:19.248930: step 4697, loss 0.172933, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:19.488913: step 4698, loss 0.116703, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:19.745198: step 4699, loss 0.0768146, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:20.012939: step 4700, loss 0.0531208, acc 1, learning_rate 0.0001
2017-10-10T11:59:20.296862: step 4701, loss 0.0221883, acc 1, learning_rate 0.0001
2017-10-10T11:59:20.585410: step 4702, loss 0.0577795, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:20.789398: step 4703, loss 0.104971, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:20.984394: step 4704, loss 0.102219, acc 0.960784, learning_rate 0.0001
2017-10-10T11:59:21.186467: step 4705, loss 0.12495, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:21.387173: step 4706, loss 0.0797235, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:21.618180: step 4707, loss 0.0504076, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:21.844801: step 4708, loss 0.0650795, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:22.071302: step 4709, loss 0.122486, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:22.355041: step 4710, loss 0.0503542, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:22.580443: step 4711, loss 0.0387666, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:22.823722: step 4712, loss 0.126856, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:23.104892: step 4713, loss 0.066063, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:23.372873: step 4714, loss 0.0545736, acc 1, learning_rate 0.0001
2017-10-10T11:59:23.597047: step 4715, loss 0.0626989, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:23.839416: step 4716, loss 0.0753866, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:24.044840: step 4717, loss 0.0541038, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:24.266285: step 4718, loss 0.0488235, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:24.587646: step 4719, loss 0.126672, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:24.815393: step 4720, loss 0.113899, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:59:25.322245: step 4720, loss 0.203085, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4720

2017-10-10T11:59:26.506901: step 4721, loss 0.0673592, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:26.734470: step 4722, loss 0.0506265, acc 1, learning_rate 0.0001
2017-10-10T11:59:26.952840: step 4723, loss 0.0569074, acc 1, learning_rate 0.0001
2017-10-10T11:59:27.205103: step 4724, loss 0.105086, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:27.435062: step 4725, loss 0.0512026, acc 1, learning_rate 0.0001
2017-10-10T11:59:27.657302: step 4726, loss 0.0277073, acc 1, learning_rate 0.0001
2017-10-10T11:59:27.914743: step 4727, loss 0.0815919, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:28.155353: step 4728, loss 0.046399, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:28.404375: step 4729, loss 0.0982522, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:28.671982: step 4730, loss 0.111458, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:28.947737: step 4731, loss 0.0866681, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:29.145034: step 4732, loss 0.090667, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:29.349957: step 4733, loss 0.0529388, acc 1, learning_rate 0.0001
2017-10-10T11:59:29.533047: step 4734, loss 0.0589494, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:29.733855: step 4735, loss 0.114176, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:29.896877: step 4736, loss 0.074889, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:30.137023: step 4737, loss 0.087562, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:30.390150: step 4738, loss 0.0506945, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:30.640598: step 4739, loss 0.116164, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:30.891344: step 4740, loss 0.0657237, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:31.125602: step 4741, loss 0.0368484, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:31.380392: step 4742, loss 0.130447, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:31.652855: step 4743, loss 0.0834794, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:31.900058: step 4744, loss 0.0901642, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:32.139385: step 4745, loss 0.0461644, acc 1, learning_rate 0.0001
2017-10-10T11:59:32.368990: step 4746, loss 0.0706589, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:32.654248: step 4747, loss 0.0788365, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:32.914232: step 4748, loss 0.0203143, acc 1, learning_rate 0.0001
2017-10-10T11:59:33.150257: step 4749, loss 0.135024, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:33.428511: step 4750, loss 0.0848009, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:33.680989: step 4751, loss 0.111406, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:33.921389: step 4752, loss 0.0953006, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:34.152869: step 4753, loss 0.0501451, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:34.398321: step 4754, loss 0.110928, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:34.740407: step 4755, loss 0.0957103, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:34.985922: step 4756, loss 0.174793, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:35.177592: step 4757, loss 0.0702093, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:35.360873: step 4758, loss 0.0784269, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:35.543134: step 4759, loss 0.0814298, acc 1, learning_rate 0.0001
2017-10-10T11:59:35.743115: step 4760, loss 0.145634, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T11:59:36.420735: step 4760, loss 0.202574, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4760

2017-10-10T11:59:37.732827: step 4761, loss 0.0694709, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:37.928407: step 4762, loss 0.0988885, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:38.148397: step 4763, loss 0.091176, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:38.364824: step 4764, loss 0.168043, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:38.575756: step 4765, loss 0.0600441, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:38.830143: step 4766, loss 0.046038, acc 1, learning_rate 0.0001
2017-10-10T11:59:39.138069: step 4767, loss 0.0914681, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:39.377284: step 4768, loss 0.0791105, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:39.632888: step 4769, loss 0.169169, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:39.877983: step 4770, loss 0.164455, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:40.091547: step 4771, loss 0.0600565, acc 1, learning_rate 0.0001
2017-10-10T11:59:40.335237: step 4772, loss 0.0564533, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:40.597231: step 4773, loss 0.0762661, acc 1, learning_rate 0.0001
2017-10-10T11:59:40.832891: step 4774, loss 0.055642, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:41.072930: step 4775, loss 0.0451905, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:41.322284: step 4776, loss 0.0349415, acc 1, learning_rate 0.0001
2017-10-10T11:59:41.557663: step 4777, loss 0.0656745, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:41.833029: step 4778, loss 0.0835148, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:42.095284: step 4779, loss 0.0494943, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:42.374788: step 4780, loss 0.0614096, acc 1, learning_rate 0.0001
2017-10-10T11:59:42.629702: step 4781, loss 0.0317917, acc 1, learning_rate 0.0001
2017-10-10T11:59:42.869864: step 4782, loss 0.202167, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:43.135454: step 4783, loss 0.0660381, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:43.364974: step 4784, loss 0.106099, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:43.672201: step 4785, loss 0.0938281, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:43.900881: step 4786, loss 0.0735136, acc 1, learning_rate 0.0001
2017-10-10T11:59:44.148708: step 4787, loss 0.0320565, acc 1, learning_rate 0.0001
2017-10-10T11:59:44.384817: step 4788, loss 0.112323, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:44.605522: step 4789, loss 0.141829, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:44.848863: step 4790, loss 0.0427308, acc 1, learning_rate 0.0001
2017-10-10T11:59:45.126622: step 4791, loss 0.0530858, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:45.356872: step 4792, loss 0.0886735, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:45.531005: step 4793, loss 0.0418645, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:45.720891: step 4794, loss 0.0380353, acc 1, learning_rate 0.0001
2017-10-10T11:59:45.944835: step 4795, loss 0.141429, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:46.166959: step 4796, loss 0.0556324, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:46.303871: step 4797, loss 0.129875, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:46.528827: step 4798, loss 0.074313, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:46.738846: step 4799, loss 0.0649417, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:46.932849: step 4800, loss 0.0852191, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T11:59:47.597720: step 4800, loss 0.20034, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4800

2017-10-10T11:59:48.633472: step 4801, loss 0.148354, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:48.848341: step 4802, loss 0.151358, acc 0.960784, learning_rate 0.0001
2017-10-10T11:59:49.100833: step 4803, loss 0.0206669, acc 1, learning_rate 0.0001
2017-10-10T11:59:49.367261: step 4804, loss 0.15779, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:49.626615: step 4805, loss 0.31203, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:49.903034: step 4806, loss 0.0425954, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:50.137935: step 4807, loss 0.0654231, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:50.368904: step 4808, loss 0.0302163, acc 1, learning_rate 0.0001
2017-10-10T11:59:50.640873: step 4809, loss 0.107105, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:50.902150: step 4810, loss 0.0496812, acc 1, learning_rate 0.0001
2017-10-10T11:59:51.126657: step 4811, loss 0.0285826, acc 1, learning_rate 0.0001
2017-10-10T11:59:51.387390: step 4812, loss 0.0789018, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:51.620492: step 4813, loss 0.0847698, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:51.868533: step 4814, loss 0.104217, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:52.090740: step 4815, loss 0.15041, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:52.371415: step 4816, loss 0.0977434, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:52.635540: step 4817, loss 0.105245, acc 0.921875, learning_rate 0.0001
2017-10-10T11:59:52.858884: step 4818, loss 0.0759536, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:53.091448: step 4819, loss 0.153664, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:53.336355: step 4820, loss 0.0934483, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:53.573815: step 4821, loss 0.0386496, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:53.799760: step 4822, loss 0.117397, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:54.059319: step 4823, loss 0.0617041, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:54.345715: step 4824, loss 0.0584766, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:54.551803: step 4825, loss 0.0858568, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:54.756855: step 4826, loss 0.0671397, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:54.958622: step 4827, loss 0.0797748, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:55.201075: step 4828, loss 0.104026, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:55.424651: step 4829, loss 0.0856358, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:55.664859: step 4830, loss 0.081042, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:55.975209: step 4831, loss 0.177236, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:56.200436: step 4832, loss 0.0785, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:56.377592: step 4833, loss 0.134579, acc 0.953125, learning_rate 0.0001
2017-10-10T11:59:56.560863: step 4834, loss 0.103383, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:56.751523: step 4835, loss 0.0562053, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:56.976978: step 4836, loss 0.0720729, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:57.237059: step 4837, loss 0.0862718, acc 0.96875, learning_rate 0.0001
2017-10-10T11:59:57.500978: step 4838, loss 0.0357577, acc 0.984375, learning_rate 0.0001
2017-10-10T11:59:57.776939: step 4839, loss 0.13927, acc 0.9375, learning_rate 0.0001
2017-10-10T11:59:58.023246: step 4840, loss 0.0599708, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T11:59:58.720796: step 4840, loss 0.204332, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4840

2017-10-10T11:59:59.937088: step 4841, loss 0.105248, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:00.172303: step 4842, loss 0.0365255, acc 1, learning_rate 0.0001
2017-10-10T12:00:00.394727: step 4843, loss 0.128271, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:00.668023: step 4844, loss 0.0645138, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:00.912841: step 4845, loss 0.0684179, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:01.163113: step 4846, loss 0.124761, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:01.433933: step 4847, loss 0.180322, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:01.675559: step 4848, loss 0.0683465, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:01.903608: step 4849, loss 0.142141, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:02.166859: step 4850, loss 0.135318, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:02.468863: step 4851, loss 0.0969098, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:02.743614: step 4852, loss 0.0421365, acc 1, learning_rate 0.0001
2017-10-10T12:00:02.947470: step 4853, loss 0.162561, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:03.142303: step 4854, loss 0.0516341, acc 1, learning_rate 0.0001
2017-10-10T12:00:03.337600: step 4855, loss 0.0413275, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:03.525512: step 4856, loss 0.046181, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:03.772793: step 4857, loss 0.0709239, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:04.037249: step 4858, loss 0.0519211, acc 1, learning_rate 0.0001
2017-10-10T12:00:04.232810: step 4859, loss 0.0423925, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:04.466137: step 4860, loss 0.137322, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:04.712846: step 4861, loss 0.099574, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:04.950464: step 4862, loss 0.078355, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:05.205702: step 4863, loss 0.0401429, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:05.439305: step 4864, loss 0.113386, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:05.670011: step 4865, loss 0.0771489, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:05.885927: step 4866, loss 0.151394, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:06.215871: step 4867, loss 0.142792, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:06.475703: step 4868, loss 0.072253, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:06.654979: step 4869, loss 0.0393853, acc 1, learning_rate 0.0001
2017-10-10T12:00:06.863953: step 4870, loss 0.0778663, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:07.036520: step 4871, loss 0.0635696, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:07.224867: step 4872, loss 0.0352744, acc 1, learning_rate 0.0001
2017-10-10T12:00:07.393878: step 4873, loss 0.0819564, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:07.660843: step 4874, loss 0.143864, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:07.899736: step 4875, loss 0.123411, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:08.172949: step 4876, loss 0.0763807, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:08.399405: step 4877, loss 0.0150156, acc 1, learning_rate 0.0001
2017-10-10T12:00:08.624884: step 4878, loss 0.0363696, acc 1, learning_rate 0.0001
2017-10-10T12:00:08.904871: step 4879, loss 0.117487, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:09.143335: step 4880, loss 0.116145, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:09.795912: step 4880, loss 0.200411, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4880

2017-10-10T12:00:11.076512: step 4881, loss 0.0476795, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:11.261187: step 4882, loss 0.217485, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:11.467231: step 4883, loss 0.0819117, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:11.646639: step 4884, loss 0.145875, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:11.845108: step 4885, loss 0.122676, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:12.066374: step 4886, loss 0.0469152, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:12.272615: step 4887, loss 0.103336, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:12.498118: step 4888, loss 0.0417468, acc 1, learning_rate 0.0001
2017-10-10T12:00:12.725005: step 4889, loss 0.169437, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:12.978554: step 4890, loss 0.122091, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:13.222447: step 4891, loss 0.0729462, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:13.464840: step 4892, loss 0.0905757, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:13.718962: step 4893, loss 0.0552191, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:13.968818: step 4894, loss 0.107909, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:14.258089: step 4895, loss 0.0490001, acc 1, learning_rate 0.0001
2017-10-10T12:00:14.492825: step 4896, loss 0.0512308, acc 1, learning_rate 0.0001
2017-10-10T12:00:14.712853: step 4897, loss 0.0767619, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:14.990323: step 4898, loss 0.0201486, acc 1, learning_rate 0.0001
2017-10-10T12:00:15.231446: step 4899, loss 0.136146, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:15.452894: step 4900, loss 0.111541, acc 0.960784, learning_rate 0.0001
2017-10-10T12:00:15.728469: step 4901, loss 0.114832, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:15.973280: step 4902, loss 0.0387344, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:16.224909: step 4903, loss 0.173781, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:16.552830: step 4904, loss 0.0517048, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:16.815123: step 4905, loss 0.0471627, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:17.018734: step 4906, loss 0.0464581, acc 1, learning_rate 0.0001
2017-10-10T12:00:17.200746: step 4907, loss 0.138701, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:17.401649: step 4908, loss 0.0381348, acc 1, learning_rate 0.0001
2017-10-10T12:00:17.582037: step 4909, loss 0.142882, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:17.751831: step 4910, loss 0.0569903, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:18.016858: step 4911, loss 0.0784264, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:18.260978: step 4912, loss 0.0591876, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:18.506798: step 4913, loss 0.081702, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:18.795521: step 4914, loss 0.0742645, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:19.044853: step 4915, loss 0.061419, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:19.244875: step 4916, loss 0.154576, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:19.472988: step 4917, loss 0.0439154, acc 1, learning_rate 0.0001
2017-10-10T12:00:19.759914: step 4918, loss 0.039781, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:19.986492: step 4919, loss 0.0362652, acc 1, learning_rate 0.0001
2017-10-10T12:00:20.191314: step 4920, loss 0.0886127, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:20.804372: step 4920, loss 0.200097, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4920

2017-10-10T12:00:21.822194: step 4921, loss 0.0779271, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:22.109116: step 4922, loss 0.0930661, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:22.353993: step 4923, loss 0.107329, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:22.615380: step 4924, loss 0.0784083, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:22.871557: step 4925, loss 0.0933519, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:23.112897: step 4926, loss 0.0340308, acc 1, learning_rate 0.0001
2017-10-10T12:00:23.392624: step 4927, loss 0.0499432, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:23.643191: step 4928, loss 0.0504522, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:23.886397: step 4929, loss 0.146974, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:24.151078: step 4930, loss 0.0907613, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:24.403177: step 4931, loss 0.127689, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:24.644870: step 4932, loss 0.0662609, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:24.894174: step 4933, loss 0.0351705, acc 1, learning_rate 0.0001
2017-10-10T12:00:25.142154: step 4934, loss 0.105599, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:25.396234: step 4935, loss 0.121219, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:25.630930: step 4936, loss 0.0483885, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:25.859810: step 4937, loss 0.0542771, acc 1, learning_rate 0.0001
2017-10-10T12:00:26.112869: step 4938, loss 0.101321, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:26.356346: step 4939, loss 0.127588, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:26.598508: step 4940, loss 0.0452612, acc 1, learning_rate 0.0001
2017-10-10T12:00:26.879294: step 4941, loss 0.139329, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:27.132918: step 4942, loss 0.0945946, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:27.391366: step 4943, loss 0.124791, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:27.676845: step 4944, loss 0.150615, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:28.036765: step 4945, loss 0.150721, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:28.205631: step 4946, loss 0.0913917, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:28.349488: step 4947, loss 0.0848497, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:28.483047: step 4948, loss 0.0960384, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:28.621975: step 4949, loss 0.0715124, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:28.758786: step 4950, loss 0.101219, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:28.960834: step 4951, loss 0.153239, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:29.204857: step 4952, loss 0.102474, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:29.461089: step 4953, loss 0.0743066, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:29.688829: step 4954, loss 0.0504264, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:29.905541: step 4955, loss 0.0625368, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:30.195357: step 4956, loss 0.0358987, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:30.508542: step 4957, loss 0.0152016, acc 1, learning_rate 0.0001
2017-10-10T12:00:30.743951: step 4958, loss 0.0783116, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:30.980844: step 4959, loss 0.0857253, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:31.218283: step 4960, loss 0.0982158, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:31.880805: step 4960, loss 0.19806, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-4960

2017-10-10T12:00:32.999218: step 4961, loss 0.0611716, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:33.267118: step 4962, loss 0.0906925, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:33.505063: step 4963, loss 0.134292, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:33.744883: step 4964, loss 0.0357911, acc 1, learning_rate 0.0001
2017-10-10T12:00:33.971488: step 4965, loss 0.121606, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:34.216236: step 4966, loss 0.0723675, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:34.451976: step 4967, loss 0.0809097, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:34.680957: step 4968, loss 0.0545317, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:34.939946: step 4969, loss 0.055031, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:35.156917: step 4970, loss 0.160788, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:35.375585: step 4971, loss 0.107157, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:35.610651: step 4972, loss 0.0505221, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:35.852489: step 4973, loss 0.117836, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:36.099624: step 4974, loss 0.22689, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:36.398120: step 4975, loss 0.0469956, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:36.634276: step 4976, loss 0.0409578, acc 1, learning_rate 0.0001
2017-10-10T12:00:36.835172: step 4977, loss 0.0968301, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:37.049520: step 4978, loss 0.0408003, acc 1, learning_rate 0.0001
2017-10-10T12:00:37.267902: step 4979, loss 0.211246, acc 0.90625, learning_rate 0.0001
2017-10-10T12:00:37.486130: step 4980, loss 0.0596278, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:37.749583: step 4981, loss 0.0815791, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:37.990674: step 4982, loss 0.0850186, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:38.308891: step 4983, loss 0.0951483, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:38.575535: step 4984, loss 0.0718378, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:38.752892: step 4985, loss 0.0962136, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:38.946780: step 4986, loss 0.0443425, acc 1, learning_rate 0.0001
2017-10-10T12:00:39.139797: step 4987, loss 0.0375674, acc 1, learning_rate 0.0001
2017-10-10T12:00:39.321630: step 4988, loss 0.0572549, acc 1, learning_rate 0.0001
2017-10-10T12:00:39.488842: step 4989, loss 0.0690146, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:39.752522: step 4990, loss 0.103384, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:39.994922: step 4991, loss 0.119909, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:40.270039: step 4992, loss 0.0926346, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:40.486448: step 4993, loss 0.0313262, acc 1, learning_rate 0.0001
2017-10-10T12:00:40.708873: step 4994, loss 0.0682281, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:40.980922: step 4995, loss 0.135278, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:41.201229: step 4996, loss 0.127834, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:41.420903: step 4997, loss 0.100388, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:41.641086: step 4998, loss 0.108561, acc 0.960784, learning_rate 0.0001
2017-10-10T12:00:41.912821: step 4999, loss 0.110869, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:42.147989: step 5000, loss 0.0833569, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:42.808529: step 5000, loss 0.199911, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5000

2017-10-10T12:00:44.103480: step 5001, loss 0.147003, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:44.359614: step 5002, loss 0.0332155, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:44.589471: step 5003, loss 0.0328569, acc 1, learning_rate 0.0001
2017-10-10T12:00:44.876855: step 5004, loss 0.111198, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:45.120863: step 5005, loss 0.0759744, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:45.333638: step 5006, loss 0.0499196, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:45.522938: step 5007, loss 0.0945275, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:45.732230: step 5008, loss 0.0924285, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:45.945252: step 5009, loss 0.097368, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:46.197773: step 5010, loss 0.0813875, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:46.418133: step 5011, loss 0.0305597, acc 1, learning_rate 0.0001
2017-10-10T12:00:46.662681: step 5012, loss 0.0321394, acc 1, learning_rate 0.0001
2017-10-10T12:00:46.905519: step 5013, loss 0.0900437, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:47.148843: step 5014, loss 0.0584637, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:47.391315: step 5015, loss 0.0420664, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:47.636314: step 5016, loss 0.0888586, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:47.881320: step 5017, loss 0.122775, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:48.148831: step 5018, loss 0.119422, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:48.396832: step 5019, loss 0.106952, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:48.732862: step 5020, loss 0.11274, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:49.017802: step 5021, loss 0.126877, acc 0.921875, learning_rate 0.0001
2017-10-10T12:00:49.187155: step 5022, loss 0.0469261, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:49.371479: step 5023, loss 0.0339119, acc 1, learning_rate 0.0001
2017-10-10T12:00:49.558003: step 5024, loss 0.0581063, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:49.749177: step 5025, loss 0.136015, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:49.962119: step 5026, loss 0.104707, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:50.204241: step 5027, loss 0.13986, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:50.441779: step 5028, loss 0.0699348, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:50.679108: step 5029, loss 0.0890328, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:50.981412: step 5030, loss 0.107488, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:51.236850: step 5031, loss 0.121713, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:51.505515: step 5032, loss 0.0605637, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:51.760617: step 5033, loss 0.0976024, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:52.008481: step 5034, loss 0.106732, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:52.264850: step 5035, loss 0.0625495, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:52.540148: step 5036, loss 0.0822417, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:52.791037: step 5037, loss 0.0328758, acc 1, learning_rate 0.0001
2017-10-10T12:00:53.086929: step 5038, loss 0.101012, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:53.368596: step 5039, loss 0.0915346, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:53.568501: step 5040, loss 0.0692474, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:00:54.185947: step 5040, loss 0.198072, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5040

2017-10-10T12:00:55.223920: step 5041, loss 0.0678413, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:55.472961: step 5042, loss 0.0930405, acc 0.953125, learning_rate 0.0001
2017-10-10T12:00:55.700857: step 5043, loss 0.0420713, acc 1, learning_rate 0.0001
2017-10-10T12:00:55.937166: step 5044, loss 0.0988457, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:56.185042: step 5045, loss 0.0411076, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:56.458450: step 5046, loss 0.0431545, acc 1, learning_rate 0.0001
2017-10-10T12:00:56.693751: step 5047, loss 0.126577, acc 0.9375, learning_rate 0.0001
2017-10-10T12:00:56.968911: step 5048, loss 0.0776175, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:57.227117: step 5049, loss 0.0416322, acc 1, learning_rate 0.0001
2017-10-10T12:00:57.474843: step 5050, loss 0.100069, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:57.681025: step 5051, loss 0.0900792, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:57.933012: step 5052, loss 0.0840886, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:58.164260: step 5053, loss 0.0870203, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:58.383870: step 5054, loss 0.144462, acc 0.96875, learning_rate 0.0001
2017-10-10T12:00:58.616599: step 5055, loss 0.0654122, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:58.885004: step 5056, loss 0.0514449, acc 1, learning_rate 0.0001
2017-10-10T12:00:59.075800: step 5057, loss 0.0590994, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:59.364823: step 5058, loss 0.0686532, acc 0.984375, learning_rate 0.0001
2017-10-10T12:00:59.663254: step 5059, loss 0.0405788, acc 1, learning_rate 0.0001
2017-10-10T12:00:59.843338: step 5060, loss 0.0820888, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:00.028959: step 5061, loss 0.0941612, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:00.216850: step 5062, loss 0.0524366, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:00.408815: step 5063, loss 0.109901, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:00.595024: step 5064, loss 0.116253, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:00.859861: step 5065, loss 0.0716205, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:01.091450: step 5066, loss 0.183803, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:01.396864: step 5067, loss 0.0338667, acc 1, learning_rate 0.0001
2017-10-10T12:01:01.649799: step 5068, loss 0.0527447, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:01.853403: step 5069, loss 0.0314855, acc 1, learning_rate 0.0001
2017-10-10T12:01:02.063601: step 5070, loss 0.0757023, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:02.256850: step 5071, loss 0.129029, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:02.458254: step 5072, loss 0.0337978, acc 1, learning_rate 0.0001
2017-10-10T12:01:02.647107: step 5073, loss 0.0583615, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:02.912048: step 5074, loss 0.0606745, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:03.200136: step 5075, loss 0.145267, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:03.448880: step 5076, loss 0.0736772, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:03.736322: step 5077, loss 0.0631856, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:03.976910: step 5078, loss 0.0684407, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:04.189130: step 5079, loss 0.0951339, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:04.450764: step 5080, loss 0.129991, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:05.159604: step 5080, loss 0.201907, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5080

2017-10-10T12:01:06.314538: step 5081, loss 0.065267, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:06.594027: step 5082, loss 0.0356664, acc 1, learning_rate 0.0001
2017-10-10T12:01:06.820334: step 5083, loss 0.121442, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:07.064387: step 5084, loss 0.0739744, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:07.309116: step 5085, loss 0.0998593, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:07.540840: step 5086, loss 0.0730314, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:07.789021: step 5087, loss 0.0836923, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:08.035409: step 5088, loss 0.0380055, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:08.264492: step 5089, loss 0.101084, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:08.482276: step 5090, loss 0.161436, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:08.737305: step 5091, loss 0.0295927, acc 1, learning_rate 0.0001
2017-10-10T12:01:08.992879: step 5092, loss 0.0830657, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:09.214655: step 5093, loss 0.119265, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:09.488820: step 5094, loss 0.0906783, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:09.706528: step 5095, loss 0.0725505, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:10.032929: step 5096, loss 0.155208, acc 0.941176, learning_rate 0.0001
2017-10-10T12:01:10.302122: step 5097, loss 0.0759027, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:10.444466: step 5098, loss 0.0977794, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:10.577723: step 5099, loss 0.0668902, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:10.708504: step 5100, loss 0.0932431, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:10.847054: step 5101, loss 0.0301406, acc 1, learning_rate 0.0001
2017-10-10T12:01:10.981652: step 5102, loss 0.0993745, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:11.225659: step 5103, loss 0.0983125, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:11.476851: step 5104, loss 0.0560402, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:11.741034: step 5105, loss 0.0562737, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:12.021297: step 5106, loss 0.0742813, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:12.270064: step 5107, loss 0.0803615, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:12.536677: step 5108, loss 0.0349416, acc 1, learning_rate 0.0001
2017-10-10T12:01:12.777193: step 5109, loss 0.0448967, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:13.028850: step 5110, loss 0.118698, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:13.296850: step 5111, loss 0.159953, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:13.551271: step 5112, loss 0.0365772, acc 1, learning_rate 0.0001
2017-10-10T12:01:13.788046: step 5113, loss 0.183424, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:14.000296: step 5114, loss 0.162203, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:14.240846: step 5115, loss 0.049198, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:14.512203: step 5116, loss 0.06147, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:14.768258: step 5117, loss 0.0469728, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:15.004773: step 5118, loss 0.0827429, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:15.241130: step 5119, loss 0.177163, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:15.481291: step 5120, loss 0.0552597, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:16.178938: step 5120, loss 0.20033, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5120

2017-10-10T12:01:17.308789: step 5121, loss 0.0346558, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:17.571794: step 5122, loss 0.0643296, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:17.806159: step 5123, loss 0.0617314, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:18.038179: step 5124, loss 0.0446796, acc 1, learning_rate 0.0001
2017-10-10T12:01:18.347299: step 5125, loss 0.126746, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:18.567679: step 5126, loss 0.137343, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:18.791195: step 5127, loss 0.0727856, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:18.982005: step 5128, loss 0.0744843, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:19.171530: step 5129, loss 0.0866195, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:19.363078: step 5130, loss 0.0446139, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:19.593155: step 5131, loss 0.064085, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:19.813767: step 5132, loss 0.104245, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:20.081967: step 5133, loss 0.0674014, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:20.340307: step 5134, loss 0.072872, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:20.620881: step 5135, loss 0.109591, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:20.840396: step 5136, loss 0.113531, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:21.034210: step 5137, loss 0.0368531, acc 1, learning_rate 0.0001
2017-10-10T12:01:21.214863: step 5138, loss 0.133564, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:21.392865: step 5139, loss 0.0541072, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:21.580242: step 5140, loss 0.0201241, acc 1, learning_rate 0.0001
2017-10-10T12:01:21.773706: step 5141, loss 0.042276, acc 1, learning_rate 0.0001
2017-10-10T12:01:22.048840: step 5142, loss 0.110498, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:22.268211: step 5143, loss 0.0948192, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:22.501104: step 5144, loss 0.0570931, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:22.756890: step 5145, loss 0.0408797, acc 1, learning_rate 0.0001
2017-10-10T12:01:23.001923: step 5146, loss 0.104086, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:23.236253: step 5147, loss 0.0826123, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:23.488197: step 5148, loss 0.0529631, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:23.755318: step 5149, loss 0.0615287, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:23.997711: step 5150, loss 0.0489747, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:24.258829: step 5151, loss 0.0746248, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:24.502864: step 5152, loss 0.0842699, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:24.716086: step 5153, loss 0.0949828, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:24.909732: step 5154, loss 0.0501092, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:25.156946: step 5155, loss 0.0896291, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:25.393227: step 5156, loss 0.0593823, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:25.633834: step 5157, loss 0.0381388, acc 1, learning_rate 0.0001
2017-10-10T12:01:25.925421: step 5158, loss 0.0525727, acc 1, learning_rate 0.0001
2017-10-10T12:01:26.159417: step 5159, loss 0.0530935, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:26.422272: step 5160, loss 0.0347644, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:27.199622: step 5160, loss 0.201849, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5160

2017-10-10T12:01:28.871596: step 5161, loss 0.0742012, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:29.108902: step 5162, loss 0.0461622, acc 1, learning_rate 0.0001
2017-10-10T12:01:29.357456: step 5163, loss 0.036005, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:29.601166: step 5164, loss 0.038662, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:29.874962: step 5165, loss 0.037548, acc 1, learning_rate 0.0001
2017-10-10T12:01:30.132982: step 5166, loss 0.0318132, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:30.351625: step 5167, loss 0.0426716, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:30.599248: step 5168, loss 0.0856211, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:30.874868: step 5169, loss 0.0606216, acc 1, learning_rate 0.0001
2017-10-10T12:01:31.188876: step 5170, loss 0.147132, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:31.484580: step 5171, loss 0.0878537, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:31.667920: step 5172, loss 0.0327342, acc 1, learning_rate 0.0001
2017-10-10T12:01:31.870409: step 5173, loss 0.0983225, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:32.048084: step 5174, loss 0.0784887, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:32.232830: step 5175, loss 0.151114, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:32.412187: step 5176, loss 0.0616306, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:32.609449: step 5177, loss 0.125534, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:32.845641: step 5178, loss 0.0330006, acc 1, learning_rate 0.0001
2017-10-10T12:01:33.085263: step 5179, loss 0.0886639, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:33.352801: step 5180, loss 0.0852009, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:33.591472: step 5181, loss 0.0841466, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:33.848868: step 5182, loss 0.04857, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:34.123820: step 5183, loss 0.0947122, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:34.424370: step 5184, loss 0.119523, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:34.663902: step 5185, loss 0.0364157, acc 1, learning_rate 0.0001
2017-10-10T12:01:34.890553: step 5186, loss 0.0623218, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:35.144914: step 5187, loss 0.103334, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:35.368849: step 5188, loss 0.0436001, acc 1, learning_rate 0.0001
2017-10-10T12:01:35.600902: step 5189, loss 0.0559305, acc 1, learning_rate 0.0001
2017-10-10T12:01:35.953105: step 5190, loss 0.108442, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:36.161746: step 5191, loss 0.059323, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:36.376860: step 5192, loss 0.0686409, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:36.587653: step 5193, loss 0.0584166, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:36.753720: step 5194, loss 0.0922868, acc 0.980392, learning_rate 0.0001
2017-10-10T12:01:36.971546: step 5195, loss 0.0897847, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:37.169774: step 5196, loss 0.190609, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:37.416228: step 5197, loss 0.11491, acc 0.921875, learning_rate 0.0001
2017-10-10T12:01:37.663889: step 5198, loss 0.0869487, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:37.910137: step 5199, loss 0.127629, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:38.201076: step 5200, loss 0.0706836, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:38.934712: step 5200, loss 0.201286, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5200

2017-10-10T12:01:39.948956: step 5201, loss 0.0219892, acc 1, learning_rate 0.0001
2017-10-10T12:01:40.219506: step 5202, loss 0.104487, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:40.450418: step 5203, loss 0.0664803, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:40.717304: step 5204, loss 0.0991919, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:40.946711: step 5205, loss 0.0651491, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:41.225876: step 5206, loss 0.075531, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:41.467582: step 5207, loss 0.0378795, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:41.776897: step 5208, loss 0.0205682, acc 1, learning_rate 0.0001
2017-10-10T12:01:42.053283: step 5209, loss 0.138602, acc 0.9375, learning_rate 0.0001
2017-10-10T12:01:42.248113: step 5210, loss 0.0386207, acc 1, learning_rate 0.0001
2017-10-10T12:01:42.421881: step 5211, loss 0.118786, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:42.589314: step 5212, loss 0.0654823, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:42.775957: step 5213, loss 0.0930561, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:42.977121: step 5214, loss 0.126156, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:43.199231: step 5215, loss 0.143925, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:43.448997: step 5216, loss 0.116548, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:43.713179: step 5217, loss 0.0802445, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:43.942684: step 5218, loss 0.0625939, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:44.216156: step 5219, loss 0.0437576, acc 1, learning_rate 0.0001
2017-10-10T12:01:44.549337: step 5220, loss 0.102102, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:44.792207: step 5221, loss 0.0965888, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:44.992878: step 5222, loss 0.0607583, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:45.186756: step 5223, loss 0.0793127, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:45.416842: step 5224, loss 0.0623044, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:45.633996: step 5225, loss 0.101362, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:45.875697: step 5226, loss 0.099972, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:46.135150: step 5227, loss 0.128577, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:46.383985: step 5228, loss 0.073384, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:46.611517: step 5229, loss 0.0475458, acc 1, learning_rate 0.0001
2017-10-10T12:01:46.871334: step 5230, loss 0.0276847, acc 1, learning_rate 0.0001
2017-10-10T12:01:47.115831: step 5231, loss 0.0940074, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:47.348804: step 5232, loss 0.0588067, acc 1, learning_rate 0.0001
2017-10-10T12:01:47.620308: step 5233, loss 0.0460864, acc 1, learning_rate 0.0001
2017-10-10T12:01:47.862506: step 5234, loss 0.0787892, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:48.117091: step 5235, loss 0.104136, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:48.372844: step 5236, loss 0.108358, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:48.610594: step 5237, loss 0.032327, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:48.860881: step 5238, loss 0.0473739, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:49.106666: step 5239, loss 0.0884185, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:49.356679: step 5240, loss 0.0860767, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:01:50.044084: step 5240, loss 0.19802, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5240

2017-10-10T12:01:51.252872: step 5241, loss 0.0865319, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:51.492167: step 5242, loss 0.124698, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:51.748840: step 5243, loss 0.0825184, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:52.104849: step 5244, loss 0.0395753, acc 1, learning_rate 0.0001
2017-10-10T12:01:52.343190: step 5245, loss 0.0818912, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:52.529804: step 5246, loss 0.116726, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:52.717615: step 5247, loss 0.0658816, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:52.948841: step 5248, loss 0.0654317, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:53.176847: step 5249, loss 0.0423464, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:53.433690: step 5250, loss 0.0894895, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:53.625185: step 5251, loss 0.0659511, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:53.834718: step 5252, loss 0.0686426, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:54.057329: step 5253, loss 0.0305102, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:54.319777: step 5254, loss 0.0663743, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:54.568718: step 5255, loss 0.0800319, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:54.812869: step 5256, loss 0.131953, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:55.065191: step 5257, loss 0.0624465, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:55.320162: step 5258, loss 0.0605581, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:55.558653: step 5259, loss 0.039567, acc 1, learning_rate 0.0001
2017-10-10T12:01:55.828890: step 5260, loss 0.0345825, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:56.076328: step 5261, loss 0.0444686, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:56.297086: step 5262, loss 0.0921135, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:56.574416: step 5263, loss 0.0795488, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:56.779960: step 5264, loss 0.0676581, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:57.038806: step 5265, loss 0.0948282, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:57.283407: step 5266, loss 0.042618, acc 1, learning_rate 0.0001
2017-10-10T12:01:57.559703: step 5267, loss 0.0532637, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:57.876578: step 5268, loss 0.0675354, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:58.138596: step 5269, loss 0.0989548, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:58.376841: step 5270, loss 0.146223, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:58.614097: step 5271, loss 0.0837881, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:58.886466: step 5272, loss 0.113579, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:59.149120: step 5273, loss 0.0949987, acc 0.96875, learning_rate 0.0001
2017-10-10T12:01:59.452844: step 5274, loss 0.0970191, acc 0.984375, learning_rate 0.0001
2017-10-10T12:01:59.674127: step 5275, loss 0.156199, acc 0.953125, learning_rate 0.0001
2017-10-10T12:01:59.937061: step 5276, loss 0.109474, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:00.202642: step 5277, loss 0.114992, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:00.428783: step 5278, loss 0.0663256, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:00.682248: step 5279, loss 0.0667161, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:00.950214: step 5280, loss 0.125348, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:01.607503: step 5280, loss 0.20346, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5280

2017-10-10T12:02:02.766456: step 5281, loss 0.0875611, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:02.914333: step 5282, loss 0.0702386, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:03.054409: step 5283, loss 0.0299374, acc 1, learning_rate 0.0001
2017-10-10T12:02:03.263668: step 5284, loss 0.0562985, acc 1, learning_rate 0.0001
2017-10-10T12:02:03.451446: step 5285, loss 0.0519105, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:03.687584: step 5286, loss 0.0855856, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:03.947839: step 5287, loss 0.117641, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:04.190586: step 5288, loss 0.0791612, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:04.429086: step 5289, loss 0.0818859, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:04.682114: step 5290, loss 0.03132, acc 1, learning_rate 0.0001
2017-10-10T12:02:04.920886: step 5291, loss 0.033506, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:05.167401: step 5292, loss 0.0664627, acc 1, learning_rate 0.0001
2017-10-10T12:02:05.403908: step 5293, loss 0.242836, acc 0.90625, learning_rate 0.0001
2017-10-10T12:02:05.639158: step 5294, loss 0.0246653, acc 1, learning_rate 0.0001
2017-10-10T12:02:05.900994: step 5295, loss 0.0354654, acc 1, learning_rate 0.0001
2017-10-10T12:02:06.158481: step 5296, loss 0.0410504, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:06.430175: step 5297, loss 0.0321278, acc 1, learning_rate 0.0001
2017-10-10T12:02:06.687486: step 5298, loss 0.102651, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:06.949562: step 5299, loss 0.0533727, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:07.181576: step 5300, loss 0.0690754, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:07.422238: step 5301, loss 0.0527956, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:07.675440: step 5302, loss 0.103607, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:07.908354: step 5303, loss 0.129903, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:08.132941: step 5304, loss 0.103875, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:08.365739: step 5305, loss 0.0473838, acc 1, learning_rate 0.0001
2017-10-10T12:02:08.600864: step 5306, loss 0.0292928, acc 1, learning_rate 0.0001
2017-10-10T12:02:08.848817: step 5307, loss 0.125881, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:09.098730: step 5308, loss 0.030273, acc 1, learning_rate 0.0001
2017-10-10T12:02:09.297497: step 5309, loss 0.0746522, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:09.573908: step 5310, loss 0.0425018, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:09.810336: step 5311, loss 0.116443, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:10.096599: step 5312, loss 0.0160739, acc 1, learning_rate 0.0001
2017-10-10T12:02:10.344891: step 5313, loss 0.0837719, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:10.651219: step 5314, loss 0.0400885, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:10.866576: step 5315, loss 0.0348148, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:11.054176: step 5316, loss 0.0311962, acc 1, learning_rate 0.0001
2017-10-10T12:02:11.251847: step 5317, loss 0.105539, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:11.446527: step 5318, loss 0.0888223, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:11.706454: step 5319, loss 0.074554, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:11.984602: step 5320, loss 0.0384959, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:12.675631: step 5320, loss 0.198882, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5320

2017-10-10T12:02:13.653402: step 5321, loss 0.0586998, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:13.834299: step 5322, loss 0.129313, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:14.022542: step 5323, loss 0.0890853, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:14.216851: step 5324, loss 0.082988, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:14.471146: step 5325, loss 0.0448267, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:14.685993: step 5326, loss 0.054611, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:14.917768: step 5327, loss 0.141383, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:15.174060: step 5328, loss 0.038249, acc 1, learning_rate 0.0001
2017-10-10T12:02:15.447372: step 5329, loss 0.0836151, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:15.704876: step 5330, loss 0.0865363, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:15.894207: step 5331, loss 0.0272373, acc 1, learning_rate 0.0001
2017-10-10T12:02:16.119882: step 5332, loss 0.0128786, acc 1, learning_rate 0.0001
2017-10-10T12:02:16.345196: step 5333, loss 0.101828, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:16.622660: step 5334, loss 0.0318302, acc 1, learning_rate 0.0001
2017-10-10T12:02:16.880543: step 5335, loss 0.0810182, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:17.145007: step 5336, loss 0.0548099, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:17.380986: step 5337, loss 0.158485, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:17.620859: step 5338, loss 0.055845, acc 1, learning_rate 0.0001
2017-10-10T12:02:17.873854: step 5339, loss 0.0658912, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:18.101952: step 5340, loss 0.130353, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:18.332669: step 5341, loss 0.0803343, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:18.539013: step 5342, loss 0.0857785, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:18.869029: step 5343, loss 0.157359, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:19.077111: step 5344, loss 0.0790363, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:19.288837: step 5345, loss 0.108378, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:19.507911: step 5346, loss 0.139643, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:19.712406: step 5347, loss 0.0541683, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:19.912370: step 5348, loss 0.0460115, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:20.169247: step 5349, loss 0.0784993, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:20.436565: step 5350, loss 0.124723, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:20.672016: step 5351, loss 0.159285, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:20.936655: step 5352, loss 0.10677, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:21.185377: step 5353, loss 0.0526561, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:21.416866: step 5354, loss 0.0284226, acc 1, learning_rate 0.0001
2017-10-10T12:02:21.662639: step 5355, loss 0.0901185, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:21.934209: step 5356, loss 0.101041, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:22.185911: step 5357, loss 0.0713333, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:22.417676: step 5358, loss 0.101629, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:22.685260: step 5359, loss 0.058641, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:22.915524: step 5360, loss 0.104903, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:23.552898: step 5360, loss 0.198657, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5360

2017-10-10T12:02:24.588855: step 5361, loss 0.0597349, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:24.845924: step 5362, loss 0.0636914, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:25.104792: step 5363, loss 0.110757, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:25.378358: step 5364, loss 0.0526496, acc 1, learning_rate 0.0001
2017-10-10T12:02:25.608383: step 5365, loss 0.0818955, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:25.852855: step 5366, loss 0.0458926, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:26.123586: step 5367, loss 0.043531, acc 1, learning_rate 0.0001
2017-10-10T12:02:26.374657: step 5368, loss 0.0858504, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:26.640834: step 5369, loss 0.118219, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:26.928073: step 5370, loss 0.160252, acc 0.890625, learning_rate 0.0001
2017-10-10T12:02:27.201173: step 5371, loss 0.0311556, acc 1, learning_rate 0.0001
2017-10-10T12:02:27.380711: step 5372, loss 0.113549, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:27.579941: step 5373, loss 0.119083, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:27.770933: step 5374, loss 0.0912615, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:27.963163: step 5375, loss 0.0356215, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:28.156456: step 5376, loss 0.0775796, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:28.345457: step 5377, loss 0.0350959, acc 1, learning_rate 0.0001
2017-10-10T12:02:28.623231: step 5378, loss 0.0759904, acc 1, learning_rate 0.0001
2017-10-10T12:02:28.853463: step 5379, loss 0.0507899, acc 1, learning_rate 0.0001
2017-10-10T12:02:29.115933: step 5380, loss 0.0710475, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:29.331292: step 5381, loss 0.129561, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:29.534632: step 5382, loss 0.123223, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:29.760914: step 5383, loss 0.0617078, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:30.036880: step 5384, loss 0.0316266, acc 1, learning_rate 0.0001
2017-10-10T12:02:30.288869: step 5385, loss 0.0771383, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:30.489171: step 5386, loss 0.0857469, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:30.745197: step 5387, loss 0.11609, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:30.967738: step 5388, loss 0.0444477, acc 1, learning_rate 0.0001
2017-10-10T12:02:31.219152: step 5389, loss 0.0383782, acc 1, learning_rate 0.0001
2017-10-10T12:02:31.377045: step 5390, loss 0.0611096, acc 0.980392, learning_rate 0.0001
2017-10-10T12:02:31.640863: step 5391, loss 0.0333376, acc 1, learning_rate 0.0001
2017-10-10T12:02:31.899429: step 5392, loss 0.0742886, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:32.142006: step 5393, loss 0.0452777, acc 1, learning_rate 0.0001
2017-10-10T12:02:32.393388: step 5394, loss 0.0691423, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:32.608429: step 5395, loss 0.0783013, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:32.855886: step 5396, loss 0.0267813, acc 1, learning_rate 0.0001
2017-10-10T12:02:33.087716: step 5397, loss 0.0624829, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:33.357361: step 5398, loss 0.132691, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:33.615239: step 5399, loss 0.0843176, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:33.854474: step 5400, loss 0.0742697, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:34.552828: step 5400, loss 0.202906, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5400

2017-10-10T12:02:35.856380: step 5401, loss 0.138012, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:36.045629: step 5402, loss 0.0584164, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:36.295850: step 5403, loss 0.0759493, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:36.550810: step 5404, loss 0.0742767, acc 1, learning_rate 0.0001
2017-10-10T12:02:36.812699: step 5405, loss 0.0430459, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:37.079868: step 5406, loss 0.029222, acc 1, learning_rate 0.0001
2017-10-10T12:02:37.338552: step 5407, loss 0.0303029, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:37.559471: step 5408, loss 0.0448164, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:37.780855: step 5409, loss 0.0520203, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:38.049296: step 5410, loss 0.131693, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:38.335572: step 5411, loss 0.165271, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:38.576458: step 5412, loss 0.0807607, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:38.816816: step 5413, loss 0.0958906, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:39.069340: step 5414, loss 0.171051, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:39.316701: step 5415, loss 0.053997, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:39.555352: step 5416, loss 0.0371891, acc 1, learning_rate 0.0001
2017-10-10T12:02:39.804804: step 5417, loss 0.0401734, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:40.089878: step 5418, loss 0.106336, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:40.326460: step 5419, loss 0.0503945, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:40.612846: step 5420, loss 0.0923231, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:40.850570: step 5421, loss 0.0433044, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:41.072969: step 5422, loss 0.0754188, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:41.322270: step 5423, loss 0.14627, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:41.542778: step 5424, loss 0.0446808, acc 1, learning_rate 0.0001
2017-10-10T12:02:41.772924: step 5425, loss 0.126361, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:42.037069: step 5426, loss 0.0373176, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:42.277415: step 5427, loss 0.0489371, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:42.508854: step 5428, loss 0.18993, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:42.739061: step 5429, loss 0.0653838, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:43.017501: step 5430, loss 0.0273257, acc 1, learning_rate 0.0001
2017-10-10T12:02:43.260858: step 5431, loss 0.111918, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:43.577257: step 5432, loss 0.0659472, acc 1, learning_rate 0.0001
2017-10-10T12:02:43.773734: step 5433, loss 0.152413, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:43.975054: step 5434, loss 0.0513286, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:44.172871: step 5435, loss 0.0322101, acc 1, learning_rate 0.0001
2017-10-10T12:02:44.366065: step 5436, loss 0.0905264, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:44.572960: step 5437, loss 0.0471585, acc 1, learning_rate 0.0001
2017-10-10T12:02:44.834513: step 5438, loss 0.0528624, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:45.142017: step 5439, loss 0.0526317, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:45.423698: step 5440, loss 0.110806, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:45.962218: step 5440, loss 0.198222, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5440

2017-10-10T12:02:46.920946: step 5441, loss 0.0329979, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:47.150374: step 5442, loss 0.0211266, acc 1, learning_rate 0.0001
2017-10-10T12:02:47.384937: step 5443, loss 0.070222, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:47.640842: step 5444, loss 0.0990712, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:47.897004: step 5445, loss 0.0683636, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:48.157335: step 5446, loss 0.0461942, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:48.396616: step 5447, loss 0.0480124, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:48.634253: step 5448, loss 0.0645091, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:48.887446: step 5449, loss 0.0229673, acc 1, learning_rate 0.0001
2017-10-10T12:02:49.156908: step 5450, loss 0.0954711, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:49.417002: step 5451, loss 0.0556503, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:49.668928: step 5452, loss 0.0658173, acc 1, learning_rate 0.0001
2017-10-10T12:02:49.905165: step 5453, loss 0.0599048, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:50.134932: step 5454, loss 0.117547, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:50.392945: step 5455, loss 0.0521043, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:50.646953: step 5456, loss 0.0899101, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:50.864357: step 5457, loss 0.157522, acc 0.9375, learning_rate 0.0001
2017-10-10T12:02:51.087494: step 5458, loss 0.118481, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:51.292864: step 5459, loss 0.0508878, acc 1, learning_rate 0.0001
2017-10-10T12:02:51.608877: step 5460, loss 0.10789, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:51.851541: step 5461, loss 0.113301, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:52.069340: step 5462, loss 0.0622346, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:52.245626: step 5463, loss 0.0821853, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:52.449118: step 5464, loss 0.0302552, acc 1, learning_rate 0.0001
2017-10-10T12:02:52.656876: step 5465, loss 0.0747838, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:52.868569: step 5466, loss 0.0517254, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:53.122199: step 5467, loss 0.0637572, acc 1, learning_rate 0.0001
2017-10-10T12:02:53.336581: step 5468, loss 0.0446331, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:53.564728: step 5469, loss 0.147442, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:53.767590: step 5470, loss 0.0795864, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:53.973413: step 5471, loss 0.10772, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:54.236068: step 5472, loss 0.0468084, acc 1, learning_rate 0.0001
2017-10-10T12:02:54.439439: step 5473, loss 0.175812, acc 0.921875, learning_rate 0.0001
2017-10-10T12:02:54.694147: step 5474, loss 0.032841, acc 1, learning_rate 0.0001
2017-10-10T12:02:54.920866: step 5475, loss 0.0943548, acc 0.953125, learning_rate 0.0001
2017-10-10T12:02:55.137535: step 5476, loss 0.0760546, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:55.360913: step 5477, loss 0.040808, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:55.592917: step 5478, loss 0.0625443, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:55.917046: step 5479, loss 0.055936, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:56.180117: step 5480, loss 0.113479, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:02:56.736872: step 5480, loss 0.196813, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5480

2017-10-10T12:02:57.695207: step 5481, loss 0.0731247, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:57.942589: step 5482, loss 0.0791054, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:58.188420: step 5483, loss 0.067736, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:58.408652: step 5484, loss 0.0932434, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:58.677880: step 5485, loss 0.0474893, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:58.921912: step 5486, loss 0.0667847, acc 0.96875, learning_rate 0.0001
2017-10-10T12:02:59.188860: step 5487, loss 0.0589573, acc 0.984375, learning_rate 0.0001
2017-10-10T12:02:59.442307: step 5488, loss 0.177591, acc 0.941176, learning_rate 0.0001
2017-10-10T12:02:59.705248: step 5489, loss 0.0393063, acc 1, learning_rate 0.0001
2017-10-10T12:03:00.016990: step 5490, loss 0.115511, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:00.255010: step 5491, loss 0.0951399, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:00.483905: step 5492, loss 0.0291146, acc 1, learning_rate 0.0001
2017-10-10T12:03:00.675025: step 5493, loss 0.0827731, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:00.860060: step 5494, loss 0.0853984, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:01.057517: step 5495, loss 0.124902, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:01.281520: step 5496, loss 0.166084, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:01.537892: step 5497, loss 0.034154, acc 1, learning_rate 0.0001
2017-10-10T12:03:01.794210: step 5498, loss 0.224113, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:02.021193: step 5499, loss 0.0986816, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:02.232832: step 5500, loss 0.0815009, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:02.476894: step 5501, loss 0.123599, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:02.737826: step 5502, loss 0.0558055, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:02.969165: step 5503, loss 0.0921773, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:03.239760: step 5504, loss 0.110259, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:03.484843: step 5505, loss 0.0381605, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:03.740868: step 5506, loss 0.15586, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:04.002307: step 5507, loss 0.0797344, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:04.248575: step 5508, loss 0.0593386, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:04.468858: step 5509, loss 0.0734788, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:04.716861: step 5510, loss 0.0674006, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:04.965120: step 5511, loss 0.192194, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:05.224971: step 5512, loss 0.064631, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:05.469548: step 5513, loss 0.049774, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:05.744139: step 5514, loss 0.0429063, acc 1, learning_rate 0.0001
2017-10-10T12:03:05.936891: step 5515, loss 0.126743, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:06.179118: step 5516, loss 0.08248, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:06.484848: step 5517, loss 0.0657433, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:06.745582: step 5518, loss 0.0289813, acc 1, learning_rate 0.0001
2017-10-10T12:03:06.926090: step 5519, loss 0.0882983, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:07.105869: step 5520, loss 0.063152, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:07.695893: step 5520, loss 0.199711, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5520

2017-10-10T12:03:08.824209: step 5521, loss 0.0357719, acc 1, learning_rate 0.0001
2017-10-10T12:03:09.036859: step 5522, loss 0.064615, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:09.210271: step 5523, loss 0.053343, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:09.432844: step 5524, loss 0.0668206, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:09.679109: step 5525, loss 0.0488281, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:09.919915: step 5526, loss 0.0405312, acc 1, learning_rate 0.0001
2017-10-10T12:03:10.208771: step 5527, loss 0.0859198, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:10.455776: step 5528, loss 0.128351, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:10.723024: step 5529, loss 0.0695001, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:10.949022: step 5530, loss 0.0450268, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:11.188868: step 5531, loss 0.044982, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:11.450887: step 5532, loss 0.0599892, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:11.684238: step 5533, loss 0.120942, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:11.908432: step 5534, loss 0.065966, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:12.139579: step 5535, loss 0.142101, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:12.360868: step 5536, loss 0.087085, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:12.593782: step 5537, loss 0.109955, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:12.857186: step 5538, loss 0.123913, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:13.076892: step 5539, loss 0.0800761, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:13.293232: step 5540, loss 0.0537215, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:13.524994: step 5541, loss 0.0281965, acc 1, learning_rate 0.0001
2017-10-10T12:03:13.780978: step 5542, loss 0.0193396, acc 1, learning_rate 0.0001
2017-10-10T12:03:14.037183: step 5543, loss 0.0477477, acc 1, learning_rate 0.0001
2017-10-10T12:03:14.316896: step 5544, loss 0.0738761, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:14.558478: step 5545, loss 0.0959597, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:14.854876: step 5546, loss 0.068757, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:15.097914: step 5547, loss 0.0524454, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:15.326483: step 5548, loss 0.0749814, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:15.582589: step 5549, loss 0.0818214, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:15.824613: step 5550, loss 0.0964368, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:16.064844: step 5551, loss 0.103124, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:16.314822: step 5552, loss 0.0770961, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:16.553634: step 5553, loss 0.067844, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:16.896930: step 5554, loss 0.0486813, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:17.182556: step 5555, loss 0.0689368, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:17.395621: step 5556, loss 0.0589121, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:17.544724: step 5557, loss 0.0326959, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:17.680835: step 5558, loss 0.0612426, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:17.819542: step 5559, loss 0.126772, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:17.966132: step 5560, loss 0.0437967, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:18.517736: step 5560, loss 0.199605, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5560

2017-10-10T12:03:19.877696: step 5561, loss 0.0657822, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:20.129580: step 5562, loss 0.113974, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:20.371844: step 5563, loss 0.0669158, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:20.619184: step 5564, loss 0.0697856, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:20.826254: step 5565, loss 0.0770842, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:21.012577: step 5566, loss 0.0572125, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:21.207643: step 5567, loss 0.0746977, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:21.410051: step 5568, loss 0.0971685, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:21.666939: step 5569, loss 0.0756316, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:21.900815: step 5570, loss 0.0498483, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:22.116299: step 5571, loss 0.119022, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:22.400932: step 5572, loss 0.0882956, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:22.616547: step 5573, loss 0.0609078, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:22.868929: step 5574, loss 0.100052, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:23.116437: step 5575, loss 0.0378378, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:23.347744: step 5576, loss 0.0718712, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:23.584870: step 5577, loss 0.0304743, acc 1, learning_rate 0.0001
2017-10-10T12:03:23.831370: step 5578, loss 0.190346, acc 0.890625, learning_rate 0.0001
2017-10-10T12:03:24.125097: step 5579, loss 0.0729228, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:24.361357: step 5580, loss 0.0757703, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:24.623741: step 5581, loss 0.129719, acc 0.921875, learning_rate 0.0001
2017-10-10T12:03:24.890270: step 5582, loss 0.0679808, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:25.108948: step 5583, loss 0.0761545, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:25.352932: step 5584, loss 0.0807249, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:25.582760: step 5585, loss 0.111895, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:25.865341: step 5586, loss 0.0273119, acc 1, learning_rate 0.0001
2017-10-10T12:03:26.116804: step 5587, loss 0.105624, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:26.316997: step 5588, loss 0.105979, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:26.493582: step 5589, loss 0.0180184, acc 1, learning_rate 0.0001
2017-10-10T12:03:26.675622: step 5590, loss 0.0977837, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:26.868900: step 5591, loss 0.0582041, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:27.104664: step 5592, loss 0.0508108, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:27.432847: step 5593, loss 0.137449, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:27.693562: step 5594, loss 0.0284905, acc 1, learning_rate 0.0001
2017-10-10T12:03:27.872888: step 5595, loss 0.0763375, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:28.066706: step 5596, loss 0.0980483, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:28.262547: step 5597, loss 0.0468499, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:28.444871: step 5598, loss 0.0761919, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:28.721146: step 5599, loss 0.0354935, acc 1, learning_rate 0.0001
2017-10-10T12:03:28.976857: step 5600, loss 0.0970402, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:29.630057: step 5600, loss 0.199389, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5600

2017-10-10T12:03:30.668836: step 5601, loss 0.0584809, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:30.938137: step 5602, loss 0.0843458, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:31.216365: step 5603, loss 0.0838792, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:31.466289: step 5604, loss 0.021812, acc 1, learning_rate 0.0001
2017-10-10T12:03:31.710310: step 5605, loss 0.0881462, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:31.945619: step 5606, loss 0.0882027, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:32.185092: step 5607, loss 0.0765233, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:32.454456: step 5608, loss 0.0599601, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:32.690122: step 5609, loss 0.0459009, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:32.948026: step 5610, loss 0.0707496, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:33.167203: step 5611, loss 0.101111, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:33.387807: step 5612, loss 0.0374324, acc 1, learning_rate 0.0001
2017-10-10T12:03:33.660106: step 5613, loss 0.0831352, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:33.877010: step 5614, loss 0.0850584, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:34.220842: step 5615, loss 0.0363529, acc 1, learning_rate 0.0001
2017-10-10T12:03:34.428880: step 5616, loss 0.0465884, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:34.630987: step 5617, loss 0.0989769, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:34.810752: step 5618, loss 0.105634, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:35.016822: step 5619, loss 0.0840627, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:35.233827: step 5620, loss 0.0473158, acc 1, learning_rate 0.0001
2017-10-10T12:03:35.485389: step 5621, loss 0.122989, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:35.719561: step 5622, loss 0.0506372, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:35.978952: step 5623, loss 0.0654917, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:36.217798: step 5624, loss 0.113047, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:36.502633: step 5625, loss 0.100018, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:36.729289: step 5626, loss 0.0772149, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:36.972551: step 5627, loss 0.0648807, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:37.228096: step 5628, loss 0.0775541, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:37.480076: step 5629, loss 0.141913, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:37.768186: step 5630, loss 0.0254306, acc 1, learning_rate 0.0001
2017-10-10T12:03:38.089086: step 5631, loss 0.0666389, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:38.282843: step 5632, loss 0.0231391, acc 1, learning_rate 0.0001
2017-10-10T12:03:38.470280: step 5633, loss 0.0410185, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:38.655017: step 5634, loss 0.0625534, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:38.823915: step 5635, loss 0.109238, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:39.011267: step 5636, loss 0.021963, acc 1, learning_rate 0.0001
2017-10-10T12:03:39.250031: step 5637, loss 0.0331748, acc 1, learning_rate 0.0001
2017-10-10T12:03:39.541393: step 5638, loss 0.080025, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:39.767443: step 5639, loss 0.0453964, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:40.009780: step 5640, loss 0.0739319, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:40.718096: step 5640, loss 0.196613, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5640

2017-10-10T12:03:41.889359: step 5641, loss 0.128209, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:42.172871: step 5642, loss 0.0775935, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:42.474034: step 5643, loss 0.0400878, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:42.666360: step 5644, loss 0.183737, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:42.867383: step 5645, loss 0.105174, acc 0.9375, learning_rate 0.0001
2017-10-10T12:03:43.062533: step 5646, loss 0.0631001, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:43.276862: step 5647, loss 0.131568, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:43.464836: step 5648, loss 0.0924426, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:43.743269: step 5649, loss 0.082084, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:43.988845: step 5650, loss 0.0847704, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:44.206270: step 5651, loss 0.0349721, acc 1, learning_rate 0.0001
2017-10-10T12:03:44.481640: step 5652, loss 0.0684941, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:44.747134: step 5653, loss 0.1254, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:44.986842: step 5654, loss 0.0928984, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:45.235001: step 5655, loss 0.0571476, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:45.482183: step 5656, loss 0.0479324, acc 1, learning_rate 0.0001
2017-10-10T12:03:45.732905: step 5657, loss 0.052997, acc 1, learning_rate 0.0001
2017-10-10T12:03:45.960344: step 5658, loss 0.0545537, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:46.220819: step 5659, loss 0.0646181, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:46.456150: step 5660, loss 0.0584058, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:46.706401: step 5661, loss 0.0371485, acc 1, learning_rate 0.0001
2017-10-10T12:03:46.963093: step 5662, loss 0.0438908, acc 1, learning_rate 0.0001
2017-10-10T12:03:47.210846: step 5663, loss 0.138177, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:47.465651: step 5664, loss 0.103065, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:47.725114: step 5665, loss 0.0634532, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:47.947675: step 5666, loss 0.0781748, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:48.196856: step 5667, loss 0.0629303, acc 1, learning_rate 0.0001
2017-10-10T12:03:48.500889: step 5668, loss 0.0444003, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:48.750951: step 5669, loss 0.0534425, acc 1, learning_rate 0.0001
2017-10-10T12:03:48.920883: step 5670, loss 0.129064, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:49.112467: step 5671, loss 0.0708622, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:49.310998: step 5672, loss 0.061268, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:49.485322: step 5673, loss 0.053208, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:49.651988: step 5674, loss 0.0818349, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:49.835933: step 5675, loss 0.0558567, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:50.085290: step 5676, loss 0.0343478, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:50.357759: step 5677, loss 0.0566649, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:50.688369: step 5678, loss 0.111247, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:50.908882: step 5679, loss 0.0626443, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:51.125768: step 5680, loss 0.0873417, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:03:51.736166: step 5680, loss 0.197406, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5680

2017-10-10T12:03:53.052960: step 5681, loss 0.0361895, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:53.319883: step 5682, loss 0.0428637, acc 1, learning_rate 0.0001
2017-10-10T12:03:53.579447: step 5683, loss 0.0395092, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:53.735102: step 5684, loss 0.0410362, acc 1, learning_rate 0.0001
2017-10-10T12:03:54.014400: step 5685, loss 0.0558887, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:54.253857: step 5686, loss 0.0683143, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:54.504886: step 5687, loss 0.0677955, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:54.788507: step 5688, loss 0.0731826, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:55.026805: step 5689, loss 0.131788, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:55.268875: step 5690, loss 0.0539481, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:55.542065: step 5691, loss 0.0614134, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:55.745781: step 5692, loss 0.0513763, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:55.965073: step 5693, loss 0.0681988, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:56.210140: step 5694, loss 0.0263989, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:56.480103: step 5695, loss 0.059344, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:56.704638: step 5696, loss 0.0660075, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:56.916846: step 5697, loss 0.0738247, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:57.176865: step 5698, loss 0.103636, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:57.403220: step 5699, loss 0.130063, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:57.636847: step 5700, loss 0.095759, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:57.884874: step 5701, loss 0.0420549, acc 1, learning_rate 0.0001
2017-10-10T12:03:58.160839: step 5702, loss 0.13226, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:58.436990: step 5703, loss 0.0627768, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:58.677063: step 5704, loss 0.02581, acc 1, learning_rate 0.0001
2017-10-10T12:03:58.950033: step 5705, loss 0.0999154, acc 0.96875, learning_rate 0.0001
2017-10-10T12:03:59.314056: step 5706, loss 0.0380283, acc 1, learning_rate 0.0001
2017-10-10T12:03:59.564364: step 5707, loss 0.0738162, acc 0.984375, learning_rate 0.0001
2017-10-10T12:03:59.698275: step 5708, loss 0.0213117, acc 1, learning_rate 0.0001
2017-10-10T12:03:59.835391: step 5709, loss 0.136458, acc 0.953125, learning_rate 0.0001
2017-10-10T12:03:59.976336: step 5710, loss 0.0291994, acc 1, learning_rate 0.0001
2017-10-10T12:04:00.107706: step 5711, loss 0.0499323, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:00.280766: step 5712, loss 0.128087, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:00.518722: step 5713, loss 0.090932, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:00.752972: step 5714, loss 0.0888804, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:01.045131: step 5715, loss 0.0452428, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:01.272817: step 5716, loss 0.132871, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:01.506019: step 5717, loss 0.0611539, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:01.742536: step 5718, loss 0.101427, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:01.969809: step 5719, loss 0.146042, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:02.200510: step 5720, loss 0.132124, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:02.861027: step 5720, loss 0.20049, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5720

2017-10-10T12:04:03.920910: step 5721, loss 0.0339483, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:04.172533: step 5722, loss 0.0459266, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:04.380826: step 5723, loss 0.0645438, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:04.636881: step 5724, loss 0.072907, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:04.888131: step 5725, loss 0.112062, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:05.120219: step 5726, loss 0.0621494, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:05.404920: step 5727, loss 0.0545708, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:05.654843: step 5728, loss 0.0301719, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:05.882110: step 5729, loss 0.0756859, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:06.136864: step 5730, loss 0.0486171, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:06.392235: step 5731, loss 0.0869944, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:06.645031: step 5732, loss 0.0687374, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:06.892866: step 5733, loss 0.0545432, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:07.197035: step 5734, loss 0.0695436, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:07.488991: step 5735, loss 0.0571391, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:07.784716: step 5736, loss 0.11754, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:07.996185: step 5737, loss 0.100375, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:08.198118: step 5738, loss 0.0899302, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:08.402213: step 5739, loss 0.0462385, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:08.616838: step 5740, loss 0.113542, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:08.848877: step 5741, loss 0.0227933, acc 1, learning_rate 0.0001
2017-10-10T12:04:09.081170: step 5742, loss 0.0868779, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:09.339280: step 5743, loss 0.0301885, acc 1, learning_rate 0.0001
2017-10-10T12:04:09.678935: step 5744, loss 0.121862, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:09.868694: step 5745, loss 0.13363, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:10.060484: step 5746, loss 0.0482296, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:10.273390: step 5747, loss 0.140596, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:10.455638: step 5748, loss 0.211967, acc 0.90625, learning_rate 0.0001
2017-10-10T12:04:10.676845: step 5749, loss 0.0745398, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:10.903602: step 5750, loss 0.0417955, acc 1, learning_rate 0.0001
2017-10-10T12:04:11.132336: step 5751, loss 0.107722, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:11.362306: step 5752, loss 0.107439, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:11.625812: step 5753, loss 0.0567961, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:11.854657: step 5754, loss 0.0304604, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:12.076906: step 5755, loss 0.0473701, acc 1, learning_rate 0.0001
2017-10-10T12:04:12.357076: step 5756, loss 0.055045, acc 1, learning_rate 0.0001
2017-10-10T12:04:12.596259: step 5757, loss 0.0577616, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:12.873985: step 5758, loss 0.0596593, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:13.129381: step 5759, loss 0.0596872, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:13.373055: step 5760, loss 0.0293956, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:14.000503: step 5760, loss 0.198493, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5760

2017-10-10T12:04:15.171128: step 5761, loss 0.0693214, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:15.389712: step 5762, loss 0.0412946, acc 1, learning_rate 0.0001
2017-10-10T12:04:15.593857: step 5763, loss 0.0542698, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:15.827000: step 5764, loss 0.112225, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:16.102314: step 5765, loss 0.0461648, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:16.320837: step 5766, loss 0.105093, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:16.504832: step 5767, loss 0.050547, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:16.708837: step 5768, loss 0.0957312, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:16.936032: step 5769, loss 0.0332419, acc 1, learning_rate 0.0001
2017-10-10T12:04:17.096841: step 5770, loss 0.104348, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:17.315960: step 5771, loss 0.0360613, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:17.542487: step 5772, loss 0.0210883, acc 1, learning_rate 0.0001
2017-10-10T12:04:17.792873: step 5773, loss 0.0800421, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:18.009631: step 5774, loss 0.0512469, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:18.264207: step 5775, loss 0.0620401, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:18.507186: step 5776, loss 0.0555429, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:18.781518: step 5777, loss 0.0584543, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:19.020879: step 5778, loss 0.0575623, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:19.250088: step 5779, loss 0.0531683, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:19.491771: step 5780, loss 0.0937564, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:19.728948: step 5781, loss 0.0282044, acc 1, learning_rate 0.0001
2017-10-10T12:04:19.984994: step 5782, loss 0.0230956, acc 1, learning_rate 0.0001
2017-10-10T12:04:20.256903: step 5783, loss 0.0339925, acc 1, learning_rate 0.0001
2017-10-10T12:04:20.432614: step 5784, loss 0.0834803, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:20.601012: step 5785, loss 0.0807859, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:20.785009: step 5786, loss 0.0513952, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:20.960926: step 5787, loss 0.0222448, acc 1, learning_rate 0.0001
2017-10-10T12:04:21.175405: step 5788, loss 0.0391945, acc 1, learning_rate 0.0001
2017-10-10T12:04:21.350064: step 5789, loss 0.0927714, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:21.545174: step 5790, loss 0.0485869, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:21.824857: step 5791, loss 0.0679394, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:22.077798: step 5792, loss 0.0612036, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:22.308838: step 5793, loss 0.0547068, acc 1, learning_rate 0.0001
2017-10-10T12:04:22.573317: step 5794, loss 0.111203, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:22.826132: step 5795, loss 0.0853161, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:23.048868: step 5796, loss 0.112971, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:23.287971: step 5797, loss 0.0489964, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:23.526078: step 5798, loss 0.113918, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:23.764829: step 5799, loss 0.0974867, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:24.003331: step 5800, loss 0.0642798, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:24.722085: step 5800, loss 0.198486, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5800

2017-10-10T12:04:25.863153: step 5801, loss 0.116364, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:26.103101: step 5802, loss 0.0833832, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:26.335290: step 5803, loss 0.0583457, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:26.568842: step 5804, loss 0.01965, acc 1, learning_rate 0.0001
2017-10-10T12:04:26.824860: step 5805, loss 0.0616484, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:27.075918: step 5806, loss 0.0625217, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:27.328902: step 5807, loss 0.0846035, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:27.576020: step 5808, loss 0.0716177, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:27.787863: step 5809, loss 0.0691498, acc 1, learning_rate 0.0001
2017-10-10T12:04:28.020885: step 5810, loss 0.0380247, acc 1, learning_rate 0.0001
2017-10-10T12:04:28.288920: step 5811, loss 0.046652, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:28.560981: step 5812, loss 0.0231068, acc 1, learning_rate 0.0001
2017-10-10T12:04:28.804589: step 5813, loss 0.0587169, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:29.008266: step 5814, loss 0.0724742, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:29.232835: step 5815, loss 0.0815259, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:29.517048: step 5816, loss 0.0691308, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:29.778509: step 5817, loss 0.0895023, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:30.032826: step 5818, loss 0.0515288, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:30.307810: step 5819, loss 0.0928386, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:30.621455: step 5820, loss 0.149741, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:30.842973: step 5821, loss 0.0362187, acc 1, learning_rate 0.0001
2017-10-10T12:04:31.020690: step 5822, loss 0.043187, acc 1, learning_rate 0.0001
2017-10-10T12:04:31.212813: step 5823, loss 0.0541797, acc 1, learning_rate 0.0001
2017-10-10T12:04:31.400723: step 5824, loss 0.0642944, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:31.574006: step 5825, loss 0.126203, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:31.832949: step 5826, loss 0.0342035, acc 1, learning_rate 0.0001
2017-10-10T12:04:32.076839: step 5827, loss 0.0576767, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:32.315599: step 5828, loss 0.13336, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:32.561118: step 5829, loss 0.091387, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:32.844408: step 5830, loss 0.0520771, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:33.090334: step 5831, loss 0.0343274, acc 1, learning_rate 0.0001
2017-10-10T12:04:33.279598: step 5832, loss 0.0508042, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:33.472862: step 5833, loss 0.0641159, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:33.695941: step 5834, loss 0.128504, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:33.930675: step 5835, loss 0.0529368, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:34.157137: step 5836, loss 0.103485, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:34.392901: step 5837, loss 0.0677454, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:34.611941: step 5838, loss 0.0470588, acc 1, learning_rate 0.0001
2017-10-10T12:04:34.857712: step 5839, loss 0.0320861, acc 1, learning_rate 0.0001
2017-10-10T12:04:35.132862: step 5840, loss 0.0562174, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:35.708915: step 5840, loss 0.199571, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5840

2017-10-10T12:04:36.760855: step 5841, loss 0.0691372, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:36.995263: step 5842, loss 0.0300267, acc 1, learning_rate 0.0001
2017-10-10T12:04:37.237272: step 5843, loss 0.150129, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:37.464543: step 5844, loss 0.103755, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:37.742353: step 5845, loss 0.0929471, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:37.960638: step 5846, loss 0.0470016, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:38.161069: step 5847, loss 0.143354, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:38.385090: step 5848, loss 0.0698033, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:38.626582: step 5849, loss 0.0435462, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:38.846787: step 5850, loss 0.0622164, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:39.068864: step 5851, loss 0.152287, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:39.305029: step 5852, loss 0.0903274, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:39.508855: step 5853, loss 0.0724991, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:39.733148: step 5854, loss 0.0785637, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:39.982516: step 5855, loss 0.0660066, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:40.229941: step 5856, loss 0.0263301, acc 1, learning_rate 0.0001
2017-10-10T12:04:40.497742: step 5857, loss 0.0780148, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:40.758865: step 5858, loss 0.144266, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:41.092942: step 5859, loss 0.0354537, acc 1, learning_rate 0.0001
2017-10-10T12:04:41.296875: step 5860, loss 0.101322, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:41.571350: step 5861, loss 0.0697913, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:41.719899: step 5862, loss 0.0441999, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:41.864592: step 5863, loss 0.0555117, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:42.022995: step 5864, loss 0.0523383, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:42.234083: step 5865, loss 0.0254856, acc 1, learning_rate 0.0001
2017-10-10T12:04:42.456612: step 5866, loss 0.0804783, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:42.710236: step 5867, loss 0.118598, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:42.939546: step 5868, loss 0.0661056, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:43.188899: step 5869, loss 0.0364435, acc 1, learning_rate 0.0001
2017-10-10T12:04:43.447102: step 5870, loss 0.0619971, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:43.704823: step 5871, loss 0.115403, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:44.013632: step 5872, loss 0.107289, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:44.267127: step 5873, loss 0.142938, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:44.539839: step 5874, loss 0.0455895, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:44.784941: step 5875, loss 0.0638089, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:45.021329: step 5876, loss 0.171155, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:45.263116: step 5877, loss 0.0330179, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:45.569233: step 5878, loss 0.0304496, acc 1, learning_rate 0.0001
2017-10-10T12:04:45.785127: step 5879, loss 0.18887, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:45.981760: step 5880, loss 0.10483, acc 0.960784, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:46.561053: step 5880, loss 0.198904, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5880

2017-10-10T12:04:47.752839: step 5881, loss 0.0312021, acc 1, learning_rate 0.0001
2017-10-10T12:04:48.003032: step 5882, loss 0.0316115, acc 1, learning_rate 0.0001
2017-10-10T12:04:48.239564: step 5883, loss 0.0763747, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:48.493594: step 5884, loss 0.0749161, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:48.736015: step 5885, loss 0.0352415, acc 1, learning_rate 0.0001
2017-10-10T12:04:48.983854: step 5886, loss 0.0491774, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:49.252564: step 5887, loss 0.0256689, acc 1, learning_rate 0.0001
2017-10-10T12:04:49.508999: step 5888, loss 0.0279139, acc 1, learning_rate 0.0001
2017-10-10T12:04:49.808857: step 5889, loss 0.0315535, acc 1, learning_rate 0.0001
2017-10-10T12:04:50.052781: step 5890, loss 0.06449, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:50.263001: step 5891, loss 0.0444052, acc 1, learning_rate 0.0001
2017-10-10T12:04:50.456843: step 5892, loss 0.130098, acc 0.9375, learning_rate 0.0001
2017-10-10T12:04:50.664946: step 5893, loss 0.0391029, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:50.872865: step 5894, loss 0.0325902, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:51.156865: step 5895, loss 0.0132097, acc 1, learning_rate 0.0001
2017-10-10T12:04:51.449036: step 5896, loss 0.0545548, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:51.652888: step 5897, loss 0.0666304, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:51.836661: step 5898, loss 0.0723464, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:52.018378: step 5899, loss 0.0696609, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:52.184820: step 5900, loss 0.0914478, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:52.379962: step 5901, loss 0.0489071, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:52.559075: step 5902, loss 0.0974993, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:52.790781: step 5903, loss 0.148913, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:53.049931: step 5904, loss 0.147554, acc 0.921875, learning_rate 0.0001
2017-10-10T12:04:53.293104: step 5905, loss 0.0444988, acc 1, learning_rate 0.0001
2017-10-10T12:04:53.534234: step 5906, loss 0.0521687, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:53.812885: step 5907, loss 0.0569295, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:54.076778: step 5908, loss 0.0840112, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:54.345468: step 5909, loss 0.0341452, acc 1, learning_rate 0.0001
2017-10-10T12:04:54.597326: step 5910, loss 0.0556952, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:54.846219: step 5911, loss 0.0388329, acc 1, learning_rate 0.0001
2017-10-10T12:04:55.101639: step 5912, loss 0.107073, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:55.376874: step 5913, loss 0.0227231, acc 1, learning_rate 0.0001
2017-10-10T12:04:55.602164: step 5914, loss 0.0651209, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:55.836900: step 5915, loss 0.0796477, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:56.096084: step 5916, loss 0.0925588, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:56.305913: step 5917, loss 0.0573922, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:56.544972: step 5918, loss 0.0536717, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:56.791723: step 5919, loss 0.113603, acc 0.953125, learning_rate 0.0001
2017-10-10T12:04:57.044872: step 5920, loss 0.0793809, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:04:57.697814: step 5920, loss 0.19846, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5920

2017-10-10T12:04:58.900830: step 5921, loss 0.146588, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:59.155305: step 5922, loss 0.0521317, acc 0.984375, learning_rate 0.0001
2017-10-10T12:04:59.390461: step 5923, loss 0.0779458, acc 0.96875, learning_rate 0.0001
2017-10-10T12:04:59.612058: step 5924, loss 0.0473299, acc 1, learning_rate 0.0001
2017-10-10T12:04:59.847576: step 5925, loss 0.126501, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:00.114625: step 5926, loss 0.100316, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:00.357062: step 5927, loss 0.0951994, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:00.602406: step 5928, loss 0.0445581, acc 1, learning_rate 0.0001
2017-10-10T12:05:00.881416: step 5929, loss 0.0379777, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:01.130347: step 5930, loss 0.106694, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:01.356724: step 5931, loss 0.0433939, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:01.701110: step 5932, loss 0.0649968, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:01.949532: step 5933, loss 0.073924, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:02.148852: step 5934, loss 0.0305504, acc 1, learning_rate 0.0001
2017-10-10T12:05:02.357606: step 5935, loss 0.115183, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:02.545180: step 5936, loss 0.0808951, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:02.820530: step 5937, loss 0.0578597, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:03.065428: step 5938, loss 0.172203, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:03.304599: step 5939, loss 0.0958104, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:03.582087: step 5940, loss 0.100719, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:03.848826: step 5941, loss 0.160012, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:04.112338: step 5942, loss 0.10102, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:04.332041: step 5943, loss 0.0924937, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:04.528952: step 5944, loss 0.0844066, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:04.760854: step 5945, loss 0.0799629, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:05.021256: step 5946, loss 0.0314001, acc 1, learning_rate 0.0001
2017-10-10T12:05:05.256484: step 5947, loss 0.0606427, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:05.492420: step 5948, loss 0.0851235, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:05.755081: step 5949, loss 0.0528948, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:05.991799: step 5950, loss 0.0538854, acc 1, learning_rate 0.0001
2017-10-10T12:05:06.220867: step 5951, loss 0.0433065, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:06.568210: step 5952, loss 0.0453661, acc 1, learning_rate 0.0001
2017-10-10T12:05:06.775441: step 5953, loss 0.0306933, acc 1, learning_rate 0.0001
2017-10-10T12:05:06.953502: step 5954, loss 0.10645, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:07.133454: step 5955, loss 0.0709926, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:07.325886: step 5956, loss 0.102045, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:07.522708: step 5957, loss 0.15985, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:07.719940: step 5958, loss 0.0876612, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:07.958346: step 5959, loss 0.102281, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:08.217768: step 5960, loss 0.0135007, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:08.801107: step 5960, loss 0.196244, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-5960

2017-10-10T12:05:09.797368: step 5961, loss 0.0890174, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:10.043171: step 5962, loss 0.03153, acc 1, learning_rate 0.0001
2017-10-10T12:05:10.305960: step 5963, loss 0.063827, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:10.557765: step 5964, loss 0.143707, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:10.797003: step 5965, loss 0.112497, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:11.013327: step 5966, loss 0.0504503, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:11.238845: step 5967, loss 0.113494, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:11.496849: step 5968, loss 0.0477844, acc 1, learning_rate 0.0001
2017-10-10T12:05:11.716912: step 5969, loss 0.0358655, acc 1, learning_rate 0.0001
2017-10-10T12:05:12.014616: step 5970, loss 0.0475559, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:12.305602: step 5971, loss 0.0360093, acc 1, learning_rate 0.0001
2017-10-10T12:05:12.486704: step 5972, loss 0.101833, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:12.678333: step 5973, loss 0.0347762, acc 1, learning_rate 0.0001
2017-10-10T12:05:12.860396: step 5974, loss 0.0820993, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:13.047396: step 5975, loss 0.112917, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:13.325040: step 5976, loss 0.15267, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:13.572397: step 5977, loss 0.0480565, acc 1, learning_rate 0.0001
2017-10-10T12:05:13.798036: step 5978, loss 0.0549917, acc 0.980392, learning_rate 0.0001
2017-10-10T12:05:14.042751: step 5979, loss 0.0657628, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:14.273213: step 5980, loss 0.0942953, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:14.512478: step 5981, loss 0.064047, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:14.776710: step 5982, loss 0.0293248, acc 1, learning_rate 0.0001
2017-10-10T12:05:15.081006: step 5983, loss 0.0622529, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:15.299801: step 5984, loss 0.0363728, acc 1, learning_rate 0.0001
2017-10-10T12:05:15.494031: step 5985, loss 0.0687552, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:15.693135: step 5986, loss 0.087907, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:15.880855: step 5987, loss 0.131132, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:16.079778: step 5988, loss 0.11419, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:16.312512: step 5989, loss 0.0415312, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:16.525145: step 5990, loss 0.0494091, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:16.776935: step 5991, loss 0.06892, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:17.058072: step 5992, loss 0.0784175, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:17.291454: step 5993, loss 0.0225636, acc 1, learning_rate 0.0001
2017-10-10T12:05:17.544722: step 5994, loss 0.0688152, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:17.818566: step 5995, loss 0.150199, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:18.065048: step 5996, loss 0.0577502, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:18.318693: step 5997, loss 0.0830075, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:18.542845: step 5998, loss 0.0366812, acc 1, learning_rate 0.0001
2017-10-10T12:05:18.795974: step 5999, loss 0.091348, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:19.064169: step 6000, loss 0.0336103, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:19.636502: step 6000, loss 0.200514, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6000

2017-10-10T12:05:20.742569: step 6001, loss 0.0717421, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:20.999874: step 6002, loss 0.0447833, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:21.252996: step 6003, loss 0.0765033, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:21.492221: step 6004, loss 0.151642, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:21.749791: step 6005, loss 0.101952, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:22.011112: step 6006, loss 0.030919, acc 1, learning_rate 0.0001
2017-10-10T12:05:22.255299: step 6007, loss 0.062158, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:22.568824: step 6008, loss 0.0474677, acc 1, learning_rate 0.0001
2017-10-10T12:05:22.814937: step 6009, loss 0.0643737, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:23.015331: step 6010, loss 0.138289, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:23.207426: step 6011, loss 0.0718727, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:23.409227: step 6012, loss 0.0663963, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:23.672924: step 6013, loss 0.0802802, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:23.885100: step 6014, loss 0.0571608, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:24.076904: step 6015, loss 0.0471501, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:24.259479: step 6016, loss 0.057443, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:24.456949: step 6017, loss 0.101121, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:24.680961: step 6018, loss 0.0424703, acc 1, learning_rate 0.0001
2017-10-10T12:05:24.936302: step 6019, loss 0.120886, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:25.198492: step 6020, loss 0.153658, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:25.432059: step 6021, loss 0.0574902, acc 1, learning_rate 0.0001
2017-10-10T12:05:25.663817: step 6022, loss 0.0491183, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:25.912300: step 6023, loss 0.0310339, acc 1, learning_rate 0.0001
2017-10-10T12:05:26.162747: step 6024, loss 0.0500553, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:26.425121: step 6025, loss 0.109248, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:26.674670: step 6026, loss 0.0886307, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:26.908981: step 6027, loss 0.100002, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:27.131416: step 6028, loss 0.105914, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:27.374543: step 6029, loss 0.0154545, acc 1, learning_rate 0.0001
2017-10-10T12:05:27.616925: step 6030, loss 0.0723695, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:27.876866: step 6031, loss 0.10386, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:28.132105: step 6032, loss 0.0474311, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:28.368169: step 6033, loss 0.0455991, acc 1, learning_rate 0.0001
2017-10-10T12:05:28.616380: step 6034, loss 0.0527632, acc 1, learning_rate 0.0001
2017-10-10T12:05:28.876449: step 6035, loss 0.0200108, acc 1, learning_rate 0.0001
2017-10-10T12:05:29.081350: step 6036, loss 0.0763228, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:29.317181: step 6037, loss 0.0423677, acc 1, learning_rate 0.0001
2017-10-10T12:05:29.545086: step 6038, loss 0.122868, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:29.787469: step 6039, loss 0.142507, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:29.998031: step 6040, loss 0.0851167, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:30.548214: step 6040, loss 0.197433, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6040

2017-10-10T12:05:31.898313: step 6041, loss 0.045439, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:32.120869: step 6042, loss 0.0450898, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:32.323267: step 6043, loss 0.0742417, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:32.508765: step 6044, loss 0.0833839, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:32.776830: step 6045, loss 0.0867531, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:32.986177: step 6046, loss 0.0182472, acc 1, learning_rate 0.0001
2017-10-10T12:05:33.171935: step 6047, loss 0.0456361, acc 1, learning_rate 0.0001
2017-10-10T12:05:33.339850: step 6048, loss 0.067671, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:33.561663: step 6049, loss 0.100633, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:33.743418: step 6050, loss 0.0503149, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:33.919076: step 6051, loss 0.025552, acc 1, learning_rate 0.0001
2017-10-10T12:05:34.196853: step 6052, loss 0.057775, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:34.440961: step 6053, loss 0.184847, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:34.688874: step 6054, loss 0.0547043, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:34.929848: step 6055, loss 0.0332474, acc 1, learning_rate 0.0001
2017-10-10T12:05:35.165295: step 6056, loss 0.104207, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:35.454284: step 6057, loss 0.0335534, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:35.686032: step 6058, loss 0.120454, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:35.920046: step 6059, loss 0.0351716, acc 1, learning_rate 0.0001
2017-10-10T12:05:36.167230: step 6060, loss 0.0652983, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:36.388894: step 6061, loss 0.0962291, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:36.642357: step 6062, loss 0.0987534, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:36.902740: step 6063, loss 0.051271, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:37.138284: step 6064, loss 0.165119, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:37.381052: step 6065, loss 0.0888322, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:37.618653: step 6066, loss 0.0346293, acc 1, learning_rate 0.0001
2017-10-10T12:05:37.851322: step 6067, loss 0.102724, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:38.110036: step 6068, loss 0.0497312, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:38.348817: step 6069, loss 0.0137002, acc 1, learning_rate 0.0001
2017-10-10T12:05:38.573007: step 6070, loss 0.0603214, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:38.788224: step 6071, loss 0.0943771, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:39.035662: step 6072, loss 0.10441, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:39.304867: step 6073, loss 0.0667922, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:39.534724: step 6074, loss 0.0600762, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:39.788909: step 6075, loss 0.0780454, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:40.006556: step 6076, loss 0.0826382, acc 0.960784, learning_rate 0.0001
2017-10-10T12:05:40.312672: step 6077, loss 0.064304, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:40.520866: step 6078, loss 0.045503, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:40.708854: step 6079, loss 0.0235294, acc 1, learning_rate 0.0001
2017-10-10T12:05:40.923971: step 6080, loss 0.0608555, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:41.373732: step 6080, loss 0.198527, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6080

2017-10-10T12:05:42.428302: step 6081, loss 0.0504362, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:42.688827: step 6082, loss 0.0298943, acc 1, learning_rate 0.0001
2017-10-10T12:05:42.968824: step 6083, loss 0.123078, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:43.196332: step 6084, loss 0.063971, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:43.503839: step 6085, loss 0.0892037, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:43.703593: step 6086, loss 0.0932564, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:43.911534: step 6087, loss 0.100541, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:44.087498: step 6088, loss 0.0885731, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:44.273005: step 6089, loss 0.00506624, acc 1, learning_rate 0.0001
2017-10-10T12:05:44.505087: step 6090, loss 0.059409, acc 1, learning_rate 0.0001
2017-10-10T12:05:44.777010: step 6091, loss 0.0364811, acc 1, learning_rate 0.0001
2017-10-10T12:05:45.016975: step 6092, loss 0.187396, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:45.249960: step 6093, loss 0.0728328, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:45.491398: step 6094, loss 0.144582, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:45.776612: step 6095, loss 0.101879, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:46.036830: step 6096, loss 0.0511471, acc 1, learning_rate 0.0001
2017-10-10T12:05:46.299233: step 6097, loss 0.165411, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:46.542364: step 6098, loss 0.0706111, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:46.762561: step 6099, loss 0.0964071, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:47.007320: step 6100, loss 0.028945, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:47.268639: step 6101, loss 0.0407681, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:47.510460: step 6102, loss 0.0622612, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:47.783921: step 6103, loss 0.100163, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:48.026153: step 6104, loss 0.0877718, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:48.273087: step 6105, loss 0.0512558, acc 1, learning_rate 0.0001
2017-10-10T12:05:48.583227: step 6106, loss 0.0420871, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:48.816487: step 6107, loss 0.0569976, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:49.016924: step 6108, loss 0.0412271, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:49.237667: step 6109, loss 0.0765439, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:49.440071: step 6110, loss 0.0989869, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:49.652851: step 6111, loss 0.0805379, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:49.897458: step 6112, loss 0.0742044, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:50.125666: step 6113, loss 0.0758527, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:50.395394: step 6114, loss 0.0748772, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:50.647825: step 6115, loss 0.159413, acc 0.921875, learning_rate 0.0001
2017-10-10T12:05:50.932380: step 6116, loss 0.0928928, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:51.156825: step 6117, loss 0.0292254, acc 1, learning_rate 0.0001
2017-10-10T12:05:51.402959: step 6118, loss 0.0773428, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:51.620534: step 6119, loss 0.110318, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:51.877447: step 6120, loss 0.0525311, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:05:52.407933: step 6120, loss 0.1982, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6120

2017-10-10T12:05:53.748843: step 6121, loss 0.0877423, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:54.013590: step 6122, loss 0.0742929, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:54.190633: step 6123, loss 0.0325196, acc 1, learning_rate 0.0001
2017-10-10T12:05:54.392252: step 6124, loss 0.0408133, acc 1, learning_rate 0.0001
2017-10-10T12:05:54.572411: step 6125, loss 0.0927974, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:54.751949: step 6126, loss 0.0618727, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:54.964018: step 6127, loss 0.113239, acc 0.9375, learning_rate 0.0001
2017-10-10T12:05:55.226041: step 6128, loss 0.0243422, acc 1, learning_rate 0.0001
2017-10-10T12:05:55.473692: step 6129, loss 0.0279104, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:55.738398: step 6130, loss 0.0719387, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:55.978890: step 6131, loss 0.0492662, acc 1, learning_rate 0.0001
2017-10-10T12:05:56.223711: step 6132, loss 0.115343, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:56.458709: step 6133, loss 0.128415, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:56.742454: step 6134, loss 0.0834268, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:56.986678: step 6135, loss 0.0856061, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:57.185776: step 6136, loss 0.0341989, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:57.390672: step 6137, loss 0.0410483, acc 1, learning_rate 0.0001
2017-10-10T12:05:57.581804: step 6138, loss 0.0654668, acc 0.96875, learning_rate 0.0001
2017-10-10T12:05:57.777961: step 6139, loss 0.0597192, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:58.012554: step 6140, loss 0.0886178, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:58.267730: step 6141, loss 0.0578958, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:58.520853: step 6142, loss 0.0545519, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:58.813016: step 6143, loss 0.0421675, acc 1, learning_rate 0.0001
2017-10-10T12:05:59.060866: step 6144, loss 0.0734598, acc 0.984375, learning_rate 0.0001
2017-10-10T12:05:59.313337: step 6145, loss 0.0922257, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:59.545482: step 6146, loss 0.113418, acc 0.953125, learning_rate 0.0001
2017-10-10T12:05:59.788548: step 6147, loss 0.0403545, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:00.019984: step 6148, loss 0.0800424, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:00.240125: step 6149, loss 0.076677, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:00.487317: step 6150, loss 0.0365617, acc 1, learning_rate 0.0001
2017-10-10T12:06:00.739471: step 6151, loss 0.0443539, acc 1, learning_rate 0.0001
2017-10-10T12:06:00.996852: step 6152, loss 0.030636, acc 1, learning_rate 0.0001
2017-10-10T12:06:01.262494: step 6153, loss 0.0947084, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:01.528292: step 6154, loss 0.1025, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:01.790122: step 6155, loss 0.0390346, acc 1, learning_rate 0.0001
2017-10-10T12:06:02.045392: step 6156, loss 0.0611786, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:02.308991: step 6157, loss 0.0296736, acc 1, learning_rate 0.0001
2017-10-10T12:06:02.546896: step 6158, loss 0.0552082, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:02.764725: step 6159, loss 0.0831845, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:03.024822: step 6160, loss 0.0510031, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:03.570458: step 6160, loss 0.196141, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6160

2017-10-10T12:06:04.671377: step 6161, loss 0.0566695, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:04.849296: step 6162, loss 0.0576838, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:05.123955: step 6163, loss 0.0384665, acc 1, learning_rate 0.0001
2017-10-10T12:06:05.283420: step 6164, loss 0.103517, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:05.494970: step 6165, loss 0.0947628, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:05.709602: step 6166, loss 0.0948004, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:05.921034: step 6167, loss 0.0513618, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:06.112782: step 6168, loss 0.0407425, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:06.355004: step 6169, loss 0.117658, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:06.596871: step 6170, loss 0.0938617, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:06.844321: step 6171, loss 0.116866, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:07.077530: step 6172, loss 0.113828, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:07.314972: step 6173, loss 0.0987159, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:07.545384: step 6174, loss 0.0450064, acc 0.980392, learning_rate 0.0001
2017-10-10T12:06:07.784378: step 6175, loss 0.0582889, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:08.052986: step 6176, loss 0.032083, acc 1, learning_rate 0.0001
2017-10-10T12:06:08.290919: step 6177, loss 0.0234093, acc 1, learning_rate 0.0001
2017-10-10T12:06:08.521041: step 6178, loss 0.0697971, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:08.770605: step 6179, loss 0.0759295, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:09.004985: step 6180, loss 0.118742, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:09.277534: step 6181, loss 0.0799822, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:09.533799: step 6182, loss 0.0843298, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:09.780816: step 6183, loss 0.0879753, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:10.023623: step 6184, loss 0.0472289, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:10.297352: step 6185, loss 0.0628207, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:10.544880: step 6186, loss 0.0273362, acc 1, learning_rate 0.0001
2017-10-10T12:06:10.786926: step 6187, loss 0.113189, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:11.030800: step 6188, loss 0.147881, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:11.258374: step 6189, loss 0.0386569, acc 1, learning_rate 0.0001
2017-10-10T12:06:11.504225: step 6190, loss 0.0683387, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:11.724992: step 6191, loss 0.0325257, acc 1, learning_rate 0.0001
2017-10-10T12:06:11.955522: step 6192, loss 0.0372535, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:12.193011: step 6193, loss 0.0384375, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:12.459766: step 6194, loss 0.079046, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:12.700856: step 6195, loss 0.0403298, acc 1, learning_rate 0.0001
2017-10-10T12:06:12.933317: step 6196, loss 0.0712581, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:13.181183: step 6197, loss 0.0940809, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:13.410479: step 6198, loss 0.104978, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:13.678066: step 6199, loss 0.0259068, acc 1, learning_rate 0.0001
2017-10-10T12:06:13.900829: step 6200, loss 0.0534158, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:14.365732: step 6200, loss 0.195169, acc 0.933813

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6200

2017-10-10T12:06:15.496844: step 6201, loss 0.0887214, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:15.694822: step 6202, loss 0.051943, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:15.936741: step 6203, loss 0.0903727, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:16.207140: step 6204, loss 0.082824, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:16.453151: step 6205, loss 0.0523348, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:16.672984: step 6206, loss 0.104268, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:16.917034: step 6207, loss 0.083346, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:17.190159: step 6208, loss 0.0719496, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:17.416809: step 6209, loss 0.0719431, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:17.660547: step 6210, loss 0.0433729, acc 1, learning_rate 0.0001
2017-10-10T12:06:17.885190: step 6211, loss 0.044154, acc 1, learning_rate 0.0001
2017-10-10T12:06:18.158618: step 6212, loss 0.0261741, acc 1, learning_rate 0.0001
2017-10-10T12:06:18.408865: step 6213, loss 0.125256, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:18.659431: step 6214, loss 0.0576118, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:18.912914: step 6215, loss 0.0849914, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:19.173275: step 6216, loss 0.07198, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:19.403659: step 6217, loss 0.0785732, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:19.662233: step 6218, loss 0.0577936, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:19.931453: step 6219, loss 0.105944, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:20.187524: step 6220, loss 0.108412, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:20.432879: step 6221, loss 0.0491806, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:20.693034: step 6222, loss 0.0382149, acc 1, learning_rate 0.0001
2017-10-10T12:06:20.929047: step 6223, loss 0.108047, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:21.188303: step 6224, loss 0.0703071, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:21.424835: step 6225, loss 0.070386, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:21.672889: step 6226, loss 0.0424525, acc 1, learning_rate 0.0001
2017-10-10T12:06:21.945684: step 6227, loss 0.050353, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:22.155084: step 6228, loss 0.0617463, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:22.372434: step 6229, loss 0.201098, acc 0.90625, learning_rate 0.0001
2017-10-10T12:06:22.570888: step 6230, loss 0.0660316, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:22.775563: step 6231, loss 0.0870319, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:23.024870: step 6232, loss 0.0220824, acc 1, learning_rate 0.0001
2017-10-10T12:06:23.256514: step 6233, loss 0.0837003, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:23.514340: step 6234, loss 0.0356936, acc 1, learning_rate 0.0001
2017-10-10T12:06:23.731040: step 6235, loss 0.0301593, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:23.964623: step 6236, loss 0.0402864, acc 1, learning_rate 0.0001
2017-10-10T12:06:24.217211: step 6237, loss 0.0196467, acc 1, learning_rate 0.0001
2017-10-10T12:06:24.468865: step 6238, loss 0.0826528, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:24.698451: step 6239, loss 0.017637, acc 1, learning_rate 0.0001
2017-10-10T12:06:24.931452: step 6240, loss 0.0678007, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:25.633232: step 6240, loss 0.193887, acc 0.933813

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6240

2017-10-10T12:06:26.473782: step 6241, loss 0.0332104, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:26.677474: step 6242, loss 0.132059, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:26.899445: step 6243, loss 0.0643506, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:27.171233: step 6244, loss 0.0405654, acc 1, learning_rate 0.0001
2017-10-10T12:06:27.406960: step 6245, loss 0.089281, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:27.656237: step 6246, loss 0.0540863, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:27.853016: step 6247, loss 0.0483563, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:28.091585: step 6248, loss 0.0912785, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:28.336902: step 6249, loss 0.0681195, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:28.608578: step 6250, loss 0.0775337, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:28.831368: step 6251, loss 0.0350729, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:29.107924: step 6252, loss 0.156987, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:29.360379: step 6253, loss 0.0838143, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:29.588459: step 6254, loss 0.0737794, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:29.891892: step 6255, loss 0.0926417, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:30.081112: step 6256, loss 0.0858767, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:30.315815: step 6257, loss 0.0457592, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:30.520314: step 6258, loss 0.0434307, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:30.729006: step 6259, loss 0.0272308, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:30.950180: step 6260, loss 0.0480371, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:31.179623: step 6261, loss 0.0838653, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:31.395903: step 6262, loss 0.121914, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:31.658628: step 6263, loss 0.0445749, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:31.904616: step 6264, loss 0.0841961, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:32.111869: step 6265, loss 0.117982, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:32.368904: step 6266, loss 0.0498436, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:32.672350: step 6267, loss 0.105099, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:32.905106: step 6268, loss 0.0545052, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:33.143503: step 6269, loss 0.120214, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:33.400593: step 6270, loss 0.0626698, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:33.644889: step 6271, loss 0.0551576, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:33.850955: step 6272, loss 0.125248, acc 0.960784, learning_rate 0.0001
2017-10-10T12:06:34.096863: step 6273, loss 0.034466, acc 1, learning_rate 0.0001
2017-10-10T12:06:34.384839: step 6274, loss 0.125572, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:34.635851: step 6275, loss 0.0851848, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:34.888845: step 6276, loss 0.102504, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:35.164292: step 6277, loss 0.0791892, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:35.403958: step 6278, loss 0.0731844, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:35.716611: step 6279, loss 0.0346114, acc 1, learning_rate 0.0001
2017-10-10T12:06:35.971106: step 6280, loss 0.145256, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:36.448323: step 6280, loss 0.196189, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6280

2017-10-10T12:06:37.455329: step 6281, loss 0.0733992, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:37.704838: step 6282, loss 0.0459748, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:37.981390: step 6283, loss 0.0581178, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:38.231873: step 6284, loss 0.021396, acc 1, learning_rate 0.0001
2017-10-10T12:06:38.439049: step 6285, loss 0.0912021, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:38.641209: step 6286, loss 0.0156033, acc 1, learning_rate 0.0001
2017-10-10T12:06:38.844225: step 6287, loss 0.0366692, acc 1, learning_rate 0.0001
2017-10-10T12:06:39.048347: step 6288, loss 0.107767, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:39.295086: step 6289, loss 0.105473, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:39.531802: step 6290, loss 0.0732266, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:39.759574: step 6291, loss 0.0568998, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:40.011272: step 6292, loss 0.0265522, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:40.250380: step 6293, loss 0.0206135, acc 1, learning_rate 0.0001
2017-10-10T12:06:40.506005: step 6294, loss 0.125615, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:40.760969: step 6295, loss 0.0429317, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:41.008878: step 6296, loss 0.0502617, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:41.220837: step 6297, loss 0.0709496, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:41.501016: step 6298, loss 0.0434174, acc 1, learning_rate 0.0001
2017-10-10T12:06:41.724447: step 6299, loss 0.0974226, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:41.980581: step 6300, loss 0.0520563, acc 1, learning_rate 0.0001
2017-10-10T12:06:42.228759: step 6301, loss 0.104552, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:42.488843: step 6302, loss 0.0357104, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:42.754237: step 6303, loss 0.0393242, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:42.979047: step 6304, loss 0.0343402, acc 1, learning_rate 0.0001
2017-10-10T12:06:43.243048: step 6305, loss 0.0491066, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:43.475353: step 6306, loss 0.0778793, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:43.717959: step 6307, loss 0.028807, acc 1, learning_rate 0.0001
2017-10-10T12:06:43.997869: step 6308, loss 0.0618283, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:44.223911: step 6309, loss 0.0357201, acc 1, learning_rate 0.0001
2017-10-10T12:06:44.509972: step 6310, loss 0.0242756, acc 1, learning_rate 0.0001
2017-10-10T12:06:44.776811: step 6311, loss 0.0901672, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:45.016869: step 6312, loss 0.0740652, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:45.267838: step 6313, loss 0.0675365, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:45.528956: step 6314, loss 0.0689016, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:45.771070: step 6315, loss 0.0792338, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:46.020684: step 6316, loss 0.114864, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:46.308854: step 6317, loss 0.0493919, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:46.648894: step 6318, loss 0.108046, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:46.859226: step 6319, loss 0.0975267, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:46.995837: step 6320, loss 0.0308969, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:47.337127: step 6320, loss 0.194053, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6320

2017-10-10T12:06:48.463840: step 6321, loss 0.0899208, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:48.708172: step 6322, loss 0.0689391, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:48.964954: step 6323, loss 0.0419757, acc 1, learning_rate 0.0001
2017-10-10T12:06:49.200810: step 6324, loss 0.0556865, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:49.470036: step 6325, loss 0.0263639, acc 1, learning_rate 0.0001
2017-10-10T12:06:49.714267: step 6326, loss 0.0539892, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:49.977821: step 6327, loss 0.0828585, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:50.203112: step 6328, loss 0.0378712, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:50.448653: step 6329, loss 0.0402455, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:50.695993: step 6330, loss 0.0231527, acc 1, learning_rate 0.0001
2017-10-10T12:06:50.960816: step 6331, loss 0.061744, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:51.201387: step 6332, loss 0.0751756, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:51.446682: step 6333, loss 0.126231, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:51.706736: step 6334, loss 0.101323, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:51.952225: step 6335, loss 0.0749266, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:52.214537: step 6336, loss 0.0189183, acc 1, learning_rate 0.0001
2017-10-10T12:06:52.466398: step 6337, loss 0.031038, acc 1, learning_rate 0.0001
2017-10-10T12:06:52.715254: step 6338, loss 0.0104424, acc 1, learning_rate 0.0001
2017-10-10T12:06:52.986962: step 6339, loss 0.0431608, acc 1, learning_rate 0.0001
2017-10-10T12:06:53.248353: step 6340, loss 0.056807, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:53.544857: step 6341, loss 0.0671094, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:53.788905: step 6342, loss 0.0565298, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:54.035675: step 6343, loss 0.0795629, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:54.264850: step 6344, loss 0.0794822, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:54.526430: step 6345, loss 0.0501861, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:54.797920: step 6346, loss 0.0695198, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:55.012597: step 6347, loss 0.0918999, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:55.217392: step 6348, loss 0.127163, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:55.414055: step 6349, loss 0.0974753, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:55.616271: step 6350, loss 0.0948073, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:55.817752: step 6351, loss 0.118597, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:56.076712: step 6352, loss 0.0732265, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:56.356855: step 6353, loss 0.149161, acc 0.9375, learning_rate 0.0001
2017-10-10T12:06:56.588838: step 6354, loss 0.119614, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:56.832884: step 6355, loss 0.0370377, acc 0.984375, learning_rate 0.0001
2017-10-10T12:06:57.166905: step 6356, loss 0.0907309, acc 0.96875, learning_rate 0.0001
2017-10-10T12:06:57.374892: step 6357, loss 0.0243742, acc 1, learning_rate 0.0001
2017-10-10T12:06:57.552561: step 6358, loss 0.137322, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:57.726196: step 6359, loss 0.0290695, acc 1, learning_rate 0.0001
2017-10-10T12:06:57.902251: step 6360, loss 0.179667, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:06:58.429518: step 6360, loss 0.197077, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6360

2017-10-10T12:06:59.429107: step 6361, loss 0.0951031, acc 0.953125, learning_rate 0.0001
2017-10-10T12:06:59.690300: step 6362, loss 0.0359372, acc 1, learning_rate 0.0001
2017-10-10T12:06:59.905895: step 6363, loss 0.141693, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:00.156689: step 6364, loss 0.0554153, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:00.396872: step 6365, loss 0.0579113, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:00.659013: step 6366, loss 0.0418272, acc 1, learning_rate 0.0001
2017-10-10T12:07:00.904598: step 6367, loss 0.0462075, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:01.166599: step 6368, loss 0.0495932, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:01.384860: step 6369, loss 0.0909199, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:01.594393: step 6370, loss 0.0301038, acc 1, learning_rate 0.0001
2017-10-10T12:07:01.822014: step 6371, loss 0.0535028, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:02.073923: step 6372, loss 0.0510425, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:02.348314: step 6373, loss 0.0572247, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:02.586206: step 6374, loss 0.0621526, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:02.788143: step 6375, loss 0.0576616, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:03.024590: step 6376, loss 0.0921634, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:03.324885: step 6377, loss 0.0551897, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:03.598203: step 6378, loss 0.0460544, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:03.804681: step 6379, loss 0.0530101, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:04.016409: step 6380, loss 0.0443266, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:04.213202: step 6381, loss 0.074436, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:04.418166: step 6382, loss 0.0430017, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:04.670986: step 6383, loss 0.0424784, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:04.898693: step 6384, loss 0.02254, acc 1, learning_rate 0.0001
2017-10-10T12:07:05.145348: step 6385, loss 0.145115, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:05.430177: step 6386, loss 0.0469941, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:05.688015: step 6387, loss 0.0679736, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:05.972944: step 6388, loss 0.060085, acc 1, learning_rate 0.0001
2017-10-10T12:07:06.212368: step 6389, loss 0.0584515, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:06.479800: step 6390, loss 0.115524, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:06.685528: step 6391, loss 0.0291872, acc 1, learning_rate 0.0001
2017-10-10T12:07:06.920967: step 6392, loss 0.0466417, acc 1, learning_rate 0.0001
2017-10-10T12:07:07.165078: step 6393, loss 0.0319655, acc 1, learning_rate 0.0001
2017-10-10T12:07:07.444847: step 6394, loss 0.0385895, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:07.760729: step 6395, loss 0.0330197, acc 1, learning_rate 0.0001
2017-10-10T12:07:07.947770: step 6396, loss 0.119047, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:08.116479: step 6397, loss 0.0688044, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:08.334154: step 6398, loss 0.0624422, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:08.528828: step 6399, loss 0.0298997, acc 1, learning_rate 0.0001
2017-10-10T12:07:08.726850: step 6400, loss 0.0409108, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:09.264875: step 6400, loss 0.199326, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6400

2017-10-10T12:07:10.377976: step 6401, loss 0.111215, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:10.617620: step 6402, loss 0.0757718, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:10.847910: step 6403, loss 0.093638, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:11.148676: step 6404, loss 0.148141, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:11.364956: step 6405, loss 0.0673487, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:11.565251: step 6406, loss 0.052888, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:11.812938: step 6407, loss 0.0503324, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:11.999911: step 6408, loss 0.0957176, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:12.208860: step 6409, loss 0.0511686, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:12.444858: step 6410, loss 0.07881, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:12.669159: step 6411, loss 0.0430591, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:12.926030: step 6412, loss 0.0225651, acc 1, learning_rate 0.0001
2017-10-10T12:07:13.153544: step 6413, loss 0.0776489, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:13.363378: step 6414, loss 0.0258189, acc 1, learning_rate 0.0001
2017-10-10T12:07:13.611715: step 6415, loss 0.0575903, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:13.859743: step 6416, loss 0.0653955, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:14.131699: step 6417, loss 0.0357483, acc 1, learning_rate 0.0001
2017-10-10T12:07:14.373525: step 6418, loss 0.114462, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:14.595036: step 6419, loss 0.0798485, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:14.843846: step 6420, loss 0.132952, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:15.091235: step 6421, loss 0.0361207, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:15.356946: step 6422, loss 0.0738921, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:15.624866: step 6423, loss 0.0658004, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:15.844806: step 6424, loss 0.0232175, acc 1, learning_rate 0.0001
2017-10-10T12:07:16.088377: step 6425, loss 0.124708, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:16.327005: step 6426, loss 0.0457628, acc 1, learning_rate 0.0001
2017-10-10T12:07:16.552324: step 6427, loss 0.0153117, acc 1, learning_rate 0.0001
2017-10-10T12:07:16.788021: step 6428, loss 0.0146036, acc 1, learning_rate 0.0001
2017-10-10T12:07:17.039392: step 6429, loss 0.0153183, acc 1, learning_rate 0.0001
2017-10-10T12:07:17.272863: step 6430, loss 0.0650925, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:17.540952: step 6431, loss 0.0845884, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:17.755369: step 6432, loss 0.0247192, acc 1, learning_rate 0.0001
2017-10-10T12:07:17.980858: step 6433, loss 0.021675, acc 1, learning_rate 0.0001
2017-10-10T12:07:18.329416: step 6434, loss 0.0557193, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:18.512619: step 6435, loss 0.0661649, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:18.680861: step 6436, loss 0.0436761, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:18.888362: step 6437, loss 0.0907153, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:19.061512: step 6438, loss 0.0214862, acc 1, learning_rate 0.0001
2017-10-10T12:07:19.226977: step 6439, loss 0.0412366, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:19.460851: step 6440, loss 0.0526217, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:20.029963: step 6440, loss 0.197055, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6440

2017-10-10T12:07:21.233443: step 6441, loss 0.0340425, acc 1, learning_rate 0.0001
2017-10-10T12:07:21.487270: step 6442, loss 0.0260456, acc 1, learning_rate 0.0001
2017-10-10T12:07:21.729009: step 6443, loss 0.0345528, acc 1, learning_rate 0.0001
2017-10-10T12:07:21.965515: step 6444, loss 0.04071, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:22.228878: step 6445, loss 0.0496361, acc 1, learning_rate 0.0001
2017-10-10T12:07:22.501662: step 6446, loss 0.0218493, acc 1, learning_rate 0.0001
2017-10-10T12:07:22.779734: step 6447, loss 0.0233798, acc 1, learning_rate 0.0001
2017-10-10T12:07:23.004636: step 6448, loss 0.0691405, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:23.264029: step 6449, loss 0.106907, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:23.501017: step 6450, loss 0.0838743, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:23.726233: step 6451, loss 0.0907696, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:23.947439: step 6452, loss 0.0670854, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:24.213474: step 6453, loss 0.0626162, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:24.461883: step 6454, loss 0.0505359, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:24.736439: step 6455, loss 0.0575243, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:24.979812: step 6456, loss 0.0436592, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:25.212889: step 6457, loss 0.0992623, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:25.433034: step 6458, loss 0.0144155, acc 1, learning_rate 0.0001
2017-10-10T12:07:25.719722: step 6459, loss 0.0669074, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:25.924060: step 6460, loss 0.0902282, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:26.199265: step 6461, loss 0.0728194, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:26.429995: step 6462, loss 0.117591, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:26.675655: step 6463, loss 0.0886193, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:26.917350: step 6464, loss 0.0335498, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:27.164191: step 6465, loss 0.0862788, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:27.409116: step 6466, loss 0.0207931, acc 1, learning_rate 0.0001
2017-10-10T12:07:27.664705: step 6467, loss 0.0403809, acc 1, learning_rate 0.0001
2017-10-10T12:07:27.963633: step 6468, loss 0.056906, acc 1, learning_rate 0.0001
2017-10-10T12:07:28.179775: step 6469, loss 0.051821, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:28.393202: step 6470, loss 0.0914972, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:28.588969: step 6471, loss 0.0313555, acc 1, learning_rate 0.0001
2017-10-10T12:07:28.831820: step 6472, loss 0.0811016, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:28.994168: step 6473, loss 0.0395885, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:29.140951: step 6474, loss 0.127659, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:29.363530: step 6475, loss 0.0717904, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:29.560842: step 6476, loss 0.0521631, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:29.786723: step 6477, loss 0.0578844, acc 1, learning_rate 0.0001
2017-10-10T12:07:30.016905: step 6478, loss 0.0783915, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:30.224102: step 6479, loss 0.045555, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:30.433406: step 6480, loss 0.132627, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:31.024823: step 6480, loss 0.196489, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6480

2017-10-10T12:07:32.469106: step 6481, loss 0.113156, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:32.731152: step 6482, loss 0.0529072, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:32.990456: step 6483, loss 0.040353, acc 1, learning_rate 0.0001
2017-10-10T12:07:33.235510: step 6484, loss 0.0448208, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:33.484935: step 6485, loss 0.0418033, acc 1, learning_rate 0.0001
2017-10-10T12:07:33.751430: step 6486, loss 0.0438022, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:34.009785: step 6487, loss 0.0498533, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:34.259149: step 6488, loss 0.104097, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:34.504848: step 6489, loss 0.0444218, acc 1, learning_rate 0.0001
2017-10-10T12:07:34.743928: step 6490, loss 0.0497088, acc 1, learning_rate 0.0001
2017-10-10T12:07:34.977842: step 6491, loss 0.110437, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:35.248956: step 6492, loss 0.0371679, acc 1, learning_rate 0.0001
2017-10-10T12:07:35.482168: step 6493, loss 0.0640534, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:35.705179: step 6494, loss 0.0810521, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:35.925951: step 6495, loss 0.0982731, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:36.213014: step 6496, loss 0.1609, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:36.450584: step 6497, loss 0.105378, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:36.663791: step 6498, loss 0.0368239, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:36.856112: step 6499, loss 0.0427676, acc 1, learning_rate 0.0001
2017-10-10T12:07:37.060800: step 6500, loss 0.0350625, acc 1, learning_rate 0.0001
2017-10-10T12:07:37.289725: step 6501, loss 0.0327353, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:37.516990: step 6502, loss 0.0249069, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:37.779147: step 6503, loss 0.0236322, acc 1, learning_rate 0.0001
2017-10-10T12:07:38.022854: step 6504, loss 0.0388673, acc 1, learning_rate 0.0001
2017-10-10T12:07:38.268879: step 6505, loss 0.10701, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:38.522567: step 6506, loss 0.0515064, acc 1, learning_rate 0.0001
2017-10-10T12:07:38.760906: step 6507, loss 0.0886454, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:39.094957: step 6508, loss 0.041566, acc 1, learning_rate 0.0001
2017-10-10T12:07:39.300905: step 6509, loss 0.0492127, acc 1, learning_rate 0.0001
2017-10-10T12:07:39.488809: step 6510, loss 0.0418846, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:39.658763: step 6511, loss 0.0827354, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:39.840417: step 6512, loss 0.0706117, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:40.017580: step 6513, loss 0.0601209, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:40.264849: step 6514, loss 0.0584957, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:40.496818: step 6515, loss 0.0984886, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:40.761740: step 6516, loss 0.0786341, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:41.015420: step 6517, loss 0.148492, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:41.260803: step 6518, loss 0.0908361, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:41.531283: step 6519, loss 0.022553, acc 1, learning_rate 0.0001
2017-10-10T12:07:41.771462: step 6520, loss 0.0370748, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:42.369569: step 6520, loss 0.197997, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6520

2017-10-10T12:07:43.508847: step 6521, loss 0.124487, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:43.795054: step 6522, loss 0.110882, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:44.035985: step 6523, loss 0.0195468, acc 1, learning_rate 0.0001
2017-10-10T12:07:44.302009: step 6524, loss 0.102084, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:44.478179: step 6525, loss 0.0444693, acc 1, learning_rate 0.0001
2017-10-10T12:07:44.676839: step 6526, loss 0.143636, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:44.874414: step 6527, loss 0.0586637, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:45.069886: step 6528, loss 0.0332255, acc 1, learning_rate 0.0001
2017-10-10T12:07:45.264950: step 6529, loss 0.0566192, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:45.519820: step 6530, loss 0.0310008, acc 1, learning_rate 0.0001
2017-10-10T12:07:45.780802: step 6531, loss 0.117055, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:46.023079: step 6532, loss 0.0857734, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:46.260172: step 6533, loss 0.0684322, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:46.529341: step 6534, loss 0.0354913, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:46.761956: step 6535, loss 0.0306222, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:47.031362: step 6536, loss 0.0504611, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:47.276749: step 6537, loss 0.0810221, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:47.536450: step 6538, loss 0.0478741, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:47.780473: step 6539, loss 0.0291352, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:48.022481: step 6540, loss 0.0522604, acc 1, learning_rate 0.0001
2017-10-10T12:07:48.288845: step 6541, loss 0.0356897, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:48.533035: step 6542, loss 0.0261406, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:48.791090: step 6543, loss 0.0534827, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:49.045999: step 6544, loss 0.109064, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:49.263011: step 6545, loss 0.0629516, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:49.522598: step 6546, loss 0.0273372, acc 1, learning_rate 0.0001
2017-10-10T12:07:49.807725: step 6547, loss 0.0451006, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:50.037341: step 6548, loss 0.0596587, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:50.252350: step 6549, loss 0.0421508, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:50.432337: step 6550, loss 0.0383538, acc 1, learning_rate 0.0001
2017-10-10T12:07:50.625682: step 6551, loss 0.0865341, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:50.806162: step 6552, loss 0.0596136, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:50.982961: step 6553, loss 0.0753934, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:51.219522: step 6554, loss 0.0338667, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:51.467537: step 6555, loss 0.164945, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:51.717988: step 6556, loss 0.0307523, acc 1, learning_rate 0.0001
2017-10-10T12:07:51.985817: step 6557, loss 0.040874, acc 1, learning_rate 0.0001
2017-10-10T12:07:52.252893: step 6558, loss 0.0660258, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:52.553228: step 6559, loss 0.0214227, acc 1, learning_rate 0.0001
2017-10-10T12:07:52.749744: step 6560, loss 0.0468065, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:07:53.196693: step 6560, loss 0.199391, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6560

2017-10-10T12:07:54.273105: step 6561, loss 0.0983966, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:54.516765: step 6562, loss 0.161172, acc 0.921875, learning_rate 0.0001
2017-10-10T12:07:54.767087: step 6563, loss 0.0791645, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:55.032196: step 6564, loss 0.145886, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:55.329671: step 6565, loss 0.0366793, acc 1, learning_rate 0.0001
2017-10-10T12:07:55.532813: step 6566, loss 0.0743775, acc 0.960784, learning_rate 0.0001
2017-10-10T12:07:55.768863: step 6567, loss 0.115158, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:56.028896: step 6568, loss 0.111971, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:56.260852: step 6569, loss 0.0377876, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:56.498452: step 6570, loss 0.0369256, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:56.733533: step 6571, loss 0.100191, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:56.981521: step 6572, loss 0.0573397, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:57.232361: step 6573, loss 0.103501, acc 0.9375, learning_rate 0.0001
2017-10-10T12:07:57.434839: step 6574, loss 0.0339653, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:57.678486: step 6575, loss 0.00983004, acc 1, learning_rate 0.0001
2017-10-10T12:07:57.922912: step 6576, loss 0.050657, acc 0.984375, learning_rate 0.0001
2017-10-10T12:07:58.176878: step 6577, loss 0.0992143, acc 0.953125, learning_rate 0.0001
2017-10-10T12:07:58.416944: step 6578, loss 0.079172, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:58.645773: step 6579, loss 0.0279505, acc 1, learning_rate 0.0001
2017-10-10T12:07:58.903032: step 6580, loss 0.0407896, acc 1, learning_rate 0.0001
2017-10-10T12:07:59.134017: step 6581, loss 0.0780051, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:59.390204: step 6582, loss 0.0820953, acc 0.96875, learning_rate 0.0001
2017-10-10T12:07:59.637983: step 6583, loss 0.0330423, acc 1, learning_rate 0.0001
2017-10-10T12:07:59.865023: step 6584, loss 0.0589348, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:00.164920: step 6585, loss 0.0573411, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:00.404897: step 6586, loss 0.0922595, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:00.586565: step 6587, loss 0.0584336, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:00.761519: step 6588, loss 0.0851112, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:01.041255: step 6589, loss 0.0658184, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:01.182991: step 6590, loss 0.0872723, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:01.353930: step 6591, loss 0.0505818, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:01.567478: step 6592, loss 0.0402846, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:01.771278: step 6593, loss 0.0476666, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:02.028524: step 6594, loss 0.105165, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:02.276938: step 6595, loss 0.058026, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:02.533771: step 6596, loss 0.0865033, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:02.773765: step 6597, loss 0.053644, acc 1, learning_rate 0.0001
2017-10-10T12:08:03.041184: step 6598, loss 0.11628, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:03.257167: step 6599, loss 0.0627171, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:03.485202: step 6600, loss 0.0860467, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:04.128939: step 6600, loss 0.195969, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6600

2017-10-10T12:08:05.329712: step 6601, loss 0.0586869, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:05.536606: step 6602, loss 0.0651648, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:05.777262: step 6603, loss 0.0187309, acc 1, learning_rate 0.0001
2017-10-10T12:08:06.039242: step 6604, loss 0.0881011, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:06.269865: step 6605, loss 0.0531109, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:06.505038: step 6606, loss 0.0983884, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:06.735163: step 6607, loss 0.0265842, acc 1, learning_rate 0.0001
2017-10-10T12:08:06.969059: step 6608, loss 0.0446891, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:07.203958: step 6609, loss 0.0319328, acc 1, learning_rate 0.0001
2017-10-10T12:08:07.466503: step 6610, loss 0.115797, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:07.735539: step 6611, loss 0.03919, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:08.004430: step 6612, loss 0.0446735, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:08.251710: step 6613, loss 0.0602577, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:08.516228: step 6614, loss 0.0992149, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:08.769512: step 6615, loss 0.0820057, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:09.002002: step 6616, loss 0.0491064, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:09.322628: step 6617, loss 0.0477943, acc 1, learning_rate 0.0001
2017-10-10T12:08:09.520864: step 6618, loss 0.0342893, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:09.705091: step 6619, loss 0.0508445, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:09.894549: step 6620, loss 0.0288526, acc 1, learning_rate 0.0001
2017-10-10T12:08:10.088855: step 6621, loss 0.0309115, acc 1, learning_rate 0.0001
2017-10-10T12:08:10.400848: step 6622, loss 0.0675627, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:10.626595: step 6623, loss 0.0575918, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:10.810147: step 6624, loss 0.0777771, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:10.987619: step 6625, loss 0.0806423, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:11.181214: step 6626, loss 0.0920043, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:11.377258: step 6627, loss 0.0445818, acc 1, learning_rate 0.0001
2017-10-10T12:08:11.656853: step 6628, loss 0.107794, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:11.948333: step 6629, loss 0.0275625, acc 1, learning_rate 0.0001
2017-10-10T12:08:12.197180: step 6630, loss 0.0386315, acc 1, learning_rate 0.0001
2017-10-10T12:08:12.424350: step 6631, loss 0.0969558, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:12.732960: step 6632, loss 0.0581154, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:12.988839: step 6633, loss 0.069745, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:13.215635: step 6634, loss 0.0676249, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:13.451900: step 6635, loss 0.0914633, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:13.708106: step 6636, loss 0.0924802, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:13.963633: step 6637, loss 0.0634382, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:14.229329: step 6638, loss 0.101213, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:14.489308: step 6639, loss 0.0148582, acc 1, learning_rate 0.0001
2017-10-10T12:08:14.757089: step 6640, loss 0.0278508, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:15.392853: step 6640, loss 0.196949, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6640

2017-10-10T12:08:16.374174: step 6641, loss 0.0582838, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:16.620398: step 6642, loss 0.0592716, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:16.853383: step 6643, loss 0.118034, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:17.122567: step 6644, loss 0.029495, acc 1, learning_rate 0.0001
2017-10-10T12:08:17.404857: step 6645, loss 0.0673811, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:17.696141: step 6646, loss 0.0206186, acc 1, learning_rate 0.0001
2017-10-10T12:08:17.922252: step 6647, loss 0.152636, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:18.116678: step 6648, loss 0.044132, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:18.308237: step 6649, loss 0.082622, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:18.496839: step 6650, loss 0.123682, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:18.681267: step 6651, loss 0.0466755, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:18.981019: step 6652, loss 0.0203481, acc 1, learning_rate 0.0001
2017-10-10T12:08:19.220543: step 6653, loss 0.0568889, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:19.469253: step 6654, loss 0.049814, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:19.720880: step 6655, loss 0.041317, acc 1, learning_rate 0.0001
2017-10-10T12:08:19.979349: step 6656, loss 0.0429698, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:20.262398: step 6657, loss 0.0278149, acc 1, learning_rate 0.0001
2017-10-10T12:08:20.579954: step 6658, loss 0.0326513, acc 1, learning_rate 0.0001
2017-10-10T12:08:20.826471: step 6659, loss 0.0491901, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:21.015834: step 6660, loss 0.0274117, acc 1, learning_rate 0.0001
2017-10-10T12:08:21.208745: step 6661, loss 0.0554831, acc 1, learning_rate 0.0001
2017-10-10T12:08:21.389121: step 6662, loss 0.038707, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:21.549877: step 6663, loss 0.078285, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:21.697168: step 6664, loss 0.0592035, acc 0.980392, learning_rate 0.0001
2017-10-10T12:08:21.928883: step 6665, loss 0.100507, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:22.137423: step 6666, loss 0.0529432, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:22.409105: step 6667, loss 0.0218973, acc 1, learning_rate 0.0001
2017-10-10T12:08:22.646620: step 6668, loss 0.126727, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:22.893062: step 6669, loss 0.0585333, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:23.187876: step 6670, loss 0.0513927, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:23.419301: step 6671, loss 0.0918759, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:23.675820: step 6672, loss 0.0721509, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:23.909171: step 6673, loss 0.0341021, acc 1, learning_rate 0.0001
2017-10-10T12:08:24.179454: step 6674, loss 0.0993325, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:24.432821: step 6675, loss 0.039404, acc 1, learning_rate 0.0001
2017-10-10T12:08:24.682851: step 6676, loss 0.0227293, acc 1, learning_rate 0.0001
2017-10-10T12:08:25.004297: step 6677, loss 0.0716571, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:25.222132: step 6678, loss 0.0595378, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:25.464707: step 6679, loss 0.0473354, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:25.718947: step 6680, loss 0.0391609, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:26.336955: step 6680, loss 0.201173, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6680

2017-10-10T12:08:27.431324: step 6681, loss 0.0424857, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:27.666861: step 6682, loss 0.160817, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:27.903793: step 6683, loss 0.100415, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:28.172893: step 6684, loss 0.108382, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:28.379592: step 6685, loss 0.0752853, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:28.630297: step 6686, loss 0.126168, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:28.902347: step 6687, loss 0.0774971, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:29.176858: step 6688, loss 0.0735382, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:29.420840: step 6689, loss 0.0469458, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:29.682498: step 6690, loss 0.0772764, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:29.952868: step 6691, loss 0.0462352, acc 1, learning_rate 0.0001
2017-10-10T12:08:30.202674: step 6692, loss 0.0666489, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:30.470698: step 6693, loss 0.0203135, acc 1, learning_rate 0.0001
2017-10-10T12:08:30.718728: step 6694, loss 0.0247051, acc 1, learning_rate 0.0001
2017-10-10T12:08:30.957601: step 6695, loss 0.0497264, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:31.292205: step 6696, loss 0.0528776, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:31.483466: step 6697, loss 0.0813379, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:31.665763: step 6698, loss 0.0242402, acc 1, learning_rate 0.0001
2017-10-10T12:08:31.845945: step 6699, loss 0.0252332, acc 1, learning_rate 0.0001
2017-10-10T12:08:32.035301: step 6700, loss 0.0548974, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:32.285851: step 6701, loss 0.0963477, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:32.528867: step 6702, loss 0.0574993, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:32.799196: step 6703, loss 0.0237367, acc 1, learning_rate 0.0001
2017-10-10T12:08:33.032848: step 6704, loss 0.0941946, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:33.240881: step 6705, loss 0.0321256, acc 1, learning_rate 0.0001
2017-10-10T12:08:33.476971: step 6706, loss 0.0494616, acc 1, learning_rate 0.0001
2017-10-10T12:08:33.740866: step 6707, loss 0.0198625, acc 1, learning_rate 0.0001
2017-10-10T12:08:33.980953: step 6708, loss 0.0373986, acc 1, learning_rate 0.0001
2017-10-10T12:08:34.191150: step 6709, loss 0.0409353, acc 1, learning_rate 0.0001
2017-10-10T12:08:34.498425: step 6710, loss 0.0315492, acc 1, learning_rate 0.0001
2017-10-10T12:08:34.721778: step 6711, loss 0.0223298, acc 1, learning_rate 0.0001
2017-10-10T12:08:34.939868: step 6712, loss 0.049184, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:35.142054: step 6713, loss 0.0333782, acc 1, learning_rate 0.0001
2017-10-10T12:08:35.332635: step 6714, loss 0.103775, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:35.532502: step 6715, loss 0.064048, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:35.780976: step 6716, loss 0.0500864, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:36.025518: step 6717, loss 0.0526599, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:36.250691: step 6718, loss 0.0475072, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:36.490676: step 6719, loss 0.0142594, acc 1, learning_rate 0.0001
2017-10-10T12:08:36.732000: step 6720, loss 0.0336614, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:37.339689: step 6720, loss 0.199285, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6720

2017-10-10T12:08:38.594443: step 6721, loss 0.0418442, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:38.871433: step 6722, loss 0.0267261, acc 1, learning_rate 0.0001
2017-10-10T12:08:39.108831: step 6723, loss 0.066307, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:39.326151: step 6724, loss 0.172173, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:39.568872: step 6725, loss 0.0195141, acc 1, learning_rate 0.0001
2017-10-10T12:08:39.800845: step 6726, loss 0.0471218, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:40.019106: step 6727, loss 0.0386731, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:40.244840: step 6728, loss 0.0612524, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:40.509013: step 6729, loss 0.0352542, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:40.765153: step 6730, loss 0.127364, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:41.028868: step 6731, loss 0.0797267, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:41.252866: step 6732, loss 0.037037, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:41.581148: step 6733, loss 0.123959, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:41.803755: step 6734, loss 0.0356026, acc 1, learning_rate 0.0001
2017-10-10T12:08:41.984416: step 6735, loss 0.115628, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:42.204118: step 6736, loss 0.137501, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:42.374053: step 6737, loss 0.151107, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:42.592886: step 6738, loss 0.060706, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:42.832833: step 6739, loss 0.0291943, acc 1, learning_rate 0.0001
2017-10-10T12:08:43.044840: step 6740, loss 0.0929405, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:43.239425: step 6741, loss 0.0316002, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:43.480787: step 6742, loss 0.0368566, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:43.682116: step 6743, loss 0.0315211, acc 1, learning_rate 0.0001
2017-10-10T12:08:43.951431: step 6744, loss 0.0845215, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:44.211983: step 6745, loss 0.0218504, acc 1, learning_rate 0.0001
2017-10-10T12:08:44.440674: step 6746, loss 0.0782058, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:44.689246: step 6747, loss 0.132238, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:44.937091: step 6748, loss 0.0404924, acc 1, learning_rate 0.0001
2017-10-10T12:08:45.182907: step 6749, loss 0.0670146, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:45.443227: step 6750, loss 0.0897699, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:45.678991: step 6751, loss 0.130811, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:45.936609: step 6752, loss 0.0450105, acc 1, learning_rate 0.0001
2017-10-10T12:08:46.213812: step 6753, loss 0.0127536, acc 1, learning_rate 0.0001
2017-10-10T12:08:46.444144: step 6754, loss 0.0500002, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:46.724821: step 6755, loss 0.0169796, acc 1, learning_rate 0.0001
2017-10-10T12:08:46.955031: step 6756, loss 0.0603651, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:47.167290: step 6757, loss 0.133079, acc 0.953125, learning_rate 0.0001
2017-10-10T12:08:47.431016: step 6758, loss 0.0948949, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:47.695633: step 6759, loss 0.101026, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:47.941475: step 6760, loss 0.120803, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:48.609170: step 6760, loss 0.196464, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6760

2017-10-10T12:08:49.555082: step 6761, loss 0.0614339, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:49.752986: step 6762, loss 0.0793021, acc 0.960784, learning_rate 0.0001
2017-10-10T12:08:49.991400: step 6763, loss 0.0550032, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:50.241187: step 6764, loss 0.0414915, acc 1, learning_rate 0.0001
2017-10-10T12:08:50.484692: step 6765, loss 0.0857031, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:50.729751: step 6766, loss 0.0893756, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:51.037909: step 6767, loss 0.04835, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:51.249679: step 6768, loss 0.0995972, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:51.472955: step 6769, loss 0.073968, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:51.720577: step 6770, loss 0.0731113, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:51.960772: step 6771, loss 0.0463434, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:52.099524: step 6772, loss 0.115566, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:52.256959: step 6773, loss 0.0447082, acc 1, learning_rate 0.0001
2017-10-10T12:08:52.456573: step 6774, loss 0.0937479, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:52.640934: step 6775, loss 0.0340029, acc 1, learning_rate 0.0001
2017-10-10T12:08:52.839941: step 6776, loss 0.127337, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:53.032113: step 6777, loss 0.0593033, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:53.253453: step 6778, loss 0.0200798, acc 1, learning_rate 0.0001
2017-10-10T12:08:53.949067: step 6779, loss 0.0285471, acc 1, learning_rate 0.0001
2017-10-10T12:08:54.206893: step 6780, loss 0.0910774, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:54.444303: step 6781, loss 0.0372266, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:54.664820: step 6782, loss 0.0203715, acc 1, learning_rate 0.0001
2017-10-10T12:08:54.876358: step 6783, loss 0.0705274, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:55.124982: step 6784, loss 0.00917944, acc 1, learning_rate 0.0001
2017-10-10T12:08:55.405019: step 6785, loss 0.069475, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:55.665144: step 6786, loss 0.0471651, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:55.864198: step 6787, loss 0.0352184, acc 1, learning_rate 0.0001
2017-10-10T12:08:56.106876: step 6788, loss 0.0517239, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:56.336387: step 6789, loss 0.097467, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:56.586173: step 6790, loss 0.051527, acc 1, learning_rate 0.0001
2017-10-10T12:08:56.815616: step 6791, loss 0.0882579, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:57.069066: step 6792, loss 0.0972514, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:57.344946: step 6793, loss 0.149985, acc 0.9375, learning_rate 0.0001
2017-10-10T12:08:57.582596: step 6794, loss 0.0980255, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:57.809895: step 6795, loss 0.121636, acc 0.96875, learning_rate 0.0001
2017-10-10T12:08:58.048864: step 6796, loss 0.0337191, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:58.326165: step 6797, loss 0.0621663, acc 1, learning_rate 0.0001
2017-10-10T12:08:58.591156: step 6798, loss 0.0430647, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:58.843376: step 6799, loss 0.0788641, acc 0.984375, learning_rate 0.0001
2017-10-10T12:08:59.091839: step 6800, loss 0.0530938, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:08:59.746193: step 6800, loss 0.197673, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6800

2017-10-10T12:09:00.905034: step 6801, loss 0.0248782, acc 1, learning_rate 0.0001
2017-10-10T12:09:01.144860: step 6802, loss 0.0845633, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:01.411228: step 6803, loss 0.0365002, acc 1, learning_rate 0.0001
2017-10-10T12:09:01.739533: step 6804, loss 0.0885393, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:02.022332: step 6805, loss 0.0220123, acc 1, learning_rate 0.0001
2017-10-10T12:09:02.212611: step 6806, loss 0.0941228, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:02.388521: step 6807, loss 0.0538651, acc 1, learning_rate 0.0001
2017-10-10T12:09:02.564970: step 6808, loss 0.0788029, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:02.754301: step 6809, loss 0.0840203, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:02.995876: step 6810, loss 0.0696818, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:03.265275: step 6811, loss 0.0214468, acc 1, learning_rate 0.0001
2017-10-10T12:09:03.513110: step 6812, loss 0.105496, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:03.706782: step 6813, loss 0.0368071, acc 1, learning_rate 0.0001
2017-10-10T12:09:03.955432: step 6814, loss 0.0539588, acc 1, learning_rate 0.0001
2017-10-10T12:09:04.218931: step 6815, loss 0.0624933, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:04.445507: step 6816, loss 0.0369352, acc 1, learning_rate 0.0001
2017-10-10T12:09:04.680351: step 6817, loss 0.0647462, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:04.930223: step 6818, loss 0.0346389, acc 1, learning_rate 0.0001
2017-10-10T12:09:05.156571: step 6819, loss 0.0250174, acc 1, learning_rate 0.0001
2017-10-10T12:09:05.391421: step 6820, loss 0.0110902, acc 1, learning_rate 0.0001
2017-10-10T12:09:05.658706: step 6821, loss 0.0731435, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:05.879925: step 6822, loss 0.0243649, acc 1, learning_rate 0.0001
2017-10-10T12:09:06.124867: step 6823, loss 0.0393603, acc 1, learning_rate 0.0001
2017-10-10T12:09:06.381671: step 6824, loss 0.0640786, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:06.636824: step 6825, loss 0.0296453, acc 1, learning_rate 0.0001
2017-10-10T12:09:06.867655: step 6826, loss 0.078844, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:07.110910: step 6827, loss 0.0823951, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:07.345702: step 6828, loss 0.0233315, acc 1, learning_rate 0.0001
2017-10-10T12:09:07.611911: step 6829, loss 0.0293709, acc 1, learning_rate 0.0001
2017-10-10T12:09:07.884891: step 6830, loss 0.0266633, acc 1, learning_rate 0.0001
2017-10-10T12:09:08.200886: step 6831, loss 0.0461152, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:08.431601: step 6832, loss 0.0512614, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:08.651985: step 6833, loss 0.124064, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:08.863675: step 6834, loss 0.0691046, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:09.070658: step 6835, loss 0.0439562, acc 1, learning_rate 0.0001
2017-10-10T12:09:09.257062: step 6836, loss 0.0360525, acc 1, learning_rate 0.0001
2017-10-10T12:09:09.504639: step 6837, loss 0.0963486, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:09.748853: step 6838, loss 0.0526009, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:09.988905: step 6839, loss 0.0370185, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:10.221946: step 6840, loss 0.132206, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:10.866435: step 6840, loss 0.195187, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6840

2017-10-10T12:09:12.204858: step 6841, loss 0.129647, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:12.429956: step 6842, loss 0.0234065, acc 1, learning_rate 0.0001
2017-10-10T12:09:12.605086: step 6843, loss 0.0910076, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:12.822275: step 6844, loss 0.0456748, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:12.995525: step 6845, loss 0.133367, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:13.184940: step 6846, loss 0.10286, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:13.385131: step 6847, loss 0.0254052, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:13.656866: step 6848, loss 0.0601722, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:13.906013: step 6849, loss 0.0637648, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:14.188860: step 6850, loss 0.123194, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:14.428867: step 6851, loss 0.0426864, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:14.648988: step 6852, loss 0.0497972, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:14.855828: step 6853, loss 0.110304, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:15.077054: step 6854, loss 0.109359, acc 0.921875, learning_rate 0.0001
2017-10-10T12:09:15.342548: step 6855, loss 0.111677, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:15.584723: step 6856, loss 0.0522863, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:15.850147: step 6857, loss 0.0781019, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:16.108796: step 6858, loss 0.0933003, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:16.396891: step 6859, loss 0.0521384, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:16.591377: step 6860, loss 0.0699829, acc 0.980392, learning_rate 0.0001
2017-10-10T12:09:16.771374: step 6861, loss 0.0281203, acc 1, learning_rate 0.0001
2017-10-10T12:09:16.960090: step 6862, loss 0.076887, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:17.164866: step 6863, loss 0.0714567, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:17.392823: step 6864, loss 0.105263, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:17.572135: step 6865, loss 0.109591, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:17.748069: step 6866, loss 0.0395078, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:17.920390: step 6867, loss 0.13176, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:18.131611: step 6868, loss 0.0435335, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:18.354321: step 6869, loss 0.0738042, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:18.574255: step 6870, loss 0.0339896, acc 1, learning_rate 0.0001
2017-10-10T12:09:18.838689: step 6871, loss 0.0416527, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:19.109575: step 6872, loss 0.0707705, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:19.345203: step 6873, loss 0.0443418, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:19.583040: step 6874, loss 0.0767604, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:19.773471: step 6875, loss 0.100353, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:20.005092: step 6876, loss 0.0767258, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:20.299251: step 6877, loss 0.0484148, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:20.517036: step 6878, loss 0.091828, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:20.768857: step 6879, loss 0.175359, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:21.021555: step 6880, loss 0.0812778, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:21.609037: step 6880, loss 0.194782, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6880

2017-10-10T12:09:22.675206: step 6881, loss 0.0274299, acc 1, learning_rate 0.0001
2017-10-10T12:09:22.877892: step 6882, loss 0.0657611, acc 1, learning_rate 0.0001
2017-10-10T12:09:23.061813: step 6883, loss 0.0517333, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:23.236373: step 6884, loss 0.0578583, acc 1, learning_rate 0.0001
2017-10-10T12:09:23.408917: step 6885, loss 0.0977439, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:23.600828: step 6886, loss 0.0883712, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:23.778719: step 6887, loss 0.0707562, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:24.031354: step 6888, loss 0.1313, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:24.287621: step 6889, loss 0.0891599, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:24.545758: step 6890, loss 0.130528, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:24.817077: step 6891, loss 0.0433335, acc 1, learning_rate 0.0001
2017-10-10T12:09:25.114890: step 6892, loss 0.0219133, acc 1, learning_rate 0.0001
2017-10-10T12:09:25.360186: step 6893, loss 0.0374144, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:25.567305: step 6894, loss 0.0488478, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:25.770326: step 6895, loss 0.0135355, acc 1, learning_rate 0.0001
2017-10-10T12:09:26.026948: step 6896, loss 0.019586, acc 1, learning_rate 0.0001
2017-10-10T12:09:26.248367: step 6897, loss 0.0632719, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:26.462043: step 6898, loss 0.0196, acc 1, learning_rate 0.0001
2017-10-10T12:09:26.719291: step 6899, loss 0.0592955, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:26.950059: step 6900, loss 0.0426544, acc 1, learning_rate 0.0001
2017-10-10T12:09:27.180896: step 6901, loss 0.0914783, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:27.451959: step 6902, loss 0.0603341, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:27.712127: step 6903, loss 0.10853, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:27.974193: step 6904, loss 0.0167195, acc 1, learning_rate 0.0001
2017-10-10T12:09:28.220868: step 6905, loss 0.0918708, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:28.467280: step 6906, loss 0.0464637, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:28.683478: step 6907, loss 0.0420486, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:28.923096: step 6908, loss 0.0820061, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:29.185044: step 6909, loss 0.0790588, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:29.463453: step 6910, loss 0.0677098, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:29.711167: step 6911, loss 0.0502734, acc 1, learning_rate 0.0001
2017-10-10T12:09:29.968826: step 6912, loss 0.0638104, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:30.253238: step 6913, loss 0.0882097, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:30.466768: step 6914, loss 0.0247349, acc 1, learning_rate 0.0001
2017-10-10T12:09:30.717487: step 6915, loss 0.099944, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:30.932705: step 6916, loss 0.0423542, acc 1, learning_rate 0.0001
2017-10-10T12:09:31.160883: step 6917, loss 0.0244218, acc 1, learning_rate 0.0001
2017-10-10T12:09:31.399484: step 6918, loss 0.0387382, acc 1, learning_rate 0.0001
2017-10-10T12:09:31.637100: step 6919, loss 0.0169993, acc 1, learning_rate 0.0001
2017-10-10T12:09:31.917827: step 6920, loss 0.0610606, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:32.551303: step 6920, loss 0.195653, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6920

2017-10-10T12:09:33.652795: step 6921, loss 0.0323499, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:33.792074: step 6922, loss 0.103444, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:33.942081: step 6923, loss 0.059215, acc 1, learning_rate 0.0001
2017-10-10T12:09:34.080701: step 6924, loss 0.0419513, acc 1, learning_rate 0.0001
2017-10-10T12:09:34.250055: step 6925, loss 0.101361, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:34.471754: step 6926, loss 0.083991, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:34.686455: step 6927, loss 0.086031, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:34.898880: step 6928, loss 0.0821933, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:35.148886: step 6929, loss 0.0365763, acc 1, learning_rate 0.0001
2017-10-10T12:09:35.410143: step 6930, loss 0.0566713, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:35.665509: step 6931, loss 0.0609582, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:35.904025: step 6932, loss 0.0388842, acc 1, learning_rate 0.0001
2017-10-10T12:09:36.107328: step 6933, loss 0.0499813, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:36.316838: step 6934, loss 0.0777208, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:36.564840: step 6935, loss 0.0891785, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:36.772862: step 6936, loss 0.0949949, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:37.041980: step 6937, loss 0.107688, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:37.294853: step 6938, loss 0.0250511, acc 1, learning_rate 0.0001
2017-10-10T12:09:37.556902: step 6939, loss 0.0505594, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:37.807737: step 6940, loss 0.0317553, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:38.057100: step 6941, loss 0.0989851, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:38.335900: step 6942, loss 0.0554461, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:38.561619: step 6943, loss 0.101796, acc 0.9375, learning_rate 0.0001
2017-10-10T12:09:38.800824: step 6944, loss 0.0969765, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:39.019705: step 6945, loss 0.0903263, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:39.279999: step 6946, loss 0.0562047, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:39.517861: step 6947, loss 0.0644051, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:39.732793: step 6948, loss 0.0205812, acc 1, learning_rate 0.0001
2017-10-10T12:09:40.001424: step 6949, loss 0.116464, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:40.211586: step 6950, loss 0.0452485, acc 1, learning_rate 0.0001
2017-10-10T12:09:40.443902: step 6951, loss 0.063045, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:40.669864: step 6952, loss 0.0751936, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:40.936738: step 6953, loss 0.0551221, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:41.192430: step 6954, loss 0.088471, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:41.425175: step 6955, loss 0.049537, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:41.720933: step 6956, loss 0.0365354, acc 1, learning_rate 0.0001
2017-10-10T12:09:41.996995: step 6957, loss 0.0483916, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:42.172922: step 6958, loss 0.0160687, acc 1, learning_rate 0.0001
2017-10-10T12:09:42.387072: step 6959, loss 0.0309461, acc 1, learning_rate 0.0001
2017-10-10T12:09:42.589819: step 6960, loss 0.0646802, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:43.135536: step 6960, loss 0.195803, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-6960

2017-10-10T12:09:44.341612: step 6961, loss 0.0336063, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:44.522406: step 6962, loss 0.0405358, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:44.777491: step 6963, loss 0.0211922, acc 1, learning_rate 0.0001
2017-10-10T12:09:44.985756: step 6964, loss 0.0500133, acc 1, learning_rate 0.0001
2017-10-10T12:09:45.226287: step 6965, loss 0.0821336, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:45.509437: step 6966, loss 0.0674146, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:45.755599: step 6967, loss 0.0593816, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:45.976858: step 6968, loss 0.0188513, acc 1, learning_rate 0.0001
2017-10-10T12:09:46.235477: step 6969, loss 0.0837267, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:46.472357: step 6970, loss 0.0889889, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:46.751424: step 6971, loss 0.0262082, acc 1, learning_rate 0.0001
2017-10-10T12:09:47.000685: step 6972, loss 0.0630156, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:47.226422: step 6973, loss 0.0397862, acc 1, learning_rate 0.0001
2017-10-10T12:09:47.473122: step 6974, loss 0.0118588, acc 1, learning_rate 0.0001
2017-10-10T12:09:47.712134: step 6975, loss 0.0974315, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:47.927012: step 6976, loss 0.0217509, acc 1, learning_rate 0.0001
2017-10-10T12:09:48.184608: step 6977, loss 0.0933329, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:48.424821: step 6978, loss 0.0405519, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:48.661100: step 6979, loss 0.0532839, acc 1, learning_rate 0.0001
2017-10-10T12:09:48.926768: step 6980, loss 0.0645158, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:49.204853: step 6981, loss 0.0580921, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:49.456848: step 6982, loss 0.0652802, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:49.696989: step 6983, loss 0.0719808, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:49.924890: step 6984, loss 0.0449219, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:50.262792: step 6985, loss 0.0950597, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:50.456186: step 6986, loss 0.0172364, acc 1, learning_rate 0.0001
2017-10-10T12:09:50.662966: step 6987, loss 0.0468455, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:50.859093: step 6988, loss 0.0619598, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:51.048910: step 6989, loss 0.0949118, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:51.313007: step 6990, loss 0.106883, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:51.608866: step 6991, loss 0.0414765, acc 1, learning_rate 0.0001
2017-10-10T12:09:51.855045: step 6992, loss 0.0867064, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:52.112487: step 6993, loss 0.0244921, acc 1, learning_rate 0.0001
2017-10-10T12:09:52.361511: step 6994, loss 0.0699581, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:52.630857: step 6995, loss 0.0585361, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:52.883064: step 6996, loss 0.058664, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:53.115497: step 6997, loss 0.0667706, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:53.364868: step 6998, loss 0.0339925, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:53.593972: step 6999, loss 0.134395, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:53.857851: step 7000, loss 0.0919582, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:09:54.668957: step 7000, loss 0.199539, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7000

2017-10-10T12:09:55.524924: step 7001, loss 0.109195, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:55.756593: step 7002, loss 0.0251216, acc 1, learning_rate 0.0001
2017-10-10T12:09:55.985416: step 7003, loss 0.101321, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:56.244914: step 7004, loss 0.0879217, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:56.494224: step 7005, loss 0.0269683, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:56.755737: step 7006, loss 0.0705484, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:57.002029: step 7007, loss 0.0197992, acc 1, learning_rate 0.0001
2017-10-10T12:09:57.225051: step 7008, loss 0.0454676, acc 1, learning_rate 0.0001
2017-10-10T12:09:57.467818: step 7009, loss 0.0538876, acc 1, learning_rate 0.0001
2017-10-10T12:09:57.710925: step 7010, loss 0.125317, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:57.996666: step 7011, loss 0.104272, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:58.219474: step 7012, loss 0.0720315, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:58.427369: step 7013, loss 0.0422247, acc 1, learning_rate 0.0001
2017-10-10T12:09:58.625947: step 7014, loss 0.161821, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:58.827080: step 7015, loss 0.0366341, acc 1, learning_rate 0.0001
2017-10-10T12:09:59.037081: step 7016, loss 0.0618534, acc 0.96875, learning_rate 0.0001
2017-10-10T12:09:59.312936: step 7017, loss 0.0548425, acc 0.984375, learning_rate 0.0001
2017-10-10T12:09:59.553150: step 7018, loss 0.102209, acc 0.953125, learning_rate 0.0001
2017-10-10T12:09:59.769928: step 7019, loss 0.0888172, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:00.014653: step 7020, loss 0.0576824, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:00.305151: step 7021, loss 0.0603107, acc 1, learning_rate 0.0001
2017-10-10T12:10:00.545346: step 7022, loss 0.0443498, acc 1, learning_rate 0.0001
2017-10-10T12:10:00.800856: step 7023, loss 0.0437513, acc 1, learning_rate 0.0001
2017-10-10T12:10:01.016779: step 7024, loss 0.101463, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:01.240324: step 7025, loss 0.0589037, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:01.466923: step 7026, loss 0.0642804, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:01.677373: step 7027, loss 0.117429, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:01.912964: step 7028, loss 0.165739, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:02.168833: step 7029, loss 0.0541442, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:02.403155: step 7030, loss 0.0589688, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:02.635615: step 7031, loss 0.0671671, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:02.902072: step 7032, loss 0.0367125, acc 1, learning_rate 0.0001
2017-10-10T12:10:03.156348: step 7033, loss 0.0625498, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:03.418534: step 7034, loss 0.171912, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:03.664836: step 7035, loss 0.0542879, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:03.880629: step 7036, loss 0.0700903, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:04.143020: step 7037, loss 0.0358497, acc 1, learning_rate 0.0001
2017-10-10T12:10:04.374872: step 7038, loss 0.0472825, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:04.644954: step 7039, loss 0.0785444, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:04.921522: step 7040, loss 0.0318456, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:05.429973: step 7040, loss 0.194364, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7040

2017-10-10T12:10:06.429186: step 7041, loss 0.049943, acc 1, learning_rate 0.0001
2017-10-10T12:10:06.624336: step 7042, loss 0.0706858, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:06.823780: step 7043, loss 0.0188802, acc 1, learning_rate 0.0001
2017-10-10T12:10:07.041597: step 7044, loss 0.0729817, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:07.231433: step 7045, loss 0.110767, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:07.448840: step 7046, loss 0.0260065, acc 1, learning_rate 0.0001
2017-10-10T12:10:07.676192: step 7047, loss 0.103285, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:07.928935: step 7048, loss 0.0386763, acc 1, learning_rate 0.0001
2017-10-10T12:10:08.193452: step 7049, loss 0.0717916, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:08.411642: step 7050, loss 0.0185719, acc 1, learning_rate 0.0001
2017-10-10T12:10:08.671643: step 7051, loss 0.135381, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:08.908824: step 7052, loss 0.0213219, acc 1, learning_rate 0.0001
2017-10-10T12:10:09.130848: step 7053, loss 0.0630059, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:09.368139: step 7054, loss 0.0606254, acc 1, learning_rate 0.0001
2017-10-10T12:10:09.626860: step 7055, loss 0.0715792, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:09.812895: step 7056, loss 0.108491, acc 0.980392, learning_rate 0.0001
2017-10-10T12:10:10.092173: step 7057, loss 0.0136759, acc 1, learning_rate 0.0001
2017-10-10T12:10:10.343910: step 7058, loss 0.0462168, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:10.572841: step 7059, loss 0.0833349, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:10.824927: step 7060, loss 0.0992499, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:11.091356: step 7061, loss 0.0268957, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:11.329425: step 7062, loss 0.0304381, acc 1, learning_rate 0.0001
2017-10-10T12:10:11.602947: step 7063, loss 0.0599238, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:11.833811: step 7064, loss 0.0516181, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:12.039675: step 7065, loss 0.037748, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:12.286201: step 7066, loss 0.0443764, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:12.525100: step 7067, loss 0.0723442, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:12.759910: step 7068, loss 0.0336773, acc 1, learning_rate 0.0001
2017-10-10T12:10:13.032844: step 7069, loss 0.0297444, acc 1, learning_rate 0.0001
2017-10-10T12:10:13.260924: step 7070, loss 0.070955, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:13.490998: step 7071, loss 0.0259245, acc 1, learning_rate 0.0001
2017-10-10T12:10:13.759051: step 7072, loss 0.0613967, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:13.981711: step 7073, loss 0.0469356, acc 1, learning_rate 0.0001
2017-10-10T12:10:14.217181: step 7074, loss 0.0723359, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:14.479970: step 7075, loss 0.0478255, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:14.752041: step 7076, loss 0.0360707, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:15.017724: step 7077, loss 0.0529197, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:15.228638: step 7078, loss 0.0522216, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:15.480842: step 7079, loss 0.0757601, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:15.718877: step 7080, loss 0.10118, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:16.235676: step 7080, loss 0.194908, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7080

2017-10-10T12:10:17.454123: step 7081, loss 0.0419603, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:17.698226: step 7082, loss 0.0381739, acc 1, learning_rate 0.0001
2017-10-10T12:10:17.964908: step 7083, loss 0.0205549, acc 1, learning_rate 0.0001
2017-10-10T12:10:18.210798: step 7084, loss 0.0463987, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:18.448885: step 7085, loss 0.0713138, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:18.656828: step 7086, loss 0.0228801, acc 1, learning_rate 0.0001
2017-10-10T12:10:18.917598: step 7087, loss 0.0415444, acc 1, learning_rate 0.0001
2017-10-10T12:10:19.152902: step 7088, loss 0.0193403, acc 1, learning_rate 0.0001
2017-10-10T12:10:19.387363: step 7089, loss 0.0580116, acc 1, learning_rate 0.0001
2017-10-10T12:10:19.644364: step 7090, loss 0.0928101, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:19.860902: step 7091, loss 0.039447, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:20.107438: step 7092, loss 0.0611715, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:20.330043: step 7093, loss 0.0458819, acc 1, learning_rate 0.0001
2017-10-10T12:10:20.555467: step 7094, loss 0.0410124, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:20.816849: step 7095, loss 0.077746, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:21.081008: step 7096, loss 0.0398399, acc 1, learning_rate 0.0001
2017-10-10T12:10:21.353091: step 7097, loss 0.0706431, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:21.600847: step 7098, loss 0.161805, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:21.832883: step 7099, loss 0.0798147, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:22.067320: step 7100, loss 0.066767, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:22.311542: step 7101, loss 0.127122, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:22.560894: step 7102, loss 0.0531948, acc 1, learning_rate 0.0001
2017-10-10T12:10:22.852847: step 7103, loss 0.0768961, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:23.141508: step 7104, loss 0.0458376, acc 1, learning_rate 0.0001
2017-10-10T12:10:23.341360: step 7105, loss 0.0385271, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:23.552142: step 7106, loss 0.128454, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:23.772843: step 7107, loss 0.0507075, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:23.968845: step 7108, loss 0.0255931, acc 1, learning_rate 0.0001
2017-10-10T12:10:24.207289: step 7109, loss 0.0450161, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:24.448878: step 7110, loss 0.0621298, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:24.688870: step 7111, loss 0.116752, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:24.941008: step 7112, loss 0.0622715, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:25.235691: step 7113, loss 0.10447, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:25.508873: step 7114, loss 0.117507, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:25.804108: step 7115, loss 0.0569551, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:25.980117: step 7116, loss 0.124889, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:26.127339: step 7117, loss 0.018265, acc 1, learning_rate 0.0001
2017-10-10T12:10:26.311084: step 7118, loss 0.0224006, acc 1, learning_rate 0.0001
2017-10-10T12:10:26.490689: step 7119, loss 0.0656776, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:26.671815: step 7120, loss 0.082101, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:27.296852: step 7120, loss 0.19454, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7120

2017-10-10T12:10:28.291856: step 7121, loss 0.0982376, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:28.560936: step 7122, loss 0.121792, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:28.783426: step 7123, loss 0.0782499, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:29.035721: step 7124, loss 0.0221982, acc 1, learning_rate 0.0001
2017-10-10T12:10:29.281353: step 7125, loss 0.0699979, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:29.528243: step 7126, loss 0.0426409, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:29.776866: step 7127, loss 0.0432545, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:30.029331: step 7128, loss 0.0284198, acc 1, learning_rate 0.0001
2017-10-10T12:10:30.253094: step 7129, loss 0.0421705, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:30.532496: step 7130, loss 0.0769202, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:30.782658: step 7131, loss 0.0368684, acc 1, learning_rate 0.0001
2017-10-10T12:10:31.031403: step 7132, loss 0.0681016, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:31.321080: step 7133, loss 0.0466132, acc 1, learning_rate 0.0001
2017-10-10T12:10:31.549149: step 7134, loss 0.0810139, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:31.747926: step 7135, loss 0.120097, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:31.949578: step 7136, loss 0.0636238, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:32.165038: step 7137, loss 0.0353871, acc 1, learning_rate 0.0001
2017-10-10T12:10:32.360837: step 7138, loss 0.13816, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:32.575087: step 7139, loss 0.0629974, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:32.827029: step 7140, loss 0.029223, acc 1, learning_rate 0.0001
2017-10-10T12:10:33.073444: step 7141, loss 0.1654, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:33.304493: step 7142, loss 0.0680374, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:33.571021: step 7143, loss 0.0677557, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:33.809740: step 7144, loss 0.056838, acc 1, learning_rate 0.0001
2017-10-10T12:10:34.061565: step 7145, loss 0.0554866, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:34.285528: step 7146, loss 0.116989, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:34.506072: step 7147, loss 0.0423277, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:34.749320: step 7148, loss 0.0897684, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:35.000987: step 7149, loss 0.0933258, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:35.224223: step 7150, loss 0.0493757, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:35.510177: step 7151, loss 0.0886283, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:35.733074: step 7152, loss 0.113575, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:35.972856: step 7153, loss 0.0684572, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:36.277334: step 7154, loss 0.0448627, acc 0.980392, learning_rate 0.0001
2017-10-10T12:10:36.507714: step 7155, loss 0.0837168, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:36.679119: step 7156, loss 0.13418, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:36.857381: step 7157, loss 0.0900721, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:37.082411: step 7158, loss 0.0543687, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:37.258448: step 7159, loss 0.0909213, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:37.472013: step 7160, loss 0.0436593, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:38.188142: step 7160, loss 0.194479, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7160

2017-10-10T12:10:39.364971: step 7161, loss 0.0478213, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:39.672910: step 7162, loss 0.0243003, acc 1, learning_rate 0.0001
2017-10-10T12:10:39.872820: step 7163, loss 0.0485825, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:40.061206: step 7164, loss 0.0377911, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:40.253209: step 7165, loss 0.0529424, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:40.456629: step 7166, loss 0.081924, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:40.746371: step 7167, loss 0.0505645, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:41.015419: step 7168, loss 0.0545131, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:41.256953: step 7169, loss 0.0385302, acc 1, learning_rate 0.0001
2017-10-10T12:10:41.483498: step 7170, loss 0.031605, acc 1, learning_rate 0.0001
2017-10-10T12:10:41.723666: step 7171, loss 0.0775042, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:41.964982: step 7172, loss 0.0564954, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:42.209280: step 7173, loss 0.015745, acc 1, learning_rate 0.0001
2017-10-10T12:10:42.495944: step 7174, loss 0.054289, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:42.731816: step 7175, loss 0.0700441, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:42.992853: step 7176, loss 0.0494994, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:43.221308: step 7177, loss 0.0216518, acc 1, learning_rate 0.0001
2017-10-10T12:10:43.521727: step 7178, loss 0.0437989, acc 1, learning_rate 0.0001
2017-10-10T12:10:43.746078: step 7179, loss 0.0752704, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:43.961820: step 7180, loss 0.0764609, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:44.235178: step 7181, loss 0.0742148, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:44.456859: step 7182, loss 0.0387613, acc 1, learning_rate 0.0001
2017-10-10T12:10:44.669881: step 7183, loss 0.111438, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:44.887513: step 7184, loss 0.084727, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:45.106344: step 7185, loss 0.0860699, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:45.362598: step 7186, loss 0.107127, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:45.620931: step 7187, loss 0.0514132, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:45.855324: step 7188, loss 0.0451906, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:46.092757: step 7189, loss 0.0130774, acc 1, learning_rate 0.0001
2017-10-10T12:10:46.359466: step 7190, loss 0.107359, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:46.603817: step 7191, loss 0.03666, acc 1, learning_rate 0.0001
2017-10-10T12:10:46.868965: step 7192, loss 0.0402921, acc 1, learning_rate 0.0001
2017-10-10T12:10:47.086254: step 7193, loss 0.117752, acc 0.921875, learning_rate 0.0001
2017-10-10T12:10:47.271514: step 7194, loss 0.0148017, acc 1, learning_rate 0.0001
2017-10-10T12:10:47.458775: step 7195, loss 0.0476362, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:47.660849: step 7196, loss 0.0487996, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:47.851434: step 7197, loss 0.0380926, acc 1, learning_rate 0.0001
2017-10-10T12:10:48.144958: step 7198, loss 0.131829, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:48.349323: step 7199, loss 0.046197, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:48.540023: step 7200, loss 0.0665513, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:10:49.138569: step 7200, loss 0.193721, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7200

2017-10-10T12:10:50.336822: step 7201, loss 0.0487189, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:50.581180: step 7202, loss 0.114614, acc 0.921875, learning_rate 0.0001
2017-10-10T12:10:50.845890: step 7203, loss 0.0609696, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:51.099537: step 7204, loss 0.0927496, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:51.325870: step 7205, loss 0.0219701, acc 1, learning_rate 0.0001
2017-10-10T12:10:51.548909: step 7206, loss 0.045878, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:51.858523: step 7207, loss 0.0532568, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:52.104014: step 7208, loss 0.158698, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:52.358214: step 7209, loss 0.0406993, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:52.600909: step 7210, loss 0.0387312, acc 1, learning_rate 0.0001
2017-10-10T12:10:52.843926: step 7211, loss 0.0502153, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:53.102042: step 7212, loss 0.0119018, acc 1, learning_rate 0.0001
2017-10-10T12:10:53.333063: step 7213, loss 0.114339, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:53.602721: step 7214, loss 0.0249362, acc 1, learning_rate 0.0001
2017-10-10T12:10:53.845458: step 7215, loss 0.0836334, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:54.116832: step 7216, loss 0.0437434, acc 1, learning_rate 0.0001
2017-10-10T12:10:54.344752: step 7217, loss 0.0425213, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:54.571054: step 7218, loss 0.11982, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:54.796745: step 7219, loss 0.0388132, acc 1, learning_rate 0.0001
2017-10-10T12:10:55.029829: step 7220, loss 0.087392, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:55.273888: step 7221, loss 0.0649007, acc 1, learning_rate 0.0001
2017-10-10T12:10:55.558159: step 7222, loss 0.0940047, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:55.792742: step 7223, loss 0.0357314, acc 1, learning_rate 0.0001
2017-10-10T12:10:56.008896: step 7224, loss 0.0381317, acc 1, learning_rate 0.0001
2017-10-10T12:10:56.268839: step 7225, loss 0.0385985, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:56.577066: step 7226, loss 0.0411589, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:56.765868: step 7227, loss 0.0153448, acc 1, learning_rate 0.0001
2017-10-10T12:10:57.043548: step 7228, loss 0.0770614, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:57.262709: step 7229, loss 0.0329259, acc 1, learning_rate 0.0001
2017-10-10T12:10:57.409496: step 7230, loss 0.132193, acc 0.9375, learning_rate 0.0001
2017-10-10T12:10:57.599013: step 7231, loss 0.0618055, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:57.784854: step 7232, loss 0.0523703, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:57.982465: step 7233, loss 0.0828087, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:58.222361: step 7234, loss 0.0924663, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:58.475107: step 7235, loss 0.0811037, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:58.712988: step 7236, loss 0.0853712, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:58.966659: step 7237, loss 0.134099, acc 0.953125, learning_rate 0.0001
2017-10-10T12:10:59.205555: step 7238, loss 0.0498104, acc 0.96875, learning_rate 0.0001
2017-10-10T12:10:59.476826: step 7239, loss 0.052853, acc 0.984375, learning_rate 0.0001
2017-10-10T12:10:59.703712: step 7240, loss 0.0243442, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:00.386169: step 7240, loss 0.196116, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7240

2017-10-10T12:11:01.617841: step 7241, loss 0.0510666, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:01.834267: step 7242, loss 0.0527144, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:02.099776: step 7243, loss 0.0439461, acc 1, learning_rate 0.0001
2017-10-10T12:11:02.340411: step 7244, loss 0.163224, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:02.555984: step 7245, loss 0.0521742, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:02.756420: step 7246, loss 0.0286109, acc 1, learning_rate 0.0001
2017-10-10T12:11:03.044696: step 7247, loss 0.018564, acc 1, learning_rate 0.0001
2017-10-10T12:11:03.273029: step 7248, loss 0.0669674, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:03.502417: step 7249, loss 0.0317965, acc 1, learning_rate 0.0001
2017-10-10T12:11:03.773466: step 7250, loss 0.0407595, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:04.030160: step 7251, loss 0.0310896, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:04.254252: step 7252, loss 0.134347, acc 0.960784, learning_rate 0.0001
2017-10-10T12:11:04.556527: step 7253, loss 0.0555862, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:04.798076: step 7254, loss 0.0792014, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:05.024355: step 7255, loss 0.0906066, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:05.220383: step 7256, loss 0.0948081, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:05.410365: step 7257, loss 0.0825264, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:05.612256: step 7258, loss 0.0703166, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:05.813206: step 7259, loss 0.03065, acc 1, learning_rate 0.0001
2017-10-10T12:11:06.042427: step 7260, loss 0.059499, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:06.248886: step 7261, loss 0.0288776, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:06.496921: step 7262, loss 0.069368, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:06.750270: step 7263, loss 0.0319105, acc 1, learning_rate 0.0001
2017-10-10T12:11:06.952888: step 7264, loss 0.0127665, acc 1, learning_rate 0.0001
2017-10-10T12:11:07.273744: step 7265, loss 0.0894588, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:07.544415: step 7266, loss 0.0613802, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:07.711255: step 7267, loss 0.0218799, acc 1, learning_rate 0.0001
2017-10-10T12:11:07.882314: step 7268, loss 0.0456555, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:08.071560: step 7269, loss 0.040602, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:08.263464: step 7270, loss 0.0167131, acc 1, learning_rate 0.0001
2017-10-10T12:11:08.463024: step 7271, loss 0.0298828, acc 1, learning_rate 0.0001
2017-10-10T12:11:08.652926: step 7272, loss 0.0977911, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:08.896955: step 7273, loss 0.0694091, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:09.139865: step 7274, loss 0.0606436, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:09.375661: step 7275, loss 0.0486268, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:09.602784: step 7276, loss 0.0460328, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:09.832959: step 7277, loss 0.0749907, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:10.121227: step 7278, loss 0.0689134, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:10.353988: step 7279, loss 0.0278858, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:10.614947: step 7280, loss 0.0884569, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:11.325681: step 7280, loss 0.193701, acc 0.933813

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7280

2017-10-10T12:11:12.276824: step 7281, loss 0.0344701, acc 1, learning_rate 0.0001
2017-10-10T12:11:12.512889: step 7282, loss 0.0988893, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:12.759487: step 7283, loss 0.0281793, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:13.065628: step 7284, loss 0.062998, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:13.269374: step 7285, loss 0.0355811, acc 1, learning_rate 0.0001
2017-10-10T12:11:13.468672: step 7286, loss 0.20144, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:13.694052: step 7287, loss 0.10461, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:13.884789: step 7288, loss 0.0486538, acc 1, learning_rate 0.0001
2017-10-10T12:11:14.122007: step 7289, loss 0.0813613, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:14.362895: step 7290, loss 0.036119, acc 1, learning_rate 0.0001
2017-10-10T12:11:14.600960: step 7291, loss 0.0684382, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:14.884848: step 7292, loss 0.0724753, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:15.080892: step 7293, loss 0.157184, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:15.321522: step 7294, loss 0.0134586, acc 1, learning_rate 0.0001
2017-10-10T12:11:15.582801: step 7295, loss 0.117832, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:15.783491: step 7296, loss 0.0384208, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:16.024570: step 7297, loss 0.0306125, acc 1, learning_rate 0.0001
2017-10-10T12:11:16.224867: step 7298, loss 0.0344628, acc 1, learning_rate 0.0001
2017-10-10T12:11:16.444391: step 7299, loss 0.0214932, acc 1, learning_rate 0.0001
2017-10-10T12:11:16.654340: step 7300, loss 0.0665165, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:16.856865: step 7301, loss 0.10736, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:17.086541: step 7302, loss 0.0975679, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:17.348822: step 7303, loss 0.0201133, acc 1, learning_rate 0.0001
2017-10-10T12:11:17.592911: step 7304, loss 0.0397752, acc 1, learning_rate 0.0001
2017-10-10T12:11:17.900853: step 7305, loss 0.139495, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:18.155158: step 7306, loss 0.11802, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:18.332047: step 7307, loss 0.087839, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:18.498824: step 7308, loss 0.0531378, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:18.689189: step 7309, loss 0.0294892, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:18.877959: step 7310, loss 0.030319, acc 1, learning_rate 0.0001
2017-10-10T12:11:19.050172: step 7311, loss 0.0277676, acc 1, learning_rate 0.0001
2017-10-10T12:11:19.232886: step 7312, loss 0.0412071, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:19.500611: step 7313, loss 0.0600131, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:19.725024: step 7314, loss 0.0720052, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:19.965254: step 7315, loss 0.0812986, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:20.201982: step 7316, loss 0.048494, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:20.429333: step 7317, loss 0.161098, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:20.646013: step 7318, loss 0.0697951, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:20.866493: step 7319, loss 0.0493241, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:21.090780: step 7320, loss 0.0373727, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:21.816060: step 7320, loss 0.194264, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7320

2017-10-10T12:11:22.792895: step 7321, loss 0.0310558, acc 1, learning_rate 0.0001
2017-10-10T12:11:23.036917: step 7322, loss 0.036703, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:23.298064: step 7323, loss 0.121209, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:23.558875: step 7324, loss 0.0786898, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:23.752753: step 7325, loss 0.0477008, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:23.988875: step 7326, loss 0.0546266, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:24.242783: step 7327, loss 0.0752365, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:24.468030: step 7328, loss 0.0319496, acc 1, learning_rate 0.0001
2017-10-10T12:11:24.715453: step 7329, loss 0.0750863, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:24.950679: step 7330, loss 0.0366186, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:25.210196: step 7331, loss 0.0401032, acc 1, learning_rate 0.0001
2017-10-10T12:11:25.496934: step 7332, loss 0.157834, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:25.720688: step 7333, loss 0.0539428, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:25.964449: step 7334, loss 0.0731246, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:26.209177: step 7335, loss 0.0672474, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:26.461784: step 7336, loss 0.101183, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:26.712891: step 7337, loss 0.0469692, acc 1, learning_rate 0.0001
2017-10-10T12:11:26.965571: step 7338, loss 0.09292, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:27.200518: step 7339, loss 0.0909434, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:27.446569: step 7340, loss 0.0371504, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:27.687375: step 7341, loss 0.0291152, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:27.933070: step 7342, loss 0.211265, acc 0.90625, learning_rate 0.0001
2017-10-10T12:11:28.228865: step 7343, loss 0.0633729, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:28.533274: step 7344, loss 0.0221165, acc 1, learning_rate 0.0001
2017-10-10T12:11:28.713905: step 7345, loss 0.0572002, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:28.896126: step 7346, loss 0.0678508, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:29.116508: step 7347, loss 0.140111, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:29.322014: step 7348, loss 0.0256802, acc 1, learning_rate 0.0001
2017-10-10T12:11:29.559931: step 7349, loss 0.0542289, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:29.792900: step 7350, loss 0.0192691, acc 1, learning_rate 0.0001
2017-10-10T12:11:30.087875: step 7351, loss 0.0465564, acc 1, learning_rate 0.0001
2017-10-10T12:11:30.287259: step 7352, loss 0.16098, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:30.513802: step 7353, loss 0.0440272, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:30.714149: step 7354, loss 0.0967054, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:30.907354: step 7355, loss 0.0409442, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:31.106865: step 7356, loss 0.0556208, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:31.331256: step 7357, loss 0.105041, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:31.592824: step 7358, loss 0.0849724, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:31.848324: step 7359, loss 0.0800132, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:32.080999: step 7360, loss 0.0296455, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:32.751190: step 7360, loss 0.195785, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7360

2017-10-10T12:11:34.068864: step 7361, loss 0.0849016, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:34.327590: step 7362, loss 0.0250483, acc 1, learning_rate 0.0001
2017-10-10T12:11:34.622271: step 7363, loss 0.0419317, acc 1, learning_rate 0.0001
2017-10-10T12:11:34.871503: step 7364, loss 0.108341, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:35.098770: step 7365, loss 0.01093, acc 1, learning_rate 0.0001
2017-10-10T12:11:35.336494: step 7366, loss 0.0786044, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:35.580969: step 7367, loss 0.0607033, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:35.818053: step 7368, loss 0.0611053, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:36.078600: step 7369, loss 0.0416939, acc 1, learning_rate 0.0001
2017-10-10T12:11:36.338640: step 7370, loss 0.043693, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:36.582614: step 7371, loss 0.105096, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:36.834524: step 7372, loss 0.0289012, acc 1, learning_rate 0.0001
2017-10-10T12:11:37.075607: step 7373, loss 0.112735, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:37.317445: step 7374, loss 0.0406223, acc 1, learning_rate 0.0001
2017-10-10T12:11:37.570203: step 7375, loss 0.025779, acc 1, learning_rate 0.0001
2017-10-10T12:11:37.823718: step 7376, loss 0.0320428, acc 1, learning_rate 0.0001
2017-10-10T12:11:38.092995: step 7377, loss 0.0948959, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:38.372883: step 7378, loss 0.0872062, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:38.736943: step 7379, loss 0.062092, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:38.924588: step 7380, loss 0.0567483, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:39.057735: step 7381, loss 0.0505012, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:39.202536: step 7382, loss 0.0662596, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:39.332998: step 7383, loss 0.0793732, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:39.472397: step 7384, loss 0.0213468, acc 1, learning_rate 0.0001
2017-10-10T12:11:39.615415: step 7385, loss 0.010079, acc 1, learning_rate 0.0001
2017-10-10T12:11:39.881574: step 7386, loss 0.0983604, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:40.142962: step 7387, loss 0.0633461, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:40.416070: step 7388, loss 0.0619934, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:40.648873: step 7389, loss 0.0161371, acc 1, learning_rate 0.0001
2017-10-10T12:11:40.880883: step 7390, loss 0.0416818, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:41.161778: step 7391, loss 0.0391347, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:41.408370: step 7392, loss 0.223771, acc 0.921875, learning_rate 0.0001
2017-10-10T12:11:41.667973: step 7393, loss 0.0746215, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:41.915576: step 7394, loss 0.0810467, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:42.165419: step 7395, loss 0.0478791, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:42.348960: step 7396, loss 0.111219, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:42.566775: step 7397, loss 0.08667, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:42.812276: step 7398, loss 0.15179, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:43.126611: step 7399, loss 0.0755669, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:43.370580: step 7400, loss 0.0729127, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:44.017109: step 7400, loss 0.195216, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7400

2017-10-10T12:11:45.015821: step 7401, loss 0.0363513, acc 1, learning_rate 0.0001
2017-10-10T12:11:45.279420: step 7402, loss 0.0246913, acc 1, learning_rate 0.0001
2017-10-10T12:11:45.504835: step 7403, loss 0.0614395, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:45.771928: step 7404, loss 0.0471021, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:46.004265: step 7405, loss 0.112817, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:46.222116: step 7406, loss 0.0729315, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:46.464854: step 7407, loss 0.0126261, acc 1, learning_rate 0.0001
2017-10-10T12:11:46.710762: step 7408, loss 0.0103877, acc 1, learning_rate 0.0001
2017-10-10T12:11:47.016308: step 7409, loss 0.0852128, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:47.282830: step 7410, loss 0.0144691, acc 1, learning_rate 0.0001
2017-10-10T12:11:47.499788: step 7411, loss 0.0741175, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:47.691342: step 7412, loss 0.0866683, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:47.881790: step 7413, loss 0.0547129, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:48.084217: step 7414, loss 0.0971404, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:48.372883: step 7415, loss 0.107352, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:48.676173: step 7416, loss 0.0482307, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:48.911823: step 7417, loss 0.0627364, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:49.095094: step 7418, loss 0.0275507, acc 1, learning_rate 0.0001
2017-10-10T12:11:49.285543: step 7419, loss 0.0387438, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:49.488668: step 7420, loss 0.00967604, acc 1, learning_rate 0.0001
2017-10-10T12:11:49.675762: step 7421, loss 0.0968021, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:49.870986: step 7422, loss 0.0760036, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:50.124843: step 7423, loss 0.123733, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:50.355794: step 7424, loss 0.148678, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:50.636153: step 7425, loss 0.0359903, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:50.884467: step 7426, loss 0.0675699, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:51.122232: step 7427, loss 0.0591943, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:51.388609: step 7428, loss 0.034115, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:51.633820: step 7429, loss 0.0800979, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:51.873328: step 7430, loss 0.0325129, acc 1, learning_rate 0.0001
2017-10-10T12:11:52.137418: step 7431, loss 0.0759694, acc 0.953125, learning_rate 0.0001
2017-10-10T12:11:52.378469: step 7432, loss 0.0450141, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:52.602609: step 7433, loss 0.0498388, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:52.832044: step 7434, loss 0.114234, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:53.102850: step 7435, loss 0.0372843, acc 1, learning_rate 0.0001
2017-10-10T12:11:53.360841: step 7436, loss 0.138755, acc 0.9375, learning_rate 0.0001
2017-10-10T12:11:53.599567: step 7437, loss 0.061663, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:53.845482: step 7438, loss 0.0288803, acc 1, learning_rate 0.0001
2017-10-10T12:11:54.090670: step 7439, loss 0.0854777, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:54.355174: step 7440, loss 0.111443, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:11:55.050188: step 7440, loss 0.194072, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7440

2017-10-10T12:11:56.111339: step 7441, loss 0.0160236, acc 1, learning_rate 0.0001
2017-10-10T12:11:56.335591: step 7442, loss 0.0239126, acc 1, learning_rate 0.0001
2017-10-10T12:11:56.577395: step 7443, loss 0.0406275, acc 1, learning_rate 0.0001
2017-10-10T12:11:56.844955: step 7444, loss 0.0753496, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:57.068905: step 7445, loss 0.0263926, acc 1, learning_rate 0.0001
2017-10-10T12:11:57.313031: step 7446, loss 0.0171187, acc 1, learning_rate 0.0001
2017-10-10T12:11:57.548496: step 7447, loss 0.0742338, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:57.737892: step 7448, loss 0.132129, acc 0.921569, learning_rate 0.0001
2017-10-10T12:11:58.071605: step 7449, loss 0.0474462, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:58.308075: step 7450, loss 0.0511227, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:58.537305: step 7451, loss 0.0268211, acc 1, learning_rate 0.0001
2017-10-10T12:11:58.830972: step 7452, loss 0.0507916, acc 1, learning_rate 0.0001
2017-10-10T12:11:59.090069: step 7453, loss 0.0398306, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:59.315607: step 7454, loss 0.0660324, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:59.496838: step 7455, loss 0.092939, acc 0.96875, learning_rate 0.0001
2017-10-10T12:11:59.689958: step 7456, loss 0.0555937, acc 0.984375, learning_rate 0.0001
2017-10-10T12:11:59.866758: step 7457, loss 0.079215, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:00.071921: step 7458, loss 0.0863694, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:00.364838: step 7459, loss 0.0485422, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:00.606991: step 7460, loss 0.0287861, acc 1, learning_rate 0.0001
2017-10-10T12:12:00.858225: step 7461, loss 0.0604538, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:01.057429: step 7462, loss 0.0271267, acc 1, learning_rate 0.0001
2017-10-10T12:12:01.323866: step 7463, loss 0.0889075, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:01.564972: step 7464, loss 0.0413708, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:01.818078: step 7465, loss 0.0635149, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:02.052050: step 7466, loss 0.0615711, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:02.285962: step 7467, loss 0.0841768, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:02.548463: step 7468, loss 0.0768145, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:02.788602: step 7469, loss 0.0171384, acc 1, learning_rate 0.0001
2017-10-10T12:12:03.030707: step 7470, loss 0.00666176, acc 1, learning_rate 0.0001
2017-10-10T12:12:03.295812: step 7471, loss 0.0391573, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:03.581186: step 7472, loss 0.0188052, acc 1, learning_rate 0.0001
2017-10-10T12:12:03.843679: step 7473, loss 0.0626298, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:04.107153: step 7474, loss 0.0546051, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:04.770006: step 7475, loss 0.0869679, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:04.984627: step 7476, loss 0.033313, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:05.183464: step 7477, loss 0.00941574, acc 1, learning_rate 0.0001
2017-10-10T12:12:05.381813: step 7478, loss 0.108729, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:05.612544: step 7479, loss 0.0188307, acc 1, learning_rate 0.0001
2017-10-10T12:12:05.832538: step 7480, loss 0.0816038, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:06.541405: step 7480, loss 0.196657, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7480

2017-10-10T12:12:07.804902: step 7481, loss 0.0370894, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:08.082606: step 7482, loss 0.0218638, acc 1, learning_rate 0.0001
2017-10-10T12:12:08.351455: step 7483, loss 0.0627695, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:08.588859: step 7484, loss 0.0540514, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:08.843149: step 7485, loss 0.0181158, acc 1, learning_rate 0.0001
2017-10-10T12:12:09.095661: step 7486, loss 0.0254349, acc 1, learning_rate 0.0001
2017-10-10T12:12:09.331264: step 7487, loss 0.0385218, acc 1, learning_rate 0.0001
2017-10-10T12:12:09.559316: step 7488, loss 0.116127, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:09.892868: step 7489, loss 0.0221486, acc 1, learning_rate 0.0001
2017-10-10T12:12:10.141507: step 7490, loss 0.0470253, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:10.327120: step 7491, loss 0.042293, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:10.532536: step 7492, loss 0.139626, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:10.723897: step 7493, loss 0.0816668, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:10.899768: step 7494, loss 0.0284286, acc 1, learning_rate 0.0001
2017-10-10T12:12:11.128858: step 7495, loss 0.0503825, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:11.380828: step 7496, loss 0.134011, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:11.648928: step 7497, loss 0.0429841, acc 1, learning_rate 0.0001
2017-10-10T12:12:11.901274: step 7498, loss 0.0927952, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:12.139308: step 7499, loss 0.0249523, acc 1, learning_rate 0.0001
2017-10-10T12:12:12.447187: step 7500, loss 0.0690395, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:12.668854: step 7501, loss 0.106609, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:12.855783: step 7502, loss 0.0591972, acc 1, learning_rate 0.0001
2017-10-10T12:12:13.079813: step 7503, loss 0.0977576, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:13.264820: step 7504, loss 0.0674266, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:13.450001: step 7505, loss 0.0364174, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:13.676830: step 7506, loss 0.14902, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:13.909992: step 7507, loss 0.110852, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:14.137198: step 7508, loss 0.0802707, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:14.377885: step 7509, loss 0.0337302, acc 1, learning_rate 0.0001
2017-10-10T12:12:14.653005: step 7510, loss 0.0505492, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:14.920873: step 7511, loss 0.0663798, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:15.182467: step 7512, loss 0.0175919, acc 1, learning_rate 0.0001
2017-10-10T12:12:15.433845: step 7513, loss 0.0257931, acc 1, learning_rate 0.0001
2017-10-10T12:12:15.693543: step 7514, loss 0.0624071, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:15.950783: step 7515, loss 0.0319918, acc 1, learning_rate 0.0001
2017-10-10T12:12:16.187663: step 7516, loss 0.104618, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:16.474990: step 7517, loss 0.0312928, acc 1, learning_rate 0.0001
2017-10-10T12:12:16.721004: step 7518, loss 0.0535959, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:16.952312: step 7519, loss 0.0407906, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:17.172895: step 7520, loss 0.0373683, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:17.830530: step 7520, loss 0.195812, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7520

2017-10-10T12:12:18.836996: step 7521, loss 0.0331128, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:19.084787: step 7522, loss 0.0717283, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:19.330442: step 7523, loss 0.116281, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:19.579108: step 7524, loss 0.0542631, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:19.864907: step 7525, loss 0.059757, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:20.140268: step 7526, loss 0.0658473, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:20.417155: step 7527, loss 0.0352209, acc 1, learning_rate 0.0001
2017-10-10T12:12:20.712083: step 7528, loss 0.0333945, acc 1, learning_rate 0.0001
2017-10-10T12:12:20.933555: step 7529, loss 0.130268, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:21.088764: step 7530, loss 0.0765342, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:21.228784: step 7531, loss 0.134655, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:21.362051: step 7532, loss 0.0398392, acc 1, learning_rate 0.0001
2017-10-10T12:12:21.504265: step 7533, loss 0.0120456, acc 1, learning_rate 0.0001
2017-10-10T12:12:21.635275: step 7534, loss 0.114543, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:21.774656: step 7535, loss 0.125036, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:22.002379: step 7536, loss 0.022102, acc 1, learning_rate 0.0001
2017-10-10T12:12:22.246785: step 7537, loss 0.0552978, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:22.532250: step 7538, loss 0.0681071, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:22.801003: step 7539, loss 0.037936, acc 1, learning_rate 0.0001
2017-10-10T12:12:23.035679: step 7540, loss 0.0478578, acc 1, learning_rate 0.0001
2017-10-10T12:12:23.258602: step 7541, loss 0.127247, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:23.510313: step 7542, loss 0.0472613, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:23.750151: step 7543, loss 0.04577, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:23.992333: step 7544, loss 0.0394368, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:24.235023: step 7545, loss 0.0765948, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:24.436664: step 7546, loss 0.0596528, acc 0.980392, learning_rate 0.0001
2017-10-10T12:12:24.636833: step 7547, loss 0.0520514, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:24.905148: step 7548, loss 0.135024, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:25.144172: step 7549, loss 0.0748382, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:25.389491: step 7550, loss 0.0264247, acc 1, learning_rate 0.0001
2017-10-10T12:12:25.666186: step 7551, loss 0.0648363, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:25.887058: step 7552, loss 0.0617316, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:26.154739: step 7553, loss 0.0695208, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:26.387615: step 7554, loss 0.0262019, acc 1, learning_rate 0.0001
2017-10-10T12:12:26.632330: step 7555, loss 0.0470248, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:26.853013: step 7556, loss 0.0358953, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:27.082668: step 7557, loss 0.026757, acc 1, learning_rate 0.0001
2017-10-10T12:12:27.356785: step 7558, loss 0.0566137, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:27.615811: step 7559, loss 0.045032, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:27.836754: step 7560, loss 0.131316, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:28.550666: step 7560, loss 0.194742, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7560

2017-10-10T12:12:29.616869: step 7561, loss 0.0464885, acc 1, learning_rate 0.0001
2017-10-10T12:12:29.846367: step 7562, loss 0.0407082, acc 1, learning_rate 0.0001
2017-10-10T12:12:30.053900: step 7563, loss 0.0324718, acc 1, learning_rate 0.0001
2017-10-10T12:12:30.272083: step 7564, loss 0.0459561, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:30.536361: step 7565, loss 0.0585921, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:30.808855: step 7566, loss 0.105501, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:31.084706: step 7567, loss 0.050138, acc 1, learning_rate 0.0001
2017-10-10T12:12:31.295486: step 7568, loss 0.0231628, acc 1, learning_rate 0.0001
2017-10-10T12:12:31.474388: step 7569, loss 0.0301035, acc 1, learning_rate 0.0001
2017-10-10T12:12:31.664826: step 7570, loss 0.061984, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:31.867985: step 7571, loss 0.0220152, acc 1, learning_rate 0.0001
2017-10-10T12:12:32.126598: step 7572, loss 0.0570284, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:32.365887: step 7573, loss 0.0520969, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:32.606839: step 7574, loss 0.0257979, acc 1, learning_rate 0.0001
2017-10-10T12:12:32.886143: step 7575, loss 0.053172, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:33.117197: step 7576, loss 0.0932671, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:33.352569: step 7577, loss 0.0189658, acc 1, learning_rate 0.0001
2017-10-10T12:12:33.597691: step 7578, loss 0.0506778, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:33.830682: step 7579, loss 0.076289, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:34.082825: step 7580, loss 0.0611076, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:34.332201: step 7581, loss 0.028716, acc 1, learning_rate 0.0001
2017-10-10T12:12:34.575603: step 7582, loss 0.0167698, acc 1, learning_rate 0.0001
2017-10-10T12:12:34.844023: step 7583, loss 0.0618449, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:35.064869: step 7584, loss 0.106278, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:35.323126: step 7585, loss 0.0769422, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:35.545751: step 7586, loss 0.125522, acc 0.9375, learning_rate 0.0001
2017-10-10T12:12:35.777113: step 7587, loss 0.0297074, acc 1, learning_rate 0.0001
2017-10-10T12:12:36.043895: step 7588, loss 0.0285372, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:36.313301: step 7589, loss 0.0590348, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:36.571725: step 7590, loss 0.0498381, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:36.803736: step 7591, loss 0.02794, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:37.046618: step 7592, loss 0.0386063, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:37.295740: step 7593, loss 0.0527693, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:37.524296: step 7594, loss 0.0766841, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:37.808472: step 7595, loss 0.118549, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:38.069579: step 7596, loss 0.0496478, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:38.281415: step 7597, loss 0.0767684, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:38.502743: step 7598, loss 0.0667072, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:38.711564: step 7599, loss 0.0308367, acc 1, learning_rate 0.0001
2017-10-10T12:12:38.955619: step 7600, loss 0.00949421, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:39.620882: step 7600, loss 0.196669, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7600

2017-10-10T12:12:40.795930: step 7601, loss 0.0915085, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:41.017614: step 7602, loss 0.0564944, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:41.242738: step 7603, loss 0.0324071, acc 1, learning_rate 0.0001
2017-10-10T12:12:41.508892: step 7604, loss 0.0842538, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:41.765225: step 7605, loss 0.0490759, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:41.961552: step 7606, loss 0.0418719, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:42.136869: step 7607, loss 0.0329439, acc 1, learning_rate 0.0001
2017-10-10T12:12:42.325644: step 7608, loss 0.0244856, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:42.508128: step 7609, loss 0.0459804, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:42.739274: step 7610, loss 0.0660748, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:43.003504: step 7611, loss 0.0195574, acc 1, learning_rate 0.0001
2017-10-10T12:12:43.243906: step 7612, loss 0.0414613, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:43.512848: step 7613, loss 0.0502553, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:43.747203: step 7614, loss 0.0477836, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:43.971833: step 7615, loss 0.0427904, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:44.202503: step 7616, loss 0.0323817, acc 1, learning_rate 0.0001
2017-10-10T12:12:44.475154: step 7617, loss 0.0218598, acc 1, learning_rate 0.0001
2017-10-10T12:12:44.696770: step 7618, loss 0.0726614, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:44.944844: step 7619, loss 0.0161359, acc 1, learning_rate 0.0001
2017-10-10T12:12:45.188122: step 7620, loss 0.0317297, acc 1, learning_rate 0.0001
2017-10-10T12:12:45.429067: step 7621, loss 0.0440682, acc 1, learning_rate 0.0001
2017-10-10T12:12:45.668013: step 7622, loss 0.0687009, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:45.934323: step 7623, loss 0.0600783, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:46.210700: step 7624, loss 0.0554149, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:46.413350: step 7625, loss 0.00952378, acc 1, learning_rate 0.0001
2017-10-10T12:12:46.628511: step 7626, loss 0.0585321, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:46.828999: step 7627, loss 0.0642227, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:47.033277: step 7628, loss 0.0796526, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:47.274906: step 7629, loss 0.037999, acc 1, learning_rate 0.0001
2017-10-10T12:12:47.480940: step 7630, loss 0.0438332, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:47.747660: step 7631, loss 0.0465368, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:48.033276: step 7632, loss 0.0378473, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:48.280810: step 7633, loss 0.117575, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:48.562390: step 7634, loss 0.142329, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:48.789585: step 7635, loss 0.0544138, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:49.045122: step 7636, loss 0.115759, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:49.328441: step 7637, loss 0.0355271, acc 1, learning_rate 0.0001
2017-10-10T12:12:49.588454: step 7638, loss 0.0458401, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:49.840937: step 7639, loss 0.0356005, acc 1, learning_rate 0.0001
2017-10-10T12:12:50.084883: step 7640, loss 0.029926, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:12:50.741119: step 7640, loss 0.196486, acc 0.932374

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7640

2017-10-10T12:12:52.084882: step 7641, loss 0.040881, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:52.287503: step 7642, loss 0.0176181, acc 1, learning_rate 0.0001
2017-10-10T12:12:52.485659: step 7643, loss 0.0430018, acc 1, learning_rate 0.0001
2017-10-10T12:12:52.660860: step 7644, loss 0.106383, acc 0.960784, learning_rate 0.0001
2017-10-10T12:12:52.869346: step 7645, loss 0.0334652, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:53.044838: step 7646, loss 0.0842265, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:53.316862: step 7647, loss 0.048353, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:53.560994: step 7648, loss 0.0163578, acc 1, learning_rate 0.0001
2017-10-10T12:12:53.817003: step 7649, loss 0.0621011, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:54.119485: step 7650, loss 0.057415, acc 1, learning_rate 0.0001
2017-10-10T12:12:54.393564: step 7651, loss 0.0475624, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:54.612938: step 7652, loss 0.0355307, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:54.846560: step 7653, loss 0.0287898, acc 1, learning_rate 0.0001
2017-10-10T12:12:55.060862: step 7654, loss 0.0427968, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:55.260161: step 7655, loss 0.0206757, acc 1, learning_rate 0.0001
2017-10-10T12:12:55.464961: step 7656, loss 0.0447814, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:55.724882: step 7657, loss 0.0315153, acc 1, learning_rate 0.0001
2017-10-10T12:12:55.972844: step 7658, loss 0.0611999, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:56.217006: step 7659, loss 0.0294896, acc 1, learning_rate 0.0001
2017-10-10T12:12:56.480966: step 7660, loss 0.0116688, acc 1, learning_rate 0.0001
2017-10-10T12:12:56.721222: step 7661, loss 0.0482643, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:56.958822: step 7662, loss 0.042738, acc 1, learning_rate 0.0001
2017-10-10T12:12:57.184864: step 7663, loss 0.0205391, acc 1, learning_rate 0.0001
2017-10-10T12:12:57.434589: step 7664, loss 0.0588793, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:57.665060: step 7665, loss 0.077508, acc 0.953125, learning_rate 0.0001
2017-10-10T12:12:57.938113: step 7666, loss 0.0244038, acc 1, learning_rate 0.0001
2017-10-10T12:12:58.194609: step 7667, loss 0.049584, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:58.401022: step 7668, loss 0.0804819, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:58.599859: step 7669, loss 0.0458883, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:58.816898: step 7670, loss 0.0272047, acc 1, learning_rate 0.0001
2017-10-10T12:12:59.057338: step 7671, loss 0.0362585, acc 1, learning_rate 0.0001
2017-10-10T12:12:59.273215: step 7672, loss 0.0587769, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:59.494963: step 7673, loss 0.127025, acc 0.96875, learning_rate 0.0001
2017-10-10T12:12:59.737306: step 7674, loss 0.0463199, acc 0.984375, learning_rate 0.0001
2017-10-10T12:12:59.996991: step 7675, loss 0.0458435, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:00.241218: step 7676, loss 0.0603935, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:00.460870: step 7677, loss 0.0486305, acc 1, learning_rate 0.0001
2017-10-10T12:13:00.691601: step 7678, loss 0.0687036, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:00.956939: step 7679, loss 0.0745237, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:01.212919: step 7680, loss 0.046582, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:01.937976: step 7680, loss 0.195673, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7680

2017-10-10T12:13:02.928932: step 7681, loss 0.0716763, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:03.077788: step 7682, loss 0.12657, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:03.220406: step 7683, loss 0.0542533, acc 1, learning_rate 0.0001
2017-10-10T12:13:03.433001: step 7684, loss 0.15919, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:03.627084: step 7685, loss 0.0123029, acc 1, learning_rate 0.0001
2017-10-10T12:13:03.824313: step 7686, loss 0.0211353, acc 1, learning_rate 0.0001
2017-10-10T12:13:04.064978: step 7687, loss 0.0617557, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:04.327364: step 7688, loss 0.0276952, acc 1, learning_rate 0.0001
2017-10-10T12:13:04.573667: step 7689, loss 0.0688104, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:04.842678: step 7690, loss 0.0805229, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:05.096362: step 7691, loss 0.0730875, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:05.340833: step 7692, loss 0.0199269, acc 1, learning_rate 0.0001
2017-10-10T12:13:05.570052: step 7693, loss 0.0725368, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:05.819989: step 7694, loss 0.115666, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:06.074568: step 7695, loss 0.0787009, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:06.319669: step 7696, loss 0.0562416, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:06.574721: step 7697, loss 0.0331227, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:06.818401: step 7698, loss 0.0804773, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:07.117123: step 7699, loss 0.154771, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:07.383283: step 7700, loss 0.129799, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:07.641350: step 7701, loss 0.040208, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:07.883201: step 7702, loss 0.0932277, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:08.133388: step 7703, loss 0.057136, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:08.385150: step 7704, loss 0.0935151, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:08.656516: step 7705, loss 0.161046, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:08.909368: step 7706, loss 0.0554876, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:09.163388: step 7707, loss 0.0261358, acc 1, learning_rate 0.0001
2017-10-10T12:13:09.384105: step 7708, loss 0.0362577, acc 1, learning_rate 0.0001
2017-10-10T12:13:09.589109: step 7709, loss 0.0748998, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:09.825281: step 7710, loss 0.0403948, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:10.056001: step 7711, loss 0.0448324, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:10.347932: step 7712, loss 0.0509926, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:10.601062: step 7713, loss 0.0321307, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:10.852908: step 7714, loss 0.0846023, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:11.126125: step 7715, loss 0.0559563, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:11.407698: step 7716, loss 0.0468308, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:11.603154: step 7717, loss 0.0404551, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:11.788830: step 7718, loss 0.0430401, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:11.980382: step 7719, loss 0.0365609, acc 1, learning_rate 0.0001
2017-10-10T12:13:12.184858: step 7720, loss 0.100811, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:12.918032: step 7720, loss 0.195134, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7720

2017-10-10T12:13:13.605866: step 7721, loss 0.0681859, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:13.804449: step 7722, loss 0.0267071, acc 1, learning_rate 0.0001
2017-10-10T12:13:13.983825: step 7723, loss 0.0207312, acc 1, learning_rate 0.0001
2017-10-10T12:13:14.161482: step 7724, loss 0.103339, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:14.340589: step 7725, loss 0.0210446, acc 1, learning_rate 0.0001
2017-10-10T12:13:14.516874: step 7726, loss 0.0519944, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:14.696212: step 7727, loss 0.057318, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:14.862668: step 7728, loss 0.0530753, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:15.048301: step 7729, loss 0.0370498, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:15.239846: step 7730, loss 0.0410307, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:15.440193: step 7731, loss 0.0563728, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:15.623028: step 7732, loss 0.0479209, acc 1, learning_rate 0.0001
2017-10-10T12:13:15.799785: step 7733, loss 0.0315091, acc 1, learning_rate 0.0001
2017-10-10T12:13:16.008140: step 7734, loss 0.128505, acc 0.921875, learning_rate 0.0001
2017-10-10T12:13:16.182997: step 7735, loss 0.0216834, acc 1, learning_rate 0.0001
2017-10-10T12:13:16.371395: step 7736, loss 0.103979, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:16.555203: step 7737, loss 0.0394697, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:16.732426: step 7738, loss 0.0306954, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:16.923636: step 7739, loss 0.0161377, acc 1, learning_rate 0.0001
2017-10-10T12:13:17.113803: step 7740, loss 0.0988668, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:17.290845: step 7741, loss 0.0243537, acc 1, learning_rate 0.0001
2017-10-10T12:13:17.416885: step 7742, loss 0.0523325, acc 0.980392, learning_rate 0.0001
2017-10-10T12:13:17.642025: step 7743, loss 0.053876, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:17.821606: step 7744, loss 0.0812453, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:18.021409: step 7745, loss 0.0466516, acc 1, learning_rate 0.0001
2017-10-10T12:13:18.211925: step 7746, loss 0.0991049, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:18.405923: step 7747, loss 0.0889499, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:18.582154: step 7748, loss 0.0641145, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:18.756384: step 7749, loss 0.0677189, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:18.936989: step 7750, loss 0.0351528, acc 1, learning_rate 0.0001
2017-10-10T12:13:19.104326: step 7751, loss 0.0730971, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:19.277847: step 7752, loss 0.0387306, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:19.463483: step 7753, loss 0.0631612, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:19.636943: step 7754, loss 0.0695783, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:19.813210: step 7755, loss 0.0410608, acc 1, learning_rate 0.0001
2017-10-10T12:13:20.023003: step 7756, loss 0.0391434, acc 1, learning_rate 0.0001
2017-10-10T12:13:20.198392: step 7757, loss 0.0909458, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:20.448909: step 7758, loss 0.139255, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:20.656244: step 7759, loss 0.044344, acc 1, learning_rate 0.0001
2017-10-10T12:13:20.787257: step 7760, loss 0.0624054, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:21.114499: step 7760, loss 0.195069, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7760

2017-10-10T12:13:21.964435: step 7761, loss 0.0425663, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:22.127522: step 7762, loss 0.0304843, acc 1, learning_rate 0.0001
2017-10-10T12:13:22.311717: step 7763, loss 0.0621769, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:22.495835: step 7764, loss 0.0667738, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:22.671276: step 7765, loss 0.0449277, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:22.866841: step 7766, loss 0.0651418, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:23.040871: step 7767, loss 0.0507496, acc 1, learning_rate 0.0001
2017-10-10T12:13:23.231859: step 7768, loss 0.0268834, acc 1, learning_rate 0.0001
2017-10-10T12:13:23.419800: step 7769, loss 0.0277979, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:23.617871: step 7770, loss 0.0822869, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:23.801010: step 7771, loss 0.0217502, acc 1, learning_rate 0.0001
2017-10-10T12:13:24.002122: step 7772, loss 0.0193498, acc 1, learning_rate 0.0001
2017-10-10T12:13:24.180985: step 7773, loss 0.0724589, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:24.359725: step 7774, loss 0.120106, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:24.540879: step 7775, loss 0.0786784, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:24.737026: step 7776, loss 0.109154, acc 0.9375, learning_rate 0.0001
2017-10-10T12:13:24.893998: step 7777, loss 0.081518, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:25.052834: step 7778, loss 0.10261, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:25.257306: step 7779, loss 0.0717582, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:25.410977: step 7780, loss 0.0560302, acc 1, learning_rate 0.0001
2017-10-10T12:13:25.602952: step 7781, loss 0.047984, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:25.780846: step 7782, loss 0.0475132, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:25.954896: step 7783, loss 0.0200527, acc 1, learning_rate 0.0001
2017-10-10T12:13:26.132057: step 7784, loss 0.0486485, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:26.316053: step 7785, loss 0.0268894, acc 1, learning_rate 0.0001
2017-10-10T12:13:26.519192: step 7786, loss 0.0544333, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:26.712473: step 7787, loss 0.0359957, acc 1, learning_rate 0.0001
2017-10-10T12:13:26.911185: step 7788, loss 0.0763245, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:27.077242: step 7789, loss 0.0284494, acc 1, learning_rate 0.0001
2017-10-10T12:13:27.271768: step 7790, loss 0.0368315, acc 1, learning_rate 0.0001
2017-10-10T12:13:27.436926: step 7791, loss 0.0994571, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:27.616865: step 7792, loss 0.0427463, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:27.792953: step 7793, loss 0.059541, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:27.976761: step 7794, loss 0.141235, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:28.208865: step 7795, loss 0.0246025, acc 1, learning_rate 0.0001
2017-10-10T12:13:28.436471: step 7796, loss 0.0693986, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:28.556268: step 7797, loss 0.0165168, acc 1, learning_rate 0.0001
2017-10-10T12:13:28.677926: step 7798, loss 0.0854524, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:28.803916: step 7799, loss 0.0690423, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:28.939481: step 7800, loss 0.0744084, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:29.403642: step 7800, loss 0.194031, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7800

2017-10-10T12:13:30.209109: step 7801, loss 0.0183432, acc 1, learning_rate 0.0001
2017-10-10T12:13:30.394509: step 7802, loss 0.0851002, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:30.562432: step 7803, loss 0.0606018, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:30.737338: step 7804, loss 0.0516146, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:30.932391: step 7805, loss 0.0317555, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:31.110365: step 7806, loss 0.0411812, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:31.302688: step 7807, loss 0.0254711, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:31.476084: step 7808, loss 0.0795222, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:31.656876: step 7809, loss 0.0418597, acc 1, learning_rate 0.0001
2017-10-10T12:13:31.828852: step 7810, loss 0.0158174, acc 1, learning_rate 0.0001
2017-10-10T12:13:32.014379: step 7811, loss 0.0967953, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:32.193101: step 7812, loss 0.0447477, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:32.351028: step 7813, loss 0.0632507, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:32.525212: step 7814, loss 0.0580998, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:32.696808: step 7815, loss 0.0380568, acc 1, learning_rate 0.0001
2017-10-10T12:13:32.879485: step 7816, loss 0.0518094, acc 1, learning_rate 0.0001
2017-10-10T12:13:33.084888: step 7817, loss 0.0542116, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:33.284315: step 7818, loss 0.0551464, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:33.459024: step 7819, loss 0.0980199, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:33.657211: step 7820, loss 0.100597, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:33.848092: step 7821, loss 0.0473071, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:34.044783: step 7822, loss 0.0780174, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:34.228943: step 7823, loss 0.0265912, acc 1, learning_rate 0.0001
2017-10-10T12:13:34.440947: step 7824, loss 0.0641122, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:34.624928: step 7825, loss 0.0350632, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:34.797569: step 7826, loss 0.0800785, acc 0.953125, learning_rate 0.0001
2017-10-10T12:13:34.999506: step 7827, loss 0.0402343, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:35.192772: step 7828, loss 0.0293084, acc 1, learning_rate 0.0001
2017-10-10T12:13:35.372497: step 7829, loss 0.0370341, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:35.552408: step 7830, loss 0.0311794, acc 1, learning_rate 0.0001
2017-10-10T12:13:35.796427: step 7831, loss 0.0695828, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:35.993787: step 7832, loss 0.0223983, acc 1, learning_rate 0.0001
2017-10-10T12:13:36.118993: step 7833, loss 0.0628735, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:36.240378: step 7834, loss 0.056087, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:36.365961: step 7835, loss 0.0632895, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:36.493017: step 7836, loss 0.0722812, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:36.611585: step 7837, loss 0.0432793, acc 0.984375, learning_rate 0.0001
2017-10-10T12:13:36.741726: step 7838, loss 0.0739127, acc 0.96875, learning_rate 0.0001
2017-10-10T12:13:36.929001: step 7839, loss 0.035882, acc 1, learning_rate 0.0001
2017-10-10T12:13:37.096571: step 7840, loss 0.0754441, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T12:13:37.521982: step 7840, loss 0.196723, acc 0.930935

Saved model checkpoint to /home/sheep/bigdata/runs/1507653618/checkpoints/model-7840

