
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=128

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/xxliu10/bigdata/runs/1507751131

Load glove file /home/xxliu10/bigdata/vec25.txt
glove file has been loaded

2017-10-11T14:45:39.041686: step 1, loss 6.69381, acc 0.140625, learning_rate 0.005
2017-10-11T14:45:39.186547: step 2, loss 6.85393, acc 0.34375, learning_rate 0.00498
2017-10-11T14:45:39.323208: step 3, loss 5.36538, acc 0.40625, learning_rate 0.00496008
2017-10-11T14:45:39.458131: step 4, loss 6.36516, acc 0.390625, learning_rate 0.00494024
2017-10-11T14:45:39.602487: step 5, loss 7.04256, acc 0.3125, learning_rate 0.00492049
2017-10-11T14:45:39.750859: step 6, loss 3.31164, acc 0.5, learning_rate 0.00490081
2017-10-11T14:45:39.904993: step 7, loss 3.67422, acc 0.484375, learning_rate 0.00488121
2017-10-11T14:45:40.051243: step 8, loss 4.20814, acc 0.359375, learning_rate 0.0048617
2017-10-11T14:45:40.211251: step 9, loss 4.22694, acc 0.390625, learning_rate 0.00484226
2017-10-11T14:45:40.370636: step 10, loss 5.01962, acc 0.375, learning_rate 0.00482291
2017-10-11T14:45:40.512573: step 11, loss 2.19927, acc 0.5625, learning_rate 0.00480363
2017-10-11T14:45:40.678449: step 12, loss 2.90219, acc 0.546875, learning_rate 0.00478443
2017-10-11T14:45:40.852053: step 13, loss 2.94029, acc 0.515625, learning_rate 0.00476531
2017-10-11T14:45:41.002174: step 14, loss 2.73349, acc 0.59375, learning_rate 0.00474627
2017-10-11T14:45:41.176673: step 15, loss 3.30399, acc 0.515625, learning_rate 0.0047273
2017-10-11T14:45:41.317403: step 16, loss 2.53569, acc 0.625, learning_rate 0.00470841
2017-10-11T14:45:41.468158: step 17, loss 2.7625, acc 0.578125, learning_rate 0.0046896
2017-10-11T14:45:41.621400: step 18, loss 1.59496, acc 0.734375, learning_rate 0.00467087
2017-10-11T14:45:41.782976: step 19, loss 0.752611, acc 0.75, learning_rate 0.00465221
2017-10-11T14:45:41.939972: step 20, loss 2.11629, acc 0.65625, learning_rate 0.00463363
2017-10-11T14:45:42.094940: step 21, loss 1.93945, acc 0.578125, learning_rate 0.00461513
2017-10-11T14:45:42.253006: step 22, loss 1.95405, acc 0.609375, learning_rate 0.0045967
2017-10-11T14:45:42.420807: step 23, loss 1.89635, acc 0.625, learning_rate 0.00457834
2017-10-11T14:45:42.581327: step 24, loss 2.08274, acc 0.578125, learning_rate 0.00456006
2017-10-11T14:45:42.743313: step 25, loss 1.83749, acc 0.671875, learning_rate 0.00454186
2017-10-11T14:45:42.901000: step 26, loss 1.05798, acc 0.734375, learning_rate 0.00452373
2017-10-11T14:45:43.066067: step 27, loss 1.18108, acc 0.765625, learning_rate 0.00450567
2017-10-11T14:45:43.212035: step 28, loss 0.993944, acc 0.75, learning_rate 0.00448769
2017-10-11T14:45:43.372733: step 29, loss 1.42441, acc 0.734375, learning_rate 0.00446978
2017-10-11T14:45:43.524785: step 30, loss 1.20662, acc 0.671875, learning_rate 0.00445194
2017-10-11T14:45:43.686566: step 31, loss 1.06667, acc 0.734375, learning_rate 0.00443418
2017-10-11T14:45:43.844342: step 32, loss 1.98443, acc 0.703125, learning_rate 0.00441649
2017-10-11T14:45:44.007223: step 33, loss 1.54476, acc 0.703125, learning_rate 0.00439887
2017-10-11T14:45:44.158461: step 34, loss 1.34538, acc 0.71875, learning_rate 0.00438132
2017-10-11T14:45:44.320265: step 35, loss 1.3966, acc 0.703125, learning_rate 0.00436385
2017-10-11T14:45:44.490088: step 36, loss 1.20228, acc 0.71875, learning_rate 0.00434644
2017-10-11T14:45:44.646267: step 37, loss 1.15576, acc 0.734375, learning_rate 0.00432911
2017-10-11T14:45:44.804719: step 38, loss 1.5682, acc 0.65625, learning_rate 0.00431185
2017-10-11T14:45:44.979010: step 39, loss 1.0059, acc 0.78125, learning_rate 0.00429465
2017-10-11T14:45:45.132308: step 40, loss 1.04611, acc 0.75, learning_rate 0.00427753

Evaluation:
2017-10-11T14:45:45.284158: step 40, loss 0.5589, acc 0.858993

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-40

2017-10-11T14:45:46.638177: step 41, loss 1.82176, acc 0.78125, learning_rate 0.00426048
2017-10-11T14:45:46.803795: step 42, loss 0.879127, acc 0.8125, learning_rate 0.0042435
2017-10-11T14:45:46.973964: step 43, loss 1.53601, acc 0.71875, learning_rate 0.00422659
2017-10-11T14:45:47.138576: step 44, loss 1.04957, acc 0.765625, learning_rate 0.00420974
2017-10-11T14:45:47.300037: step 45, loss 1.20918, acc 0.765625, learning_rate 0.00419297
2017-10-11T14:45:47.467754: step 46, loss 1.37504, acc 0.71875, learning_rate 0.00417626
2017-10-11T14:45:47.623803: step 47, loss 1.2896, acc 0.828125, learning_rate 0.00415962
2017-10-11T14:45:47.788183: step 48, loss 1.29456, acc 0.71875, learning_rate 0.00414305
2017-10-11T14:45:47.952200: step 49, loss 1.63801, acc 0.640625, learning_rate 0.00412655
2017-10-11T14:45:48.122366: step 50, loss 1.1529, acc 0.75, learning_rate 0.00411011
2017-10-11T14:45:48.299897: step 51, loss 0.906281, acc 0.828125, learning_rate 0.00409375
2017-10-11T14:45:48.476828: step 52, loss 1.09107, acc 0.71875, learning_rate 0.00407744
2017-10-11T14:45:48.636956: step 53, loss 0.863658, acc 0.78125, learning_rate 0.00406121
2017-10-11T14:45:48.809325: step 54, loss 0.794726, acc 0.828125, learning_rate 0.00404504
2017-10-11T14:45:48.982125: step 55, loss 0.678055, acc 0.84375, learning_rate 0.00402894
2017-10-11T14:45:49.148779: step 56, loss 1.54836, acc 0.765625, learning_rate 0.0040129
2017-10-11T14:45:49.314429: step 57, loss 1.43417, acc 0.703125, learning_rate 0.00399693
2017-10-11T14:45:49.472541: step 58, loss 0.888048, acc 0.78125, learning_rate 0.00398102
2017-10-11T14:45:49.637200: step 59, loss 0.870153, acc 0.734375, learning_rate 0.00396518
2017-10-11T14:45:49.809829: step 60, loss 1.1897, acc 0.75, learning_rate 0.00394941
2017-10-11T14:45:49.971426: step 61, loss 1.14992, acc 0.84375, learning_rate 0.00393369
2017-10-11T14:45:50.138518: step 62, loss 0.622093, acc 0.828125, learning_rate 0.00391804
2017-10-11T14:45:50.307163: step 63, loss 1.08185, acc 0.765625, learning_rate 0.00390246
2017-10-11T14:45:50.476412: step 64, loss 1.20805, acc 0.8125, learning_rate 0.00388694
2017-10-11T14:45:50.657046: step 65, loss 0.601152, acc 0.859375, learning_rate 0.00387148
2017-10-11T14:45:50.830554: step 66, loss 0.83308, acc 0.796875, learning_rate 0.00385609
2017-10-11T14:45:51.005625: step 67, loss 1.15115, acc 0.75, learning_rate 0.00384076
2017-10-11T14:45:51.171674: step 68, loss 1.01333, acc 0.765625, learning_rate 0.00382549
2017-10-11T14:45:51.348062: step 69, loss 0.852568, acc 0.765625, learning_rate 0.00381028
2017-10-11T14:45:51.510045: step 70, loss 0.882968, acc 0.796875, learning_rate 0.00379514
2017-10-11T14:45:51.677038: step 71, loss 0.906774, acc 0.8125, learning_rate 0.00378005
2017-10-11T14:45:51.846035: step 72, loss 0.563691, acc 0.796875, learning_rate 0.00376503
2017-10-11T14:45:52.017693: step 73, loss 0.935319, acc 0.78125, learning_rate 0.00375007
2017-10-11T14:45:52.189543: step 74, loss 0.588326, acc 0.875, learning_rate 0.00373517
2017-10-11T14:45:52.376292: step 75, loss 0.451418, acc 0.875, learning_rate 0.00372034
2017-10-11T14:45:52.539542: step 76, loss 0.575684, acc 0.796875, learning_rate 0.00370556
2017-10-11T14:45:52.719196: step 77, loss 0.686837, acc 0.84375, learning_rate 0.00369084
2017-10-11T14:45:52.902203: step 78, loss 0.451878, acc 0.90625, learning_rate 0.00367619
2017-10-11T14:45:53.070234: step 79, loss 0.656095, acc 0.890625, learning_rate 0.00366159
2017-10-11T14:45:53.246023: step 80, loss 0.551229, acc 0.8125, learning_rate 0.00364705

Evaluation:
2017-10-11T14:45:53.402187: step 80, loss 0.438581, acc 0.883453

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-80

2017-10-11T14:45:54.750180: step 81, loss 1.1193, acc 0.78125, learning_rate 0.00363257
2017-10-11T14:45:54.931137: step 82, loss 0.65088, acc 0.8125, learning_rate 0.00361815
2017-10-11T14:45:55.105914: step 83, loss 1.20991, acc 0.8125, learning_rate 0.00360379
2017-10-11T14:45:55.290471: step 84, loss 0.816149, acc 0.78125, learning_rate 0.00358949
2017-10-11T14:45:55.468141: step 85, loss 0.721703, acc 0.78125, learning_rate 0.00357525
2017-10-11T14:45:55.649312: step 86, loss 0.489017, acc 0.890625, learning_rate 0.00356106
2017-10-11T14:45:55.825878: step 87, loss 1.26267, acc 0.75, learning_rate 0.00354694
2017-10-11T14:45:55.995080: step 88, loss 0.586334, acc 0.859375, learning_rate 0.00353287
2017-10-11T14:45:56.168974: step 89, loss 0.76741, acc 0.828125, learning_rate 0.00351885
2017-10-11T14:45:56.348439: step 90, loss 0.587932, acc 0.84375, learning_rate 0.0035049
2017-10-11T14:45:56.519807: step 91, loss 1.0195, acc 0.796875, learning_rate 0.003491
2017-10-11T14:45:56.685651: step 92, loss 0.549378, acc 0.84375, learning_rate 0.00347716
2017-10-11T14:45:56.851455: step 93, loss 0.748618, acc 0.796875, learning_rate 0.00346338
2017-10-11T14:45:57.029257: step 94, loss 0.363551, acc 0.921875, learning_rate 0.00344965
2017-10-11T14:45:57.214617: step 95, loss 0.858466, acc 0.78125, learning_rate 0.00343597
2017-10-11T14:45:57.391656: step 96, loss 0.6667, acc 0.859375, learning_rate 0.00342236
2017-10-11T14:45:57.561751: step 97, loss 1.12274, acc 0.796875, learning_rate 0.0034088
2017-10-11T14:45:57.714722: step 98, loss 0.756174, acc 0.803922, learning_rate 0.00339529
2017-10-11T14:45:57.901395: step 99, loss 0.519488, acc 0.859375, learning_rate 0.00338184
2017-10-11T14:45:58.075691: step 100, loss 0.916338, acc 0.828125, learning_rate 0.00336844
2017-10-11T14:45:58.246241: step 101, loss 1.10566, acc 0.796875, learning_rate 0.0033551
2017-10-11T14:45:58.421054: step 102, loss 0.904304, acc 0.78125, learning_rate 0.00334182
2017-10-11T14:45:58.595562: step 103, loss 0.373028, acc 0.84375, learning_rate 0.00332858
2017-10-11T14:45:58.761671: step 104, loss 0.387186, acc 0.875, learning_rate 0.00331541
2017-10-11T14:45:58.947201: step 105, loss 0.894619, acc 0.765625, learning_rate 0.00330228
2017-10-11T14:45:59.118593: step 106, loss 0.686509, acc 0.84375, learning_rate 0.00328921
2017-10-11T14:45:59.289375: step 107, loss 0.704275, acc 0.828125, learning_rate 0.00327619
2017-10-11T14:45:59.464151: step 108, loss 0.699833, acc 0.84375, learning_rate 0.00326323
2017-10-11T14:45:59.628575: step 109, loss 0.442732, acc 0.90625, learning_rate 0.00325032
2017-10-11T14:45:59.805279: step 110, loss 1.30194, acc 0.75, learning_rate 0.00323746
2017-10-11T14:45:59.980587: step 111, loss 0.680828, acc 0.828125, learning_rate 0.00322465
2017-10-11T14:46:00.165586: step 112, loss 1.02794, acc 0.8125, learning_rate 0.0032119
2017-10-11T14:46:00.334405: step 113, loss 0.540067, acc 0.828125, learning_rate 0.0031992
2017-10-11T14:46:00.517919: step 114, loss 0.600807, acc 0.90625, learning_rate 0.00318655
2017-10-11T14:46:00.689640: step 115, loss 0.689747, acc 0.859375, learning_rate 0.00317395
2017-10-11T14:46:00.866521: step 116, loss 1.14765, acc 0.71875, learning_rate 0.0031614
2017-10-11T14:46:01.042541: step 117, loss 0.392976, acc 0.890625, learning_rate 0.0031489
2017-10-11T14:46:01.223454: step 118, loss 0.822777, acc 0.859375, learning_rate 0.00313646
2017-10-11T14:46:01.393055: step 119, loss 0.717883, acc 0.78125, learning_rate 0.00312407
2017-10-11T14:46:01.566183: step 120, loss 0.678401, acc 0.859375, learning_rate 0.00311172

Evaluation:
2017-10-11T14:46:01.709473: step 120, loss 0.404415, acc 0.889209

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-120

2017-10-11T14:46:03.061573: step 121, loss 0.372641, acc 0.90625, learning_rate 0.00309943
2017-10-11T14:46:03.232543: step 122, loss 0.508054, acc 0.828125, learning_rate 0.00308719
2017-10-11T14:46:03.414628: step 123, loss 0.617658, acc 0.859375, learning_rate 0.00307499
2017-10-11T14:46:03.593197: step 124, loss 0.741647, acc 0.8125, learning_rate 0.00306285
2017-10-11T14:46:03.767785: step 125, loss 0.503309, acc 0.78125, learning_rate 0.00305076
2017-10-11T14:46:03.950144: step 126, loss 0.46473, acc 0.890625, learning_rate 0.00303871
2017-10-11T14:46:04.124262: step 127, loss 0.775976, acc 0.890625, learning_rate 0.00302672
2017-10-11T14:46:04.307168: step 128, loss 0.721627, acc 0.84375, learning_rate 0.00301477
2017-10-11T14:46:04.481656: step 129, loss 0.627397, acc 0.8125, learning_rate 0.00300287
2017-10-11T14:46:04.652344: step 130, loss 0.658708, acc 0.84375, learning_rate 0.00299102
2017-10-11T14:46:04.830143: step 131, loss 0.488063, acc 0.890625, learning_rate 0.00297922
2017-10-11T14:46:05.006413: step 132, loss 0.400736, acc 0.90625, learning_rate 0.00296747
2017-10-11T14:46:05.182904: step 133, loss 0.615317, acc 0.859375, learning_rate 0.00295577
2017-10-11T14:46:05.366679: step 134, loss 0.328677, acc 0.921875, learning_rate 0.00294411
2017-10-11T14:46:05.540590: step 135, loss 0.49697, acc 0.84375, learning_rate 0.0029325
2017-10-11T14:46:05.717888: step 136, loss 0.60553, acc 0.875, learning_rate 0.00292094
2017-10-11T14:46:05.891652: step 137, loss 0.577408, acc 0.84375, learning_rate 0.00290943
2017-10-11T14:46:06.078100: step 138, loss 0.863962, acc 0.8125, learning_rate 0.00289796
2017-10-11T14:46:06.262972: step 139, loss 0.755519, acc 0.796875, learning_rate 0.00288654
2017-10-11T14:46:06.428442: step 140, loss 0.351544, acc 0.84375, learning_rate 0.00287516
2017-10-11T14:46:06.595430: step 141, loss 0.625105, acc 0.875, learning_rate 0.00286384
2017-10-11T14:46:06.767718: step 142, loss 0.666082, acc 0.8125, learning_rate 0.00285256
2017-10-11T14:46:06.948547: step 143, loss 0.899861, acc 0.765625, learning_rate 0.00284132
2017-10-11T14:46:07.113146: step 144, loss 0.446292, acc 0.875, learning_rate 0.00283013
2017-10-11T14:46:07.285579: step 145, loss 0.195065, acc 0.9375, learning_rate 0.00281899
2017-10-11T14:46:07.463872: step 146, loss 0.771723, acc 0.828125, learning_rate 0.00280789
2017-10-11T14:46:07.631456: step 147, loss 0.524265, acc 0.859375, learning_rate 0.00279684
2017-10-11T14:46:07.795083: step 148, loss 0.515123, acc 0.84375, learning_rate 0.00278583
2017-10-11T14:46:07.967478: step 149, loss 0.25665, acc 0.90625, learning_rate 0.00277486
2017-10-11T14:46:08.140023: step 150, loss 0.593502, acc 0.8125, learning_rate 0.00276395
2017-10-11T14:46:08.314434: step 151, loss 0.922493, acc 0.78125, learning_rate 0.00275307
2017-10-11T14:46:08.492355: step 152, loss 0.709313, acc 0.796875, learning_rate 0.00274224
2017-10-11T14:46:08.663703: step 153, loss 0.564725, acc 0.828125, learning_rate 0.00273146
2017-10-11T14:46:08.857275: step 154, loss 0.537232, acc 0.890625, learning_rate 0.00272072
2017-10-11T14:46:09.047463: step 155, loss 0.268491, acc 0.9375, learning_rate 0.00271002
2017-10-11T14:46:09.232396: step 156, loss 0.640022, acc 0.875, learning_rate 0.00269937
2017-10-11T14:46:09.415881: step 157, loss 0.356031, acc 0.9375, learning_rate 0.00268876
2017-10-11T14:46:09.600687: step 158, loss 0.356369, acc 0.90625, learning_rate 0.00267819
2017-10-11T14:46:09.790911: step 159, loss 0.828156, acc 0.828125, learning_rate 0.00266767
2017-10-11T14:46:09.956519: step 160, loss 0.405208, acc 0.828125, learning_rate 0.00265719

Evaluation:
2017-10-11T14:46:10.120234: step 160, loss 0.341882, acc 0.884892

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-160

2017-10-11T14:46:11.485983: step 161, loss 0.18849, acc 0.9375, learning_rate 0.00264675
2017-10-11T14:46:11.666281: step 162, loss 0.404758, acc 0.890625, learning_rate 0.00263635
2017-10-11T14:46:11.857334: step 163, loss 0.439774, acc 0.859375, learning_rate 0.002626
2017-10-11T14:46:12.042275: step 164, loss 0.387384, acc 0.859375, learning_rate 0.00261569
2017-10-11T14:46:12.219150: step 165, loss 0.453929, acc 0.859375, learning_rate 0.00260542
2017-10-11T14:46:12.393723: step 166, loss 0.621838, acc 0.875, learning_rate 0.0025952
2017-10-11T14:46:12.566293: step 167, loss 0.0764585, acc 0.96875, learning_rate 0.00258501
2017-10-11T14:46:12.737419: step 168, loss 0.44752, acc 0.90625, learning_rate 0.00257487
2017-10-11T14:46:12.920132: step 169, loss 0.269766, acc 0.921875, learning_rate 0.00256477
2017-10-11T14:46:13.097689: step 170, loss 0.367966, acc 0.875, learning_rate 0.0025547
2017-10-11T14:46:13.282118: step 171, loss 0.618873, acc 0.78125, learning_rate 0.00254469
2017-10-11T14:46:13.454976: step 172, loss 0.304915, acc 0.875, learning_rate 0.00253471
2017-10-11T14:46:13.627165: step 173, loss 0.428073, acc 0.90625, learning_rate 0.00252477
2017-10-11T14:46:13.801573: step 174, loss 0.553037, acc 0.890625, learning_rate 0.00251487
2017-10-11T14:46:13.972436: step 175, loss 0.232591, acc 0.90625, learning_rate 0.00250501
2017-10-11T14:46:14.149154: step 176, loss 0.205128, acc 0.953125, learning_rate 0.0024952
2017-10-11T14:46:14.335308: step 177, loss 0.640936, acc 0.859375, learning_rate 0.00248542
2017-10-11T14:46:14.522382: step 178, loss 0.339589, acc 0.875, learning_rate 0.00247568
2017-10-11T14:46:14.695808: step 179, loss 0.35394, acc 0.9375, learning_rate 0.00246599
2017-10-11T14:46:14.872817: step 180, loss 0.610925, acc 0.859375, learning_rate 0.00245633
2017-10-11T14:46:15.043573: step 181, loss 0.496448, acc 0.84375, learning_rate 0.00244671
2017-10-11T14:46:15.232491: step 182, loss 0.69104, acc 0.84375, learning_rate 0.00243713
2017-10-11T14:46:15.418813: step 183, loss 0.411517, acc 0.890625, learning_rate 0.00242759
2017-10-11T14:46:15.600778: step 184, loss 0.440249, acc 0.890625, learning_rate 0.00241809
2017-10-11T14:46:15.780506: step 185, loss 0.519902, acc 0.84375, learning_rate 0.00240863
2017-10-11T14:46:15.962359: step 186, loss 0.217377, acc 0.90625, learning_rate 0.00239921
2017-10-11T14:46:16.138269: step 187, loss 0.408414, acc 0.90625, learning_rate 0.00238982
2017-10-11T14:46:16.318313: step 188, loss 0.22729, acc 0.890625, learning_rate 0.00238048
2017-10-11T14:46:16.485893: step 189, loss 0.355871, acc 0.921875, learning_rate 0.00237117
2017-10-11T14:46:16.676805: step 190, loss 0.373199, acc 0.890625, learning_rate 0.0023619
2017-10-11T14:46:16.858885: step 191, loss 0.339559, acc 0.890625, learning_rate 0.00235267
2017-10-11T14:46:17.036904: step 192, loss 0.558995, acc 0.84375, learning_rate 0.00234347
2017-10-11T14:46:17.209820: step 193, loss 0.337746, acc 0.90625, learning_rate 0.00233431
2017-10-11T14:46:17.390207: step 194, loss 0.478646, acc 0.84375, learning_rate 0.00232519
2017-10-11T14:46:17.576991: step 195, loss 0.413002, acc 0.875, learning_rate 0.00231611
2017-10-11T14:46:17.736511: step 196, loss 0.586308, acc 0.862745, learning_rate 0.00230707
2017-10-11T14:46:17.913876: step 197, loss 0.266922, acc 0.90625, learning_rate 0.00229806
2017-10-11T14:46:18.092036: step 198, loss 0.245202, acc 0.921875, learning_rate 0.00228908
2017-10-11T14:46:18.264193: step 199, loss 0.386193, acc 0.90625, learning_rate 0.00228015
2017-10-11T14:46:18.449809: step 200, loss 0.334699, acc 0.890625, learning_rate 0.00227125

Evaluation:
2017-10-11T14:46:18.615761: step 200, loss 0.331288, acc 0.894964

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-200

2017-10-11T14:46:19.679465: step 201, loss 0.673993, acc 0.828125, learning_rate 0.00226239
2017-10-11T14:46:19.859102: step 202, loss 0.536123, acc 0.8125, learning_rate 0.00225356
2017-10-11T14:46:20.032583: step 203, loss 0.288965, acc 0.890625, learning_rate 0.00224477
2017-10-11T14:46:20.211537: step 204, loss 0.382645, acc 0.921875, learning_rate 0.00223602
2017-10-11T14:46:20.384092: step 205, loss 0.063888, acc 0.96875, learning_rate 0.0022273
2017-10-11T14:46:20.558860: step 206, loss 0.166775, acc 0.90625, learning_rate 0.00221862
2017-10-11T14:46:20.758490: step 207, loss 0.313518, acc 0.875, learning_rate 0.00220997
2017-10-11T14:46:20.941415: step 208, loss 0.299147, acc 0.90625, learning_rate 0.00220136
2017-10-11T14:46:21.125176: step 209, loss 0.408396, acc 0.84375, learning_rate 0.00219278
2017-10-11T14:46:21.293885: step 210, loss 0.27439, acc 0.875, learning_rate 0.00218424
2017-10-11T14:46:21.476460: step 211, loss 0.332671, acc 0.921875, learning_rate 0.00217573
2017-10-11T14:46:21.663912: step 212, loss 0.108952, acc 0.953125, learning_rate 0.00216726
2017-10-11T14:46:21.835254: step 213, loss 0.237633, acc 0.9375, learning_rate 0.00215882
2017-10-11T14:46:22.025360: step 214, loss 0.570352, acc 0.90625, learning_rate 0.00215041
2017-10-11T14:46:22.213265: step 215, loss 0.248387, acc 0.875, learning_rate 0.00214204
2017-10-11T14:46:22.397784: step 216, loss 0.287189, acc 0.90625, learning_rate 0.00213371
2017-10-11T14:46:22.567989: step 217, loss 0.24647, acc 0.921875, learning_rate 0.00212541
2017-10-11T14:46:22.758540: step 218, loss 0.268263, acc 0.953125, learning_rate 0.00211714
2017-10-11T14:46:22.949701: step 219, loss 0.533553, acc 0.875, learning_rate 0.00210891
2017-10-11T14:46:23.129004: step 220, loss 0.432238, acc 0.90625, learning_rate 0.00210071
2017-10-11T14:46:23.313503: step 221, loss 0.201856, acc 0.9375, learning_rate 0.00209254
2017-10-11T14:46:23.498644: step 222, loss 0.119627, acc 0.9375, learning_rate 0.00208441
2017-10-11T14:46:23.688817: step 223, loss 0.361199, acc 0.890625, learning_rate 0.00207631
2017-10-11T14:46:23.864909: step 224, loss 0.296675, acc 0.90625, learning_rate 0.00206824
2017-10-11T14:46:24.038516: step 225, loss 0.312472, acc 0.9375, learning_rate 0.00206021
2017-10-11T14:46:24.218063: step 226, loss 0.136289, acc 0.96875, learning_rate 0.00205221
2017-10-11T14:46:24.410425: step 227, loss 0.269865, acc 0.890625, learning_rate 0.00204424
2017-10-11T14:46:24.581472: step 228, loss 0.219073, acc 0.921875, learning_rate 0.0020363
2017-10-11T14:46:24.760347: step 229, loss 0.414322, acc 0.875, learning_rate 0.0020284
2017-10-11T14:46:24.944079: step 230, loss 0.329071, acc 0.875, learning_rate 0.00202053
2017-10-11T14:46:25.118683: step 231, loss 0.651992, acc 0.8125, learning_rate 0.00201269
2017-10-11T14:46:25.289831: step 232, loss 0.191043, acc 0.890625, learning_rate 0.00200488
2017-10-11T14:46:25.465022: step 233, loss 0.188939, acc 0.921875, learning_rate 0.00199711
2017-10-11T14:46:25.654730: step 234, loss 0.426956, acc 0.90625, learning_rate 0.00198936
2017-10-11T14:46:25.834768: step 235, loss 0.261996, acc 0.890625, learning_rate 0.00198165
2017-10-11T14:46:26.030111: step 236, loss 0.218751, acc 0.9375, learning_rate 0.00197397
2017-10-11T14:46:26.212694: step 237, loss 0.311252, acc 0.859375, learning_rate 0.00196632
2017-10-11T14:46:26.386807: step 238, loss 0.435104, acc 0.90625, learning_rate 0.0019587
2017-10-11T14:46:26.560992: step 239, loss 0.404207, acc 0.890625, learning_rate 0.00195112
2017-10-11T14:46:26.744751: step 240, loss 0.322624, acc 0.90625, learning_rate 0.00194356

Evaluation:
2017-10-11T14:46:26.908150: step 240, loss 0.305367, acc 0.894964

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-240

2017-10-11T14:46:28.536814: step 241, loss 0.167628, acc 0.953125, learning_rate 0.00193604
2017-10-11T14:46:28.723545: step 242, loss 0.317037, acc 0.890625, learning_rate 0.00192854
2017-10-11T14:46:28.904239: step 243, loss 0.357995, acc 0.859375, learning_rate 0.00192108
2017-10-11T14:46:29.076850: step 244, loss 0.295477, acc 0.890625, learning_rate 0.00191364
2017-10-11T14:46:29.261039: step 245, loss 0.409668, acc 0.890625, learning_rate 0.00190624
2017-10-11T14:46:29.443875: step 246, loss 0.345048, acc 0.890625, learning_rate 0.00189887
2017-10-11T14:46:29.628589: step 247, loss 0.484, acc 0.875, learning_rate 0.00189153
2017-10-11T14:46:29.808359: step 248, loss 0.435404, acc 0.921875, learning_rate 0.00188421
2017-10-11T14:46:30.002069: step 249, loss 0.310206, acc 0.890625, learning_rate 0.00187693
2017-10-11T14:46:30.180259: step 250, loss 0.510042, acc 0.875, learning_rate 0.00186968
2017-10-11T14:46:30.368232: step 251, loss 0.309806, acc 0.90625, learning_rate 0.00186245
2017-10-11T14:46:30.554040: step 252, loss 0.0250338, acc 1, learning_rate 0.00185526
2017-10-11T14:46:30.721466: step 253, loss 0.194868, acc 0.890625, learning_rate 0.0018481
2017-10-11T14:46:30.884941: step 254, loss 0.296616, acc 0.859375, learning_rate 0.00184096
2017-10-11T14:46:31.045900: step 255, loss 0.26354, acc 0.9375, learning_rate 0.00183385
2017-10-11T14:46:31.228803: step 256, loss 0.45331, acc 0.875, learning_rate 0.00182678
2017-10-11T14:46:31.401552: step 257, loss 0.263288, acc 0.921875, learning_rate 0.00181973
2017-10-11T14:46:31.573476: step 258, loss 0.243067, acc 0.921875, learning_rate 0.00181271
2017-10-11T14:46:31.742222: step 259, loss 0.210573, acc 0.921875, learning_rate 0.00180572
2017-10-11T14:46:31.907069: step 260, loss 0.423527, acc 0.8125, learning_rate 0.00179876
2017-10-11T14:46:32.080649: step 261, loss 0.230916, acc 0.921875, learning_rate 0.00179182
2017-10-11T14:46:32.253567: step 262, loss 0.228518, acc 0.9375, learning_rate 0.00178492
2017-10-11T14:46:32.423688: step 263, loss 0.410621, acc 0.875, learning_rate 0.00177804
2017-10-11T14:46:32.601342: step 264, loss 0.366401, acc 0.90625, learning_rate 0.00177119
2017-10-11T14:46:32.798789: step 265, loss 0.122679, acc 0.9375, learning_rate 0.00176437
2017-10-11T14:46:32.983659: step 266, loss 0.351949, acc 0.859375, learning_rate 0.00175758
2017-10-11T14:46:33.160078: step 267, loss 0.359974, acc 0.875, learning_rate 0.00175081
2017-10-11T14:46:33.339691: step 268, loss 0.466985, acc 0.90625, learning_rate 0.00174407
2017-10-11T14:46:33.509394: step 269, loss 0.50748, acc 0.828125, learning_rate 0.00173736
2017-10-11T14:46:33.680014: step 270, loss 0.190244, acc 0.96875, learning_rate 0.00173068
2017-10-11T14:46:33.864793: step 271, loss 0.314219, acc 0.921875, learning_rate 0.00172402
2017-10-11T14:46:34.040131: step 272, loss 0.251794, acc 0.9375, learning_rate 0.00171739
2017-10-11T14:46:34.210914: step 273, loss 0.108442, acc 0.96875, learning_rate 0.00171079
2017-10-11T14:46:34.390473: step 274, loss 0.662377, acc 0.84375, learning_rate 0.00170422
2017-10-11T14:46:34.565650: step 275, loss 0.307551, acc 0.90625, learning_rate 0.00169767
2017-10-11T14:46:34.735133: step 276, loss 0.205852, acc 0.90625, learning_rate 0.00169115
2017-10-11T14:46:34.916606: step 277, loss 0.43198, acc 0.84375, learning_rate 0.00168465
2017-10-11T14:46:35.084420: step 278, loss 0.427561, acc 0.875, learning_rate 0.00167818
2017-10-11T14:46:35.264058: step 279, loss 0.515504, acc 0.84375, learning_rate 0.00167174
2017-10-11T14:46:35.443320: step 280, loss 0.218598, acc 0.90625, learning_rate 0.00166533

Evaluation:
2017-10-11T14:46:35.600965: step 280, loss 0.280812, acc 0.899281

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-280

2017-10-11T14:46:36.675364: step 281, loss 0.456988, acc 0.859375, learning_rate 0.00165894
2017-10-11T14:46:36.848815: step 282, loss 0.631416, acc 0.828125, learning_rate 0.00165257
2017-10-11T14:46:37.022765: step 283, loss 0.263093, acc 0.90625, learning_rate 0.00164624
2017-10-11T14:46:37.208721: step 284, loss 0.511236, acc 0.8125, learning_rate 0.00163993
2017-10-11T14:46:37.380369: step 285, loss 0.283648, acc 0.875, learning_rate 0.00163364
2017-10-11T14:46:37.559637: step 286, loss 0.207564, acc 0.890625, learning_rate 0.00162738
2017-10-11T14:46:37.737267: step 287, loss 0.213874, acc 0.921875, learning_rate 0.00162115
2017-10-11T14:46:37.915738: step 288, loss 0.270671, acc 0.921875, learning_rate 0.00161494
2017-10-11T14:46:38.093960: step 289, loss 0.530369, acc 0.828125, learning_rate 0.00160875
2017-10-11T14:46:38.267920: step 290, loss 0.290023, acc 0.875, learning_rate 0.00160259
2017-10-11T14:46:38.440796: step 291, loss 0.297809, acc 0.921875, learning_rate 0.00159646
2017-10-11T14:46:38.628443: step 292, loss 0.128425, acc 0.96875, learning_rate 0.00159035
2017-10-11T14:46:38.802603: step 293, loss 0.434996, acc 0.859375, learning_rate 0.00158427
2017-10-11T14:46:38.961409: step 294, loss 0.409841, acc 0.901961, learning_rate 0.00157821
2017-10-11T14:46:39.145850: step 295, loss 0.173274, acc 0.921875, learning_rate 0.00157218
2017-10-11T14:46:39.321630: step 296, loss 0.145717, acc 0.96875, learning_rate 0.00156617
2017-10-11T14:46:39.502492: step 297, loss 0.299099, acc 0.90625, learning_rate 0.00156018
2017-10-11T14:46:39.677413: step 298, loss 0.153606, acc 0.921875, learning_rate 0.00155422
2017-10-11T14:46:39.858443: step 299, loss 0.33546, acc 0.90625, learning_rate 0.00154829
2017-10-11T14:46:40.028863: step 300, loss 0.0592518, acc 0.984375, learning_rate 0.00154238
2017-10-11T14:46:40.191254: step 301, loss 0.61557, acc 0.84375, learning_rate 0.00153649
2017-10-11T14:46:40.356325: step 302, loss 0.314185, acc 0.90625, learning_rate 0.00153063
2017-10-11T14:46:40.546423: step 303, loss 0.0808101, acc 0.96875, learning_rate 0.00152479
2017-10-11T14:46:40.727375: step 304, loss 0.150356, acc 0.953125, learning_rate 0.00151897
2017-10-11T14:46:40.922831: step 305, loss 0.16978, acc 0.953125, learning_rate 0.00151318
2017-10-11T14:46:41.101713: step 306, loss 0.494082, acc 0.859375, learning_rate 0.00150741
2017-10-11T14:46:41.281841: step 307, loss 0.316551, acc 0.859375, learning_rate 0.00150167
2017-10-11T14:46:41.476450: step 308, loss 0.175833, acc 0.9375, learning_rate 0.00149594
2017-10-11T14:46:41.645935: step 309, loss 0.12105, acc 0.953125, learning_rate 0.00149025
2017-10-11T14:46:41.826902: step 310, loss 0.214987, acc 0.921875, learning_rate 0.00148457
2017-10-11T14:46:42.015528: step 311, loss 0.210093, acc 0.90625, learning_rate 0.00147892
2017-10-11T14:46:42.201607: step 312, loss 0.171554, acc 0.9375, learning_rate 0.00147329
2017-10-11T14:46:42.374451: step 313, loss 0.22538, acc 0.90625, learning_rate 0.00146769
2017-10-11T14:46:42.558929: step 314, loss 0.477462, acc 0.859375, learning_rate 0.0014621
2017-10-11T14:46:42.749278: step 315, loss 0.350554, acc 0.9375, learning_rate 0.00145654
2017-10-11T14:46:42.934054: step 316, loss 0.403324, acc 0.890625, learning_rate 0.00145101
2017-10-11T14:46:43.111063: step 317, loss 0.143504, acc 0.953125, learning_rate 0.00144549
2017-10-11T14:46:43.290945: step 318, loss 0.123126, acc 0.953125, learning_rate 0.00144
2017-10-11T14:46:43.468529: step 319, loss 0.408784, acc 0.890625, learning_rate 0.00143453
2017-10-11T14:46:43.646963: step 320, loss 0.398339, acc 0.890625, learning_rate 0.00142908

Evaluation:
2017-10-11T14:46:43.805091: step 320, loss 0.275892, acc 0.903597

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-320

2017-10-11T14:46:45.127571: step 321, loss 0.179107, acc 0.921875, learning_rate 0.00142366
2017-10-11T14:46:45.304630: step 322, loss 0.204434, acc 0.9375, learning_rate 0.00141826
2017-10-11T14:46:45.470736: step 323, loss 0.363144, acc 0.90625, learning_rate 0.00141288
2017-10-11T14:46:45.654690: step 324, loss 0.344901, acc 0.890625, learning_rate 0.00140752
2017-10-11T14:46:45.830766: step 325, loss 0.367345, acc 0.859375, learning_rate 0.00140218
2017-10-11T14:46:46.000514: step 326, loss 0.196712, acc 0.921875, learning_rate 0.00139686
2017-10-11T14:46:46.177224: step 327, loss 0.2403, acc 0.9375, learning_rate 0.00139157
2017-10-11T14:46:46.352179: step 328, loss 0.366057, acc 0.890625, learning_rate 0.0013863
2017-10-11T14:46:46.533520: step 329, loss 0.280467, acc 0.921875, learning_rate 0.00138105
2017-10-11T14:46:46.716022: step 330, loss 0.120772, acc 0.9375, learning_rate 0.00137582
2017-10-11T14:46:46.890825: step 331, loss 0.219168, acc 0.9375, learning_rate 0.00137061
2017-10-11T14:46:47.080683: step 332, loss 0.166932, acc 0.921875, learning_rate 0.00136543
2017-10-11T14:46:47.262568: step 333, loss 0.17128, acc 0.921875, learning_rate 0.00136026
2017-10-11T14:46:47.442163: step 334, loss 0.423395, acc 0.859375, learning_rate 0.00135512
2017-10-11T14:46:47.618041: step 335, loss 0.264616, acc 0.90625, learning_rate 0.00134999
2017-10-11T14:46:47.813806: step 336, loss 0.29249, acc 0.921875, learning_rate 0.00134489
2017-10-11T14:46:48.001266: step 337, loss 0.360367, acc 0.8125, learning_rate 0.00133981
2017-10-11T14:46:48.180688: step 338, loss 0.274169, acc 0.921875, learning_rate 0.00133475
2017-10-11T14:46:48.364328: step 339, loss 0.213055, acc 0.890625, learning_rate 0.00132971
2017-10-11T14:46:48.557910: step 340, loss 0.253859, acc 0.9375, learning_rate 0.00132469
2017-10-11T14:46:48.748209: step 341, loss 0.230135, acc 0.9375, learning_rate 0.00131969
2017-10-11T14:46:48.931372: step 342, loss 0.201975, acc 0.9375, learning_rate 0.00131471
2017-10-11T14:46:49.114088: step 343, loss 0.133749, acc 0.953125, learning_rate 0.00130975
2017-10-11T14:46:49.300329: step 344, loss 0.322793, acc 0.921875, learning_rate 0.00130482
2017-10-11T14:46:49.486840: step 345, loss 0.263938, acc 0.90625, learning_rate 0.0012999
2017-10-11T14:46:49.668954: step 346, loss 0.375991, acc 0.859375, learning_rate 0.001295
2017-10-11T14:46:49.842742: step 347, loss 0.29917, acc 0.890625, learning_rate 0.00129012
2017-10-11T14:46:50.023713: step 348, loss 0.220307, acc 0.953125, learning_rate 0.00128527
2017-10-11T14:46:50.199889: step 349, loss 0.398977, acc 0.875, learning_rate 0.00128043
2017-10-11T14:46:50.383632: step 350, loss 0.114728, acc 0.96875, learning_rate 0.00127561
2017-10-11T14:46:50.570144: step 351, loss 0.242701, acc 0.921875, learning_rate 0.00127081
2017-10-11T14:46:50.755825: step 352, loss 0.138526, acc 0.9375, learning_rate 0.00126603
2017-10-11T14:46:50.944512: step 353, loss 0.625277, acc 0.828125, learning_rate 0.00126127
2017-10-11T14:46:51.115720: step 354, loss 0.195399, acc 0.9375, learning_rate 0.00125653
2017-10-11T14:46:51.302460: step 355, loss 0.128553, acc 0.953125, learning_rate 0.00125181
2017-10-11T14:46:51.475045: step 356, loss 0.100061, acc 0.984375, learning_rate 0.00124711
2017-10-11T14:46:51.656655: step 357, loss 0.267444, acc 0.921875, learning_rate 0.00124243
2017-10-11T14:46:51.841150: step 358, loss 0.278125, acc 0.921875, learning_rate 0.00123777
2017-10-11T14:46:52.030082: step 359, loss 0.265936, acc 0.921875, learning_rate 0.00123312
2017-10-11T14:46:52.206552: step 360, loss 0.386946, acc 0.875, learning_rate 0.0012285

Evaluation:
2017-10-11T14:46:52.363281: step 360, loss 0.278092, acc 0.903597

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-360

2017-10-11T14:46:53.974640: step 361, loss 0.144512, acc 0.9375, learning_rate 0.00122389
2017-10-11T14:46:54.156138: step 362, loss 0.292994, acc 0.90625, learning_rate 0.0012193
2017-10-11T14:46:54.336121: step 363, loss 0.220566, acc 0.953125, learning_rate 0.00121473
2017-10-11T14:46:54.519598: step 364, loss 0.498448, acc 0.859375, learning_rate 0.00121018
2017-10-11T14:46:54.702215: step 365, loss 0.164767, acc 0.9375, learning_rate 0.00120565
2017-10-11T14:46:54.881835: step 366, loss 0.190051, acc 0.921875, learning_rate 0.00120114
2017-10-11T14:46:55.072365: step 367, loss 0.158289, acc 0.921875, learning_rate 0.00119664
2017-10-11T14:46:55.244778: step 368, loss 0.15299, acc 0.90625, learning_rate 0.00119217
2017-10-11T14:46:55.420378: step 369, loss 0.254495, acc 0.90625, learning_rate 0.00118771
2017-10-11T14:46:55.592907: step 370, loss 0.26127, acc 0.921875, learning_rate 0.00118327
2017-10-11T14:46:55.764411: step 371, loss 0.13545, acc 0.921875, learning_rate 0.00117885
2017-10-11T14:46:55.944602: step 372, loss 0.192594, acc 0.921875, learning_rate 0.00117445
2017-10-11T14:46:56.136009: step 373, loss 0.283256, acc 0.875, learning_rate 0.00117006
2017-10-11T14:46:56.319719: step 374, loss 0.299889, acc 0.890625, learning_rate 0.00116569
2017-10-11T14:46:56.503277: step 375, loss 0.355893, acc 0.921875, learning_rate 0.00116134
2017-10-11T14:46:56.675523: step 376, loss 0.338872, acc 0.921875, learning_rate 0.00115701
2017-10-11T14:46:56.862583: step 377, loss 0.223289, acc 0.921875, learning_rate 0.0011527
2017-10-11T14:46:57.030333: step 378, loss 0.0587543, acc 0.984375, learning_rate 0.0011484
2017-10-11T14:46:57.213580: step 379, loss 0.206079, acc 0.953125, learning_rate 0.00114412
2017-10-11T14:46:57.404553: step 380, loss 0.189695, acc 0.921875, learning_rate 0.00113986
2017-10-11T14:46:57.590651: step 381, loss 0.298013, acc 0.921875, learning_rate 0.00113561
2017-10-11T14:46:57.776650: step 382, loss 0.348891, acc 0.890625, learning_rate 0.00113139
2017-10-11T14:46:57.952953: step 383, loss 0.281806, acc 0.921875, learning_rate 0.00112718
2017-10-11T14:46:58.131461: step 384, loss 0.173699, acc 0.9375, learning_rate 0.00112298
2017-10-11T14:46:58.315064: step 385, loss 0.250746, acc 0.90625, learning_rate 0.00111881
2017-10-11T14:46:58.499809: step 386, loss 0.0545869, acc 0.984375, learning_rate 0.00111465
2017-10-11T14:46:58.688168: step 387, loss 0.197692, acc 0.9375, learning_rate 0.00111051
2017-10-11T14:46:58.868939: step 388, loss 0.29084, acc 0.9375, learning_rate 0.00110638
2017-10-11T14:46:59.053722: step 389, loss 0.173626, acc 0.9375, learning_rate 0.00110228
2017-10-11T14:46:59.237341: step 390, loss 0.133971, acc 0.9375, learning_rate 0.00109818
2017-10-11T14:46:59.431861: step 391, loss 0.289355, acc 0.9375, learning_rate 0.00109411
2017-10-11T14:46:59.585753: step 392, loss 0.20901, acc 0.941176, learning_rate 0.00109005
2017-10-11T14:46:59.762047: step 393, loss 0.14492, acc 0.953125, learning_rate 0.00108601
2017-10-11T14:46:59.932879: step 394, loss 0.20893, acc 0.921875, learning_rate 0.00108199
2017-10-11T14:47:00.117249: step 395, loss 0.218959, acc 0.90625, learning_rate 0.00107798
2017-10-11T14:47:00.303923: step 396, loss 0.0736609, acc 0.96875, learning_rate 0.00107399
2017-10-11T14:47:00.486994: step 397, loss 0.43141, acc 0.890625, learning_rate 0.00107001
2017-10-11T14:47:00.680367: step 398, loss 0.293712, acc 0.90625, learning_rate 0.00106605
2017-10-11T14:47:00.857837: step 399, loss 0.155013, acc 0.9375, learning_rate 0.00106211
2017-10-11T14:47:01.034659: step 400, loss 0.141736, acc 0.953125, learning_rate 0.00105818

Evaluation:
2017-10-11T14:47:01.189784: step 400, loss 0.270494, acc 0.906475

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-400

2017-10-11T14:47:02.270855: step 401, loss 0.163987, acc 0.90625, learning_rate 0.00105427
2017-10-11T14:47:02.447518: step 402, loss 0.367515, acc 0.875, learning_rate 0.00105037
2017-10-11T14:47:02.626836: step 403, loss 0.253671, acc 0.953125, learning_rate 0.0010465
2017-10-11T14:47:02.800832: step 404, loss 0.26204, acc 0.9375, learning_rate 0.00104263
2017-10-11T14:47:02.984928: step 405, loss 0.338178, acc 0.9375, learning_rate 0.00103878
2017-10-11T14:47:03.157161: step 406, loss 0.133065, acc 0.921875, learning_rate 0.00103495
2017-10-11T14:47:03.348967: step 407, loss 0.373548, acc 0.875, learning_rate 0.00103114
2017-10-11T14:47:03.545652: step 408, loss 0.0928348, acc 0.96875, learning_rate 0.00102734
2017-10-11T14:47:03.726077: step 409, loss 0.399047, acc 0.890625, learning_rate 0.00102355
2017-10-11T14:47:03.909269: step 410, loss 0.516316, acc 0.875, learning_rate 0.00101978
2017-10-11T14:47:04.097565: step 411, loss 0.201203, acc 0.90625, learning_rate 0.00101603
2017-10-11T14:47:04.274386: step 412, loss 0.147988, acc 0.984375, learning_rate 0.00101229
2017-10-11T14:47:04.463082: step 413, loss 0.330736, acc 0.90625, learning_rate 0.00100856
2017-10-11T14:47:04.643109: step 414, loss 0.236397, acc 0.921875, learning_rate 0.00100486
2017-10-11T14:47:04.823252: step 415, loss 0.126653, acc 0.96875, learning_rate 0.00100116
2017-10-11T14:47:05.011269: step 416, loss 0.0857077, acc 0.984375, learning_rate 0.000997483
2017-10-11T14:47:05.200001: step 417, loss 0.159497, acc 0.953125, learning_rate 0.00099382
2017-10-11T14:47:05.371527: step 418, loss 0.16707, acc 0.921875, learning_rate 0.000990172
2017-10-11T14:47:05.553245: step 419, loss 0.125238, acc 0.953125, learning_rate 0.000986538
2017-10-11T14:47:05.734709: step 420, loss 0.175375, acc 0.9375, learning_rate 0.00098292
2017-10-11T14:47:05.909255: step 421, loss 0.16748, acc 0.953125, learning_rate 0.000979316
2017-10-11T14:47:06.095472: step 422, loss 0.187691, acc 0.921875, learning_rate 0.000975727
2017-10-11T14:47:06.282491: step 423, loss 0.209062, acc 0.953125, learning_rate 0.000972152
2017-10-11T14:47:06.475512: step 424, loss 0.234762, acc 0.921875, learning_rate 0.000968592
2017-10-11T14:47:06.654138: step 425, loss 0.247113, acc 0.921875, learning_rate 0.000965047
2017-10-11T14:47:06.837708: step 426, loss 0.327901, acc 0.921875, learning_rate 0.000961516
2017-10-11T14:47:07.019964: step 427, loss 0.0374588, acc 1, learning_rate 0.000958
2017-10-11T14:47:07.208951: step 428, loss 0.125308, acc 0.953125, learning_rate 0.000954497
2017-10-11T14:47:07.386183: step 429, loss 0.384405, acc 0.90625, learning_rate 0.00095101
2017-10-11T14:47:07.572688: step 430, loss 0.123659, acc 0.9375, learning_rate 0.000947536
2017-10-11T14:47:07.745904: step 431, loss 0.158164, acc 0.921875, learning_rate 0.000944076
2017-10-11T14:47:07.928461: step 432, loss 0.28587, acc 0.921875, learning_rate 0.000940631
2017-10-11T14:47:08.106956: step 433, loss 0.168859, acc 0.9375, learning_rate 0.0009372
2017-10-11T14:47:08.285164: step 434, loss 0.286437, acc 0.90625, learning_rate 0.000933783
2017-10-11T14:47:08.466028: step 435, loss 0.140533, acc 0.921875, learning_rate 0.000930379
2017-10-11T14:47:08.646646: step 436, loss 0.289532, acc 0.875, learning_rate 0.00092699
2017-10-11T14:47:08.833267: step 437, loss 0.254479, acc 0.890625, learning_rate 0.000923614
2017-10-11T14:47:09.011335: step 438, loss 0.167604, acc 0.96875, learning_rate 0.000920253
2017-10-11T14:47:09.207311: step 439, loss 0.28825, acc 0.921875, learning_rate 0.000916905
2017-10-11T14:47:09.391020: step 440, loss 0.191164, acc 0.9375, learning_rate 0.00091357

Evaluation:
2017-10-11T14:47:09.548224: step 440, loss 0.262808, acc 0.907914

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-440

2017-10-11T14:47:10.875928: step 441, loss 0.096302, acc 0.953125, learning_rate 0.000910249
2017-10-11T14:47:11.060825: step 442, loss 0.138121, acc 0.9375, learning_rate 0.000906942
2017-10-11T14:47:11.240289: step 443, loss 0.245514, acc 0.890625, learning_rate 0.000903648
2017-10-11T14:47:11.435470: step 444, loss 0.374278, acc 0.859375, learning_rate 0.000900368
2017-10-11T14:47:11.623047: step 445, loss 0.230056, acc 0.890625, learning_rate 0.000897101
2017-10-11T14:47:11.809847: step 446, loss 0.293698, acc 0.9375, learning_rate 0.000893848
2017-10-11T14:47:11.995167: step 447, loss 0.214154, acc 0.921875, learning_rate 0.000890607
2017-10-11T14:47:12.168960: step 448, loss 0.231751, acc 0.953125, learning_rate 0.00088738
2017-10-11T14:47:12.348226: step 449, loss 0.306029, acc 0.875, learning_rate 0.000884166
2017-10-11T14:47:12.534861: step 450, loss 0.240967, acc 0.921875, learning_rate 0.000880966
2017-10-11T14:47:12.719076: step 451, loss 0.081131, acc 0.953125, learning_rate 0.000877778
2017-10-11T14:47:12.902308: step 452, loss 0.231351, acc 0.921875, learning_rate 0.000874603
2017-10-11T14:47:13.082522: step 453, loss 0.359028, acc 0.9375, learning_rate 0.000871441
2017-10-11T14:47:13.276862: step 454, loss 0.181761, acc 0.9375, learning_rate 0.000868293
2017-10-11T14:47:13.458372: step 455, loss 0.185849, acc 0.90625, learning_rate 0.000865157
2017-10-11T14:47:13.631969: step 456, loss 0.246232, acc 0.890625, learning_rate 0.000862033
2017-10-11T14:47:13.824575: step 457, loss 0.161327, acc 0.9375, learning_rate 0.000858923
2017-10-11T14:47:14.011296: step 458, loss 0.257516, acc 0.921875, learning_rate 0.000855825
2017-10-11T14:47:14.195549: step 459, loss 0.249, acc 0.921875, learning_rate 0.00085274
2017-10-11T14:47:14.385746: step 460, loss 0.223406, acc 0.921875, learning_rate 0.000849668
2017-10-11T14:47:14.565799: step 461, loss 0.232456, acc 0.90625, learning_rate 0.000846608
2017-10-11T14:47:14.750071: step 462, loss 0.109645, acc 0.953125, learning_rate 0.00084356
2017-10-11T14:47:14.926524: step 463, loss 0.175073, acc 0.9375, learning_rate 0.000840525
2017-10-11T14:47:15.089918: step 464, loss 0.108737, acc 0.96875, learning_rate 0.000837502
2017-10-11T14:47:15.257896: step 465, loss 0.248487, acc 0.90625, learning_rate 0.000834492
2017-10-11T14:47:15.440491: step 466, loss 0.0835243, acc 0.96875, learning_rate 0.000831494
2017-10-11T14:47:15.622678: step 467, loss 0.233166, acc 0.921875, learning_rate 0.000828508
2017-10-11T14:47:15.800084: step 468, loss 0.139764, acc 0.953125, learning_rate 0.000825535
2017-10-11T14:47:15.991877: step 469, loss 0.253308, acc 0.921875, learning_rate 0.000822573
2017-10-11T14:47:16.182041: step 470, loss 0.173158, acc 0.9375, learning_rate 0.000819624
2017-10-11T14:47:16.359577: step 471, loss 0.31725, acc 0.921875, learning_rate 0.000816687
2017-10-11T14:47:16.540577: step 472, loss 0.135352, acc 0.96875, learning_rate 0.000813761
2017-10-11T14:47:16.718199: step 473, loss 0.256902, acc 0.875, learning_rate 0.000810848
2017-10-11T14:47:16.900574: step 474, loss 0.143015, acc 0.9375, learning_rate 0.000807946
2017-10-11T14:47:17.085951: step 475, loss 0.265537, acc 0.9375, learning_rate 0.000805057
2017-10-11T14:47:17.262687: step 476, loss 0.0728958, acc 1, learning_rate 0.000802179
2017-10-11T14:47:17.438044: step 477, loss 0.19665, acc 0.921875, learning_rate 0.000799313
2017-10-11T14:47:17.618211: step 478, loss 0.268956, acc 0.953125, learning_rate 0.000796458
2017-10-11T14:47:17.803259: step 479, loss 0.292502, acc 0.890625, learning_rate 0.000793616
2017-10-11T14:47:17.971437: step 480, loss 0.281945, acc 0.953125, learning_rate 0.000790784

Evaluation:
2017-10-11T14:47:18.113745: step 480, loss 0.256864, acc 0.910791

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-480

2017-10-11T14:47:19.727720: step 481, loss 0.199124, acc 0.953125, learning_rate 0.000787965
2017-10-11T14:47:19.916281: step 482, loss 0.18955, acc 0.90625, learning_rate 0.000785157
2017-10-11T14:47:20.107520: step 483, loss 0.319542, acc 0.9375, learning_rate 0.00078236
2017-10-11T14:47:20.292545: step 484, loss 0.0949083, acc 0.96875, learning_rate 0.000779575
2017-10-11T14:47:20.462666: step 485, loss 0.0861497, acc 0.96875, learning_rate 0.000776801
2017-10-11T14:47:20.658115: step 486, loss 0.126643, acc 0.9375, learning_rate 0.000774038
2017-10-11T14:47:20.836599: step 487, loss 0.159618, acc 0.90625, learning_rate 0.000771287
2017-10-11T14:47:21.031846: step 488, loss 0.238049, acc 0.921875, learning_rate 0.000768547
2017-10-11T14:47:21.212079: step 489, loss 0.26355, acc 0.90625, learning_rate 0.000765818
2017-10-11T14:47:21.372618: step 490, loss 0.248676, acc 0.921569, learning_rate 0.000763101
2017-10-11T14:47:21.553729: step 491, loss 0.233897, acc 0.9375, learning_rate 0.000760394
2017-10-11T14:47:21.735443: step 492, loss 0.144057, acc 0.953125, learning_rate 0.000757698
2017-10-11T14:47:21.922355: step 493, loss 0.160676, acc 0.953125, learning_rate 0.000755014
2017-10-11T14:47:22.095554: step 494, loss 0.198053, acc 0.9375, learning_rate 0.00075234
2017-10-11T14:47:22.278367: step 495, loss 0.285434, acc 0.90625, learning_rate 0.000749677
2017-10-11T14:47:22.462651: step 496, loss 0.15302, acc 0.9375, learning_rate 0.000747026
2017-10-11T14:47:22.652080: step 497, loss 0.298068, acc 0.921875, learning_rate 0.000744385
2017-10-11T14:47:22.839481: step 498, loss 0.190357, acc 0.921875, learning_rate 0.000741754
2017-10-11T14:47:23.025322: step 499, loss 0.0444325, acc 1, learning_rate 0.000739135
2017-10-11T14:47:23.198140: step 500, loss 0.0335849, acc 1, learning_rate 0.000736526
2017-10-11T14:47:23.382136: step 501, loss 0.104208, acc 0.953125, learning_rate 0.000733928
2017-10-11T14:47:23.564597: step 502, loss 0.212598, acc 0.921875, learning_rate 0.00073134
2017-10-11T14:47:23.747675: step 503, loss 0.246069, acc 0.890625, learning_rate 0.000728763
2017-10-11T14:47:23.923652: step 504, loss 0.0990152, acc 0.984375, learning_rate 0.000726197
2017-10-11T14:47:24.103015: step 505, loss 0.0945783, acc 0.96875, learning_rate 0.000723641
2017-10-11T14:47:24.289675: step 506, loss 0.219871, acc 0.90625, learning_rate 0.000721095
2017-10-11T14:47:24.471025: step 507, loss 0.148895, acc 0.9375, learning_rate 0.00071856
2017-10-11T14:47:24.651161: step 508, loss 0.172379, acc 0.90625, learning_rate 0.000716036
2017-10-11T14:47:24.848406: step 509, loss 0.246283, acc 0.921875, learning_rate 0.000713521
2017-10-11T14:47:25.035156: step 510, loss 0.134529, acc 0.953125, learning_rate 0.000711017
2017-10-11T14:47:25.220486: step 511, loss 0.134282, acc 0.921875, learning_rate 0.000708523
2017-10-11T14:47:25.405019: step 512, loss 0.29518, acc 0.875, learning_rate 0.000706039
2017-10-11T14:47:25.581589: step 513, loss 0.448148, acc 0.875, learning_rate 0.000703565
2017-10-11T14:47:25.768039: step 514, loss 0.0503131, acc 0.984375, learning_rate 0.000701102
2017-10-11T14:47:25.949685: step 515, loss 0.370696, acc 0.90625, learning_rate 0.000698648
2017-10-11T14:47:26.151417: step 516, loss 0.129108, acc 0.953125, learning_rate 0.000696204
2017-10-11T14:47:26.335678: step 517, loss 0.371042, acc 0.859375, learning_rate 0.000693771
2017-10-11T14:47:26.534136: step 518, loss 0.152508, acc 0.9375, learning_rate 0.000691347
2017-10-11T14:47:26.719609: step 519, loss 0.273716, acc 0.90625, learning_rate 0.000688934
2017-10-11T14:47:26.898252: step 520, loss 0.213422, acc 0.9375, learning_rate 0.00068653

Evaluation:
2017-10-11T14:47:27.054563: step 520, loss 0.252164, acc 0.907914

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-520

2017-10-11T14:47:28.130627: step 521, loss 0.20346, acc 0.90625, learning_rate 0.000684136
2017-10-11T14:47:28.300570: step 522, loss 0.180821, acc 0.953125, learning_rate 0.000681751
2017-10-11T14:47:28.479511: step 523, loss 0.314383, acc 0.9375, learning_rate 0.000679377
2017-10-11T14:47:28.656415: step 524, loss 0.246078, acc 0.90625, learning_rate 0.000677012
2017-10-11T14:47:28.829005: step 525, loss 0.0748094, acc 0.984375, learning_rate 0.000674657
2017-10-11T14:47:29.013365: step 526, loss 0.219918, acc 0.921875, learning_rate 0.000672311
2017-10-11T14:47:29.202833: step 527, loss 0.0615389, acc 0.96875, learning_rate 0.000669975
2017-10-11T14:47:29.383275: step 528, loss 0.217143, acc 0.921875, learning_rate 0.000667648
2017-10-11T14:47:29.563859: step 529, loss 0.250018, acc 0.890625, learning_rate 0.000665331
2017-10-11T14:47:29.747229: step 530, loss 0.213553, acc 0.953125, learning_rate 0.000663024
2017-10-11T14:47:29.926523: step 531, loss 0.12933, acc 0.9375, learning_rate 0.000660726
2017-10-11T14:47:30.115216: step 532, loss 0.352822, acc 0.9375, learning_rate 0.000658437
2017-10-11T14:47:30.288574: step 533, loss 0.252708, acc 0.921875, learning_rate 0.000656158
2017-10-11T14:47:30.470319: step 534, loss 0.198273, acc 0.90625, learning_rate 0.000653888
2017-10-11T14:47:30.650720: step 535, loss 0.175385, acc 0.9375, learning_rate 0.000651627
2017-10-11T14:47:30.835995: step 536, loss 0.277123, acc 0.9375, learning_rate 0.000649375
2017-10-11T14:47:31.023236: step 537, loss 0.350401, acc 0.875, learning_rate 0.000647133
2017-10-11T14:47:31.207229: step 538, loss 0.166211, acc 0.953125, learning_rate 0.000644899
2017-10-11T14:47:31.380797: step 539, loss 0.254143, acc 0.921875, learning_rate 0.000642675
2017-10-11T14:47:31.560041: step 540, loss 0.103892, acc 0.984375, learning_rate 0.00064046
2017-10-11T14:47:31.739968: step 541, loss 0.183971, acc 0.9375, learning_rate 0.000638254
2017-10-11T14:47:31.915243: step 542, loss 0.158952, acc 0.921875, learning_rate 0.000636057
2017-10-11T14:47:32.087384: step 543, loss 0.112988, acc 0.953125, learning_rate 0.000633869
2017-10-11T14:47:32.270266: step 544, loss 0.287646, acc 0.90625, learning_rate 0.00063169
2017-10-11T14:47:32.454700: step 545, loss 0.102847, acc 0.96875, learning_rate 0.00062952
2017-10-11T14:47:32.648814: step 546, loss 0.124358, acc 0.96875, learning_rate 0.000627358
2017-10-11T14:47:32.843456: step 547, loss 0.237606, acc 0.921875, learning_rate 0.000625206
2017-10-11T14:47:33.033215: step 548, loss 0.0591896, acc 1, learning_rate 0.000623062
2017-10-11T14:47:33.218030: step 549, loss 0.246964, acc 0.90625, learning_rate 0.000620927
2017-10-11T14:47:33.404005: step 550, loss 0.0758052, acc 0.96875, learning_rate 0.000618801
2017-10-11T14:47:33.588624: step 551, loss 0.103389, acc 0.953125, learning_rate 0.000616683
2017-10-11T14:47:33.776739: step 552, loss 0.147344, acc 0.953125, learning_rate 0.000614574
2017-10-11T14:47:33.964490: step 553, loss 0.130595, acc 0.9375, learning_rate 0.000612474
2017-10-11T14:47:34.145117: step 554, loss 0.180091, acc 0.90625, learning_rate 0.000610382
2017-10-11T14:47:34.341658: step 555, loss 0.084771, acc 0.9375, learning_rate 0.000608299
2017-10-11T14:47:34.522775: step 556, loss 0.156888, acc 0.9375, learning_rate 0.000606224
2017-10-11T14:47:34.714048: step 557, loss 0.0719164, acc 0.96875, learning_rate 0.000604158
2017-10-11T14:47:34.908781: step 558, loss 0.317524, acc 0.90625, learning_rate 0.0006021
2017-10-11T14:47:35.093950: step 559, loss 0.119489, acc 0.921875, learning_rate 0.00060005
2017-10-11T14:47:35.273412: step 560, loss 0.101873, acc 0.96875, learning_rate 0.000598009

Evaluation:
2017-10-11T14:47:35.424505: step 560, loss 0.25873, acc 0.902158

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-560

2017-10-11T14:47:37.055423: step 561, loss 0.126832, acc 0.953125, learning_rate 0.000595977
2017-10-11T14:47:37.238423: step 562, loss 0.493797, acc 0.859375, learning_rate 0.000593952
2017-10-11T14:47:37.426785: step 563, loss 0.0727514, acc 0.96875, learning_rate 0.000591936
2017-10-11T14:47:37.611686: step 564, loss 0.127805, acc 0.953125, learning_rate 0.000589928
2017-10-11T14:47:37.802316: step 565, loss 0.0856907, acc 0.984375, learning_rate 0.000587928
2017-10-11T14:47:37.985070: step 566, loss 0.293086, acc 0.890625, learning_rate 0.000585937
2017-10-11T14:47:38.177033: step 567, loss 0.117642, acc 0.953125, learning_rate 0.000583953
2017-10-11T14:47:38.355932: step 568, loss 0.172264, acc 0.9375, learning_rate 0.000581978
2017-10-11T14:47:38.539727: step 569, loss 0.1916, acc 0.953125, learning_rate 0.00058001
2017-10-11T14:47:38.720411: step 570, loss 0.307871, acc 0.90625, learning_rate 0.000578051
2017-10-11T14:47:38.902486: step 571, loss 0.089836, acc 0.984375, learning_rate 0.0005761
2017-10-11T14:47:39.081250: step 572, loss 0.207489, acc 0.921875, learning_rate 0.000574157
2017-10-11T14:47:39.263655: step 573, loss 0.272646, acc 0.921875, learning_rate 0.000572221
2017-10-11T14:47:39.435505: step 574, loss 0.142911, acc 0.921875, learning_rate 0.000570294
2017-10-11T14:47:39.620055: step 575, loss 0.0688584, acc 0.984375, learning_rate 0.000568374
2017-10-11T14:47:39.801471: step 576, loss 0.230215, acc 0.9375, learning_rate 0.000566462
2017-10-11T14:47:39.966781: step 577, loss 0.238016, acc 0.953125, learning_rate 0.000564558
2017-10-11T14:47:40.143577: step 578, loss 0.107976, acc 0.953125, learning_rate 0.000562662
2017-10-11T14:47:40.324424: step 579, loss 0.185422, acc 0.921875, learning_rate 0.000560774
2017-10-11T14:47:40.502989: step 580, loss 0.205462, acc 0.921875, learning_rate 0.000558893
2017-10-11T14:47:40.680810: step 581, loss 0.473746, acc 0.90625, learning_rate 0.00055702
2017-10-11T14:47:40.852481: step 582, loss 0.054841, acc 0.984375, learning_rate 0.000555154
2017-10-11T14:47:41.034080: step 583, loss 0.221573, acc 0.921875, learning_rate 0.000553296
2017-10-11T14:47:41.214528: step 584, loss 0.191682, acc 0.953125, learning_rate 0.000551446
2017-10-11T14:47:41.397270: step 585, loss 0.154631, acc 0.96875, learning_rate 0.000549604
2017-10-11T14:47:41.583577: step 586, loss 0.252951, acc 0.953125, learning_rate 0.000547768
2017-10-11T14:47:41.764680: step 587, loss 0.0874489, acc 0.96875, learning_rate 0.000545941
2017-10-11T14:47:41.916401: step 588, loss 0.34877, acc 0.862745, learning_rate 0.00054412
2017-10-11T14:47:42.099765: step 589, loss 0.23283, acc 0.9375, learning_rate 0.000542308
2017-10-11T14:47:42.289811: step 590, loss 0.10101, acc 0.96875, learning_rate 0.000540502
2017-10-11T14:47:42.473172: step 591, loss 0.135098, acc 0.9375, learning_rate 0.000538704
2017-10-11T14:47:42.662173: step 592, loss 0.130905, acc 0.953125, learning_rate 0.000536914
2017-10-11T14:47:42.845663: step 593, loss 0.336485, acc 0.90625, learning_rate 0.00053513
2017-10-11T14:47:43.023993: step 594, loss 0.0989371, acc 0.9375, learning_rate 0.000533354
2017-10-11T14:47:43.208258: step 595, loss 0.0665005, acc 0.96875, learning_rate 0.000531585
2017-10-11T14:47:43.394114: step 596, loss 0.313424, acc 0.890625, learning_rate 0.000529824
2017-10-11T14:47:43.580888: step 597, loss 0.223053, acc 0.953125, learning_rate 0.000528069
2017-10-11T14:47:43.753663: step 598, loss 0.17059, acc 0.9375, learning_rate 0.000526322
2017-10-11T14:47:43.927760: step 599, loss 0.252202, acc 0.90625, learning_rate 0.000524582
2017-10-11T14:47:44.113052: step 600, loss 0.255345, acc 0.921875, learning_rate 0.000522849

Evaluation:
2017-10-11T14:47:44.283496: step 600, loss 0.24801, acc 0.909353

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-600

2017-10-11T14:47:45.394572: step 601, loss 0.104845, acc 0.96875, learning_rate 0.000521123
2017-10-11T14:47:45.570786: step 602, loss 0.124442, acc 0.953125, learning_rate 0.000519404
2017-10-11T14:47:45.744258: step 603, loss 0.137771, acc 0.9375, learning_rate 0.000517692
2017-10-11T14:47:45.935305: step 604, loss 0.160998, acc 0.921875, learning_rate 0.000515987
2017-10-11T14:47:46.115496: step 605, loss 0.206995, acc 0.953125, learning_rate 0.000514289
2017-10-11T14:47:46.298809: step 606, loss 0.155715, acc 0.9375, learning_rate 0.000512598
2017-10-11T14:47:46.479389: step 607, loss 0.111045, acc 0.96875, learning_rate 0.000510914
2017-10-11T14:47:46.673023: step 608, loss 0.260519, acc 0.921875, learning_rate 0.000509237
2017-10-11T14:47:46.859718: step 609, loss 0.282494, acc 0.875, learning_rate 0.000507566
2017-10-11T14:47:47.040833: step 610, loss 0.182281, acc 0.9375, learning_rate 0.000505903
2017-10-11T14:47:47.220111: step 611, loss 0.0682277, acc 0.984375, learning_rate 0.000504246
2017-10-11T14:47:47.398018: step 612, loss 0.320766, acc 0.890625, learning_rate 0.000502596
2017-10-11T14:47:47.569121: step 613, loss 0.248726, acc 0.921875, learning_rate 0.000500953
2017-10-11T14:47:47.746947: step 614, loss 0.311182, acc 0.90625, learning_rate 0.000499316
2017-10-11T14:47:47.929956: step 615, loss 0.228304, acc 0.921875, learning_rate 0.000497686
2017-10-11T14:47:48.115353: step 616, loss 0.130855, acc 0.953125, learning_rate 0.000496063
2017-10-11T14:47:48.299862: step 617, loss 0.301617, acc 0.921875, learning_rate 0.000494446
2017-10-11T14:47:48.495401: step 618, loss 0.153924, acc 0.953125, learning_rate 0.000492836
2017-10-11T14:47:48.682480: step 619, loss 0.174825, acc 0.96875, learning_rate 0.000491233
2017-10-11T14:47:48.868905: step 620, loss 0.114054, acc 0.953125, learning_rate 0.000489636
2017-10-11T14:47:49.045300: step 621, loss 0.0781686, acc 0.96875, learning_rate 0.000488045
2017-10-11T14:47:49.231231: step 622, loss 0.118471, acc 0.96875, learning_rate 0.000486461
2017-10-11T14:47:49.407684: step 623, loss 0.0798756, acc 0.96875, learning_rate 0.000484884
2017-10-11T14:47:49.590030: step 624, loss 0.192185, acc 0.9375, learning_rate 0.000483313
2017-10-11T14:47:49.766374: step 625, loss 0.196073, acc 0.9375, learning_rate 0.000481748
2017-10-11T14:47:49.955816: step 626, loss 0.232647, acc 0.90625, learning_rate 0.00048019
2017-10-11T14:47:50.136235: step 627, loss 0.25321, acc 0.9375, learning_rate 0.000478638
2017-10-11T14:47:50.328365: step 628, loss 0.344524, acc 0.90625, learning_rate 0.000477093
2017-10-11T14:47:50.511801: step 629, loss 0.0706387, acc 0.984375, learning_rate 0.000475554
2017-10-11T14:47:50.697490: step 630, loss 0.0369819, acc 0.984375, learning_rate 0.000474021
2017-10-11T14:47:50.872705: step 631, loss 0.167778, acc 0.9375, learning_rate 0.000472494
2017-10-11T14:47:51.050469: step 632, loss 0.147849, acc 0.921875, learning_rate 0.000470974
2017-10-11T14:47:51.227251: step 633, loss 0.241907, acc 0.890625, learning_rate 0.000469459
2017-10-11T14:47:51.409001: step 634, loss 0.173642, acc 0.90625, learning_rate 0.000467951
2017-10-11T14:47:51.596627: step 635, loss 0.163207, acc 0.9375, learning_rate 0.000466449
2017-10-11T14:47:51.774875: step 636, loss 0.185811, acc 0.9375, learning_rate 0.000464954
2017-10-11T14:47:51.962475: step 637, loss 0.252944, acc 0.921875, learning_rate 0.000463464
2017-10-11T14:47:52.157403: step 638, loss 0.129465, acc 0.9375, learning_rate 0.00046198
2017-10-11T14:47:52.338152: step 639, loss 0.268496, acc 0.90625, learning_rate 0.000460503
2017-10-11T14:47:52.526502: step 640, loss 0.159879, acc 0.90625, learning_rate 0.000459031

Evaluation:
2017-10-11T14:47:52.688163: step 640, loss 0.242913, acc 0.910791

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-640

2017-10-11T14:47:53.992705: step 641, loss 0.13503, acc 0.921875, learning_rate 0.000457566
2017-10-11T14:47:54.177051: step 642, loss 0.0667454, acc 0.984375, learning_rate 0.000456106
2017-10-11T14:47:54.350254: step 643, loss 0.160002, acc 0.953125, learning_rate 0.000454653
2017-10-11T14:47:54.532023: step 644, loss 0.330031, acc 0.859375, learning_rate 0.000453205
2017-10-11T14:47:54.719131: step 645, loss 0.272623, acc 0.859375, learning_rate 0.000451764
2017-10-11T14:47:54.905406: step 646, loss 0.300172, acc 0.890625, learning_rate 0.000450328
2017-10-11T14:47:55.081669: step 647, loss 0.0591678, acc 0.96875, learning_rate 0.000448898
2017-10-11T14:47:55.273322: step 648, loss 0.195611, acc 0.953125, learning_rate 0.000447474
2017-10-11T14:47:55.455256: step 649, loss 0.203953, acc 0.9375, learning_rate 0.000446055
2017-10-11T14:47:55.629179: step 650, loss 0.0559135, acc 0.984375, learning_rate 0.000444643
2017-10-11T14:47:55.814157: step 651, loss 0.212742, acc 0.921875, learning_rate 0.000443236
2017-10-11T14:47:56.008378: step 652, loss 0.116372, acc 0.96875, learning_rate 0.000441835
2017-10-11T14:47:56.181498: step 653, loss 0.170987, acc 0.921875, learning_rate 0.00044044
2017-10-11T14:47:56.372203: step 654, loss 0.0960682, acc 0.96875, learning_rate 0.00043905
2017-10-11T14:47:56.547859: step 655, loss 0.117665, acc 0.96875, learning_rate 0.000437666
2017-10-11T14:47:56.739878: step 656, loss 0.161924, acc 0.9375, learning_rate 0.000436288
2017-10-11T14:47:56.921984: step 657, loss 0.132046, acc 0.953125, learning_rate 0.000434915
2017-10-11T14:47:57.098023: step 658, loss 0.107668, acc 0.96875, learning_rate 0.000433548
2017-10-11T14:47:57.288038: step 659, loss 0.199598, acc 0.9375, learning_rate 0.000432187
2017-10-11T14:47:57.463861: step 660, loss 0.371924, acc 0.859375, learning_rate 0.000430831
2017-10-11T14:47:57.651381: step 661, loss 0.0426791, acc 1, learning_rate 0.000429481
2017-10-11T14:47:57.824328: step 662, loss 0.0848565, acc 0.96875, learning_rate 0.000428136
2017-10-11T14:47:58.011217: step 663, loss 0.126328, acc 0.9375, learning_rate 0.000426796
2017-10-11T14:47:58.196970: step 664, loss 0.187711, acc 0.90625, learning_rate 0.000425463
2017-10-11T14:47:58.384353: step 665, loss 0.0956068, acc 0.96875, learning_rate 0.000424134
2017-10-11T14:47:58.567801: step 666, loss 0.29248, acc 0.921875, learning_rate 0.000422811
2017-10-11T14:47:58.754704: step 667, loss 0.157389, acc 0.953125, learning_rate 0.000421493
2017-10-11T14:47:58.933583: step 668, loss 0.136789, acc 0.921875, learning_rate 0.000420181
2017-10-11T14:47:59.116928: step 669, loss 0.12941, acc 0.96875, learning_rate 0.000418874
2017-10-11T14:47:59.289306: step 670, loss 0.166866, acc 0.9375, learning_rate 0.000417573
2017-10-11T14:47:59.466656: step 671, loss 0.167073, acc 0.921875, learning_rate 0.000416276
2017-10-11T14:47:59.648123: step 672, loss 0.167821, acc 0.921875, learning_rate 0.000414985
2017-10-11T14:47:59.825015: step 673, loss 0.25817, acc 0.921875, learning_rate 0.0004137
2017-10-11T14:48:00.025154: step 674, loss 0.0895926, acc 0.96875, learning_rate 0.000412419
2017-10-11T14:48:00.216403: step 675, loss 0.13727, acc 0.9375, learning_rate 0.000411144
2017-10-11T14:48:00.400373: step 676, loss 0.0678488, acc 0.984375, learning_rate 0.000409874
2017-10-11T14:48:00.580695: step 677, loss 0.161383, acc 0.921875, learning_rate 0.000408609
2017-10-11T14:48:00.766065: step 678, loss 0.155124, acc 0.96875, learning_rate 0.00040735
2017-10-11T14:48:00.949889: step 679, loss 0.267259, acc 0.890625, learning_rate 0.000406095
2017-10-11T14:48:01.135038: step 680, loss 0.145087, acc 0.9375, learning_rate 0.000404846

Evaluation:
2017-10-11T14:48:01.297281: step 680, loss 0.243615, acc 0.907914

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-680

2017-10-11T14:48:02.833787: step 681, loss 0.169361, acc 0.96875, learning_rate 0.000403601
2017-10-11T14:48:03.019195: step 682, loss 0.13659, acc 0.953125, learning_rate 0.000402362
2017-10-11T14:48:03.207896: step 683, loss 0.0545357, acc 0.984375, learning_rate 0.000401128
2017-10-11T14:48:03.390530: step 684, loss 0.190064, acc 0.9375, learning_rate 0.000399899
2017-10-11T14:48:03.581031: step 685, loss 0.172889, acc 0.9375, learning_rate 0.000398675
2017-10-11T14:48:03.737218: step 686, loss 0.226992, acc 0.941176, learning_rate 0.000397456
2017-10-11T14:48:03.911257: step 687, loss 0.0654602, acc 0.984375, learning_rate 0.000396241
2017-10-11T14:48:04.100668: step 688, loss 0.175884, acc 0.90625, learning_rate 0.000395032
2017-10-11T14:48:04.281104: step 689, loss 0.117956, acc 0.9375, learning_rate 0.000393828
2017-10-11T14:48:04.469080: step 690, loss 0.103077, acc 0.953125, learning_rate 0.000392629
2017-10-11T14:48:04.651279: step 691, loss 0.196608, acc 0.9375, learning_rate 0.000391434
2017-10-11T14:48:04.836336: step 692, loss 0.221397, acc 0.9375, learning_rate 0.000390245
2017-10-11T14:48:05.023846: step 693, loss 0.142282, acc 0.921875, learning_rate 0.00038906
2017-10-11T14:48:05.204977: step 694, loss 0.0842463, acc 0.953125, learning_rate 0.00038788
2017-10-11T14:48:05.387497: step 695, loss 0.1669, acc 0.9375, learning_rate 0.000386705
2017-10-11T14:48:05.569352: step 696, loss 0.171818, acc 0.9375, learning_rate 0.000385535
2017-10-11T14:48:05.742416: step 697, loss 0.0904058, acc 0.96875, learning_rate 0.000384369
2017-10-11T14:48:05.926085: step 698, loss 0.111482, acc 0.953125, learning_rate 0.000383209
2017-10-11T14:48:06.114896: step 699, loss 0.184896, acc 0.90625, learning_rate 0.000382053
2017-10-11T14:48:06.306293: step 700, loss 0.242069, acc 0.9375, learning_rate 0.000380901
2017-10-11T14:48:06.490308: step 701, loss 0.0829881, acc 0.984375, learning_rate 0.000379755
2017-10-11T14:48:06.685275: step 702, loss 0.150035, acc 0.9375, learning_rate 0.000378613
2017-10-11T14:48:06.871377: step 703, loss 0.294907, acc 0.90625, learning_rate 0.000377476
2017-10-11T14:48:07.044167: step 704, loss 0.18015, acc 0.953125, learning_rate 0.000376343
2017-10-11T14:48:07.228588: step 705, loss 0.200556, acc 0.921875, learning_rate 0.000375215
2017-10-11T14:48:07.418013: step 706, loss 0.0953673, acc 0.96875, learning_rate 0.000374092
2017-10-11T14:48:07.604326: step 707, loss 0.179009, acc 0.921875, learning_rate 0.000372973
2017-10-11T14:48:07.791936: step 708, loss 0.088757, acc 0.953125, learning_rate 0.000371859
2017-10-11T14:48:07.982397: step 709, loss 0.224488, acc 0.921875, learning_rate 0.000370749
2017-10-11T14:48:08.168753: step 710, loss 0.128345, acc 0.96875, learning_rate 0.000369644
2017-10-11T14:48:08.353777: step 711, loss 0.246184, acc 0.921875, learning_rate 0.000368543
2017-10-11T14:48:08.538258: step 712, loss 0.173537, acc 0.90625, learning_rate 0.000367447
2017-10-11T14:48:08.723139: step 713, loss 0.07694, acc 0.984375, learning_rate 0.000366356
2017-10-11T14:48:08.911284: step 714, loss 0.181168, acc 0.953125, learning_rate 0.000365268
2017-10-11T14:48:09.095542: step 715, loss 0.0925295, acc 0.984375, learning_rate 0.000364186
2017-10-11T14:48:09.264675: step 716, loss 0.213566, acc 0.921875, learning_rate 0.000363107
2017-10-11T14:48:09.441292: step 717, loss 0.342938, acc 0.90625, learning_rate 0.000362033
2017-10-11T14:48:09.628806: step 718, loss 0.119661, acc 0.96875, learning_rate 0.000360964
2017-10-11T14:48:09.816018: step 719, loss 0.109562, acc 0.984375, learning_rate 0.000359899
2017-10-11T14:48:10.005374: step 720, loss 0.173684, acc 0.953125, learning_rate 0.000358838

Evaluation:
2017-10-11T14:48:10.169955: step 720, loss 0.23797, acc 0.910791

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-720

2017-10-11T14:48:11.293835: step 721, loss 0.324375, acc 0.890625, learning_rate 0.000357781
2017-10-11T14:48:11.481698: step 722, loss 0.162327, acc 0.953125, learning_rate 0.000356729
2017-10-11T14:48:11.669320: step 723, loss 0.208533, acc 0.921875, learning_rate 0.000355681
2017-10-11T14:48:11.846396: step 724, loss 0.157673, acc 0.96875, learning_rate 0.000354637
2017-10-11T14:48:12.021929: step 725, loss 0.0782441, acc 0.953125, learning_rate 0.000353598
2017-10-11T14:48:12.202854: step 726, loss 0.231012, acc 0.953125, learning_rate 0.000352563
2017-10-11T14:48:12.395034: step 727, loss 0.10556, acc 0.96875, learning_rate 0.000351532
2017-10-11T14:48:12.571549: step 728, loss 0.597697, acc 0.859375, learning_rate 0.000350505
2017-10-11T14:48:12.753972: step 729, loss 0.09423, acc 0.953125, learning_rate 0.000349483
2017-10-11T14:48:12.937833: step 730, loss 0.174208, acc 0.953125, learning_rate 0.000348465
2017-10-11T14:48:13.125761: step 731, loss 0.353418, acc 0.90625, learning_rate 0.00034745
2017-10-11T14:48:13.305870: step 732, loss 0.0247468, acc 1, learning_rate 0.00034644
2017-10-11T14:48:13.485845: step 733, loss 0.135361, acc 0.953125, learning_rate 0.000345434
2017-10-11T14:48:13.668664: step 734, loss 0.128559, acc 0.9375, learning_rate 0.000344433
2017-10-11T14:48:13.857432: step 735, loss 0.0524279, acc 1, learning_rate 0.000343435
2017-10-11T14:48:14.039318: step 736, loss 0.0816713, acc 0.9375, learning_rate 0.000342441
2017-10-11T14:48:14.222202: step 737, loss 0.0897194, acc 0.96875, learning_rate 0.000341452
2017-10-11T14:48:14.396957: step 738, loss 0.208478, acc 0.90625, learning_rate 0.000340466
2017-10-11T14:48:14.579772: step 739, loss 0.181059, acc 0.96875, learning_rate 0.000339485
2017-10-11T14:48:14.759818: step 740, loss 0.104494, acc 0.96875, learning_rate 0.000338507
2017-10-11T14:48:14.947577: step 741, loss 0.142292, acc 0.96875, learning_rate 0.000337534
2017-10-11T14:48:15.140001: step 742, loss 0.176838, acc 0.9375, learning_rate 0.000336564
2017-10-11T14:48:15.316288: step 743, loss 0.208341, acc 0.9375, learning_rate 0.000335598
2017-10-11T14:48:15.492908: step 744, loss 0.13706, acc 0.96875, learning_rate 0.000334637
2017-10-11T14:48:15.676767: step 745, loss 0.217776, acc 0.90625, learning_rate 0.000333679
2017-10-11T14:48:15.862192: step 746, loss 0.0916837, acc 0.984375, learning_rate 0.000332725
2017-10-11T14:48:16.053538: step 747, loss 0.103399, acc 0.953125, learning_rate 0.000331775
2017-10-11T14:48:16.239821: step 748, loss 0.219465, acc 0.953125, learning_rate 0.000330829
2017-10-11T14:48:16.428024: step 749, loss 0.0649547, acc 1, learning_rate 0.000329887
2017-10-11T14:48:16.610822: step 750, loss 0.0812977, acc 0.984375, learning_rate 0.000328949
2017-10-11T14:48:16.793839: step 751, loss 0.23327, acc 0.953125, learning_rate 0.000328014
2017-10-11T14:48:16.981131: step 752, loss 0.0999108, acc 0.96875, learning_rate 0.000327083
2017-10-11T14:48:17.164647: step 753, loss 0.211555, acc 0.9375, learning_rate 0.000326157
2017-10-11T14:48:17.345572: step 754, loss 0.445268, acc 0.875, learning_rate 0.000325233
2017-10-11T14:48:17.530730: step 755, loss 0.10951, acc 0.953125, learning_rate 0.000324314
2017-10-11T14:48:17.712091: step 756, loss 0.0420344, acc 0.984375, learning_rate 0.000323399
2017-10-11T14:48:17.894501: step 757, loss 0.0659189, acc 0.984375, learning_rate 0.000322487
2017-10-11T14:48:18.074090: step 758, loss 0.149754, acc 0.921875, learning_rate 0.000321579
2017-10-11T14:48:18.249447: step 759, loss 0.212095, acc 0.90625, learning_rate 0.000320674
2017-10-11T14:48:18.440763: step 760, loss 0.144654, acc 0.96875, learning_rate 0.000319773

Evaluation:
2017-10-11T14:48:18.597255: step 760, loss 0.246983, acc 0.907914

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-760

2017-10-11T14:48:20.035966: step 761, loss 0.226263, acc 0.921875, learning_rate 0.000318876
2017-10-11T14:48:20.217793: step 762, loss 0.137304, acc 0.953125, learning_rate 0.000317983
2017-10-11T14:48:20.409884: step 763, loss 0.109878, acc 0.96875, learning_rate 0.000317093
2017-10-11T14:48:20.591393: step 764, loss 0.118666, acc 0.96875, learning_rate 0.000316207
2017-10-11T14:48:20.774990: step 765, loss 0.111597, acc 0.96875, learning_rate 0.000315325
2017-10-11T14:48:20.952469: step 766, loss 0.215931, acc 0.953125, learning_rate 0.000314446
2017-10-11T14:48:21.121431: step 767, loss 0.0900602, acc 0.953125, learning_rate 0.00031357
2017-10-11T14:48:21.311438: step 768, loss 0.206116, acc 0.953125, learning_rate 0.000312699
2017-10-11T14:48:21.499010: step 769, loss 0.240429, acc 0.859375, learning_rate 0.00031183
2017-10-11T14:48:21.694104: step 770, loss 0.348101, acc 0.859375, learning_rate 0.000310966
2017-10-11T14:48:21.865062: step 771, loss 0.204087, acc 0.921875, learning_rate 0.000310105
2017-10-11T14:48:22.048776: step 772, loss 0.158576, acc 0.9375, learning_rate 0.000309247
2017-10-11T14:48:22.243987: step 773, loss 0.235707, acc 0.90625, learning_rate 0.000308393
2017-10-11T14:48:22.436665: step 774, loss 0.121428, acc 0.96875, learning_rate 0.000307542
2017-10-11T14:48:22.626560: step 775, loss 0.203912, acc 0.921875, learning_rate 0.000306695
2017-10-11T14:48:22.810532: step 776, loss 0.0502709, acc 0.96875, learning_rate 0.000305852
2017-10-11T14:48:22.999923: step 777, loss 0.113929, acc 0.953125, learning_rate 0.000305011
2017-10-11T14:48:23.195492: step 778, loss 0.0959284, acc 0.96875, learning_rate 0.000304174
2017-10-11T14:48:23.372837: step 779, loss 0.184836, acc 0.953125, learning_rate 0.000303341
2017-10-11T14:48:23.565739: step 780, loss 0.172491, acc 0.890625, learning_rate 0.000302511
2017-10-11T14:48:23.751663: step 781, loss 0.163321, acc 0.953125, learning_rate 0.000301684
2017-10-11T14:48:23.941283: step 782, loss 0.300602, acc 0.90625, learning_rate 0.000300861
2017-10-11T14:48:24.117745: step 783, loss 0.113562, acc 0.9375, learning_rate 0.000300041
2017-10-11T14:48:24.273018: step 784, loss 0.226974, acc 0.921569, learning_rate 0.000299225
2017-10-11T14:48:24.453675: step 785, loss 0.183182, acc 0.890625, learning_rate 0.000298412
2017-10-11T14:48:24.644709: step 786, loss 0.207432, acc 0.9375, learning_rate 0.000297602
2017-10-11T14:48:24.834659: step 787, loss 0.164406, acc 0.90625, learning_rate 0.000296795
2017-10-11T14:48:25.006892: step 788, loss 0.115133, acc 0.96875, learning_rate 0.000295992
2017-10-11T14:48:25.201251: step 789, loss 0.213742, acc 0.890625, learning_rate 0.000295192
2017-10-11T14:48:25.380776: step 790, loss 0.126222, acc 0.90625, learning_rate 0.000294395
2017-10-11T14:48:25.562301: step 791, loss 0.124945, acc 0.96875, learning_rate 0.000293602
2017-10-11T14:48:25.746407: step 792, loss 0.1786, acc 0.953125, learning_rate 0.000292812
2017-10-11T14:48:25.925719: step 793, loss 0.126928, acc 0.953125, learning_rate 0.000292025
2017-10-11T14:48:26.121861: step 794, loss 0.218046, acc 0.9375, learning_rate 0.000291241
2017-10-11T14:48:26.295326: step 795, loss 0.225413, acc 0.953125, learning_rate 0.00029046
2017-10-11T14:48:26.480851: step 796, loss 0.0743834, acc 0.984375, learning_rate 0.000289683
2017-10-11T14:48:26.670664: step 797, loss 0.169562, acc 0.9375, learning_rate 0.000288908
2017-10-11T14:48:26.856275: step 798, loss 0.15562, acc 0.9375, learning_rate 0.000288137
2017-10-11T14:48:27.048880: step 799, loss 0.152527, acc 0.9375, learning_rate 0.000287369
2017-10-11T14:48:27.232333: step 800, loss 0.0695391, acc 0.96875, learning_rate 0.000286605

Evaluation:
2017-10-11T14:48:27.392409: step 800, loss 0.245408, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-800

2017-10-11T14:48:28.936215: step 801, loss 0.224006, acc 0.90625, learning_rate 0.000285843
2017-10-11T14:48:29.111001: step 802, loss 0.0686524, acc 0.96875, learning_rate 0.000285084
2017-10-11T14:48:29.283613: step 803, loss 0.143774, acc 0.953125, learning_rate 0.000284329
2017-10-11T14:48:29.480913: step 804, loss 0.262185, acc 0.9375, learning_rate 0.000283577
2017-10-11T14:48:29.663629: step 805, loss 0.401986, acc 0.875, learning_rate 0.000282827
2017-10-11T14:48:29.840725: step 806, loss 0.108301, acc 0.9375, learning_rate 0.000282081
2017-10-11T14:48:30.024553: step 807, loss 0.145798, acc 0.96875, learning_rate 0.000281338
2017-10-11T14:48:30.206867: step 808, loss 0.134579, acc 0.9375, learning_rate 0.000280598
2017-10-11T14:48:30.394488: step 809, loss 0.114239, acc 0.96875, learning_rate 0.00027986
2017-10-11T14:48:30.577008: step 810, loss 0.262429, acc 0.875, learning_rate 0.000279126
2017-10-11T14:48:30.759467: step 811, loss 0.19187, acc 0.9375, learning_rate 0.000278395
2017-10-11T14:48:30.955974: step 812, loss 0.101651, acc 0.953125, learning_rate 0.000277667
2017-10-11T14:48:31.135572: step 813, loss 0.164186, acc 0.921875, learning_rate 0.000276942
2017-10-11T14:48:31.318295: step 814, loss 0.146899, acc 0.96875, learning_rate 0.00027622
2017-10-11T14:48:31.506166: step 815, loss 0.157563, acc 0.96875, learning_rate 0.0002755
2017-10-11T14:48:31.697398: step 816, loss 0.196698, acc 0.921875, learning_rate 0.000274784
2017-10-11T14:48:31.881279: step 817, loss 0.152988, acc 0.9375, learning_rate 0.000274071
2017-10-11T14:48:32.064170: step 818, loss 0.153284, acc 0.953125, learning_rate 0.00027336
2017-10-11T14:48:32.245628: step 819, loss 0.11675, acc 0.96875, learning_rate 0.000272652
2017-10-11T14:48:32.438005: step 820, loss 0.136428, acc 0.9375, learning_rate 0.000271948
2017-10-11T14:48:32.620895: step 821, loss 0.191449, acc 0.921875, learning_rate 0.000271246
2017-10-11T14:48:32.806535: step 822, loss 0.150182, acc 0.890625, learning_rate 0.000270547
2017-10-11T14:48:32.993042: step 823, loss 0.199132, acc 0.90625, learning_rate 0.000269851
2017-10-11T14:48:33.178971: step 824, loss 0.17702, acc 0.9375, learning_rate 0.000269157
2017-10-11T14:48:33.362459: step 825, loss 0.108253, acc 0.953125, learning_rate 0.000268467
2017-10-11T14:48:33.538390: step 826, loss 0.226306, acc 0.9375, learning_rate 0.000267779
2017-10-11T14:48:33.726975: step 827, loss 0.205072, acc 0.890625, learning_rate 0.000267094
2017-10-11T14:48:33.919087: step 828, loss 0.0867862, acc 0.96875, learning_rate 0.000266412
2017-10-11T14:48:34.097798: step 829, loss 0.0324456, acc 1, learning_rate 0.000265733
2017-10-11T14:48:34.274143: step 830, loss 0.163185, acc 0.9375, learning_rate 0.000265057
2017-10-11T14:48:34.462314: step 831, loss 0.273076, acc 0.9375, learning_rate 0.000264383
2017-10-11T14:48:34.641805: step 832, loss 0.139173, acc 0.9375, learning_rate 0.000263712
2017-10-11T14:48:34.817796: step 833, loss 0.0715329, acc 0.96875, learning_rate 0.000263044
2017-10-11T14:48:34.991561: step 834, loss 0.151717, acc 0.90625, learning_rate 0.000262378
2017-10-11T14:48:35.185215: step 835, loss 0.111426, acc 0.96875, learning_rate 0.000261715
2017-10-11T14:48:35.368476: step 836, loss 0.168102, acc 0.9375, learning_rate 0.000261055
2017-10-11T14:48:35.544651: step 837, loss 0.155427, acc 0.9375, learning_rate 0.000260398
2017-10-11T14:48:35.732613: step 838, loss 0.189341, acc 0.90625, learning_rate 0.000259743
2017-10-11T14:48:35.930213: step 839, loss 0.0774148, acc 0.96875, learning_rate 0.000259091
2017-10-11T14:48:36.107986: step 840, loss 0.163042, acc 0.953125, learning_rate 0.000258442

Evaluation:
2017-10-11T14:48:36.269004: step 840, loss 0.242816, acc 0.907914

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-840

2017-10-11T14:48:37.352459: step 841, loss 0.201212, acc 0.9375, learning_rate 0.000257795
2017-10-11T14:48:37.522334: step 842, loss 0.233, acc 0.921875, learning_rate 0.000257151
2017-10-11T14:48:37.703339: step 843, loss 0.281907, acc 0.90625, learning_rate 0.00025651
2017-10-11T14:48:37.885270: step 844, loss 0.166294, acc 0.953125, learning_rate 0.000255871
2017-10-11T14:48:38.062172: step 845, loss 0.121251, acc 0.921875, learning_rate 0.000255235
2017-10-11T14:48:38.253557: step 846, loss 0.142583, acc 0.953125, learning_rate 0.000254601
2017-10-11T14:48:38.436137: step 847, loss 0.144238, acc 0.953125, learning_rate 0.00025397
2017-10-11T14:48:38.611882: step 848, loss 0.222169, acc 0.96875, learning_rate 0.000253341
2017-10-11T14:48:38.793292: step 849, loss 0.115563, acc 0.921875, learning_rate 0.000252716
2017-10-11T14:48:38.976858: step 850, loss 0.133468, acc 0.921875, learning_rate 0.000252092
2017-10-11T14:48:39.146793: step 851, loss 0.105364, acc 0.953125, learning_rate 0.000251471
2017-10-11T14:48:39.333411: step 852, loss 0.116429, acc 0.953125, learning_rate 0.000250853
2017-10-11T14:48:39.530110: step 853, loss 0.0699132, acc 0.96875, learning_rate 0.000250237
2017-10-11T14:48:39.714918: step 854, loss 0.0631391, acc 0.984375, learning_rate 0.000249624
2017-10-11T14:48:39.908796: step 855, loss 0.135302, acc 0.953125, learning_rate 0.000249013
2017-10-11T14:48:40.085182: step 856, loss 0.0873095, acc 0.96875, learning_rate 0.000248405
2017-10-11T14:48:40.273856: step 857, loss 0.249827, acc 0.9375, learning_rate 0.000247799
2017-10-11T14:48:40.456928: step 858, loss 0.124531, acc 0.953125, learning_rate 0.000247196
2017-10-11T14:48:40.635164: step 859, loss 0.0469338, acc 0.984375, learning_rate 0.000246595
2017-10-11T14:48:40.832647: step 860, loss 0.175366, acc 0.921875, learning_rate 0.000245997
2017-10-11T14:48:41.010914: step 861, loss 0.0429481, acc 0.984375, learning_rate 0.000245401
2017-10-11T14:48:41.199103: step 862, loss 0.223997, acc 0.859375, learning_rate 0.000244808
2017-10-11T14:48:41.382405: step 863, loss 0.0911017, acc 0.96875, learning_rate 0.000244216
2017-10-11T14:48:41.572534: step 864, loss 0.242777, acc 0.90625, learning_rate 0.000243628
2017-10-11T14:48:41.761988: step 865, loss 0.0853492, acc 0.953125, learning_rate 0.000243042
2017-10-11T14:48:41.956880: step 866, loss 0.216402, acc 0.921875, learning_rate 0.000242458
2017-10-11T14:48:42.132236: step 867, loss 0.126961, acc 0.96875, learning_rate 0.000241876
2017-10-11T14:48:42.323721: step 868, loss 0.0824513, acc 0.984375, learning_rate 0.000241297
2017-10-11T14:48:42.493044: step 869, loss 0.126568, acc 0.953125, learning_rate 0.00024072
2017-10-11T14:48:42.674505: step 870, loss 0.305966, acc 0.875, learning_rate 0.000240146
2017-10-11T14:48:42.847262: step 871, loss 0.147873, acc 0.953125, learning_rate 0.000239574
2017-10-11T14:48:43.023451: step 872, loss 0.133651, acc 0.953125, learning_rate 0.000239004
2017-10-11T14:48:43.206587: step 873, loss 0.216582, acc 0.9375, learning_rate 0.000238437
2017-10-11T14:48:43.372797: step 874, loss 0.0188973, acc 1, learning_rate 0.000237872
2017-10-11T14:48:43.565652: step 875, loss 0.136345, acc 0.96875, learning_rate 0.000237309
2017-10-11T14:48:43.755538: step 876, loss 0.0565372, acc 1, learning_rate 0.000236749
2017-10-11T14:48:43.934712: step 877, loss 0.123508, acc 0.953125, learning_rate 0.00023619
2017-10-11T14:48:44.122661: step 878, loss 0.116863, acc 0.953125, learning_rate 0.000235635
2017-10-11T14:48:44.320391: step 879, loss 0.162619, acc 0.9375, learning_rate 0.000235081
2017-10-11T14:48:44.513004: step 880, loss 0.220216, acc 0.9375, learning_rate 0.00023453

Evaluation:
2017-10-11T14:48:44.667708: step 880, loss 0.23873, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-880

2017-10-11T14:48:46.222857: step 881, loss 0.12517, acc 0.953125, learning_rate 0.00023398
2017-10-11T14:48:46.379530: step 882, loss 0.0576419, acc 0.980392, learning_rate 0.000233434
2017-10-11T14:48:46.579487: step 883, loss 0.171316, acc 0.9375, learning_rate 0.000232889
2017-10-11T14:48:46.755890: step 884, loss 0.260085, acc 0.90625, learning_rate 0.000232346
2017-10-11T14:48:46.933823: step 885, loss 0.158863, acc 0.921875, learning_rate 0.000231806
2017-10-11T14:48:47.125957: step 886, loss 0.0342745, acc 0.984375, learning_rate 0.000231268
2017-10-11T14:48:47.311946: step 887, loss 0.0846131, acc 0.96875, learning_rate 0.000230732
2017-10-11T14:48:47.505779: step 888, loss 0.220434, acc 0.9375, learning_rate 0.000230199
2017-10-11T14:48:47.691186: step 889, loss 0.183581, acc 0.9375, learning_rate 0.000229667
2017-10-11T14:48:47.883284: step 890, loss 0.119446, acc 0.921875, learning_rate 0.000229138
2017-10-11T14:48:48.062906: step 891, loss 0.110516, acc 0.953125, learning_rate 0.000228611
2017-10-11T14:48:48.248595: step 892, loss 0.248955, acc 0.921875, learning_rate 0.000228086
2017-10-11T14:48:48.435965: step 893, loss 0.135957, acc 0.9375, learning_rate 0.000227563
2017-10-11T14:48:48.622665: step 894, loss 0.15425, acc 0.953125, learning_rate 0.000227043
2017-10-11T14:48:48.800496: step 895, loss 0.242933, acc 0.953125, learning_rate 0.000226524
2017-10-11T14:48:48.986855: step 896, loss 0.0546869, acc 0.953125, learning_rate 0.000226008
2017-10-11T14:48:49.156091: step 897, loss 0.0827531, acc 0.984375, learning_rate 0.000225493
2017-10-11T14:48:49.352937: step 898, loss 0.265529, acc 0.859375, learning_rate 0.000224981
2017-10-11T14:48:49.527600: step 899, loss 0.113358, acc 0.984375, learning_rate 0.000224471
2017-10-11T14:48:49.719899: step 900, loss 0.196702, acc 0.90625, learning_rate 0.000223963
2017-10-11T14:48:49.900056: step 901, loss 0.0541137, acc 0.984375, learning_rate 0.000223457
2017-10-11T14:48:50.089667: step 902, loss 0.0735027, acc 0.96875, learning_rate 0.000222953
2017-10-11T14:48:50.271617: step 903, loss 0.0785118, acc 0.96875, learning_rate 0.000222451
2017-10-11T14:48:50.451390: step 904, loss 0.21424, acc 0.890625, learning_rate 0.000221951
2017-10-11T14:48:50.622908: step 905, loss 0.156339, acc 0.921875, learning_rate 0.000221453
2017-10-11T14:48:50.819547: step 906, loss 0.233322, acc 0.953125, learning_rate 0.000220958
2017-10-11T14:48:50.993787: step 907, loss 0.238309, acc 0.90625, learning_rate 0.000220464
2017-10-11T14:48:51.175151: step 908, loss 0.231087, acc 0.90625, learning_rate 0.000219972
2017-10-11T14:48:51.355967: step 909, loss 0.175954, acc 0.953125, learning_rate 0.000219483
2017-10-11T14:48:51.547242: step 910, loss 0.164615, acc 0.921875, learning_rate 0.000218995
2017-10-11T14:48:51.727405: step 911, loss 0.0719248, acc 0.984375, learning_rate 0.000218509
2017-10-11T14:48:51.911962: step 912, loss 0.198127, acc 0.921875, learning_rate 0.000218025
2017-10-11T14:48:52.096368: step 913, loss 0.239423, acc 0.921875, learning_rate 0.000217544
2017-10-11T14:48:52.273649: step 914, loss 0.133724, acc 0.953125, learning_rate 0.000217064
2017-10-11T14:48:52.457912: step 915, loss 0.215208, acc 0.921875, learning_rate 0.000216586
2017-10-11T14:48:52.638903: step 916, loss 0.196399, acc 0.921875, learning_rate 0.00021611
2017-10-11T14:48:52.823251: step 917, loss 0.113382, acc 0.96875, learning_rate 0.000215636
2017-10-11T14:48:53.004283: step 918, loss 0.0884681, acc 0.953125, learning_rate 0.000215164
2017-10-11T14:48:53.185289: step 919, loss 0.058999, acc 0.96875, learning_rate 0.000214694
2017-10-11T14:48:53.367833: step 920, loss 0.226026, acc 0.921875, learning_rate 0.000214226

Evaluation:
2017-10-11T14:48:53.535795: step 920, loss 0.238837, acc 0.909353

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-920

2017-10-11T14:48:54.653247: step 921, loss 0.137635, acc 0.921875, learning_rate 0.00021376
2017-10-11T14:48:54.840161: step 922, loss 0.1739, acc 0.953125, learning_rate 0.000213295
2017-10-11T14:48:55.022289: step 923, loss 0.119534, acc 0.953125, learning_rate 0.000212833
2017-10-11T14:48:55.215036: step 924, loss 0.157934, acc 0.953125, learning_rate 0.000212372
2017-10-11T14:48:55.389012: step 925, loss 0.164046, acc 0.921875, learning_rate 0.000211914
2017-10-11T14:48:55.565390: step 926, loss 0.0796028, acc 0.96875, learning_rate 0.000211457
2017-10-11T14:48:55.743888: step 927, loss 0.137137, acc 0.9375, learning_rate 0.000211002
2017-10-11T14:48:55.929761: step 928, loss 0.23982, acc 0.953125, learning_rate 0.000210549
2017-10-11T14:48:56.111392: step 929, loss 0.120679, acc 0.953125, learning_rate 0.000210098
2017-10-11T14:48:56.302306: step 930, loss 0.144661, acc 0.9375, learning_rate 0.000209648
2017-10-11T14:48:56.493432: step 931, loss 0.21823, acc 0.90625, learning_rate 0.000209201
2017-10-11T14:48:56.679043: step 932, loss 0.0593637, acc 0.984375, learning_rate 0.000208755
2017-10-11T14:48:56.858257: step 933, loss 0.0486059, acc 1, learning_rate 0.000208311
2017-10-11T14:48:57.028508: step 934, loss 0.0680547, acc 0.984375, learning_rate 0.000207869
2017-10-11T14:48:57.224468: step 935, loss 0.204326, acc 0.921875, learning_rate 0.000207429
2017-10-11T14:48:57.414312: step 936, loss 0.187305, acc 0.921875, learning_rate 0.00020699
2017-10-11T14:48:57.597996: step 937, loss 0.0417349, acc 0.984375, learning_rate 0.000206554
2017-10-11T14:48:57.783895: step 938, loss 0.0650208, acc 0.984375, learning_rate 0.000206119
2017-10-11T14:48:57.957613: step 939, loss 0.244873, acc 0.921875, learning_rate 0.000205685
2017-10-11T14:48:58.146055: step 940, loss 0.0950024, acc 0.96875, learning_rate 0.000205254
2017-10-11T14:48:58.335242: step 941, loss 0.135003, acc 0.953125, learning_rate 0.000204824
2017-10-11T14:48:58.510209: step 942, loss 0.317004, acc 0.90625, learning_rate 0.000204397
2017-10-11T14:48:58.700938: step 943, loss 0.0442529, acc 1, learning_rate 0.00020397
2017-10-11T14:48:58.886957: step 944, loss 0.115217, acc 0.9375, learning_rate 0.000203546
2017-10-11T14:48:59.068196: step 945, loss 0.181511, acc 0.9375, learning_rate 0.000203123
2017-10-11T14:48:59.250279: step 946, loss 0.256107, acc 0.921875, learning_rate 0.000202702
2017-10-11T14:48:59.433053: step 947, loss 0.147556, acc 0.984375, learning_rate 0.000202283
2017-10-11T14:48:59.610362: step 948, loss 0.0324204, acc 1, learning_rate 0.000201866
2017-10-11T14:48:59.811815: step 949, loss 0.0764675, acc 1, learning_rate 0.00020145
2017-10-11T14:49:00.001026: step 950, loss 0.203085, acc 0.9375, learning_rate 0.000201036
2017-10-11T14:49:00.190304: step 951, loss 0.196769, acc 0.90625, learning_rate 0.000200623
2017-10-11T14:49:00.375668: step 952, loss 0.101118, acc 0.96875, learning_rate 0.000200213
2017-10-11T14:49:00.557454: step 953, loss 0.0954242, acc 0.984375, learning_rate 0.000199804
2017-10-11T14:49:00.748467: step 954, loss 0.205886, acc 0.90625, learning_rate 0.000199396
2017-10-11T14:49:00.923021: step 955, loss 0.172796, acc 0.921875, learning_rate 0.000198991
2017-10-11T14:49:01.105784: step 956, loss 0.131823, acc 0.953125, learning_rate 0.000198587
2017-10-11T14:49:01.283439: step 957, loss 0.175642, acc 0.96875, learning_rate 0.000198184
2017-10-11T14:49:01.469004: step 958, loss 0.0657844, acc 1, learning_rate 0.000197783
2017-10-11T14:49:01.649022: step 959, loss 0.146611, acc 0.9375, learning_rate 0.000197384
2017-10-11T14:49:01.828267: step 960, loss 0.27197, acc 0.890625, learning_rate 0.000196987

Evaluation:
2017-10-11T14:49:01.990067: step 960, loss 0.240499, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-960

2017-10-11T14:49:03.438645: step 961, loss 0.14529, acc 0.96875, learning_rate 0.000196591
2017-10-11T14:49:03.617148: step 962, loss 0.112082, acc 0.9375, learning_rate 0.000196197
2017-10-11T14:49:03.808522: step 963, loss 0.163408, acc 0.953125, learning_rate 0.000195804
2017-10-11T14:49:04.002198: step 964, loss 0.151482, acc 0.953125, learning_rate 0.000195413
2017-10-11T14:49:04.180390: step 965, loss 0.066948, acc 0.984375, learning_rate 0.000195023
2017-10-11T14:49:04.370761: step 966, loss 0.113562, acc 0.984375, learning_rate 0.000194636
2017-10-11T14:49:04.551953: step 967, loss 0.103413, acc 0.96875, learning_rate 0.000194249
2017-10-11T14:49:04.738560: step 968, loss 0.188435, acc 0.9375, learning_rate 0.000193865
2017-10-11T14:49:04.928072: step 969, loss 0.0841595, acc 0.984375, learning_rate 0.000193482
2017-10-11T14:49:05.113342: step 970, loss 0.222312, acc 0.9375, learning_rate 0.0001931
2017-10-11T14:49:05.297153: step 971, loss 0.0809184, acc 0.96875, learning_rate 0.00019272
2017-10-11T14:49:05.489980: step 972, loss 0.0600277, acc 1, learning_rate 0.000192341
2017-10-11T14:49:05.670949: step 973, loss 0.0913663, acc 0.953125, learning_rate 0.000191965
2017-10-11T14:49:05.861614: step 974, loss 0.142447, acc 0.9375, learning_rate 0.000191589
2017-10-11T14:49:06.058134: step 975, loss 0.17343, acc 0.9375, learning_rate 0.000191215
2017-10-11T14:49:06.248860: step 976, loss 0.0931134, acc 0.953125, learning_rate 0.000190843
2017-10-11T14:49:06.437998: step 977, loss 0.0667415, acc 0.96875, learning_rate 0.000190472
2017-10-11T14:49:06.627828: step 978, loss 0.145967, acc 0.953125, learning_rate 0.000190103
2017-10-11T14:49:06.804084: step 979, loss 0.090333, acc 0.953125, learning_rate 0.000189735
2017-10-11T14:49:06.966081: step 980, loss 0.103939, acc 0.941176, learning_rate 0.000189369
2017-10-11T14:49:07.144966: step 981, loss 0.1468, acc 0.9375, learning_rate 0.000189004
2017-10-11T14:49:07.331003: step 982, loss 0.246142, acc 0.90625, learning_rate 0.000188641
2017-10-11T14:49:07.519232: step 983, loss 0.160878, acc 0.953125, learning_rate 0.000188279
2017-10-11T14:49:07.712538: step 984, loss 0.106618, acc 0.953125, learning_rate 0.000187919
2017-10-11T14:49:07.887111: step 985, loss 0.0729497, acc 0.96875, learning_rate 0.00018756
2017-10-11T14:49:08.075950: step 986, loss 0.196972, acc 0.921875, learning_rate 0.000187202
2017-10-11T14:49:08.249615: step 987, loss 0.134949, acc 0.921875, learning_rate 0.000186846
2017-10-11T14:49:08.433911: step 988, loss 0.0389007, acc 0.984375, learning_rate 0.000186492
2017-10-11T14:49:08.619518: step 989, loss 0.0762817, acc 0.984375, learning_rate 0.000186139
2017-10-11T14:49:08.812087: step 990, loss 0.0795866, acc 0.984375, learning_rate 0.000185787
2017-10-11T14:49:08.986907: step 991, loss 0.146508, acc 0.953125, learning_rate 0.000185437
2017-10-11T14:49:09.164466: step 992, loss 0.123945, acc 0.953125, learning_rate 0.000185088
2017-10-11T14:49:09.346931: step 993, loss 0.0803822, acc 0.96875, learning_rate 0.000184741
2017-10-11T14:49:09.530268: step 994, loss 0.0390576, acc 0.984375, learning_rate 0.000184395
2017-10-11T14:49:09.724513: step 995, loss 0.140204, acc 0.921875, learning_rate 0.000184051
2017-10-11T14:49:09.907739: step 996, loss 0.113913, acc 0.9375, learning_rate 0.000183708
2017-10-11T14:49:10.094199: step 997, loss 0.097257, acc 0.9375, learning_rate 0.000183366
2017-10-11T14:49:10.282256: step 998, loss 0.166567, acc 0.96875, learning_rate 0.000183026
2017-10-11T14:49:10.474903: step 999, loss 0.242882, acc 0.921875, learning_rate 0.000182687
2017-10-11T14:49:10.664679: step 1000, loss 0.092303, acc 0.984375, learning_rate 0.000182349

Evaluation:
2017-10-11T14:49:10.833271: step 1000, loss 0.239096, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1000

2017-10-11T14:49:12.666612: step 1001, loss 0.059239, acc 0.984375, learning_rate 0.000182013
2017-10-11T14:49:12.843885: step 1002, loss 0.190048, acc 0.953125, learning_rate 0.000181678
2017-10-11T14:49:13.036740: step 1003, loss 0.12873, acc 0.953125, learning_rate 0.000181345
2017-10-11T14:49:13.223520: step 1004, loss 0.171389, acc 0.90625, learning_rate 0.000181013
2017-10-11T14:49:13.413479: step 1005, loss 0.129395, acc 0.953125, learning_rate 0.000180682
2017-10-11T14:49:13.602438: step 1006, loss 0.181252, acc 0.921875, learning_rate 0.000180353
2017-10-11T14:49:13.782220: step 1007, loss 0.159167, acc 0.9375, learning_rate 0.000180025
2017-10-11T14:49:13.972762: step 1008, loss 0.099662, acc 0.96875, learning_rate 0.000179698
2017-10-11T14:49:14.162242: step 1009, loss 0.159038, acc 0.984375, learning_rate 0.000179373
2017-10-11T14:49:14.342174: step 1010, loss 0.154088, acc 0.921875, learning_rate 0.000179049
2017-10-11T14:49:14.521069: step 1011, loss 0.174512, acc 0.90625, learning_rate 0.000178726
2017-10-11T14:49:14.717832: step 1012, loss 0.0938158, acc 0.96875, learning_rate 0.000178405
2017-10-11T14:49:14.908932: step 1013, loss 0.172241, acc 0.921875, learning_rate 0.000178085
2017-10-11T14:49:15.100539: step 1014, loss 0.144021, acc 0.9375, learning_rate 0.000177766
2017-10-11T14:49:15.292202: step 1015, loss 0.124974, acc 0.953125, learning_rate 0.000177449
2017-10-11T14:49:15.480994: step 1016, loss 0.244158, acc 0.9375, learning_rate 0.000177133
2017-10-11T14:49:15.663004: step 1017, loss 0.120184, acc 0.96875, learning_rate 0.000176818
2017-10-11T14:49:15.853324: step 1018, loss 0.175788, acc 0.96875, learning_rate 0.000176504
2017-10-11T14:49:16.032837: step 1019, loss 0.134369, acc 0.953125, learning_rate 0.000176192
2017-10-11T14:49:16.221179: step 1020, loss 0.0447875, acc 0.984375, learning_rate 0.000175881
2017-10-11T14:49:16.400979: step 1021, loss 0.148984, acc 0.953125, learning_rate 0.000175571
2017-10-11T14:49:16.586200: step 1022, loss 0.247242, acc 0.9375, learning_rate 0.000175263
2017-10-11T14:49:16.780477: step 1023, loss 0.0824971, acc 0.984375, learning_rate 0.000174956
2017-10-11T14:49:16.966309: step 1024, loss 0.147009, acc 0.9375, learning_rate 0.00017465
2017-10-11T14:49:17.149274: step 1025, loss 0.0963086, acc 0.953125, learning_rate 0.000174345
2017-10-11T14:49:17.319082: step 1026, loss 0.0361757, acc 0.984375, learning_rate 0.000174042
2017-10-11T14:49:17.505857: step 1027, loss 0.288976, acc 0.953125, learning_rate 0.000173739
2017-10-11T14:49:17.684063: step 1028, loss 0.135458, acc 0.953125, learning_rate 0.000173438
2017-10-11T14:49:17.873271: step 1029, loss 0.0640341, acc 0.96875, learning_rate 0.000173139
2017-10-11T14:49:18.061121: step 1030, loss 0.291573, acc 0.9375, learning_rate 0.00017284
2017-10-11T14:49:18.242824: step 1031, loss 0.079087, acc 0.96875, learning_rate 0.000172543
2017-10-11T14:49:18.436732: step 1032, loss 0.209122, acc 0.921875, learning_rate 0.000172247
2017-10-11T14:49:18.623918: step 1033, loss 0.234462, acc 0.90625, learning_rate 0.000171952
2017-10-11T14:49:18.811580: step 1034, loss 0.142227, acc 0.953125, learning_rate 0.000171658
2017-10-11T14:49:18.993958: step 1035, loss 0.265649, acc 0.921875, learning_rate 0.000171366
2017-10-11T14:49:19.186372: step 1036, loss 0.239293, acc 0.9375, learning_rate 0.000171074
2017-10-11T14:49:19.375319: step 1037, loss 0.0558205, acc 1, learning_rate 0.000170784
2017-10-11T14:49:19.562869: step 1038, loss 0.0913252, acc 0.984375, learning_rate 0.000170495
2017-10-11T14:49:19.758591: step 1039, loss 0.130805, acc 0.96875, learning_rate 0.000170208
2017-10-11T14:49:19.944653: step 1040, loss 0.166128, acc 0.96875, learning_rate 0.000169921

Evaluation:
2017-10-11T14:49:20.122598: step 1040, loss 0.241146, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1040

2017-10-11T14:49:21.229774: step 1041, loss 0.152185, acc 0.9375, learning_rate 0.000169636
2017-10-11T14:49:21.412107: step 1042, loss 0.101383, acc 0.96875, learning_rate 0.000169351
2017-10-11T14:49:21.595047: step 1043, loss 0.0812664, acc 0.96875, learning_rate 0.000169068
2017-10-11T14:49:21.783487: step 1044, loss 0.262523, acc 0.90625, learning_rate 0.000168786
2017-10-11T14:49:21.972106: step 1045, loss 0.122057, acc 0.96875, learning_rate 0.000168506
2017-10-11T14:49:22.162180: step 1046, loss 0.145948, acc 0.9375, learning_rate 0.000168226
2017-10-11T14:49:22.335665: step 1047, loss 0.094864, acc 0.984375, learning_rate 0.000167947
2017-10-11T14:49:22.505084: step 1048, loss 0.0727579, acc 0.984375, learning_rate 0.00016767
2017-10-11T14:49:22.678081: step 1049, loss 0.189468, acc 0.9375, learning_rate 0.000167394
2017-10-11T14:49:22.866276: step 1050, loss 0.163733, acc 0.984375, learning_rate 0.000167119
2017-10-11T14:49:23.036481: step 1051, loss 0.0895509, acc 0.96875, learning_rate 0.000166845
2017-10-11T14:49:23.220791: step 1052, loss 0.215755, acc 0.921875, learning_rate 0.000166572
2017-10-11T14:49:23.421574: step 1053, loss 0.0960284, acc 0.953125, learning_rate 0.0001663
2017-10-11T14:49:23.609421: step 1054, loss 0.0559945, acc 0.96875, learning_rate 0.00016603
2017-10-11T14:49:23.794884: step 1055, loss 0.122573, acc 0.953125, learning_rate 0.00016576
2017-10-11T14:49:23.974143: step 1056, loss 0.0686551, acc 0.984375, learning_rate 0.000165492
2017-10-11T14:49:24.152561: step 1057, loss 0.0375455, acc 1, learning_rate 0.000165224
2017-10-11T14:49:24.330796: step 1058, loss 0.22365, acc 0.890625, learning_rate 0.000164958
2017-10-11T14:49:24.514553: step 1059, loss 0.217542, acc 0.921875, learning_rate 0.000164693
2017-10-11T14:49:24.710494: step 1060, loss 0.264773, acc 0.921875, learning_rate 0.000164429
2017-10-11T14:49:24.894360: step 1061, loss 0.122505, acc 0.96875, learning_rate 0.000164166
2017-10-11T14:49:25.077644: step 1062, loss 0.107199, acc 0.953125, learning_rate 0.000163904
2017-10-11T14:49:25.284625: step 1063, loss 0.169228, acc 0.921875, learning_rate 0.000163643
2017-10-11T14:49:25.465826: step 1064, loss 0.11773, acc 0.953125, learning_rate 0.000163383
2017-10-11T14:49:25.652764: step 1065, loss 0.0227185, acc 1, learning_rate 0.000163125
2017-10-11T14:49:25.842564: step 1066, loss 0.17953, acc 0.9375, learning_rate 0.000162867
2017-10-11T14:49:26.026479: step 1067, loss 0.0439721, acc 0.984375, learning_rate 0.00016261
2017-10-11T14:49:26.204295: step 1068, loss 0.124528, acc 0.953125, learning_rate 0.000162355
2017-10-11T14:49:26.392727: step 1069, loss 0.0349072, acc 0.984375, learning_rate 0.0001621
2017-10-11T14:49:26.584319: step 1070, loss 0.130128, acc 0.953125, learning_rate 0.000161847
2017-10-11T14:49:26.766765: step 1071, loss 0.161728, acc 0.921875, learning_rate 0.000161594
2017-10-11T14:49:26.950133: step 1072, loss 0.0888981, acc 0.984375, learning_rate 0.000161343
2017-10-11T14:49:27.139730: step 1073, loss 0.0890054, acc 0.96875, learning_rate 0.000161093
2017-10-11T14:49:27.328767: step 1074, loss 0.155884, acc 0.921875, learning_rate 0.000160843
2017-10-11T14:49:27.510640: step 1075, loss 0.209085, acc 0.921875, learning_rate 0.000160595
2017-10-11T14:49:27.696122: step 1076, loss 0.0471554, acc 0.984375, learning_rate 0.000160348
2017-10-11T14:49:27.878630: step 1077, loss 0.204044, acc 0.9375, learning_rate 0.000160101
2017-10-11T14:49:28.047753: step 1078, loss 0.0829026, acc 0.980392, learning_rate 0.000159856
2017-10-11T14:49:28.236303: step 1079, loss 0.222601, acc 0.921875, learning_rate 0.000159612
2017-10-11T14:49:28.441287: step 1080, loss 0.154616, acc 0.96875, learning_rate 0.000159368

Evaluation:
2017-10-11T14:49:28.603855: step 1080, loss 0.236007, acc 0.910791

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1080

2017-10-11T14:49:30.048257: step 1081, loss 0.0960966, acc 0.96875, learning_rate 0.000159126
2017-10-11T14:49:30.230648: step 1082, loss 0.159111, acc 0.921875, learning_rate 0.000158885
2017-10-11T14:49:30.436886: step 1083, loss 0.0934172, acc 0.96875, learning_rate 0.000158644
2017-10-11T14:49:30.612629: step 1084, loss 0.114736, acc 0.96875, learning_rate 0.000158405
2017-10-11T14:49:30.797186: step 1085, loss 0.142147, acc 0.953125, learning_rate 0.000158167
2017-10-11T14:49:30.989328: step 1086, loss 0.115283, acc 0.953125, learning_rate 0.000157929
2017-10-11T14:49:31.169451: step 1087, loss 0.140171, acc 0.90625, learning_rate 0.000157693
2017-10-11T14:49:31.360976: step 1088, loss 0.201977, acc 0.921875, learning_rate 0.000157457
2017-10-11T14:49:31.549142: step 1089, loss 0.107042, acc 0.953125, learning_rate 0.000157223
2017-10-11T14:49:31.736586: step 1090, loss 0.123492, acc 0.953125, learning_rate 0.000156989
2017-10-11T14:49:31.926106: step 1091, loss 0.246673, acc 0.921875, learning_rate 0.000156757
2017-10-11T14:49:32.103999: step 1092, loss 0.183524, acc 0.890625, learning_rate 0.000156525
2017-10-11T14:49:32.296192: step 1093, loss 0.130423, acc 0.953125, learning_rate 0.000156294
2017-10-11T14:49:32.482386: step 1094, loss 0.206116, acc 0.9375, learning_rate 0.000156064
2017-10-11T14:49:32.673660: step 1095, loss 0.091986, acc 0.953125, learning_rate 0.000155836
2017-10-11T14:49:32.870132: step 1096, loss 0.28541, acc 0.9375, learning_rate 0.000155608
2017-10-11T14:49:33.059641: step 1097, loss 0.0799306, acc 0.96875, learning_rate 0.000155381
2017-10-11T14:49:33.241762: step 1098, loss 0.193266, acc 0.953125, learning_rate 0.000155155
2017-10-11T14:49:33.420959: step 1099, loss 0.135668, acc 0.9375, learning_rate 0.000154929
2017-10-11T14:49:33.605352: step 1100, loss 0.194513, acc 0.9375, learning_rate 0.000154705
2017-10-11T14:49:33.791709: step 1101, loss 0.209166, acc 0.890625, learning_rate 0.000154482
2017-10-11T14:49:33.976935: step 1102, loss 0.171078, acc 0.953125, learning_rate 0.00015426
2017-10-11T14:49:34.166924: step 1103, loss 0.398158, acc 0.875, learning_rate 0.000154038
2017-10-11T14:49:34.351338: step 1104, loss 0.123543, acc 0.9375, learning_rate 0.000153818
2017-10-11T14:49:34.536402: step 1105, loss 0.0920875, acc 0.96875, learning_rate 0.000153598
2017-10-11T14:49:34.717617: step 1106, loss 0.237417, acc 0.859375, learning_rate 0.000153379
2017-10-11T14:49:34.906728: step 1107, loss 0.145082, acc 0.921875, learning_rate 0.000153161
2017-10-11T14:49:35.090111: step 1108, loss 0.105632, acc 0.984375, learning_rate 0.000152944
2017-10-11T14:49:35.272841: step 1109, loss 0.0710546, acc 0.984375, learning_rate 0.000152728
2017-10-11T14:49:35.467233: step 1110, loss 0.0466079, acc 0.984375, learning_rate 0.000152513
2017-10-11T14:49:35.656056: step 1111, loss 0.0614153, acc 0.953125, learning_rate 0.000152299
2017-10-11T14:49:35.833141: step 1112, loss 0.163481, acc 0.9375, learning_rate 0.000152085
2017-10-11T14:49:36.020505: step 1113, loss 0.0487956, acc 0.984375, learning_rate 0.000151872
2017-10-11T14:49:36.211717: step 1114, loss 0.167783, acc 0.953125, learning_rate 0.000151661
2017-10-11T14:49:36.399106: step 1115, loss 0.0936112, acc 0.9375, learning_rate 0.00015145
2017-10-11T14:49:36.594157: step 1116, loss 0.182197, acc 0.96875, learning_rate 0.00015124
2017-10-11T14:49:36.783692: step 1117, loss 0.0709304, acc 0.984375, learning_rate 0.000151031
2017-10-11T14:49:36.966297: step 1118, loss 0.0908976, acc 0.984375, learning_rate 0.000150822
2017-10-11T14:49:37.133740: step 1119, loss 0.238927, acc 0.90625, learning_rate 0.000150615
2017-10-11T14:49:37.323880: step 1120, loss 0.129723, acc 0.9375, learning_rate 0.000150408

Evaluation:
2017-10-11T14:49:37.482933: step 1120, loss 0.235749, acc 0.909353

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1120

2017-10-11T14:49:39.309707: step 1121, loss 0.119072, acc 0.9375, learning_rate 0.000150203
2017-10-11T14:49:39.494329: step 1122, loss 0.227039, acc 0.921875, learning_rate 0.000149998
2017-10-11T14:49:39.680059: step 1123, loss 0.0517825, acc 0.984375, learning_rate 0.000149794
2017-10-11T14:49:39.859205: step 1124, loss 0.111402, acc 0.953125, learning_rate 0.00014959
2017-10-11T14:49:40.055335: step 1125, loss 0.183224, acc 0.9375, learning_rate 0.000149388
2017-10-11T14:49:40.245489: step 1126, loss 0.120813, acc 0.953125, learning_rate 0.000149186
2017-10-11T14:49:40.442582: step 1127, loss 0.12428, acc 0.90625, learning_rate 0.000148986
2017-10-11T14:49:40.627485: step 1128, loss 0.163288, acc 0.921875, learning_rate 0.000148786
2017-10-11T14:49:40.811022: step 1129, loss 0.200544, acc 0.96875, learning_rate 0.000148587
2017-10-11T14:49:40.995917: step 1130, loss 0.0979491, acc 0.96875, learning_rate 0.000148388
2017-10-11T14:49:41.185037: step 1131, loss 0.207806, acc 0.921875, learning_rate 0.000148191
2017-10-11T14:49:41.370323: step 1132, loss 0.0926064, acc 0.984375, learning_rate 0.000147994
2017-10-11T14:49:41.553718: step 1133, loss 0.0586263, acc 0.984375, learning_rate 0.000147798
2017-10-11T14:49:41.744016: step 1134, loss 0.162961, acc 0.953125, learning_rate 0.000147603
2017-10-11T14:49:41.928852: step 1135, loss 0.0639594, acc 0.984375, learning_rate 0.000147409
2017-10-11T14:49:42.109448: step 1136, loss 0.139605, acc 0.9375, learning_rate 0.000147215
2017-10-11T14:49:42.293377: step 1137, loss 0.171589, acc 0.9375, learning_rate 0.000147022
2017-10-11T14:49:42.473320: step 1138, loss 0.102844, acc 0.984375, learning_rate 0.000146831
2017-10-11T14:49:42.660037: step 1139, loss 0.0970795, acc 0.953125, learning_rate 0.000146639
2017-10-11T14:49:42.842902: step 1140, loss 0.0800905, acc 0.96875, learning_rate 0.000146449
2017-10-11T14:49:43.031162: step 1141, loss 0.173805, acc 0.921875, learning_rate 0.000146259
2017-10-11T14:49:43.219634: step 1142, loss 0.0768181, acc 0.984375, learning_rate 0.000146071
2017-10-11T14:49:43.398537: step 1143, loss 0.0601326, acc 0.984375, learning_rate 0.000145883
2017-10-11T14:49:43.585001: step 1144, loss 0.0481959, acc 0.984375, learning_rate 0.000145695
2017-10-11T14:49:43.777635: step 1145, loss 0.311021, acc 0.90625, learning_rate 0.000145509
2017-10-11T14:49:43.966368: step 1146, loss 0.0970026, acc 0.984375, learning_rate 0.000145323
2017-10-11T14:49:44.157830: step 1147, loss 0.133188, acc 0.96875, learning_rate 0.000145138
2017-10-11T14:49:44.340524: step 1148, loss 0.0834802, acc 0.984375, learning_rate 0.000144954
2017-10-11T14:49:44.519153: step 1149, loss 0.160311, acc 0.9375, learning_rate 0.00014477
2017-10-11T14:49:44.718758: step 1150, loss 0.08975, acc 0.96875, learning_rate 0.000144588
2017-10-11T14:49:44.906773: step 1151, loss 0.0648413, acc 0.96875, learning_rate 0.000144406
2017-10-11T14:49:45.088892: step 1152, loss 0.160569, acc 0.9375, learning_rate 0.000144224
2017-10-11T14:49:45.276622: step 1153, loss 0.0598069, acc 0.984375, learning_rate 0.000144044
2017-10-11T14:49:45.464148: step 1154, loss 0.0696758, acc 0.96875, learning_rate 0.000143864
2017-10-11T14:49:45.646375: step 1155, loss 0.0847749, acc 0.984375, learning_rate 0.000143685
2017-10-11T14:49:45.827426: step 1156, loss 0.311393, acc 0.9375, learning_rate 0.000143507
2017-10-11T14:49:46.004899: step 1157, loss 0.0954656, acc 0.96875, learning_rate 0.000143329
2017-10-11T14:49:46.194579: step 1158, loss 0.163871, acc 0.9375, learning_rate 0.000143152
2017-10-11T14:49:46.386272: step 1159, loss 0.0771048, acc 0.96875, learning_rate 0.000142976
2017-10-11T14:49:46.582654: step 1160, loss 0.07195, acc 0.96875, learning_rate 0.000142801

Evaluation:
2017-10-11T14:49:46.754575: step 1160, loss 0.235648, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1160

2017-10-11T14:49:47.860996: step 1161, loss 0.149364, acc 0.921875, learning_rate 0.000142626
2017-10-11T14:49:48.044351: step 1162, loss 0.208886, acc 0.921875, learning_rate 0.000142452
2017-10-11T14:49:48.230089: step 1163, loss 0.108834, acc 0.9375, learning_rate 0.000142279
2017-10-11T14:49:48.405808: step 1164, loss 0.0932331, acc 0.96875, learning_rate 0.000142106
2017-10-11T14:49:48.598584: step 1165, loss 0.169925, acc 0.921875, learning_rate 0.000141934
2017-10-11T14:49:48.785843: step 1166, loss 0.133289, acc 0.953125, learning_rate 0.000141763
2017-10-11T14:49:48.972091: step 1167, loss 0.153518, acc 0.96875, learning_rate 0.000141593
2017-10-11T14:49:49.163924: step 1168, loss 0.119825, acc 0.953125, learning_rate 0.000141423
2017-10-11T14:49:49.342290: step 1169, loss 0.0752534, acc 0.953125, learning_rate 0.000141254
2017-10-11T14:49:49.535085: step 1170, loss 0.140752, acc 0.9375, learning_rate 0.000141085
2017-10-11T14:49:49.717582: step 1171, loss 0.0333344, acc 1, learning_rate 0.000140918
2017-10-11T14:49:49.906073: step 1172, loss 0.0804473, acc 0.984375, learning_rate 0.000140751
2017-10-11T14:49:50.092033: step 1173, loss 0.0878178, acc 0.96875, learning_rate 0.000140584
2017-10-11T14:49:50.275380: step 1174, loss 0.128121, acc 0.9375, learning_rate 0.000140419
2017-10-11T14:49:50.449790: step 1175, loss 0.112841, acc 0.953125, learning_rate 0.000140254
2017-10-11T14:49:50.605970: step 1176, loss 0.0681129, acc 0.980392, learning_rate 0.000140089
2017-10-11T14:49:50.792026: step 1177, loss 0.0434063, acc 0.984375, learning_rate 0.000139926
2017-10-11T14:49:50.988223: step 1178, loss 0.149601, acc 0.9375, learning_rate 0.000139763
2017-10-11T14:49:51.175450: step 1179, loss 0.0457413, acc 1, learning_rate 0.0001396
2017-10-11T14:49:51.354130: step 1180, loss 0.136321, acc 0.953125, learning_rate 0.000139439
2017-10-11T14:49:51.536277: step 1181, loss 0.111397, acc 0.953125, learning_rate 0.000139278
2017-10-11T14:49:51.721963: step 1182, loss 0.233424, acc 0.921875, learning_rate 0.000139118
2017-10-11T14:49:51.917564: step 1183, loss 0.0453482, acc 0.984375, learning_rate 0.000138958
2017-10-11T14:49:52.097606: step 1184, loss 0.0602547, acc 0.984375, learning_rate 0.000138799
2017-10-11T14:49:52.278356: step 1185, loss 0.140899, acc 0.953125, learning_rate 0.00013864
2017-10-11T14:49:52.451370: step 1186, loss 0.114391, acc 0.953125, learning_rate 0.000138483
2017-10-11T14:49:52.631765: step 1187, loss 0.169443, acc 0.953125, learning_rate 0.000138326
2017-10-11T14:49:52.826149: step 1188, loss 0.194758, acc 0.921875, learning_rate 0.000138169
2017-10-11T14:49:53.013463: step 1189, loss 0.162709, acc 0.953125, learning_rate 0.000138013
2017-10-11T14:49:53.200467: step 1190, loss 0.0884549, acc 0.984375, learning_rate 0.000137858
2017-10-11T14:49:53.385786: step 1191, loss 0.0637123, acc 0.96875, learning_rate 0.000137704
2017-10-11T14:49:53.579771: step 1192, loss 0.169479, acc 0.921875, learning_rate 0.00013755
2017-10-11T14:49:53.758843: step 1193, loss 0.167675, acc 0.9375, learning_rate 0.000137397
2017-10-11T14:49:53.940117: step 1194, loss 0.100948, acc 0.96875, learning_rate 0.000137244
2017-10-11T14:49:54.117983: step 1195, loss 0.207774, acc 0.921875, learning_rate 0.000137092
2017-10-11T14:49:54.303932: step 1196, loss 0.117445, acc 0.953125, learning_rate 0.000136941
2017-10-11T14:49:54.475605: step 1197, loss 0.209777, acc 0.921875, learning_rate 0.00013679
2017-10-11T14:49:54.663786: step 1198, loss 0.141124, acc 0.953125, learning_rate 0.00013664
2017-10-11T14:49:54.838796: step 1199, loss 0.181832, acc 0.953125, learning_rate 0.00013649
2017-10-11T14:49:55.019441: step 1200, loss 0.125379, acc 0.9375, learning_rate 0.000136341

Evaluation:
2017-10-11T14:49:55.184402: step 1200, loss 0.23373, acc 0.907914

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1200

2017-10-11T14:49:56.804113: step 1201, loss 0.0829564, acc 0.953125, learning_rate 0.000136193
2017-10-11T14:49:56.991241: step 1202, loss 0.0864233, acc 0.96875, learning_rate 0.000136045
2017-10-11T14:49:57.189635: step 1203, loss 0.0550636, acc 0.984375, learning_rate 0.000135898
2017-10-11T14:49:57.389302: step 1204, loss 0.105519, acc 0.953125, learning_rate 0.000135751
2017-10-11T14:49:57.573464: step 1205, loss 0.219798, acc 0.90625, learning_rate 0.000135605
2017-10-11T14:49:57.753824: step 1206, loss 0.129149, acc 0.96875, learning_rate 0.00013546
2017-10-11T14:49:57.942733: step 1207, loss 0.158673, acc 0.96875, learning_rate 0.000135315
2017-10-11T14:49:58.135475: step 1208, loss 0.150318, acc 0.9375, learning_rate 0.000135171
2017-10-11T14:49:58.317341: step 1209, loss 0.132391, acc 0.953125, learning_rate 0.000135028
2017-10-11T14:49:58.503660: step 1210, loss 0.082214, acc 0.984375, learning_rate 0.000134885
2017-10-11T14:49:58.692330: step 1211, loss 0.128471, acc 0.953125, learning_rate 0.000134742
2017-10-11T14:49:58.886313: step 1212, loss 0.133644, acc 0.953125, learning_rate 0.0001346
2017-10-11T14:49:59.063118: step 1213, loss 0.067597, acc 0.96875, learning_rate 0.000134459
2017-10-11T14:49:59.240931: step 1214, loss 0.0934206, acc 0.96875, learning_rate 0.000134319
2017-10-11T14:49:59.420795: step 1215, loss 0.255031, acc 0.90625, learning_rate 0.000134178
2017-10-11T14:49:59.607709: step 1216, loss 0.118239, acc 0.96875, learning_rate 0.000134039
2017-10-11T14:49:59.792285: step 1217, loss 0.0982519, acc 0.96875, learning_rate 0.0001339
2017-10-11T14:49:59.966729: step 1218, loss 0.23804, acc 0.90625, learning_rate 0.000133762
2017-10-11T14:50:00.151981: step 1219, loss 0.110586, acc 0.953125, learning_rate 0.000133624
2017-10-11T14:50:00.341643: step 1220, loss 0.100841, acc 0.984375, learning_rate 0.000133487
2017-10-11T14:50:00.518938: step 1221, loss 0.0483193, acc 1, learning_rate 0.00013335
2017-10-11T14:50:00.698069: step 1222, loss 0.0520274, acc 1, learning_rate 0.000133214
2017-10-11T14:50:00.880884: step 1223, loss 0.108683, acc 0.984375, learning_rate 0.000133078
2017-10-11T14:50:01.063095: step 1224, loss 0.0853711, acc 0.984375, learning_rate 0.000132943
2017-10-11T14:50:01.252434: step 1225, loss 0.0482604, acc 1, learning_rate 0.000132809
2017-10-11T14:50:01.438422: step 1226, loss 0.0523595, acc 0.984375, learning_rate 0.000132675
2017-10-11T14:50:01.616279: step 1227, loss 0.110231, acc 0.9375, learning_rate 0.000132541
2017-10-11T14:50:01.801449: step 1228, loss 0.0318967, acc 0.984375, learning_rate 0.000132409
2017-10-11T14:50:01.982165: step 1229, loss 0.053522, acc 0.984375, learning_rate 0.000132276
2017-10-11T14:50:02.170383: step 1230, loss 0.186016, acc 0.921875, learning_rate 0.000132145
2017-10-11T14:50:02.355849: step 1231, loss 0.0611315, acc 0.96875, learning_rate 0.000132013
2017-10-11T14:50:02.533360: step 1232, loss 0.175211, acc 0.96875, learning_rate 0.000131883
2017-10-11T14:50:02.716397: step 1233, loss 0.319749, acc 0.890625, learning_rate 0.000131753
2017-10-11T14:50:02.901658: step 1234, loss 0.088844, acc 0.953125, learning_rate 0.000131623
2017-10-11T14:50:03.092008: step 1235, loss 0.0749448, acc 0.96875, learning_rate 0.000131494
2017-10-11T14:50:03.271668: step 1236, loss 0.0920495, acc 0.96875, learning_rate 0.000131365
2017-10-11T14:50:03.453546: step 1237, loss 0.0881225, acc 0.96875, learning_rate 0.000131237
2017-10-11T14:50:03.641859: step 1238, loss 0.0753746, acc 0.984375, learning_rate 0.00013111
2017-10-11T14:50:03.828060: step 1239, loss 0.0575768, acc 0.984375, learning_rate 0.000130983
2017-10-11T14:50:04.014634: step 1240, loss 0.0520796, acc 0.984375, learning_rate 0.000130856

Evaluation:
2017-10-11T14:50:04.178001: step 1240, loss 0.233463, acc 0.910791

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1240

2017-10-11T14:50:05.290938: step 1241, loss 0.179602, acc 0.9375, learning_rate 0.00013073
2017-10-11T14:50:05.471796: step 1242, loss 0.199138, acc 0.90625, learning_rate 0.000130605
2017-10-11T14:50:05.664456: step 1243, loss 0.0853899, acc 0.984375, learning_rate 0.00013048
2017-10-11T14:50:05.846597: step 1244, loss 0.0459088, acc 0.984375, learning_rate 0.000130356
2017-10-11T14:50:06.034546: step 1245, loss 0.138299, acc 0.9375, learning_rate 0.000130232
2017-10-11T14:50:06.209840: step 1246, loss 0.137351, acc 0.984375, learning_rate 0.000130108
2017-10-11T14:50:06.396112: step 1247, loss 0.0597328, acc 0.96875, learning_rate 0.000129985
2017-10-11T14:50:06.578126: step 1248, loss 0.203327, acc 0.921875, learning_rate 0.000129863
2017-10-11T14:50:06.756138: step 1249, loss 0.136554, acc 0.96875, learning_rate 0.000129741
2017-10-11T14:50:06.938557: step 1250, loss 0.0163295, acc 1, learning_rate 0.00012962
2017-10-11T14:50:07.119080: step 1251, loss 0.293217, acc 0.90625, learning_rate 0.000129499
2017-10-11T14:50:07.312940: step 1252, loss 0.0937113, acc 0.953125, learning_rate 0.000129378
2017-10-11T14:50:07.500884: step 1253, loss 0.0302903, acc 1, learning_rate 0.000129259
2017-10-11T14:50:07.683356: step 1254, loss 0.0599329, acc 0.984375, learning_rate 0.000129139
2017-10-11T14:50:07.860130: step 1255, loss 0.0504564, acc 0.96875, learning_rate 0.00012902
2017-10-11T14:50:08.039872: step 1256, loss 0.115671, acc 0.953125, learning_rate 0.000128902
2017-10-11T14:50:08.232304: step 1257, loss 0.111133, acc 0.96875, learning_rate 0.000128784
2017-10-11T14:50:08.423866: step 1258, loss 0.105223, acc 0.953125, learning_rate 0.000128666
2017-10-11T14:50:08.604561: step 1259, loss 0.180215, acc 0.9375, learning_rate 0.000128549
2017-10-11T14:50:08.774182: step 1260, loss 0.272319, acc 0.9375, learning_rate 0.000128433
2017-10-11T14:50:08.960196: step 1261, loss 0.208803, acc 0.9375, learning_rate 0.000128317
2017-10-11T14:50:09.145877: step 1262, loss 0.0671559, acc 0.984375, learning_rate 0.000128201
2017-10-11T14:50:09.318897: step 1263, loss 0.114147, acc 0.953125, learning_rate 0.000128086
2017-10-11T14:50:09.509466: step 1264, loss 0.128992, acc 0.96875, learning_rate 0.000127971
2017-10-11T14:50:09.701270: step 1265, loss 0.0368057, acc 0.984375, learning_rate 0.000127857
2017-10-11T14:50:09.889839: step 1266, loss 0.289825, acc 0.90625, learning_rate 0.000127743
2017-10-11T14:50:10.061820: step 1267, loss 0.0550945, acc 0.984375, learning_rate 0.00012763
2017-10-11T14:50:10.241105: step 1268, loss 0.0859239, acc 0.984375, learning_rate 0.000127517
2017-10-11T14:50:10.436233: step 1269, loss 0.190484, acc 0.921875, learning_rate 0.000127405
2017-10-11T14:50:10.621286: step 1270, loss 0.174975, acc 0.9375, learning_rate 0.000127293
2017-10-11T14:50:10.813216: step 1271, loss 0.19376, acc 0.9375, learning_rate 0.000127182
2017-10-11T14:50:11.001939: step 1272, loss 0.163672, acc 0.953125, learning_rate 0.000127071
2017-10-11T14:50:11.179200: step 1273, loss 0.197995, acc 0.953125, learning_rate 0.00012696
2017-10-11T14:50:11.335646: step 1274, loss 0.189918, acc 0.921569, learning_rate 0.00012685
2017-10-11T14:50:11.513784: step 1275, loss 0.0440765, acc 0.984375, learning_rate 0.000126741
2017-10-11T14:50:11.705760: step 1276, loss 0.106943, acc 0.953125, learning_rate 0.000126632
2017-10-11T14:50:11.899963: step 1277, loss 0.048704, acc 1, learning_rate 0.000126523
2017-10-11T14:50:12.064612: step 1278, loss 0.0412151, acc 1, learning_rate 0.000126415
2017-10-11T14:50:12.240409: step 1279, loss 0.0911007, acc 0.953125, learning_rate 0.000126307
2017-10-11T14:50:12.418457: step 1280, loss 0.0830553, acc 0.96875, learning_rate 0.000126199

Evaluation:
2017-10-11T14:50:12.572795: step 1280, loss 0.237625, acc 0.909353

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1280

2017-10-11T14:50:13.905113: step 1281, loss 0.10471, acc 0.953125, learning_rate 0.000126093
2017-10-11T14:50:14.096086: step 1282, loss 0.218239, acc 0.90625, learning_rate 0.000125986
2017-10-11T14:50:14.284594: step 1283, loss 0.153595, acc 0.921875, learning_rate 0.00012588
2017-10-11T14:50:14.468797: step 1284, loss 0.111164, acc 0.9375, learning_rate 0.000125774
2017-10-11T14:50:14.651820: step 1285, loss 0.0863665, acc 0.96875, learning_rate 0.000125669
2017-10-11T14:50:14.828987: step 1286, loss 0.112645, acc 0.9375, learning_rate 0.000125564
2017-10-11T14:50:15.009613: step 1287, loss 0.0370747, acc 1, learning_rate 0.00012546
2017-10-11T14:50:15.189649: step 1288, loss 0.177824, acc 0.9375, learning_rate 0.000125356
2017-10-11T14:50:15.372085: step 1289, loss 0.213614, acc 0.90625, learning_rate 0.000125253
2017-10-11T14:50:15.559033: step 1290, loss 0.0994019, acc 0.96875, learning_rate 0.00012515
2017-10-11T14:50:15.737190: step 1291, loss 0.160179, acc 0.953125, learning_rate 0.000125047
2017-10-11T14:50:15.915409: step 1292, loss 0.280163, acc 0.921875, learning_rate 0.000124945
2017-10-11T14:50:16.104636: step 1293, loss 0.212951, acc 0.921875, learning_rate 0.000124843
2017-10-11T14:50:16.283442: step 1294, loss 0.103822, acc 0.96875, learning_rate 0.000124741
2017-10-11T14:50:16.479545: step 1295, loss 0.133049, acc 0.96875, learning_rate 0.00012464
2017-10-11T14:50:16.669147: step 1296, loss 0.204275, acc 0.9375, learning_rate 0.00012454
2017-10-11T14:50:16.852537: step 1297, loss 0.180739, acc 0.953125, learning_rate 0.00012444
2017-10-11T14:50:17.033226: step 1298, loss 0.160583, acc 0.9375, learning_rate 0.00012434
2017-10-11T14:50:17.214929: step 1299, loss 0.0699537, acc 0.984375, learning_rate 0.000124241
2017-10-11T14:50:17.396573: step 1300, loss 0.0322418, acc 1, learning_rate 0.000124142
2017-10-11T14:50:17.576059: step 1301, loss 0.0230909, acc 1, learning_rate 0.000124043
2017-10-11T14:50:17.760762: step 1302, loss 0.0903361, acc 0.953125, learning_rate 0.000123945
2017-10-11T14:50:17.933725: step 1303, loss 0.145854, acc 0.9375, learning_rate 0.000123847
2017-10-11T14:50:18.120858: step 1304, loss 0.104086, acc 0.96875, learning_rate 0.00012375
2017-10-11T14:50:18.296141: step 1305, loss 0.162881, acc 0.953125, learning_rate 0.000123653
2017-10-11T14:50:18.477520: step 1306, loss 0.0309065, acc 0.984375, learning_rate 0.000123556
2017-10-11T14:50:18.660957: step 1307, loss 0.0914274, acc 0.96875, learning_rate 0.00012346
2017-10-11T14:50:18.850016: step 1308, loss 0.0603004, acc 0.984375, learning_rate 0.000123364
2017-10-11T14:50:19.034629: step 1309, loss 0.0305016, acc 0.984375, learning_rate 0.000123269
2017-10-11T14:50:19.225706: step 1310, loss 0.0654456, acc 0.984375, learning_rate 0.000123174
2017-10-11T14:50:19.420071: step 1311, loss 0.166689, acc 0.90625, learning_rate 0.00012308
2017-10-11T14:50:19.601832: step 1312, loss 0.107366, acc 0.96875, learning_rate 0.000122985
2017-10-11T14:50:19.777622: step 1313, loss 0.139786, acc 0.953125, learning_rate 0.000122892
2017-10-11T14:50:19.968638: step 1314, loss 0.09009, acc 0.953125, learning_rate 0.000122798
2017-10-11T14:50:20.149933: step 1315, loss 0.0950691, acc 0.984375, learning_rate 0.000122705
2017-10-11T14:50:20.327670: step 1316, loss 0.124029, acc 0.9375, learning_rate 0.000122612
2017-10-11T14:50:20.506418: step 1317, loss 0.140123, acc 0.96875, learning_rate 0.00012252
2017-10-11T14:50:20.693718: step 1318, loss 0.210113, acc 0.9375, learning_rate 0.000122428
2017-10-11T14:50:20.875980: step 1319, loss 0.0906927, acc 0.96875, learning_rate 0.000122337
2017-10-11T14:50:21.056429: step 1320, loss 0.137238, acc 0.921875, learning_rate 0.000122245

Evaluation:
2017-10-11T14:50:21.223284: step 1320, loss 0.231664, acc 0.907914

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1320

2017-10-11T14:50:22.833294: step 1321, loss 0.217443, acc 0.9375, learning_rate 0.000122155
2017-10-11T14:50:23.021090: step 1322, loss 0.247836, acc 0.9375, learning_rate 0.000122064
2017-10-11T14:50:23.196582: step 1323, loss 0.130122, acc 0.96875, learning_rate 0.000121974
2017-10-11T14:50:23.383148: step 1324, loss 0.163551, acc 0.953125, learning_rate 0.000121884
2017-10-11T14:50:23.567622: step 1325, loss 0.133263, acc 0.96875, learning_rate 0.000121795
2017-10-11T14:50:23.753972: step 1326, loss 0.141599, acc 0.96875, learning_rate 0.000121706
2017-10-11T14:50:23.936928: step 1327, loss 0.0511794, acc 0.984375, learning_rate 0.000121618
2017-10-11T14:50:24.123461: step 1328, loss 0.106104, acc 0.96875, learning_rate 0.000121529
2017-10-11T14:50:24.318667: step 1329, loss 0.114133, acc 0.953125, learning_rate 0.000121441
2017-10-11T14:50:24.502827: step 1330, loss 0.0829874, acc 0.9375, learning_rate 0.000121354
2017-10-11T14:50:24.689672: step 1331, loss 0.209073, acc 0.921875, learning_rate 0.000121267
2017-10-11T14:50:24.874152: step 1332, loss 0.0503258, acc 0.984375, learning_rate 0.00012118
2017-10-11T14:50:25.060339: step 1333, loss 0.199028, acc 0.953125, learning_rate 0.000121093
2017-10-11T14:50:25.241339: step 1334, loss 0.0926232, acc 0.953125, learning_rate 0.000121007
2017-10-11T14:50:25.434857: step 1335, loss 0.200649, acc 0.921875, learning_rate 0.000120922
2017-10-11T14:50:25.621130: step 1336, loss 0.151639, acc 0.9375, learning_rate 0.000120836
2017-10-11T14:50:25.817645: step 1337, loss 0.073303, acc 0.953125, learning_rate 0.000120751
2017-10-11T14:50:25.997829: step 1338, loss 0.0806029, acc 0.984375, learning_rate 0.000120666
2017-10-11T14:50:26.189446: step 1339, loss 0.0798761, acc 0.984375, learning_rate 0.000120582
2017-10-11T14:50:26.358042: step 1340, loss 0.211541, acc 0.953125, learning_rate 0.000120498
2017-10-11T14:50:26.540561: step 1341, loss 0.0948503, acc 0.96875, learning_rate 0.000120414
2017-10-11T14:50:26.714094: step 1342, loss 0.114654, acc 0.96875, learning_rate 0.000120331
2017-10-11T14:50:26.895964: step 1343, loss 0.0956406, acc 0.96875, learning_rate 0.000120248
2017-10-11T14:50:27.094844: step 1344, loss 0.0928774, acc 0.953125, learning_rate 0.000120165
2017-10-11T14:50:27.264642: step 1345, loss 0.12967, acc 0.953125, learning_rate 0.000120083
2017-10-11T14:50:27.453815: step 1346, loss 0.143102, acc 0.953125, learning_rate 0.000120001
2017-10-11T14:50:28.082468: step 1347, loss 0.130498, acc 0.9375, learning_rate 0.00011992
2017-10-11T14:50:28.270040: step 1348, loss 0.258637, acc 0.90625, learning_rate 0.000119838
2017-10-11T14:50:28.452157: step 1349, loss 0.176906, acc 0.9375, learning_rate 0.000119757
2017-10-11T14:50:28.633230: step 1350, loss 0.110446, acc 0.96875, learning_rate 0.000119677
2017-10-11T14:50:28.820205: step 1351, loss 0.260971, acc 0.90625, learning_rate 0.000119596
2017-10-11T14:50:29.014967: step 1352, loss 0.114194, acc 0.9375, learning_rate 0.000119516
2017-10-11T14:50:29.205892: step 1353, loss 0.106058, acc 0.953125, learning_rate 0.000119437
2017-10-11T14:50:29.398022: step 1354, loss 0.160815, acc 0.9375, learning_rate 0.000119357
2017-10-11T14:50:29.585918: step 1355, loss 0.138962, acc 0.9375, learning_rate 0.000119278
2017-10-11T14:50:29.782905: step 1356, loss 0.0837019, acc 0.96875, learning_rate 0.0001192
2017-10-11T14:50:29.973389: step 1357, loss 0.0506956, acc 0.984375, learning_rate 0.000119121
2017-10-11T14:50:30.164311: step 1358, loss 0.106479, acc 0.96875, learning_rate 0.000119043
2017-10-11T14:50:30.352346: step 1359, loss 0.171282, acc 0.953125, learning_rate 0.000118965
2017-10-11T14:50:30.528504: step 1360, loss 0.0377929, acc 0.984375, learning_rate 0.000118888

Evaluation:
2017-10-11T14:50:30.685738: step 1360, loss 0.239263, acc 0.909353

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1360

2017-10-11T14:50:31.785480: step 1361, loss 0.146894, acc 0.9375, learning_rate 0.000118811
2017-10-11T14:50:31.971945: step 1362, loss 0.0624926, acc 0.984375, learning_rate 0.000118734
2017-10-11T14:50:32.155765: step 1363, loss 0.241067, acc 0.9375, learning_rate 0.000118658
2017-10-11T14:50:32.345637: step 1364, loss 0.105997, acc 0.984375, learning_rate 0.000118582
2017-10-11T14:50:32.539063: step 1365, loss 0.143209, acc 0.96875, learning_rate 0.000118506
2017-10-11T14:50:32.735314: step 1366, loss 0.0674518, acc 0.953125, learning_rate 0.00011843
2017-10-11T14:50:32.918015: step 1367, loss 0.249983, acc 0.9375, learning_rate 0.000118355
2017-10-11T14:50:33.097304: step 1368, loss 0.0280555, acc 1, learning_rate 0.00011828
2017-10-11T14:50:33.285117: step 1369, loss 0.290316, acc 0.953125, learning_rate 0.000118205
2017-10-11T14:50:33.473495: step 1370, loss 0.195801, acc 0.953125, learning_rate 0.000118131
2017-10-11T14:50:33.667761: step 1371, loss 0.0661751, acc 0.984375, learning_rate 0.000118057
2017-10-11T14:50:33.833702: step 1372, loss 0.0846059, acc 0.960784, learning_rate 0.000117983
2017-10-11T14:50:34.020635: step 1373, loss 0.20529, acc 0.9375, learning_rate 0.00011791
2017-10-11T14:50:34.192888: step 1374, loss 0.0457058, acc 1, learning_rate 0.000117837
2017-10-11T14:50:34.369462: step 1375, loss 0.0420115, acc 1, learning_rate 0.000117764
2017-10-11T14:50:34.548040: step 1376, loss 0.0845949, acc 0.984375, learning_rate 0.000117692
2017-10-11T14:50:34.731489: step 1377, loss 0.0633219, acc 0.984375, learning_rate 0.000117619
2017-10-11T14:50:34.915705: step 1378, loss 0.198392, acc 0.9375, learning_rate 0.000117547
2017-10-11T14:50:35.094659: step 1379, loss 0.0470258, acc 1, learning_rate 0.000117476
2017-10-11T14:50:35.286001: step 1380, loss 0.0634467, acc 0.96875, learning_rate 0.000117404
2017-10-11T14:50:35.476897: step 1381, loss 0.176853, acc 0.921875, learning_rate 0.000117333
2017-10-11T14:50:35.661506: step 1382, loss 0.149782, acc 0.9375, learning_rate 0.000117263
2017-10-11T14:50:35.844973: step 1383, loss 0.151589, acc 0.953125, learning_rate 0.000117192
2017-10-11T14:50:36.033948: step 1384, loss 0.0530728, acc 0.96875, learning_rate 0.000117122
2017-10-11T14:50:36.205252: step 1385, loss 0.205113, acc 0.953125, learning_rate 0.000117052
2017-10-11T14:50:36.377068: step 1386, loss 0.070834, acc 0.96875, learning_rate 0.000116983
2017-10-11T14:50:36.558618: step 1387, loss 0.165467, acc 0.953125, learning_rate 0.000116913
2017-10-11T14:50:36.734575: step 1388, loss 0.131673, acc 0.953125, learning_rate 0.000116844
2017-10-11T14:50:36.905551: step 1389, loss 0.0622673, acc 0.984375, learning_rate 0.000116775
2017-10-11T14:50:37.083311: step 1390, loss 0.171627, acc 0.9375, learning_rate 0.000116707
2017-10-11T14:50:37.255427: step 1391, loss 0.202332, acc 0.890625, learning_rate 0.000116639
2017-10-11T14:50:37.439725: step 1392, loss 0.151161, acc 0.9375, learning_rate 0.000116571
2017-10-11T14:50:37.633492: step 1393, loss 0.186394, acc 0.953125, learning_rate 0.000116503
2017-10-11T14:50:37.833685: step 1394, loss 0.242634, acc 0.9375, learning_rate 0.000116436
2017-10-11T14:50:38.025294: step 1395, loss 0.226304, acc 0.90625, learning_rate 0.000116369
2017-10-11T14:50:38.213391: step 1396, loss 0.0589194, acc 0.96875, learning_rate 0.000116302
2017-10-11T14:50:38.397863: step 1397, loss 0.0640965, acc 0.96875, learning_rate 0.000116235
2017-10-11T14:50:38.575713: step 1398, loss 0.185672, acc 0.890625, learning_rate 0.000116169
2017-10-11T14:50:38.760735: step 1399, loss 0.143372, acc 0.96875, learning_rate 0.000116103
2017-10-11T14:50:38.953423: step 1400, loss 0.116487, acc 0.953125, learning_rate 0.000116037

Evaluation:
2017-10-11T14:50:39.110735: step 1400, loss 0.232726, acc 0.909353

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1400

2017-10-11T14:50:40.454094: step 1401, loss 0.0915496, acc 0.96875, learning_rate 0.000115972
2017-10-11T14:50:40.638833: step 1402, loss 0.0642221, acc 0.984375, learning_rate 0.000115907
2017-10-11T14:50:40.825992: step 1403, loss 0.142956, acc 0.96875, learning_rate 0.000115842
2017-10-11T14:50:41.012362: step 1404, loss 0.099466, acc 0.96875, learning_rate 0.000115777
2017-10-11T14:50:41.200775: step 1405, loss 0.0770992, acc 0.96875, learning_rate 0.000115713
2017-10-11T14:50:41.377357: step 1406, loss 0.0663709, acc 1, learning_rate 0.000115649
2017-10-11T14:50:41.559812: step 1407, loss 0.0581935, acc 0.984375, learning_rate 0.000115585
2017-10-11T14:50:41.739882: step 1408, loss 0.173189, acc 0.90625, learning_rate 0.000115521
2017-10-11T14:50:41.929449: step 1409, loss 0.0886155, acc 0.96875, learning_rate 0.000115458
2017-10-11T14:50:42.113758: step 1410, loss 0.0563756, acc 0.96875, learning_rate 0.000115395
2017-10-11T14:50:42.289158: step 1411, loss 0.111632, acc 0.96875, learning_rate 0.000115332
2017-10-11T14:50:42.479203: step 1412, loss 0.0649363, acc 0.984375, learning_rate 0.000115269
2017-10-11T14:50:42.658078: step 1413, loss 0.0445273, acc 1, learning_rate 0.000115207
2017-10-11T14:50:42.844054: step 1414, loss 0.0905894, acc 0.96875, learning_rate 0.000115145
2017-10-11T14:50:43.033142: step 1415, loss 0.140666, acc 0.921875, learning_rate 0.000115083
2017-10-11T14:50:43.210619: step 1416, loss 0.0873822, acc 0.96875, learning_rate 0.000115022
2017-10-11T14:50:43.401682: step 1417, loss 0.18784, acc 0.9375, learning_rate 0.00011496
2017-10-11T14:50:43.583191: step 1418, loss 0.0893693, acc 0.984375, learning_rate 0.000114899
2017-10-11T14:50:43.771500: step 1419, loss 0.220196, acc 0.9375, learning_rate 0.000114838
2017-10-11T14:50:43.949030: step 1420, loss 0.0342227, acc 1, learning_rate 0.000114778
2017-10-11T14:50:44.134081: step 1421, loss 0.105652, acc 0.9375, learning_rate 0.000114717
2017-10-11T14:50:44.310821: step 1422, loss 0.111292, acc 0.96875, learning_rate 0.000114657
2017-10-11T14:50:44.491469: step 1423, loss 0.0138092, acc 1, learning_rate 0.000114598
2017-10-11T14:50:44.681403: step 1424, loss 0.0322633, acc 1, learning_rate 0.000114538
2017-10-11T14:50:44.863502: step 1425, loss 0.253349, acc 0.921875, learning_rate 0.000114479
2017-10-11T14:50:45.051405: step 1426, loss 0.0858324, acc 0.953125, learning_rate 0.00011442
2017-10-11T14:50:45.241122: step 1427, loss 0.181471, acc 0.921875, learning_rate 0.000114361
2017-10-11T14:50:45.417586: step 1428, loss 0.104142, acc 0.96875, learning_rate 0.000114302
2017-10-11T14:50:45.599023: step 1429, loss 0.278837, acc 0.921875, learning_rate 0.000114244
2017-10-11T14:50:45.784696: step 1430, loss 0.0508581, acc 0.96875, learning_rate 0.000114186
2017-10-11T14:50:45.960605: step 1431, loss 0.228274, acc 0.90625, learning_rate 0.000114128
2017-10-11T14:50:46.141931: step 1432, loss 0.0911023, acc 0.953125, learning_rate 0.00011407
2017-10-11T14:50:46.326393: step 1433, loss 0.119191, acc 0.9375, learning_rate 0.000114013
2017-10-11T14:50:46.519571: step 1434, loss 0.156591, acc 0.90625, learning_rate 0.000113955
2017-10-11T14:50:46.700870: step 1435, loss 0.190191, acc 0.9375, learning_rate 0.000113898
2017-10-11T14:50:46.870168: step 1436, loss 0.0869119, acc 0.96875, learning_rate 0.000113842
2017-10-11T14:50:47.046282: step 1437, loss 0.11779, acc 0.96875, learning_rate 0.000113785
2017-10-11T14:50:47.231213: step 1438, loss 0.146278, acc 0.90625, learning_rate 0.000113729
2017-10-11T14:50:47.406440: step 1439, loss 0.186306, acc 0.9375, learning_rate 0.000113673
2017-10-11T14:50:47.585592: step 1440, loss 0.188309, acc 0.921875, learning_rate 0.000113617

Evaluation:
2017-10-11T14:50:47.748345: step 1440, loss 0.231065, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1440

2017-10-11T14:50:49.362085: step 1441, loss 0.130157, acc 0.953125, learning_rate 0.000113561
2017-10-11T14:50:49.540086: step 1442, loss 0.0984512, acc 0.9375, learning_rate 0.000113506
2017-10-11T14:50:49.736412: step 1443, loss 0.107278, acc 0.96875, learning_rate 0.000113451
2017-10-11T14:50:49.926278: step 1444, loss 0.0970854, acc 0.984375, learning_rate 0.000113396
2017-10-11T14:50:50.107857: step 1445, loss 0.125706, acc 0.921875, learning_rate 0.000113341
2017-10-11T14:50:50.281666: step 1446, loss 0.113561, acc 0.953125, learning_rate 0.000113287
2017-10-11T14:50:50.473047: step 1447, loss 0.119143, acc 0.953125, learning_rate 0.000113233
2017-10-11T14:50:50.652592: step 1448, loss 0.213028, acc 0.921875, learning_rate 0.000113179
2017-10-11T14:50:50.838484: step 1449, loss 0.179852, acc 0.921875, learning_rate 0.000113125
2017-10-11T14:50:51.018076: step 1450, loss 0.128281, acc 0.9375, learning_rate 0.000113071
2017-10-11T14:50:51.198234: step 1451, loss 0.0863304, acc 0.96875, learning_rate 0.000113018
2017-10-11T14:50:51.380025: step 1452, loss 0.191099, acc 0.9375, learning_rate 0.000112965
2017-10-11T14:50:51.552355: step 1453, loss 0.142042, acc 0.9375, learning_rate 0.000112912
2017-10-11T14:50:51.743248: step 1454, loss 0.180699, acc 0.9375, learning_rate 0.000112859
2017-10-11T14:50:51.927293: step 1455, loss 0.13101, acc 0.9375, learning_rate 0.000112807
2017-10-11T14:50:52.108984: step 1456, loss 0.0647323, acc 0.984375, learning_rate 0.000112754
2017-10-11T14:50:52.289371: step 1457, loss 0.179685, acc 0.953125, learning_rate 0.000112702
2017-10-11T14:50:52.461903: step 1458, loss 0.120215, acc 0.953125, learning_rate 0.000112651
2017-10-11T14:50:52.634210: step 1459, loss 0.16158, acc 0.953125, learning_rate 0.000112599
2017-10-11T14:50:52.814646: step 1460, loss 0.150625, acc 0.9375, learning_rate 0.000112547
2017-10-11T14:50:52.997113: step 1461, loss 0.184817, acc 0.921875, learning_rate 0.000112496
2017-10-11T14:50:53.188090: step 1462, loss 0.0947151, acc 0.96875, learning_rate 0.000112445
2017-10-11T14:50:53.387171: step 1463, loss 0.107919, acc 0.96875, learning_rate 0.000112394
2017-10-11T14:50:53.572709: step 1464, loss 0.206634, acc 0.9375, learning_rate 0.000112344
2017-10-11T14:50:53.754147: step 1465, loss 0.164452, acc 0.921875, learning_rate 0.000112293
2017-10-11T14:50:53.950589: step 1466, loss 0.24297, acc 0.953125, learning_rate 0.000112243
2017-10-11T14:50:54.147011: step 1467, loss 0.133729, acc 0.953125, learning_rate 0.000112193
2017-10-11T14:50:54.332486: step 1468, loss 0.0268595, acc 1, learning_rate 0.000112144
2017-10-11T14:50:54.525108: step 1469, loss 0.161281, acc 0.9375, learning_rate 0.000112094
2017-10-11T14:50:54.678253: step 1470, loss 0.0675919, acc 0.980392, learning_rate 0.000112045
2017-10-11T14:50:54.875734: step 1471, loss 0.21981, acc 0.921875, learning_rate 0.000111995
2017-10-11T14:50:55.067696: step 1472, loss 0.0551056, acc 0.984375, learning_rate 0.000111946
2017-10-11T14:50:55.250670: step 1473, loss 0.115984, acc 0.953125, learning_rate 0.000111898
2017-10-11T14:50:55.450996: step 1474, loss 0.123328, acc 0.96875, learning_rate 0.000111849
2017-10-11T14:50:55.641656: step 1475, loss 0.085733, acc 1, learning_rate 0.000111801
2017-10-11T14:50:55.821765: step 1476, loss 0.100926, acc 0.96875, learning_rate 0.000111753
2017-10-11T14:50:56.007860: step 1477, loss 0.144648, acc 0.96875, learning_rate 0.000111705
2017-10-11T14:50:56.190053: step 1478, loss 0.18512, acc 0.90625, learning_rate 0.000111657
2017-10-11T14:50:56.373499: step 1479, loss 0.061562, acc 0.984375, learning_rate 0.000111609
2017-10-11T14:50:56.553010: step 1480, loss 0.177933, acc 0.921875, learning_rate 0.000111562

Evaluation:
2017-10-11T14:50:56.709467: step 1480, loss 0.231879, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1480

2017-10-11T14:50:57.803677: step 1481, loss 0.075621, acc 0.96875, learning_rate 0.000111515
2017-10-11T14:50:57.989714: step 1482, loss 0.152455, acc 0.921875, learning_rate 0.000111468
2017-10-11T14:50:58.175929: step 1483, loss 0.0886738, acc 0.953125, learning_rate 0.000111421
2017-10-11T14:50:58.452066: step 1484, loss 0.0768782, acc 0.96875, learning_rate 0.000111374
2017-10-11T14:50:58.646381: step 1485, loss 0.206534, acc 0.953125, learning_rate 0.000111328
2017-10-11T14:50:58.833325: step 1486, loss 0.172392, acc 0.9375, learning_rate 0.000111282
2017-10-11T14:50:59.010753: step 1487, loss 0.157093, acc 0.96875, learning_rate 0.000111236
2017-10-11T14:50:59.203334: step 1488, loss 0.0499299, acc 0.984375, learning_rate 0.00011119
2017-10-11T14:50:59.378933: step 1489, loss 0.141124, acc 0.953125, learning_rate 0.000111144
2017-10-11T14:50:59.561260: step 1490, loss 0.296943, acc 0.90625, learning_rate 0.000111099
2017-10-11T14:50:59.754879: step 1491, loss 0.127272, acc 0.96875, learning_rate 0.000111053
2017-10-11T14:50:59.939015: step 1492, loss 0.0575158, acc 0.96875, learning_rate 0.000111008
2017-10-11T14:51:00.129980: step 1493, loss 0.0471434, acc 1, learning_rate 0.000110963
2017-10-11T14:51:00.304288: step 1494, loss 0.2985, acc 0.90625, learning_rate 0.000110918
2017-10-11T14:51:00.498234: step 1495, loss 0.126207, acc 0.921875, learning_rate 0.000110874
2017-10-11T14:51:00.678311: step 1496, loss 0.0922797, acc 0.984375, learning_rate 0.00011083
2017-10-11T14:51:00.871443: step 1497, loss 0.0788148, acc 0.96875, learning_rate 0.000110785
2017-10-11T14:51:01.060864: step 1498, loss 0.0896175, acc 0.984375, learning_rate 0.000110741
2017-10-11T14:51:01.246894: step 1499, loss 0.054979, acc 0.984375, learning_rate 0.000110697
2017-10-11T14:51:01.435626: step 1500, loss 0.262884, acc 0.921875, learning_rate 0.000110654
2017-10-11T14:51:01.620198: step 1501, loss 0.103312, acc 0.953125, learning_rate 0.00011061
2017-10-11T14:51:01.809069: step 1502, loss 0.0548456, acc 0.984375, learning_rate 0.000110567
2017-10-11T14:51:01.998786: step 1503, loss 0.078666, acc 0.953125, learning_rate 0.000110524
2017-10-11T14:51:02.185490: step 1504, loss 0.115812, acc 0.96875, learning_rate 0.000110481
2017-10-11T14:51:02.361644: step 1505, loss 0.0810611, acc 0.96875, learning_rate 0.000110438
2017-10-11T14:51:02.552976: step 1506, loss 0.0906619, acc 0.953125, learning_rate 0.000110396
2017-10-11T14:51:02.739804: step 1507, loss 0.0551724, acc 0.984375, learning_rate 0.000110353
2017-10-11T14:51:02.913154: step 1508, loss 0.0650475, acc 0.96875, learning_rate 0.000110311
2017-10-11T14:51:03.096441: step 1509, loss 0.105924, acc 0.953125, learning_rate 0.000110269
2017-10-11T14:51:03.270811: step 1510, loss 0.12575, acc 0.96875, learning_rate 0.000110227
2017-10-11T14:51:03.584611: step 1511, loss 0.0814988, acc 0.984375, learning_rate 0.000110185
2017-10-11T14:51:03.770481: step 1512, loss 0.0731897, acc 0.953125, learning_rate 0.000110144
2017-10-11T14:51:03.933769: step 1513, loss 0.0394991, acc 1, learning_rate 0.000110102
2017-10-11T14:51:04.125573: step 1514, loss 0.211577, acc 0.890625, learning_rate 0.000110061
2017-10-11T14:51:04.302871: step 1515, loss 0.183086, acc 0.9375, learning_rate 0.00011002
2017-10-11T14:51:04.485748: step 1516, loss 0.17613, acc 0.90625, learning_rate 0.000109979
2017-10-11T14:51:04.665049: step 1517, loss 0.197491, acc 0.90625, learning_rate 0.000109938
2017-10-11T14:51:04.853546: step 1518, loss 0.154074, acc 0.953125, learning_rate 0.000109898
2017-10-11T14:51:05.034908: step 1519, loss 0.124344, acc 0.9375, learning_rate 0.000109857
2017-10-11T14:51:05.216232: step 1520, loss 0.182785, acc 0.921875, learning_rate 0.000109817

Evaluation:
2017-10-11T14:51:05.376089: step 1520, loss 0.232464, acc 0.910791

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1520

2017-10-11T14:51:06.975913: step 1521, loss 0.194188, acc 0.921875, learning_rate 0.000109777
2017-10-11T14:51:07.156647: step 1522, loss 0.0576838, acc 0.984375, learning_rate 0.000109737
2017-10-11T14:51:07.340012: step 1523, loss 0.0430077, acc 0.984375, learning_rate 0.000109697
2017-10-11T14:51:07.532585: step 1524, loss 0.246017, acc 0.921875, learning_rate 0.000109658
2017-10-11T14:51:07.705063: step 1525, loss 0.196307, acc 0.890625, learning_rate 0.000109618
2017-10-11T14:51:07.876926: step 1526, loss 0.0673731, acc 0.96875, learning_rate 0.000109579
2017-10-11T14:51:08.064317: step 1527, loss 0.208333, acc 0.90625, learning_rate 0.00010954
2017-10-11T14:51:08.244874: step 1528, loss 0.103447, acc 0.953125, learning_rate 0.000109501
2017-10-11T14:51:08.431213: step 1529, loss 0.0920787, acc 0.96875, learning_rate 0.000109462
2017-10-11T14:51:08.611140: step 1530, loss 0.24349, acc 0.921875, learning_rate 0.000109424
2017-10-11T14:51:08.805606: step 1531, loss 0.106802, acc 0.953125, learning_rate 0.000109385
2017-10-11T14:51:08.993295: step 1532, loss 0.0910245, acc 0.96875, learning_rate 0.000109347
2017-10-11T14:51:09.183307: step 1533, loss 0.0667612, acc 0.96875, learning_rate 0.000109309
2017-10-11T14:51:09.364417: step 1534, loss 0.142396, acc 0.984375, learning_rate 0.000109271
2017-10-11T14:51:09.541828: step 1535, loss 0.116259, acc 0.953125, learning_rate 0.000109233
2017-10-11T14:51:09.713978: step 1536, loss 0.156059, acc 0.921875, learning_rate 0.000109195
2017-10-11T14:51:09.891685: step 1537, loss 0.189721, acc 0.921875, learning_rate 0.000109158
2017-10-11T14:51:10.081245: step 1538, loss 0.0635074, acc 0.984375, learning_rate 0.00010912
2017-10-11T14:51:10.255169: step 1539, loss 0.100644, acc 0.9375, learning_rate 0.000109083
2017-10-11T14:51:10.444417: step 1540, loss 0.0525036, acc 0.984375, learning_rate 0.000109046
2017-10-11T14:51:10.621801: step 1541, loss 0.213306, acc 0.90625, learning_rate 0.000109009
2017-10-11T14:51:10.801635: step 1542, loss 0.179241, acc 0.921875, learning_rate 0.000108972
2017-10-11T14:51:10.990139: step 1543, loss 0.135932, acc 0.9375, learning_rate 0.000108936
2017-10-11T14:51:11.167206: step 1544, loss 0.0401901, acc 1, learning_rate 0.000108899
2017-10-11T14:51:11.362162: step 1545, loss 0.0788968, acc 0.984375, learning_rate 0.000108863
2017-10-11T14:51:11.557771: step 1546, loss 0.0999241, acc 0.96875, learning_rate 0.000108827
2017-10-11T14:51:11.743245: step 1547, loss 0.221622, acc 0.9375, learning_rate 0.000108791
2017-10-11T14:51:11.929633: step 1548, loss 0.121219, acc 0.9375, learning_rate 0.000108755
2017-10-11T14:51:12.105906: step 1549, loss 0.0745457, acc 0.984375, learning_rate 0.000108719
2017-10-11T14:51:12.287107: step 1550, loss 0.101454, acc 0.984375, learning_rate 0.000108683
2017-10-11T14:51:12.472271: step 1551, loss 0.218778, acc 0.9375, learning_rate 0.000108648
2017-10-11T14:51:12.657297: step 1552, loss 0.0666104, acc 0.96875, learning_rate 0.000108613
2017-10-11T14:51:12.835129: step 1553, loss 0.0694249, acc 0.984375, learning_rate 0.000108577
2017-10-11T14:51:13.024063: step 1554, loss 0.167032, acc 0.921875, learning_rate 0.000108542
2017-10-11T14:51:13.216199: step 1555, loss 0.120854, acc 0.953125, learning_rate 0.000108508
2017-10-11T14:51:13.406469: step 1556, loss 0.107862, acc 0.96875, learning_rate 0.000108473
2017-10-11T14:51:13.648338: step 1557, loss 0.0361312, acc 0.984375, learning_rate 0.000108438
2017-10-11T14:51:13.838172: step 1558, loss 0.0548347, acc 0.984375, learning_rate 0.000108404
2017-10-11T14:51:14.029358: step 1559, loss 0.192297, acc 0.90625, learning_rate 0.00010837
2017-10-11T14:51:14.220674: step 1560, loss 0.0816155, acc 0.96875, learning_rate 0.000108335

Evaluation:
2017-10-11T14:51:14.377673: step 1560, loss 0.230405, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1560

2017-10-11T14:51:15.473991: step 1561, loss 0.0349664, acc 1, learning_rate 0.000108301
2017-10-11T14:51:15.647345: step 1562, loss 0.106997, acc 0.953125, learning_rate 0.000108267
2017-10-11T14:51:15.835624: step 1563, loss 0.100912, acc 0.984375, learning_rate 0.000108234
2017-10-11T14:51:16.026363: step 1564, loss 0.339256, acc 0.90625, learning_rate 0.0001082
2017-10-11T14:51:16.199164: step 1565, loss 0.0422084, acc 1, learning_rate 0.000108167
2017-10-11T14:51:16.389507: step 1566, loss 0.0870563, acc 0.96875, learning_rate 0.000108133
2017-10-11T14:51:16.578888: step 1567, loss 0.153549, acc 0.953125, learning_rate 0.0001081
2017-10-11T14:51:16.739969: step 1568, loss 0.0750512, acc 0.980392, learning_rate 0.000108067
2017-10-11T14:51:16.940036: step 1569, loss 0.223679, acc 0.921875, learning_rate 0.000108034
2017-10-11T14:51:17.124366: step 1570, loss 0.101238, acc 0.96875, learning_rate 0.000108001
2017-10-11T14:51:17.308605: step 1571, loss 0.0954666, acc 0.96875, learning_rate 0.000107969
2017-10-11T14:51:17.491897: step 1572, loss 0.134102, acc 0.953125, learning_rate 0.000107936
2017-10-11T14:51:17.678462: step 1573, loss 0.173357, acc 0.921875, learning_rate 0.000107904
2017-10-11T14:51:17.867642: step 1574, loss 0.0382986, acc 1, learning_rate 0.000107871
2017-10-11T14:51:18.058598: step 1575, loss 0.154156, acc 0.921875, learning_rate 0.000107839
2017-10-11T14:51:18.234574: step 1576, loss 0.120171, acc 0.953125, learning_rate 0.000107807
2017-10-11T14:51:18.414918: step 1577, loss 0.24731, acc 0.875, learning_rate 0.000107775
2017-10-11T14:51:18.608402: step 1578, loss 0.177312, acc 0.9375, learning_rate 0.000107744
2017-10-11T14:51:18.792383: step 1579, loss 0.0707679, acc 0.984375, learning_rate 0.000107712
2017-10-11T14:51:18.982083: step 1580, loss 0.131276, acc 0.9375, learning_rate 0.000107681
2017-10-11T14:51:19.163262: step 1581, loss 0.075269, acc 0.953125, learning_rate 0.000107649
2017-10-11T14:51:19.353791: step 1582, loss 0.0870673, acc 0.96875, learning_rate 0.000107618
2017-10-11T14:51:19.548642: step 1583, loss 0.174526, acc 0.9375, learning_rate 0.000107587
2017-10-11T14:51:19.729327: step 1584, loss 0.0175494, acc 1, learning_rate 0.000107556
2017-10-11T14:51:19.919186: step 1585, loss 0.122739, acc 0.9375, learning_rate 0.000107525
2017-10-11T14:51:20.103495: step 1586, loss 0.134916, acc 0.9375, learning_rate 0.000107494
2017-10-11T14:51:20.287380: step 1587, loss 0.103095, acc 0.984375, learning_rate 0.000107464
2017-10-11T14:51:20.471377: step 1588, loss 0.0531622, acc 0.984375, learning_rate 0.000107433
2017-10-11T14:51:20.651963: step 1589, loss 0.168673, acc 0.9375, learning_rate 0.000107403
2017-10-11T14:51:20.831276: step 1590, loss 0.0546477, acc 0.984375, learning_rate 0.000107373
2017-10-11T14:51:21.009886: step 1591, loss 0.160893, acc 0.9375, learning_rate 0.000107343
2017-10-11T14:51:21.206393: step 1592, loss 0.180196, acc 0.9375, learning_rate 0.000107313
2017-10-11T14:51:21.380689: step 1593, loss 0.0700471, acc 0.96875, learning_rate 0.000107283
2017-10-11T14:51:21.571245: step 1594, loss 0.112342, acc 0.984375, learning_rate 0.000107253
2017-10-11T14:51:21.766164: step 1595, loss 0.094514, acc 0.984375, learning_rate 0.000107224
2017-10-11T14:51:21.947670: step 1596, loss 0.169681, acc 0.9375, learning_rate 0.000107194
2017-10-11T14:51:22.124899: step 1597, loss 0.120262, acc 0.9375, learning_rate 0.000107165
2017-10-11T14:51:22.314674: step 1598, loss 0.077515, acc 0.96875, learning_rate 0.000107136
2017-10-11T14:51:22.497678: step 1599, loss 0.118025, acc 0.921875, learning_rate 0.000107106
2017-10-11T14:51:22.681429: step 1600, loss 0.18359, acc 0.9375, learning_rate 0.000107077

Evaluation:
2017-10-11T14:51:22.840551: step 1600, loss 0.230775, acc 0.909353

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1600

2017-10-11T14:51:24.166786: step 1601, loss 0.208203, acc 0.9375, learning_rate 0.000107048
2017-10-11T14:51:24.357096: step 1602, loss 0.0746851, acc 0.984375, learning_rate 0.00010702
2017-10-11T14:51:24.539030: step 1603, loss 0.0736171, acc 0.984375, learning_rate 0.000106991
2017-10-11T14:51:24.727685: step 1604, loss 0.208546, acc 0.90625, learning_rate 0.000106963
2017-10-11T14:51:24.914065: step 1605, loss 0.161303, acc 0.921875, learning_rate 0.000106934
2017-10-11T14:51:25.086458: step 1606, loss 0.0779511, acc 0.984375, learning_rate 0.000106906
2017-10-11T14:51:25.273810: step 1607, loss 0.298974, acc 0.90625, learning_rate 0.000106878
2017-10-11T14:51:25.456848: step 1608, loss 0.151547, acc 0.96875, learning_rate 0.00010685
2017-10-11T14:51:25.634378: step 1609, loss 0.141501, acc 0.96875, learning_rate 0.000106822
2017-10-11T14:51:25.814529: step 1610, loss 0.105613, acc 0.96875, learning_rate 0.000106794
2017-10-11T14:51:26.004213: step 1611, loss 0.042142, acc 1, learning_rate 0.000106766
2017-10-11T14:51:26.180902: step 1612, loss 0.163136, acc 0.90625, learning_rate 0.000106738
2017-10-11T14:51:26.360580: step 1613, loss 0.112969, acc 0.953125, learning_rate 0.000106711
2017-10-11T14:51:26.526563: step 1614, loss 0.0605479, acc 0.984375, learning_rate 0.000106684
2017-10-11T14:51:26.703379: step 1615, loss 0.0671357, acc 0.984375, learning_rate 0.000106656
2017-10-11T14:51:26.882296: step 1616, loss 0.111286, acc 0.984375, learning_rate 0.000106629
2017-10-11T14:51:27.070660: step 1617, loss 0.0693892, acc 0.984375, learning_rate 0.000106602
2017-10-11T14:51:27.260048: step 1618, loss 0.191433, acc 0.921875, learning_rate 0.000106575
2017-10-11T14:51:27.439889: step 1619, loss 0.0251003, acc 1, learning_rate 0.000106548
2017-10-11T14:51:27.617172: step 1620, loss 0.128206, acc 0.953125, learning_rate 0.000106521
2017-10-11T14:51:27.797642: step 1621, loss 0.0566612, acc 0.984375, learning_rate 0.000106495
2017-10-11T14:51:27.975246: step 1622, loss 0.221956, acc 0.921875, learning_rate 0.000106468
2017-10-11T14:51:28.155317: step 1623, loss 0.184896, acc 0.96875, learning_rate 0.000106442
2017-10-11T14:51:28.340941: step 1624, loss 0.0771837, acc 0.96875, learning_rate 0.000106416
2017-10-11T14:51:28.531212: step 1625, loss 0.213165, acc 0.921875, learning_rate 0.000106389
2017-10-11T14:51:28.718962: step 1626, loss 0.0929174, acc 0.984375, learning_rate 0.000106363
2017-10-11T14:51:28.906651: step 1627, loss 0.141064, acc 0.953125, learning_rate 0.000106337
2017-10-11T14:51:29.093304: step 1628, loss 0.081119, acc 0.953125, learning_rate 0.000106312
2017-10-11T14:51:29.288204: step 1629, loss 0.164938, acc 0.921875, learning_rate 0.000106286
2017-10-11T14:51:29.471963: step 1630, loss 0.136238, acc 0.96875, learning_rate 0.00010626
2017-10-11T14:51:29.661389: step 1631, loss 0.172329, acc 0.9375, learning_rate 0.000106235
2017-10-11T14:51:29.842284: step 1632, loss 0.0680246, acc 0.984375, learning_rate 0.000106209
2017-10-11T14:51:30.030953: step 1633, loss 0.197489, acc 0.90625, learning_rate 0.000106184
2017-10-11T14:51:30.206307: step 1634, loss 0.0695083, acc 0.984375, learning_rate 0.000106159
2017-10-11T14:51:30.390614: step 1635, loss 0.0431156, acc 0.984375, learning_rate 0.000106133
2017-10-11T14:51:30.573337: step 1636, loss 0.0625076, acc 0.96875, learning_rate 0.000106108
2017-10-11T14:51:30.763482: step 1637, loss 0.164862, acc 0.9375, learning_rate 0.000106083
2017-10-11T14:51:30.955784: step 1638, loss 0.215656, acc 0.9375, learning_rate 0.000106059
2017-10-11T14:51:31.130546: step 1639, loss 0.169589, acc 0.953125, learning_rate 0.000106034
2017-10-11T14:51:31.313280: step 1640, loss 0.040574, acc 0.984375, learning_rate 0.000106009

Evaluation:
2017-10-11T14:51:31.480216: step 1640, loss 0.228847, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1640

2017-10-11T14:51:33.077773: step 1641, loss 0.112167, acc 0.96875, learning_rate 0.000105985
2017-10-11T14:51:33.247938: step 1642, loss 0.100321, acc 0.953125, learning_rate 0.00010596
2017-10-11T14:51:33.432670: step 1643, loss 0.216595, acc 0.90625, learning_rate 0.000105936
2017-10-11T14:51:33.611825: step 1644, loss 0.0972652, acc 0.953125, learning_rate 0.000105912
2017-10-11T14:51:33.798331: step 1645, loss 0.279845, acc 0.875, learning_rate 0.000105888
2017-10-11T14:51:33.979469: step 1646, loss 0.127805, acc 0.953125, learning_rate 0.000105864
2017-10-11T14:51:34.165968: step 1647, loss 0.0671277, acc 0.96875, learning_rate 0.00010584
2017-10-11T14:51:34.348940: step 1648, loss 0.207469, acc 0.9375, learning_rate 0.000105816
2017-10-11T14:51:34.532613: step 1649, loss 0.0842667, acc 0.953125, learning_rate 0.000105792
2017-10-11T14:51:34.714105: step 1650, loss 0.0893715, acc 0.984375, learning_rate 0.000105768
2017-10-11T14:51:34.907374: step 1651, loss 0.0430666, acc 0.984375, learning_rate 0.000105745
2017-10-11T14:51:35.099523: step 1652, loss 0.14135, acc 0.9375, learning_rate 0.000105721
2017-10-11T14:51:35.274029: step 1653, loss 0.113557, acc 0.96875, learning_rate 0.000105698
2017-10-11T14:51:35.448803: step 1654, loss 0.111611, acc 0.96875, learning_rate 0.000105675
2017-10-11T14:51:35.647459: step 1655, loss 0.126845, acc 0.953125, learning_rate 0.000105652
2017-10-11T14:51:35.832977: step 1656, loss 0.128427, acc 0.96875, learning_rate 0.000105629
2017-10-11T14:51:36.021807: step 1657, loss 0.0584337, acc 0.984375, learning_rate 0.000105606
2017-10-11T14:51:36.201385: step 1658, loss 0.0872512, acc 0.984375, learning_rate 0.000105583
2017-10-11T14:51:36.389397: step 1659, loss 0.197102, acc 0.953125, learning_rate 0.00010556
2017-10-11T14:51:36.576932: step 1660, loss 0.127755, acc 0.96875, learning_rate 0.000105537
2017-10-11T14:51:36.757559: step 1661, loss 0.0733027, acc 0.984375, learning_rate 0.000105515
2017-10-11T14:51:36.946603: step 1662, loss 0.0325938, acc 0.984375, learning_rate 0.000105492
2017-10-11T14:51:37.144822: step 1663, loss 0.0806028, acc 0.96875, learning_rate 0.00010547
2017-10-11T14:51:37.335143: step 1664, loss 0.100022, acc 0.953125, learning_rate 0.000105447
2017-10-11T14:51:37.516543: step 1665, loss 0.051341, acc 0.984375, learning_rate 0.000105425
2017-10-11T14:51:37.669589: step 1666, loss 0.15868, acc 0.960784, learning_rate 0.000105403
2017-10-11T14:51:37.854296: step 1667, loss 0.0607359, acc 0.984375, learning_rate 0.000105381
2017-10-11T14:51:38.047539: step 1668, loss 0.264271, acc 0.875, learning_rate 0.000105359
2017-10-11T14:51:38.226307: step 1669, loss 0.0993314, acc 0.96875, learning_rate 0.000105337
2017-10-11T14:51:38.413306: step 1670, loss 0.111308, acc 0.984375, learning_rate 0.000105315
2017-10-11T14:51:38.594453: step 1671, loss 0.168674, acc 0.921875, learning_rate 0.000105294
2017-10-11T14:51:38.791764: step 1672, loss 0.158871, acc 0.9375, learning_rate 0.000105272
2017-10-11T14:51:38.980633: step 1673, loss 0.12509, acc 0.96875, learning_rate 0.000105251
2017-10-11T14:51:39.159144: step 1674, loss 0.151265, acc 0.9375, learning_rate 0.000105229
2017-10-11T14:51:39.339057: step 1675, loss 0.065116, acc 0.984375, learning_rate 0.000105208
2017-10-11T14:51:39.517065: step 1676, loss 0.121637, acc 0.96875, learning_rate 0.000105186
2017-10-11T14:51:39.705899: step 1677, loss 0.0457299, acc 1, learning_rate 0.000105165
2017-10-11T14:51:39.890315: step 1678, loss 0.0570671, acc 0.984375, learning_rate 0.000105144
2017-10-11T14:51:40.067839: step 1679, loss 0.0685752, acc 0.96875, learning_rate 0.000105123
2017-10-11T14:51:40.255197: step 1680, loss 0.127843, acc 0.96875, learning_rate 0.000105102

Evaluation:
2017-10-11T14:51:40.409056: step 1680, loss 0.230469, acc 0.909353

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1680

2017-10-11T14:51:41.511797: step 1681, loss 0.134516, acc 0.921875, learning_rate 0.000105081
2017-10-11T14:51:41.697697: step 1682, loss 0.0365759, acc 1, learning_rate 0.000105061
2017-10-11T14:51:41.886535: step 1683, loss 0.140323, acc 0.953125, learning_rate 0.00010504
2017-10-11T14:51:42.069394: step 1684, loss 0.107726, acc 0.953125, learning_rate 0.00010502
2017-10-11T14:51:42.252716: step 1685, loss 0.0763099, acc 0.984375, learning_rate 0.000104999
2017-10-11T14:51:42.425820: step 1686, loss 0.0998714, acc 0.96875, learning_rate 0.000104979
2017-10-11T14:51:42.611309: step 1687, loss 0.140578, acc 0.9375, learning_rate 0.000104958
2017-10-11T14:51:42.794864: step 1688, loss 0.0558181, acc 0.984375, learning_rate 0.000104938
2017-10-11T14:51:42.976714: step 1689, loss 0.141508, acc 0.96875, learning_rate 0.000104918
2017-10-11T14:51:43.164469: step 1690, loss 0.0905523, acc 0.96875, learning_rate 0.000104898
2017-10-11T14:51:43.348535: step 1691, loss 0.0460066, acc 0.96875, learning_rate 0.000104878
2017-10-11T14:51:43.520511: step 1692, loss 0.150146, acc 0.953125, learning_rate 0.000104858
2017-10-11T14:51:43.703468: step 1693, loss 0.280235, acc 0.953125, learning_rate 0.000104838
2017-10-11T14:51:43.880241: step 1694, loss 0.047331, acc 1, learning_rate 0.000104818
2017-10-11T14:51:44.068975: step 1695, loss 0.0834765, acc 0.953125, learning_rate 0.000104799
2017-10-11T14:51:44.251358: step 1696, loss 0.0794651, acc 0.984375, learning_rate 0.000104779
2017-10-11T14:51:44.434538: step 1697, loss 0.107488, acc 0.96875, learning_rate 0.00010476
2017-10-11T14:51:44.615782: step 1698, loss 0.0595277, acc 0.984375, learning_rate 0.00010474
2017-10-11T14:51:44.800812: step 1699, loss 0.0202794, acc 1, learning_rate 0.000104721
2017-10-11T14:51:44.978067: step 1700, loss 0.0771739, acc 0.96875, learning_rate 0.000104702
2017-10-11T14:51:45.155118: step 1701, loss 0.112077, acc 0.96875, learning_rate 0.000104682
2017-10-11T14:51:45.336977: step 1702, loss 0.174514, acc 0.96875, learning_rate 0.000104663
2017-10-11T14:51:45.511607: step 1703, loss 0.0568573, acc 0.984375, learning_rate 0.000104644
2017-10-11T14:51:45.685681: step 1704, loss 0.126969, acc 0.9375, learning_rate 0.000104625
2017-10-11T14:51:45.864549: step 1705, loss 0.250602, acc 0.875, learning_rate 0.000104606
2017-10-11T14:51:46.044058: step 1706, loss 0.13885, acc 0.96875, learning_rate 0.000104588
2017-10-11T14:51:46.208720: step 1707, loss 0.17997, acc 0.921875, learning_rate 0.000104569
2017-10-11T14:51:46.387479: step 1708, loss 0.081847, acc 0.96875, learning_rate 0.00010455
2017-10-11T14:51:46.568578: step 1709, loss 0.0293148, acc 1, learning_rate 0.000104532
2017-10-11T14:51:46.750045: step 1710, loss 0.126825, acc 0.921875, learning_rate 0.000104513
2017-10-11T14:51:46.927624: step 1711, loss 0.115075, acc 0.9375, learning_rate 0.000104495
2017-10-11T14:51:47.112187: step 1712, loss 0.0734242, acc 0.96875, learning_rate 0.000104476
2017-10-11T14:51:47.301600: step 1713, loss 0.0976358, acc 0.953125, learning_rate 0.000104458
2017-10-11T14:51:47.480871: step 1714, loss 0.0829531, acc 0.96875, learning_rate 0.00010444
2017-10-11T14:51:47.675962: step 1715, loss 0.183574, acc 0.921875, learning_rate 0.000104422
2017-10-11T14:51:47.869955: step 1716, loss 0.055141, acc 0.984375, learning_rate 0.000104404
2017-10-11T14:51:48.066765: step 1717, loss 0.143676, acc 0.984375, learning_rate 0.000104386
2017-10-11T14:51:48.241107: step 1718, loss 0.0905745, acc 0.984375, learning_rate 0.000104368
2017-10-11T14:51:48.420087: step 1719, loss 0.142327, acc 0.96875, learning_rate 0.00010435
2017-10-11T14:51:48.607366: step 1720, loss 0.105434, acc 0.953125, learning_rate 0.000104332

Evaluation:
2017-10-11T14:51:48.770764: step 1720, loss 0.233903, acc 0.910791

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1720

2017-10-11T14:51:50.144855: step 1721, loss 0.259546, acc 0.90625, learning_rate 0.000104315
2017-10-11T14:51:50.332921: step 1722, loss 0.100331, acc 0.953125, learning_rate 0.000104297
2017-10-11T14:51:50.519095: step 1723, loss 0.226672, acc 0.90625, learning_rate 0.000104279
2017-10-11T14:51:50.710004: step 1724, loss 0.0751198, acc 0.984375, learning_rate 0.000104262
2017-10-11T14:51:50.889961: step 1725, loss 0.152231, acc 0.9375, learning_rate 0.000104245
2017-10-11T14:51:51.075128: step 1726, loss 0.0325136, acc 0.984375, learning_rate 0.000104227
2017-10-11T14:51:51.251222: step 1727, loss 0.0813353, acc 0.96875, learning_rate 0.00010421
2017-10-11T14:51:51.441877: step 1728, loss 0.193695, acc 0.90625, learning_rate 0.000104193
2017-10-11T14:51:51.624865: step 1729, loss 0.208883, acc 0.90625, learning_rate 0.000104176
2017-10-11T14:51:51.812170: step 1730, loss 0.266139, acc 0.921875, learning_rate 0.000104159
2017-10-11T14:51:51.995577: step 1731, loss 0.0953822, acc 0.96875, learning_rate 0.000104142
2017-10-11T14:51:52.189691: step 1732, loss 0.169188, acc 0.953125, learning_rate 0.000104125
2017-10-11T14:51:52.373197: step 1733, loss 0.039726, acc 0.984375, learning_rate 0.000104108
2017-10-11T14:51:52.557799: step 1734, loss 0.175573, acc 0.921875, learning_rate 0.000104091
2017-10-11T14:51:52.746270: step 1735, loss 0.209376, acc 0.9375, learning_rate 0.000104074
2017-10-11T14:51:52.936738: step 1736, loss 0.171178, acc 0.953125, learning_rate 0.000104058
2017-10-11T14:51:53.115914: step 1737, loss 0.066161, acc 0.984375, learning_rate 0.000104041
2017-10-11T14:51:53.295241: step 1738, loss 0.174467, acc 0.9375, learning_rate 0.000104025
2017-10-11T14:51:53.485105: step 1739, loss 0.237639, acc 0.921875, learning_rate 0.000104008
2017-10-11T14:51:53.673228: step 1740, loss 0.181486, acc 0.96875, learning_rate 0.000103992
2017-10-11T14:51:53.857323: step 1741, loss 0.0210235, acc 0.984375, learning_rate 0.000103976
2017-10-11T14:51:54.046544: step 1742, loss 0.0836361, acc 0.96875, learning_rate 0.000103959
2017-10-11T14:51:54.228458: step 1743, loss 0.128614, acc 0.953125, learning_rate 0.000103943
2017-10-11T14:51:54.410448: step 1744, loss 0.135779, acc 0.953125, learning_rate 0.000103927
2017-10-11T14:51:54.583804: step 1745, loss 0.116465, acc 0.96875, learning_rate 0.000103911
2017-10-11T14:51:54.774186: step 1746, loss 0.129289, acc 0.953125, learning_rate 0.000103895
2017-10-11T14:51:54.963284: step 1747, loss 0.0666779, acc 0.984375, learning_rate 0.000103879
2017-10-11T14:51:55.148334: step 1748, loss 0.124141, acc 0.9375, learning_rate 0.000103863
2017-10-11T14:51:55.335136: step 1749, loss 0.0533317, acc 0.984375, learning_rate 0.000103848
2017-10-11T14:51:55.526791: step 1750, loss 0.148833, acc 0.9375, learning_rate 0.000103832
2017-10-11T14:51:55.708987: step 1751, loss 0.135188, acc 0.9375, learning_rate 0.000103816
2017-10-11T14:51:55.899381: step 1752, loss 0.105283, acc 0.953125, learning_rate 0.000103801
2017-10-11T14:51:56.084098: step 1753, loss 0.206959, acc 0.921875, learning_rate 0.000103785
2017-10-11T14:51:56.268996: step 1754, loss 0.16493, acc 0.9375, learning_rate 0.00010377
2017-10-11T14:51:56.455326: step 1755, loss 0.159327, acc 0.953125, learning_rate 0.000103754
2017-10-11T14:51:56.628260: step 1756, loss 0.0649425, acc 0.96875, learning_rate 0.000103739
2017-10-11T14:51:56.810836: step 1757, loss 0.10602, acc 0.9375, learning_rate 0.000103724
2017-10-11T14:51:57.003003: step 1758, loss 0.0762743, acc 0.96875, learning_rate 0.000103709
2017-10-11T14:51:57.180413: step 1759, loss 0.129931, acc 0.953125, learning_rate 0.000103694
2017-10-11T14:51:57.361367: step 1760, loss 0.0490819, acc 1, learning_rate 0.000103678

Evaluation:
2017-10-11T14:51:57.523980: step 1760, loss 0.228092, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1760

2017-10-11T14:51:59.100567: step 1761, loss 0.145901, acc 0.9375, learning_rate 0.000103663
2017-10-11T14:51:59.290361: step 1762, loss 0.0276099, acc 1, learning_rate 0.000103648
2017-10-11T14:51:59.473219: step 1763, loss 0.175204, acc 0.9375, learning_rate 0.000103634
2017-10-11T14:51:59.630817: step 1764, loss 0.121652, acc 0.960784, learning_rate 0.000103619
2017-10-11T14:51:59.818959: step 1765, loss 0.176175, acc 0.953125, learning_rate 0.000103604
2017-10-11T14:52:00.012804: step 1766, loss 0.131608, acc 0.9375, learning_rate 0.000103589
2017-10-11T14:52:00.202656: step 1767, loss 0.0804503, acc 0.96875, learning_rate 0.000103575
2017-10-11T14:52:00.382233: step 1768, loss 0.0301039, acc 0.984375, learning_rate 0.00010356
2017-10-11T14:52:00.560087: step 1769, loss 0.157039, acc 0.953125, learning_rate 0.000103545
2017-10-11T14:52:00.745267: step 1770, loss 0.101845, acc 0.953125, learning_rate 0.000103531
2017-10-11T14:52:00.933188: step 1771, loss 0.148571, acc 0.953125, learning_rate 0.000103517
2017-10-11T14:52:01.123205: step 1772, loss 0.0863223, acc 0.984375, learning_rate 0.000103502
2017-10-11T14:52:01.310546: step 1773, loss 0.0526065, acc 0.984375, learning_rate 0.000103488
2017-10-11T14:52:01.491247: step 1774, loss 0.0645677, acc 0.96875, learning_rate 0.000103474
2017-10-11T14:52:01.677745: step 1775, loss 0.0818643, acc 0.984375, learning_rate 0.00010346
2017-10-11T14:52:01.859130: step 1776, loss 0.127381, acc 0.96875, learning_rate 0.000103445
2017-10-11T14:52:02.039651: step 1777, loss 0.0266784, acc 1, learning_rate 0.000103431
2017-10-11T14:52:02.224590: step 1778, loss 0.352143, acc 0.90625, learning_rate 0.000103417
2017-10-11T14:52:02.411111: step 1779, loss 0.0756909, acc 0.9375, learning_rate 0.000103403
2017-10-11T14:52:02.599908: step 1780, loss 0.124315, acc 0.96875, learning_rate 0.00010339
2017-10-11T14:52:02.787555: step 1781, loss 0.0916053, acc 0.96875, learning_rate 0.000103376
2017-10-11T14:52:02.974799: step 1782, loss 0.0460144, acc 0.984375, learning_rate 0.000103362
2017-10-11T14:52:03.158243: step 1783, loss 0.108572, acc 0.9375, learning_rate 0.000103348
2017-10-11T14:52:03.327844: step 1784, loss 0.340436, acc 0.9375, learning_rate 0.000103335
2017-10-11T14:52:03.517229: step 1785, loss 0.052414, acc 1, learning_rate 0.000103321
2017-10-11T14:52:03.701768: step 1786, loss 0.135493, acc 0.953125, learning_rate 0.000103307
2017-10-11T14:52:03.888667: step 1787, loss 0.058202, acc 0.984375, learning_rate 0.000103294
2017-10-11T14:52:04.078763: step 1788, loss 0.119492, acc 0.921875, learning_rate 0.00010328
2017-10-11T14:52:04.260911: step 1789, loss 0.0878613, acc 0.953125, learning_rate 0.000103267
2017-10-11T14:52:04.443624: step 1790, loss 0.10209, acc 0.953125, learning_rate 0.000103254
2017-10-11T14:52:04.629411: step 1791, loss 0.190719, acc 0.9375, learning_rate 0.00010324
2017-10-11T14:52:04.815345: step 1792, loss 0.124782, acc 0.953125, learning_rate 0.000103227
2017-10-11T14:52:04.998170: step 1793, loss 0.0628854, acc 0.96875, learning_rate 0.000103214
2017-10-11T14:52:05.187720: step 1794, loss 0.113701, acc 0.921875, learning_rate 0.000103201
2017-10-11T14:52:05.381506: step 1795, loss 0.14271, acc 0.9375, learning_rate 0.000103188
2017-10-11T14:52:05.565099: step 1796, loss 0.0971381, acc 0.96875, learning_rate 0.000103175
2017-10-11T14:52:05.772526: step 1797, loss 0.115743, acc 0.953125, learning_rate 0.000103162
2017-10-11T14:52:05.961330: step 1798, loss 0.239166, acc 0.921875, learning_rate 0.000103149
2017-10-11T14:52:06.155916: step 1799, loss 0.199611, acc 0.9375, learning_rate 0.000103136
2017-10-11T14:52:06.355503: step 1800, loss 0.0408319, acc 0.984375, learning_rate 0.000103123

Evaluation:
2017-10-11T14:52:06.513708: step 1800, loss 0.231449, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1800

2017-10-11T14:52:07.624793: step 1801, loss 0.121302, acc 0.953125, learning_rate 0.000103111
2017-10-11T14:52:07.813108: step 1802, loss 0.203895, acc 0.9375, learning_rate 0.000103098
2017-10-11T14:52:08.002568: step 1803, loss 0.0491604, acc 1, learning_rate 0.000103085
2017-10-11T14:52:08.193214: step 1804, loss 0.132528, acc 0.953125, learning_rate 0.000103073
2017-10-11T14:52:08.376926: step 1805, loss 0.110814, acc 0.921875, learning_rate 0.00010306
2017-10-11T14:52:08.563765: step 1806, loss 0.047011, acc 1, learning_rate 0.000103048
2017-10-11T14:52:08.750956: step 1807, loss 0.134459, acc 0.96875, learning_rate 0.000103035
2017-10-11T14:52:08.936164: step 1808, loss 0.152301, acc 0.921875, learning_rate 0.000103023
2017-10-11T14:52:09.126994: step 1809, loss 0.0420275, acc 0.984375, learning_rate 0.00010301
2017-10-11T14:52:09.298622: step 1810, loss 0.163587, acc 0.90625, learning_rate 0.000102998
2017-10-11T14:52:09.475240: step 1811, loss 0.0353246, acc 1, learning_rate 0.000102986
2017-10-11T14:52:09.667534: step 1812, loss 0.187165, acc 0.890625, learning_rate 0.000102974
2017-10-11T14:52:09.847816: step 1813, loss 0.142538, acc 0.921875, learning_rate 0.000102962
2017-10-11T14:52:10.024552: step 1814, loss 0.0998882, acc 0.984375, learning_rate 0.000102949
2017-10-11T14:52:10.211335: step 1815, loss 0.0802764, acc 0.953125, learning_rate 0.000102937
2017-10-11T14:52:10.397335: step 1816, loss 0.0771769, acc 0.96875, learning_rate 0.000102925
2017-10-11T14:52:10.584176: step 1817, loss 0.096157, acc 0.953125, learning_rate 0.000102913
2017-10-11T14:52:10.759210: step 1818, loss 0.20022, acc 0.9375, learning_rate 0.000102902
2017-10-11T14:52:10.935157: step 1819, loss 0.145703, acc 0.953125, learning_rate 0.00010289
2017-10-11T14:52:11.108465: step 1820, loss 0.0714125, acc 0.96875, learning_rate 0.000102878
2017-10-11T14:52:11.294210: step 1821, loss 0.0879844, acc 0.953125, learning_rate 0.000102866
2017-10-11T14:52:11.481702: step 1822, loss 0.137835, acc 0.9375, learning_rate 0.000102855
2017-10-11T14:52:11.662904: step 1823, loss 0.136308, acc 0.953125, learning_rate 0.000102843
2017-10-11T14:52:11.852380: step 1824, loss 0.152553, acc 0.9375, learning_rate 0.000102831
2017-10-11T14:52:12.021894: step 1825, loss 0.130371, acc 0.984375, learning_rate 0.00010282
2017-10-11T14:52:12.204960: step 1826, loss 0.152545, acc 0.9375, learning_rate 0.000102808
2017-10-11T14:52:12.383200: step 1827, loss 0.132021, acc 0.953125, learning_rate 0.000102797
2017-10-11T14:52:12.554888: step 1828, loss 0.0844318, acc 0.953125, learning_rate 0.000102785
2017-10-11T14:52:12.744115: step 1829, loss 0.064984, acc 0.984375, learning_rate 0.000102774
2017-10-11T14:52:12.933903: step 1830, loss 0.204512, acc 0.9375, learning_rate 0.000102763
2017-10-11T14:52:13.127101: step 1831, loss 0.0714056, acc 0.96875, learning_rate 0.000102751
2017-10-11T14:52:13.322401: step 1832, loss 0.0695101, acc 0.96875, learning_rate 0.00010274
2017-10-11T14:52:13.512415: step 1833, loss 0.144506, acc 0.953125, learning_rate 0.000102729
2017-10-11T14:52:13.714696: step 1834, loss 0.0550215, acc 0.96875, learning_rate 0.000102718
2017-10-11T14:52:13.906221: step 1835, loss 0.073938, acc 0.984375, learning_rate 0.000102707
2017-10-11T14:52:14.105862: step 1836, loss 0.180214, acc 0.953125, learning_rate 0.000102696
2017-10-11T14:52:14.309697: step 1837, loss 0.14789, acc 0.953125, learning_rate 0.000102685
2017-10-11T14:52:14.519569: step 1838, loss 0.241696, acc 0.953125, learning_rate 0.000102674
2017-10-11T14:52:14.738502: step 1839, loss 0.0948068, acc 0.984375, learning_rate 0.000102663
2017-10-11T14:52:14.957748: step 1840, loss 0.122781, acc 0.984375, learning_rate 0.000102652

Evaluation:
2017-10-11T14:52:15.153268: step 1840, loss 0.230138, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1840

2017-10-11T14:52:17.588829: step 1841, loss 0.117671, acc 0.96875, learning_rate 0.000102641
2017-10-11T14:52:17.796333: step 1842, loss 0.0661824, acc 0.984375, learning_rate 0.00010263
2017-10-11T14:52:18.033029: step 1843, loss 0.0536344, acc 0.984375, learning_rate 0.00010262
2017-10-11T14:52:18.253176: step 1844, loss 0.0859199, acc 0.984375, learning_rate 0.000102609
2017-10-11T14:52:18.469041: step 1845, loss 0.135971, acc 0.953125, learning_rate 0.000102598
2017-10-11T14:52:18.682580: step 1846, loss 0.179177, acc 0.9375, learning_rate 0.000102588
2017-10-11T14:52:18.903694: step 1847, loss 0.156627, acc 0.9375, learning_rate 0.000102577
2017-10-11T14:52:19.118565: step 1848, loss 0.0294575, acc 1, learning_rate 0.000102567
2017-10-11T14:52:19.327637: step 1849, loss 0.0952941, acc 0.984375, learning_rate 0.000102556
2017-10-11T14:52:19.554426: step 1850, loss 0.0883722, acc 0.96875, learning_rate 0.000102546
2017-10-11T14:52:19.784064: step 1851, loss 0.0830974, acc 0.96875, learning_rate 0.000102535
2017-10-11T14:52:19.999275: step 1852, loss 0.0746408, acc 0.984375, learning_rate 0.000102525
2017-10-11T14:52:20.218514: step 1853, loss 0.0913701, acc 0.984375, learning_rate 0.000102515
2017-10-11T14:52:20.444070: step 1854, loss 0.109704, acc 0.96875, learning_rate 0.000102504
2017-10-11T14:52:20.660439: step 1855, loss 0.107713, acc 0.953125, learning_rate 0.000102494
2017-10-11T14:52:20.887059: step 1856, loss 0.0510488, acc 0.984375, learning_rate 0.000102484
2017-10-11T14:52:21.119781: step 1857, loss 0.130182, acc 0.921875, learning_rate 0.000102474
2017-10-11T14:52:21.349068: step 1858, loss 0.0612309, acc 0.984375, learning_rate 0.000102464
2017-10-11T14:52:21.552591: step 1859, loss 0.0414283, acc 0.984375, learning_rate 0.000102454
2017-10-11T14:52:21.785099: step 1860, loss 0.0862492, acc 0.984375, learning_rate 0.000102444
2017-10-11T14:52:21.974126: step 1861, loss 0.0514724, acc 0.96875, learning_rate 0.000102434
2017-10-11T14:52:22.138312: step 1862, loss 0.182413, acc 0.901961, learning_rate 0.000102424
2017-10-11T14:52:22.328355: step 1863, loss 0.067903, acc 0.96875, learning_rate 0.000102414
2017-10-11T14:52:22.516442: step 1864, loss 0.238644, acc 0.921875, learning_rate 0.000102404
2017-10-11T14:52:22.698827: step 1865, loss 0.0961534, acc 0.96875, learning_rate 0.000102394
2017-10-11T14:52:22.883389: step 1866, loss 0.0284977, acc 1, learning_rate 0.000102384
2017-10-11T14:52:23.057880: step 1867, loss 0.123087, acc 0.953125, learning_rate 0.000102375
2017-10-11T14:52:23.244826: step 1868, loss 0.0871888, acc 0.96875, learning_rate 0.000102365
2017-10-11T14:52:23.433601: step 1869, loss 0.178195, acc 0.90625, learning_rate 0.000102355
2017-10-11T14:52:23.653619: step 1870, loss 0.0179999, acc 1, learning_rate 0.000102346
2017-10-11T14:52:23.887994: step 1871, loss 0.202895, acc 0.96875, learning_rate 0.000102336
2017-10-11T14:52:24.128499: step 1872, loss 0.122765, acc 0.9375, learning_rate 0.000102327
2017-10-11T14:52:24.347484: step 1873, loss 0.142651, acc 0.96875, learning_rate 0.000102317
2017-10-11T14:52:24.574121: step 1874, loss 0.258126, acc 0.890625, learning_rate 0.000102308
2017-10-11T14:52:24.793704: step 1875, loss 0.110154, acc 0.96875, learning_rate 0.000102298
2017-10-11T14:52:25.004305: step 1876, loss 0.185485, acc 0.953125, learning_rate 0.000102289
2017-10-11T14:52:25.220257: step 1877, loss 0.0796789, acc 0.953125, learning_rate 0.000102279
2017-10-11T14:52:25.446108: step 1878, loss 0.155133, acc 0.921875, learning_rate 0.00010227
2017-10-11T14:52:25.677086: step 1879, loss 0.0707901, acc 0.984375, learning_rate 0.000102261
2017-10-11T14:52:25.901159: step 1880, loss 0.0851402, acc 0.984375, learning_rate 0.000102252

Evaluation:
2017-10-11T14:52:26.125249: step 1880, loss 0.231882, acc 0.910791

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1880

2017-10-11T14:52:27.718930: step 1881, loss 0.114915, acc 0.96875, learning_rate 0.000102242
2017-10-11T14:52:27.947856: step 1882, loss 0.197918, acc 0.921875, learning_rate 0.000102233
2017-10-11T14:52:28.186748: step 1883, loss 0.114639, acc 0.96875, learning_rate 0.000102224
2017-10-11T14:52:28.426056: step 1884, loss 0.126596, acc 0.953125, learning_rate 0.000102215
2017-10-11T14:52:28.672663: step 1885, loss 0.0779082, acc 0.984375, learning_rate 0.000102206
2017-10-11T14:52:28.895150: step 1886, loss 0.0593341, acc 0.984375, learning_rate 0.000102197
2017-10-11T14:52:29.128459: step 1887, loss 0.125303, acc 0.9375, learning_rate 0.000102188
2017-10-11T14:52:29.351028: step 1888, loss 0.212613, acc 0.921875, learning_rate 0.000102179
2017-10-11T14:52:29.569623: step 1889, loss 0.129265, acc 0.96875, learning_rate 0.00010217
2017-10-11T14:52:29.809249: step 1890, loss 0.148887, acc 0.9375, learning_rate 0.000102161
2017-10-11T14:52:30.045916: step 1891, loss 0.0418069, acc 1, learning_rate 0.000102153
2017-10-11T14:52:30.258130: step 1892, loss 0.109997, acc 0.953125, learning_rate 0.000102144
2017-10-11T14:52:30.480236: step 1893, loss 0.134534, acc 0.96875, learning_rate 0.000102135
2017-10-11T14:52:30.713440: step 1894, loss 0.0691242, acc 0.96875, learning_rate 0.000102126
2017-10-11T14:52:30.953597: step 1895, loss 0.0393943, acc 0.984375, learning_rate 0.000102118
2017-10-11T14:52:31.193890: step 1896, loss 0.070346, acc 0.984375, learning_rate 0.000102109
2017-10-11T14:52:31.436204: step 1897, loss 0.134654, acc 0.96875, learning_rate 0.0001021
2017-10-11T14:52:31.662989: step 1898, loss 0.178879, acc 0.9375, learning_rate 0.000102092
2017-10-11T14:52:31.887666: step 1899, loss 0.197374, acc 0.953125, learning_rate 0.000102083
2017-10-11T14:52:32.103015: step 1900, loss 0.0994841, acc 0.96875, learning_rate 0.000102075
2017-10-11T14:52:32.288095: step 1901, loss 0.0990763, acc 0.953125, learning_rate 0.000102066
2017-10-11T14:52:32.473188: step 1902, loss 0.0966615, acc 0.953125, learning_rate 0.000102058
2017-10-11T14:52:32.650599: step 1903, loss 0.101977, acc 0.953125, learning_rate 0.00010205
2017-10-11T14:52:32.830321: step 1904, loss 0.0969718, acc 0.953125, learning_rate 0.000102041
2017-10-11T14:52:33.017638: step 1905, loss 0.154692, acc 0.90625, learning_rate 0.000102033
2017-10-11T14:52:33.199884: step 1906, loss 0.0398355, acc 0.984375, learning_rate 0.000102025
2017-10-11T14:52:33.376288: step 1907, loss 0.0455909, acc 0.984375, learning_rate 0.000102016
2017-10-11T14:52:33.572543: step 1908, loss 0.090965, acc 0.9375, learning_rate 0.000102008
2017-10-11T14:52:33.760895: step 1909, loss 0.0372389, acc 1, learning_rate 0.000102
2017-10-11T14:52:33.939412: step 1910, loss 0.0625945, acc 0.96875, learning_rate 0.000101992
2017-10-11T14:52:34.130184: step 1911, loss 0.138174, acc 0.96875, learning_rate 0.000101984
2017-10-11T14:52:34.308709: step 1912, loss 0.183153, acc 0.9375, learning_rate 0.000101975
2017-10-11T14:52:34.530828: step 1913, loss 0.120642, acc 0.96875, learning_rate 0.000101967
2017-10-11T14:52:34.749544: step 1914, loss 0.0550414, acc 0.984375, learning_rate 0.000101959
2017-10-11T14:52:34.968833: step 1915, loss 0.100878, acc 0.953125, learning_rate 0.000101951
2017-10-11T14:52:35.219137: step 1916, loss 0.119532, acc 0.96875, learning_rate 0.000101943
2017-10-11T14:52:35.449337: step 1917, loss 0.0756892, acc 0.984375, learning_rate 0.000101935
2017-10-11T14:52:35.688343: step 1918, loss 0.0768826, acc 0.953125, learning_rate 0.000101928
2017-10-11T14:52:35.918941: step 1919, loss 0.133183, acc 0.96875, learning_rate 0.00010192
2017-10-11T14:52:36.144939: step 1920, loss 0.229866, acc 0.890625, learning_rate 0.000101912

Evaluation:
2017-10-11T14:52:36.335660: step 1920, loss 0.230499, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1920

2017-10-11T14:52:38.539696: step 1921, loss 0.04256, acc 0.984375, learning_rate 0.000101904
2017-10-11T14:52:38.772971: step 1922, loss 0.163159, acc 0.90625, learning_rate 0.000101896
2017-10-11T14:52:39.013710: step 1923, loss 0.214078, acc 0.875, learning_rate 0.000101889
2017-10-11T14:52:39.233132: step 1924, loss 0.0762558, acc 1, learning_rate 0.000101881
2017-10-11T14:52:39.498548: step 1925, loss 0.0787025, acc 0.96875, learning_rate 0.000101873
2017-10-11T14:52:39.728913: step 1926, loss 0.0737098, acc 0.984375, learning_rate 0.000101865
2017-10-11T14:52:40.009319: step 1927, loss 0.137766, acc 0.96875, learning_rate 0.000101858
2017-10-11T14:52:40.233069: step 1928, loss 0.102521, acc 0.953125, learning_rate 0.00010185
2017-10-11T14:52:40.465224: step 1929, loss 0.0361331, acc 1, learning_rate 0.000101843
2017-10-11T14:52:40.688255: step 1930, loss 0.0319556, acc 0.984375, learning_rate 0.000101835
2017-10-11T14:52:40.904519: step 1931, loss 0.0907818, acc 0.9375, learning_rate 0.000101828
2017-10-11T14:52:41.158774: step 1932, loss 0.0948882, acc 0.96875, learning_rate 0.00010182
2017-10-11T14:52:41.398843: step 1933, loss 0.204997, acc 0.921875, learning_rate 0.000101813
2017-10-11T14:52:41.672290: step 1934, loss 0.0500206, acc 0.984375, learning_rate 0.000101805
2017-10-11T14:52:41.936707: step 1935, loss 0.163552, acc 0.9375, learning_rate 0.000101798
2017-10-11T14:52:42.204090: step 1936, loss 0.0403519, acc 1, learning_rate 0.000101791
2017-10-11T14:52:42.474631: step 1937, loss 0.0826641, acc 0.984375, learning_rate 0.000101783
2017-10-11T14:52:42.735775: step 1938, loss 0.0894022, acc 0.96875, learning_rate 0.000101776
2017-10-11T14:52:42.982223: step 1939, loss 0.06054, acc 0.984375, learning_rate 0.000101769
2017-10-11T14:52:43.235536: step 1940, loss 0.157488, acc 0.9375, learning_rate 0.000101762
2017-10-11T14:52:43.503740: step 1941, loss 0.0993582, acc 0.96875, learning_rate 0.000101754
2017-10-11T14:52:43.720676: step 1942, loss 0.0916735, acc 0.96875, learning_rate 0.000101747
2017-10-11T14:52:43.932271: step 1943, loss 0.0465288, acc 0.984375, learning_rate 0.00010174
2017-10-11T14:52:44.121898: step 1944, loss 0.131859, acc 0.953125, learning_rate 0.000101733
2017-10-11T14:52:44.344389: step 1945, loss 0.113042, acc 0.96875, learning_rate 0.000101726
2017-10-11T14:52:44.577099: step 1946, loss 0.189343, acc 0.953125, learning_rate 0.000101719
2017-10-11T14:52:44.780271: step 1947, loss 0.104039, acc 0.96875, learning_rate 0.000101712
2017-10-11T14:52:45.011482: step 1948, loss 0.205309, acc 0.953125, learning_rate 0.000101705
2017-10-11T14:52:45.236623: step 1949, loss 0.167247, acc 0.953125, learning_rate 0.000101698
2017-10-11T14:52:45.463019: step 1950, loss 0.1915, acc 0.9375, learning_rate 0.000101691
2017-10-11T14:52:45.685536: step 1951, loss 0.125872, acc 0.953125, learning_rate 0.000101684
2017-10-11T14:52:45.895551: step 1952, loss 0.134866, acc 0.953125, learning_rate 0.000101677
2017-10-11T14:52:46.171145: step 1953, loss 0.0890422, acc 0.953125, learning_rate 0.00010167
2017-10-11T14:52:46.448608: step 1954, loss 0.104066, acc 0.96875, learning_rate 0.000101664
2017-10-11T14:52:46.689305: step 1955, loss 0.13495, acc 0.9375, learning_rate 0.000101657
2017-10-11T14:52:46.954082: step 1956, loss 0.105441, acc 0.96875, learning_rate 0.00010165
2017-10-11T14:52:47.215563: step 1957, loss 0.0313884, acc 1, learning_rate 0.000101643
2017-10-11T14:52:47.441245: step 1958, loss 0.0597481, acc 0.984375, learning_rate 0.000101637
2017-10-11T14:52:47.665761: step 1959, loss 0.169131, acc 0.953125, learning_rate 0.00010163
2017-10-11T14:52:47.860284: step 1960, loss 0.0854352, acc 0.960784, learning_rate 0.000101623

Evaluation:
2017-10-11T14:52:48.055340: step 1960, loss 0.227912, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-1960

2017-10-11T14:52:50.182024: step 1961, loss 0.0668149, acc 0.96875, learning_rate 0.000101617
2017-10-11T14:52:50.435131: step 1962, loss 0.0673793, acc 0.984375, learning_rate 0.00010161
2017-10-11T14:52:50.698900: step 1963, loss 0.0656656, acc 0.96875, learning_rate 0.000101604
2017-10-11T14:52:50.968880: step 1964, loss 0.123977, acc 0.984375, learning_rate 0.000101597
2017-10-11T14:52:51.224987: step 1965, loss 0.208601, acc 0.9375, learning_rate 0.00010159
2017-10-11T14:52:51.491706: step 1966, loss 0.101937, acc 0.96875, learning_rate 0.000101584
2017-10-11T14:52:51.759135: step 1967, loss 0.132059, acc 0.953125, learning_rate 0.000101577
2017-10-11T14:52:52.040682: step 1968, loss 0.0934092, acc 0.96875, learning_rate 0.000101571
2017-10-11T14:52:52.303062: step 1969, loss 0.19554, acc 0.921875, learning_rate 0.000101565
2017-10-11T14:52:52.567222: step 1970, loss 0.0818722, acc 0.96875, learning_rate 0.000101558
2017-10-11T14:52:52.843275: step 1971, loss 0.0917848, acc 0.984375, learning_rate 0.000101552
2017-10-11T14:52:53.126168: step 1972, loss 0.273926, acc 0.90625, learning_rate 0.000101546
2017-10-11T14:52:53.372665: step 1973, loss 0.232494, acc 0.921875, learning_rate 0.000101539
2017-10-11T14:52:53.647048: step 1974, loss 0.120901, acc 0.953125, learning_rate 0.000101533
2017-10-11T14:52:53.933972: step 1975, loss 0.190717, acc 0.953125, learning_rate 0.000101527
2017-10-11T14:52:54.226950: step 1976, loss 0.113506, acc 0.9375, learning_rate 0.00010152
2017-10-11T14:52:54.466413: step 1977, loss 0.174359, acc 0.953125, learning_rate 0.000101514
2017-10-11T14:52:54.746698: step 1978, loss 0.0328308, acc 0.984375, learning_rate 0.000101508
2017-10-11T14:52:55.016696: step 1979, loss 0.0714436, acc 0.96875, learning_rate 0.000101502
2017-10-11T14:52:55.267420: step 1980, loss 0.0572021, acc 0.984375, learning_rate 0.000101496
2017-10-11T14:52:55.554491: step 1981, loss 0.103984, acc 0.96875, learning_rate 0.00010149
2017-10-11T14:52:55.839858: step 1982, loss 0.0659479, acc 0.984375, learning_rate 0.000101484
2017-10-11T14:52:56.143512: step 1983, loss 0.150193, acc 0.9375, learning_rate 0.000101478
2017-10-11T14:52:56.386148: step 1984, loss 0.110213, acc 0.953125, learning_rate 0.000101472
2017-10-11T14:52:56.669840: step 1985, loss 0.114559, acc 0.9375, learning_rate 0.000101466
2017-10-11T14:52:56.931538: step 1986, loss 0.172472, acc 0.9375, learning_rate 0.00010146
2017-10-11T14:52:57.174216: step 1987, loss 0.064326, acc 0.984375, learning_rate 0.000101454
2017-10-11T14:52:57.425096: step 1988, loss 0.172121, acc 0.953125, learning_rate 0.000101448
2017-10-11T14:52:57.674819: step 1989, loss 0.0759943, acc 0.96875, learning_rate 0.000101442
2017-10-11T14:52:57.980481: step 1990, loss 0.132292, acc 0.96875, learning_rate 0.000101436
2017-10-11T14:52:58.297881: step 1991, loss 0.102667, acc 0.953125, learning_rate 0.00010143
2017-10-11T14:52:58.597041: step 1992, loss 0.0672365, acc 0.984375, learning_rate 0.000101424
2017-10-11T14:52:58.842493: step 1993, loss 0.0698168, acc 0.984375, learning_rate 0.000101418
2017-10-11T14:52:59.109104: step 1994, loss 0.25747, acc 0.90625, learning_rate 0.000101413
2017-10-11T14:52:59.403818: step 1995, loss 0.0239302, acc 1, learning_rate 0.000101407
2017-10-11T14:52:59.689715: step 1996, loss 0.0329219, acc 1, learning_rate 0.000101401
2017-10-11T14:52:59.925754: step 1997, loss 0.124552, acc 0.96875, learning_rate 0.000101395
2017-10-11T14:53:00.202639: step 1998, loss 0.10855, acc 0.921875, learning_rate 0.00010139
2017-10-11T14:53:00.482209: step 1999, loss 0.0885707, acc 0.953125, learning_rate 0.000101384
2017-10-11T14:53:00.735652: step 2000, loss 0.116632, acc 0.953125, learning_rate 0.000101378

Evaluation:
2017-10-11T14:53:00.974405: step 2000, loss 0.230463, acc 0.910791

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2000

2017-10-11T14:53:02.727347: step 2001, loss 0.0144389, acc 1, learning_rate 0.000101373
2017-10-11T14:53:03.004744: step 2002, loss 0.194941, acc 0.921875, learning_rate 0.000101367
2017-10-11T14:53:03.284580: step 2003, loss 0.0990227, acc 0.953125, learning_rate 0.000101362
2017-10-11T14:53:03.577741: step 2004, loss 0.085192, acc 0.984375, learning_rate 0.000101356
2017-10-11T14:53:03.875994: step 2005, loss 0.100237, acc 0.96875, learning_rate 0.00010135
2017-10-11T14:53:04.175551: step 2006, loss 0.0540799, acc 0.96875, learning_rate 0.000101345
2017-10-11T14:53:04.478765: step 2007, loss 0.148025, acc 0.96875, learning_rate 0.000101339
2017-10-11T14:53:04.737301: step 2008, loss 0.101309, acc 0.96875, learning_rate 0.000101334
2017-10-11T14:53:05.023727: step 2009, loss 0.0358241, acc 1, learning_rate 0.000101328
2017-10-11T14:53:05.320699: step 2010, loss 0.160379, acc 0.953125, learning_rate 0.000101323
2017-10-11T14:53:05.627122: step 2011, loss 0.148644, acc 0.9375, learning_rate 0.000101318
2017-10-11T14:53:05.943713: step 2012, loss 0.136911, acc 0.921875, learning_rate 0.000101312
2017-10-11T14:53:06.233343: step 2013, loss 0.142807, acc 0.96875, learning_rate 0.000101307
2017-10-11T14:53:06.513427: step 2014, loss 0.0473159, acc 0.984375, learning_rate 0.000101302
2017-10-11T14:53:06.809593: step 2015, loss 0.135721, acc 0.9375, learning_rate 0.000101296
2017-10-11T14:53:07.102043: step 2016, loss 0.0506985, acc 1, learning_rate 0.000101291
2017-10-11T14:53:07.390063: step 2017, loss 0.194362, acc 0.921875, learning_rate 0.000101286
2017-10-11T14:53:07.670766: step 2018, loss 0.125342, acc 0.96875, learning_rate 0.00010128
2017-10-11T14:53:07.954545: step 2019, loss 0.0252873, acc 1, learning_rate 0.000101275
2017-10-11T14:53:08.226497: step 2020, loss 0.132746, acc 0.96875, learning_rate 0.00010127
2017-10-11T14:53:08.515575: step 2021, loss 0.15556, acc 0.9375, learning_rate 0.000101265
2017-10-11T14:53:08.771964: step 2022, loss 0.208068, acc 0.9375, learning_rate 0.00010126
2017-10-11T14:53:09.012459: step 2023, loss 0.0523556, acc 0.953125, learning_rate 0.000101255
2017-10-11T14:53:09.258100: step 2024, loss 0.0574219, acc 1, learning_rate 0.000101249
2017-10-11T14:53:09.525388: step 2025, loss 0.169104, acc 0.921875, learning_rate 0.000101244
2017-10-11T14:53:09.775653: step 2026, loss 0.0407381, acc 1, learning_rate 0.000101239
2017-10-11T14:53:10.062676: step 2027, loss 0.0970219, acc 0.984375, learning_rate 0.000101234
2017-10-11T14:53:10.297145: step 2028, loss 0.173507, acc 0.953125, learning_rate 0.000101229
2017-10-11T14:53:10.572110: step 2029, loss 0.118697, acc 0.953125, learning_rate 0.000101224
2017-10-11T14:53:10.850075: step 2030, loss 0.0881722, acc 0.96875, learning_rate 0.000101219
2017-10-11T14:53:11.181503: step 2031, loss 0.0905803, acc 0.984375, learning_rate 0.000101214
2017-10-11T14:53:11.497049: step 2032, loss 0.0477248, acc 0.984375, learning_rate 0.000101209
2017-10-11T14:53:11.782203: step 2033, loss 0.141747, acc 0.953125, learning_rate 0.000101204
2017-10-11T14:53:12.117388: step 2034, loss 0.0916933, acc 0.953125, learning_rate 0.000101199
2017-10-11T14:53:12.434419: step 2035, loss 0.0548195, acc 0.984375, learning_rate 0.000101194
2017-10-11T14:53:12.739914: step 2036, loss 0.0763591, acc 0.953125, learning_rate 0.00010119
2017-10-11T14:53:13.059389: step 2037, loss 0.117423, acc 0.953125, learning_rate 0.000101185
2017-10-11T14:53:13.326111: step 2038, loss 0.195411, acc 0.96875, learning_rate 0.00010118
2017-10-11T14:53:13.585108: step 2039, loss 0.102104, acc 0.953125, learning_rate 0.000101175
2017-10-11T14:53:13.855293: step 2040, loss 0.113634, acc 0.96875, learning_rate 0.00010117

Evaluation:
2017-10-11T14:53:14.119974: step 2040, loss 0.227852, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2040

2017-10-11T14:53:16.396041: step 2041, loss 0.143797, acc 0.9375, learning_rate 0.000101166
2017-10-11T14:53:16.627849: step 2042, loss 0.158165, acc 0.984375, learning_rate 0.000101161
2017-10-11T14:53:16.921139: step 2043, loss 0.175111, acc 0.9375, learning_rate 0.000101156
2017-10-11T14:53:17.212578: step 2044, loss 0.160126, acc 0.953125, learning_rate 0.000101151
2017-10-11T14:53:17.553827: step 2045, loss 0.109528, acc 0.953125, learning_rate 0.000101147
2017-10-11T14:53:17.853737: step 2046, loss 0.182754, acc 0.9375, learning_rate 0.000101142
2017-10-11T14:53:18.150015: step 2047, loss 0.311245, acc 0.90625, learning_rate 0.000101137
2017-10-11T14:53:18.437016: step 2048, loss 0.0622345, acc 0.984375, learning_rate 0.000101133
2017-10-11T14:53:18.746281: step 2049, loss 0.127672, acc 0.953125, learning_rate 0.000101128
2017-10-11T14:53:19.026148: step 2050, loss 0.119874, acc 0.953125, learning_rate 0.000101123
2017-10-11T14:53:19.349368: step 2051, loss 0.107432, acc 0.96875, learning_rate 0.000101119
2017-10-11T14:53:19.652489: step 2052, loss 0.105033, acc 0.96875, learning_rate 0.000101114
2017-10-11T14:53:19.975114: step 2053, loss 0.0516529, acc 0.984375, learning_rate 0.00010111
2017-10-11T14:53:20.295092: step 2054, loss 0.056835, acc 1, learning_rate 0.000101105
2017-10-11T14:53:20.610373: step 2055, loss 0.168509, acc 0.921875, learning_rate 0.000101101
2017-10-11T14:53:20.890880: step 2056, loss 0.0727703, acc 0.984375, learning_rate 0.000101096
2017-10-11T14:53:21.134391: step 2057, loss 0.0737734, acc 0.96875, learning_rate 0.000101092
2017-10-11T14:53:21.383512: step 2058, loss 0.176349, acc 0.960784, learning_rate 0.000101087
2017-10-11T14:53:21.664215: step 2059, loss 0.0572092, acc 0.96875, learning_rate 0.000101083
2017-10-11T14:53:21.966721: step 2060, loss 0.0364252, acc 0.984375, learning_rate 0.000101078
2017-10-11T14:53:22.249737: step 2061, loss 0.111202, acc 0.96875, learning_rate 0.000101074
2017-10-11T14:53:22.565141: step 2062, loss 0.0727076, acc 0.96875, learning_rate 0.00010107
2017-10-11T14:53:22.833444: step 2063, loss 0.180874, acc 0.953125, learning_rate 0.000101065
2017-10-11T14:53:23.092731: step 2064, loss 0.0889056, acc 0.953125, learning_rate 0.000101061
2017-10-11T14:53:23.343349: step 2065, loss 0.0540052, acc 0.984375, learning_rate 0.000101057
2017-10-11T14:53:23.582123: step 2066, loss 0.142351, acc 0.953125, learning_rate 0.000101052
2017-10-11T14:53:23.858358: step 2067, loss 0.0966085, acc 0.96875, learning_rate 0.000101048
2017-10-11T14:53:24.136690: step 2068, loss 0.0250681, acc 1, learning_rate 0.000101044
2017-10-11T14:53:24.402128: step 2069, loss 0.0486136, acc 0.984375, learning_rate 0.000101039
2017-10-11T14:53:24.666867: step 2070, loss 0.130063, acc 0.953125, learning_rate 0.000101035
2017-10-11T14:53:25.000199: step 2071, loss 0.163331, acc 0.96875, learning_rate 0.000101031
2017-10-11T14:53:25.315490: step 2072, loss 0.0480682, acc 0.984375, learning_rate 0.000101027
2017-10-11T14:53:25.614005: step 2073, loss 0.117306, acc 0.953125, learning_rate 0.000101023
2017-10-11T14:53:25.892820: step 2074, loss 0.122383, acc 0.953125, learning_rate 0.000101018
2017-10-11T14:53:26.167559: step 2075, loss 0.136834, acc 0.96875, learning_rate 0.000101014
2017-10-11T14:53:26.451337: step 2076, loss 0.162425, acc 0.9375, learning_rate 0.00010101
2017-10-11T14:53:26.725304: step 2077, loss 0.178647, acc 0.953125, learning_rate 0.000101006
2017-10-11T14:53:27.038966: step 2078, loss 0.208827, acc 0.90625, learning_rate 0.000101002
2017-10-11T14:53:27.303089: step 2079, loss 0.214228, acc 0.921875, learning_rate 0.000100998
2017-10-11T14:53:27.588963: step 2080, loss 0.207606, acc 0.9375, learning_rate 0.000100994

Evaluation:
2017-10-11T14:53:27.849447: step 2080, loss 0.22825, acc 0.910791

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2080

2017-10-11T14:53:30.479743: step 2081, loss 0.106227, acc 0.96875, learning_rate 0.00010099
2017-10-11T14:53:30.779505: step 2082, loss 0.111759, acc 0.953125, learning_rate 0.000100986
2017-10-11T14:53:31.080917: step 2083, loss 0.198512, acc 0.90625, learning_rate 0.000100982
2017-10-11T14:53:31.385226: step 2084, loss 0.0887139, acc 0.953125, learning_rate 0.000100978
2017-10-11T14:53:31.698588: step 2085, loss 0.0729593, acc 0.96875, learning_rate 0.000100974
2017-10-11T14:53:32.039890: step 2086, loss 0.0746625, acc 0.96875, learning_rate 0.00010097
2017-10-11T14:53:32.413844: step 2087, loss 0.0667657, acc 0.984375, learning_rate 0.000100966
2017-10-11T14:53:32.743470: step 2088, loss 0.150475, acc 0.921875, learning_rate 0.000100962
2017-10-11T14:53:33.064658: step 2089, loss 0.135625, acc 0.96875, learning_rate 0.000100958
2017-10-11T14:53:33.393959: step 2090, loss 0.0484856, acc 1, learning_rate 0.000100954
2017-10-11T14:53:33.681547: step 2091, loss 0.0744486, acc 0.984375, learning_rate 0.00010095
2017-10-11T14:53:33.936059: step 2092, loss 0.0780146, acc 0.984375, learning_rate 0.000100946
2017-10-11T14:53:34.222680: step 2093, loss 0.102297, acc 0.96875, learning_rate 0.000100942
2017-10-11T14:53:34.501603: step 2094, loss 0.102579, acc 0.96875, learning_rate 0.000100938
2017-10-11T14:53:34.785476: step 2095, loss 0.0722263, acc 0.96875, learning_rate 0.000100935
2017-10-11T14:53:35.061480: step 2096, loss 0.0980851, acc 0.953125, learning_rate 0.000100931
2017-10-11T14:53:35.319327: step 2097, loss 0.08896, acc 0.984375, learning_rate 0.000100927
2017-10-11T14:53:35.576112: step 2098, loss 0.145944, acc 0.953125, learning_rate 0.000100923
2017-10-11T14:53:35.864351: step 2099, loss 0.110974, acc 0.96875, learning_rate 0.000100919
2017-10-11T14:53:36.192948: step 2100, loss 0.181792, acc 0.953125, learning_rate 0.000100916
2017-10-11T14:53:36.488853: step 2101, loss 0.0592019, acc 0.984375, learning_rate 0.000100912
2017-10-11T14:53:36.745389: step 2102, loss 0.0891668, acc 0.984375, learning_rate 0.000100908
2017-10-11T14:53:37.013492: step 2103, loss 0.081942, acc 0.984375, learning_rate 0.000100904
2017-10-11T14:53:37.276607: step 2104, loss 0.102804, acc 0.953125, learning_rate 0.000100901
2017-10-11T14:53:37.528425: step 2105, loss 0.0599524, acc 0.984375, learning_rate 0.000100897
2017-10-11T14:53:37.804949: step 2106, loss 0.075086, acc 0.96875, learning_rate 0.000100893
2017-10-11T14:53:38.089226: step 2107, loss 0.0597184, acc 1, learning_rate 0.00010089
2017-10-11T14:53:38.393493: step 2108, loss 0.144423, acc 0.9375, learning_rate 0.000100886
2017-10-11T14:53:38.730139: step 2109, loss 0.113826, acc 0.953125, learning_rate 0.000100883
2017-10-11T14:53:39.041333: step 2110, loss 0.0865328, acc 0.96875, learning_rate 0.000100879
2017-10-11T14:53:39.338790: step 2111, loss 0.176732, acc 0.953125, learning_rate 0.000100875
2017-10-11T14:53:39.598997: step 2112, loss 0.337596, acc 0.921875, learning_rate 0.000100872
2017-10-11T14:53:39.911282: step 2113, loss 0.0708009, acc 0.984375, learning_rate 0.000100868
2017-10-11T14:53:40.193712: step 2114, loss 0.22653, acc 0.953125, learning_rate 0.000100865
2017-10-11T14:53:40.474390: step 2115, loss 0.064139, acc 0.96875, learning_rate 0.000100861
2017-10-11T14:53:40.808412: step 2116, loss 0.039842, acc 0.984375, learning_rate 0.000100858
2017-10-11T14:53:41.081617: step 2117, loss 0.0843741, acc 0.96875, learning_rate 0.000100854
2017-10-11T14:53:41.369898: step 2118, loss 0.126285, acc 0.96875, learning_rate 0.000100851
2017-10-11T14:53:41.637009: step 2119, loss 0.102719, acc 0.9375, learning_rate 0.000100847
2017-10-11T14:53:41.899732: step 2120, loss 0.116987, acc 0.96875, learning_rate 0.000100844

Evaluation:
2017-10-11T14:53:42.161448: step 2120, loss 0.227109, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2120

2017-10-11T14:53:43.899471: step 2121, loss 0.0203381, acc 1, learning_rate 0.00010084
2017-10-11T14:53:44.197029: step 2122, loss 0.21869, acc 0.9375, learning_rate 0.000100837
2017-10-11T14:53:44.519939: step 2123, loss 0.110547, acc 0.984375, learning_rate 0.000100833
2017-10-11T14:53:44.793509: step 2124, loss 0.0967373, acc 0.953125, learning_rate 0.00010083
2017-10-11T14:53:45.105381: step 2125, loss 0.0288383, acc 1, learning_rate 0.000100827
2017-10-11T14:53:45.401115: step 2126, loss 0.127071, acc 0.9375, learning_rate 0.000100823
2017-10-11T14:53:45.705166: step 2127, loss 0.167563, acc 0.953125, learning_rate 0.00010082
2017-10-11T14:53:45.983234: step 2128, loss 0.0556441, acc 0.96875, learning_rate 0.000100817
2017-10-11T14:53:46.277839: step 2129, loss 0.124629, acc 0.921875, learning_rate 0.000100813
2017-10-11T14:53:46.575420: step 2130, loss 0.1371, acc 0.953125, learning_rate 0.00010081
2017-10-11T14:53:46.898262: step 2131, loss 0.10667, acc 0.96875, learning_rate 0.000100807
2017-10-11T14:53:47.189994: step 2132, loss 0.0891076, acc 0.96875, learning_rate 0.000100803
2017-10-11T14:53:47.516933: step 2133, loss 0.084906, acc 0.984375, learning_rate 0.0001008
2017-10-11T14:53:47.789477: step 2134, loss 0.123205, acc 0.9375, learning_rate 0.000100797
2017-10-11T14:53:48.084655: step 2135, loss 0.0964459, acc 0.953125, learning_rate 0.000100793
2017-10-11T14:53:48.367402: step 2136, loss 0.119958, acc 0.953125, learning_rate 0.00010079
2017-10-11T14:53:48.639348: step 2137, loss 0.0474037, acc 0.984375, learning_rate 0.000100787
2017-10-11T14:53:48.892405: step 2138, loss 0.154392, acc 0.90625, learning_rate 0.000100784
2017-10-11T14:53:49.145087: step 2139, loss 0.056083, acc 0.984375, learning_rate 0.000100781
2017-10-11T14:53:49.423154: step 2140, loss 0.175422, acc 0.9375, learning_rate 0.000100777
2017-10-11T14:53:49.630600: step 2141, loss 0.13382, acc 0.9375, learning_rate 0.000100774
2017-10-11T14:53:49.874234: step 2142, loss 0.0304793, acc 1, learning_rate 0.000100771
2017-10-11T14:53:50.161365: step 2143, loss 0.0855167, acc 0.953125, learning_rate 0.000100768
2017-10-11T14:53:50.420678: step 2144, loss 0.0874989, acc 0.96875, learning_rate 0.000100765
2017-10-11T14:53:50.697519: step 2145, loss 0.0869005, acc 0.96875, learning_rate 0.000100762
2017-10-11T14:53:50.972601: step 2146, loss 0.191909, acc 0.890625, learning_rate 0.000100759
2017-10-11T14:53:51.223415: step 2147, loss 0.0638113, acc 0.984375, learning_rate 0.000100755
2017-10-11T14:53:51.465104: step 2148, loss 0.11895, acc 0.96875, learning_rate 0.000100752
2017-10-11T14:53:51.688034: step 2149, loss 0.170561, acc 0.953125, learning_rate 0.000100749
2017-10-11T14:53:51.991038: step 2150, loss 0.189692, acc 0.921875, learning_rate 0.000100746
2017-10-11T14:53:52.312263: step 2151, loss 0.157294, acc 0.9375, learning_rate 0.000100743
2017-10-11T14:53:52.618830: step 2152, loss 0.198396, acc 0.953125, learning_rate 0.00010074
2017-10-11T14:53:52.909978: step 2153, loss 0.0501086, acc 0.984375, learning_rate 0.000100737
2017-10-11T14:53:53.188693: step 2154, loss 0.0838909, acc 0.984375, learning_rate 0.000100734
2017-10-11T14:53:53.487301: step 2155, loss 0.172796, acc 0.96875, learning_rate 0.000100731
2017-10-11T14:53:53.744835: step 2156, loss 0.148756, acc 0.901961, learning_rate 0.000100728
2017-10-11T14:53:53.991722: step 2157, loss 0.110369, acc 0.96875, learning_rate 0.000100725
2017-10-11T14:53:54.282780: step 2158, loss 0.0811288, acc 0.96875, learning_rate 0.000100722
2017-10-11T14:53:54.539850: step 2159, loss 0.075158, acc 0.96875, learning_rate 0.000100719
2017-10-11T14:53:54.793576: step 2160, loss 0.0979452, acc 0.96875, learning_rate 0.000100716

Evaluation:
2017-10-11T14:53:55.033018: step 2160, loss 0.226851, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2160

2017-10-11T14:53:57.727460: step 2161, loss 0.099066, acc 0.9375, learning_rate 0.000100713
2017-10-11T14:53:58.010620: step 2162, loss 0.0948008, acc 0.96875, learning_rate 0.000100711
2017-10-11T14:53:58.285084: step 2163, loss 0.0430104, acc 0.984375, learning_rate 0.000100708
2017-10-11T14:53:58.548998: step 2164, loss 0.114487, acc 0.953125, learning_rate 0.000100705
2017-10-11T14:53:58.863931: step 2165, loss 0.165621, acc 0.953125, learning_rate 0.000100702
2017-10-11T14:53:59.180765: step 2166, loss 0.210275, acc 0.890625, learning_rate 0.000100699
2017-10-11T14:53:59.445828: step 2167, loss 0.168636, acc 0.9375, learning_rate 0.000100696
2017-10-11T14:53:59.760126: step 2168, loss 0.188125, acc 0.921875, learning_rate 0.000100693
2017-10-11T14:54:00.103701: step 2169, loss 0.0598466, acc 0.984375, learning_rate 0.00010069
2017-10-11T14:54:00.462558: step 2170, loss 0.066921, acc 0.984375, learning_rate 0.000100688
2017-10-11T14:54:00.773111: step 2171, loss 0.155723, acc 0.953125, learning_rate 0.000100685
2017-10-11T14:54:01.105005: step 2172, loss 0.167811, acc 0.9375, learning_rate 0.000100682
2017-10-11T14:54:01.375038: step 2173, loss 0.0777953, acc 0.96875, learning_rate 0.000100679
2017-10-11T14:54:01.609168: step 2174, loss 0.116114, acc 0.96875, learning_rate 0.000100677
2017-10-11T14:54:01.882813: step 2175, loss 0.116189, acc 0.9375, learning_rate 0.000100674
2017-10-11T14:54:02.145353: step 2176, loss 0.0502656, acc 0.984375, learning_rate 0.000100671
2017-10-11T14:54:02.446608: step 2177, loss 0.0406359, acc 0.984375, learning_rate 0.000100668
2017-10-11T14:54:02.764505: step 2178, loss 0.097421, acc 0.953125, learning_rate 0.000100666
2017-10-11T14:54:03.105164: step 2179, loss 0.188334, acc 0.9375, learning_rate 0.000100663
2017-10-11T14:54:03.415477: step 2180, loss 0.115268, acc 0.953125, learning_rate 0.00010066
2017-10-11T14:54:03.678284: step 2181, loss 0.268823, acc 0.890625, learning_rate 0.000100657
2017-10-11T14:54:03.960241: step 2182, loss 0.143132, acc 0.953125, learning_rate 0.000100655
2017-10-11T14:54:04.234053: step 2183, loss 0.0643361, acc 0.984375, learning_rate 0.000100652
2017-10-11T14:54:04.490348: step 2184, loss 0.215781, acc 0.921875, learning_rate 0.000100649
2017-10-11T14:54:04.778758: step 2185, loss 0.050447, acc 0.984375, learning_rate 0.000100647
2017-10-11T14:54:05.033193: step 2186, loss 0.0684581, acc 0.96875, learning_rate 0.000100644
2017-10-11T14:54:05.313968: step 2187, loss 0.0606511, acc 0.96875, learning_rate 0.000100641
2017-10-11T14:54:05.550247: step 2188, loss 0.259296, acc 0.90625, learning_rate 0.000100639
2017-10-11T14:54:05.819328: step 2189, loss 0.106445, acc 0.953125, learning_rate 0.000100636
2017-10-11T14:54:06.136129: step 2190, loss 0.034057, acc 0.984375, learning_rate 0.000100634
2017-10-11T14:54:06.443465: step 2191, loss 0.180508, acc 0.921875, learning_rate 0.000100631
2017-10-11T14:54:06.749506: step 2192, loss 0.186736, acc 0.9375, learning_rate 0.000100628
2017-10-11T14:54:07.065880: step 2193, loss 0.220948, acc 0.953125, learning_rate 0.000100626
2017-10-11T14:54:07.387590: step 2194, loss 0.050661, acc 0.96875, learning_rate 0.000100623
2017-10-11T14:54:07.660719: step 2195, loss 0.0567874, acc 0.984375, learning_rate 0.000100621
2017-10-11T14:54:07.949591: step 2196, loss 0.0894276, acc 0.96875, learning_rate 0.000100618
2017-10-11T14:54:08.258927: step 2197, loss 0.05066, acc 0.96875, learning_rate 0.000100616
2017-10-11T14:54:08.539919: step 2198, loss 0.0324913, acc 1, learning_rate 0.000100613
2017-10-11T14:54:08.827391: step 2199, loss 0.141098, acc 0.953125, learning_rate 0.000100611
2017-10-11T14:54:09.092383: step 2200, loss 0.260578, acc 0.90625, learning_rate 0.000100608

Evaluation:
2017-10-11T14:54:09.331951: step 2200, loss 0.227986, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2200

2017-10-11T14:54:11.122483: step 2201, loss 0.0546608, acc 1, learning_rate 0.000100606
2017-10-11T14:54:11.358600: step 2202, loss 0.103377, acc 0.96875, learning_rate 0.000100603
2017-10-11T14:54:11.651603: step 2203, loss 0.0439009, acc 0.984375, learning_rate 0.000100601
2017-10-11T14:54:11.955521: step 2204, loss 0.095391, acc 0.953125, learning_rate 0.000100598
2017-10-11T14:54:12.245833: step 2205, loss 0.0913689, acc 0.96875, learning_rate 0.000100596
2017-10-11T14:54:12.558375: step 2206, loss 0.090526, acc 0.96875, learning_rate 0.000100594
2017-10-11T14:54:12.864941: step 2207, loss 0.106289, acc 0.953125, learning_rate 0.000100591
2017-10-11T14:54:13.170107: step 2208, loss 0.0217059, acc 1, learning_rate 0.000100589
2017-10-11T14:54:13.465194: step 2209, loss 0.186419, acc 0.96875, learning_rate 0.000100586
2017-10-11T14:54:13.756816: step 2210, loss 0.0801699, acc 0.953125, learning_rate 0.000100584
2017-10-11T14:54:14.073349: step 2211, loss 0.0446381, acc 0.984375, learning_rate 0.000100581
2017-10-11T14:54:14.356738: step 2212, loss 0.0918016, acc 0.953125, learning_rate 0.000100579
2017-10-11T14:54:14.637835: step 2213, loss 0.052165, acc 1, learning_rate 0.000100577
2017-10-11T14:54:14.929581: step 2214, loss 0.0899824, acc 0.96875, learning_rate 0.000100574
2017-10-11T14:54:15.184249: step 2215, loss 0.159992, acc 0.96875, learning_rate 0.000100572
2017-10-11T14:54:15.452130: step 2216, loss 0.11766, acc 0.96875, learning_rate 0.00010057
2017-10-11T14:54:15.744893: step 2217, loss 0.0383893, acc 1, learning_rate 0.000100567
2017-10-11T14:54:16.018428: step 2218, loss 0.180372, acc 0.96875, learning_rate 0.000100565
2017-10-11T14:54:16.304820: step 2219, loss 0.204905, acc 0.953125, learning_rate 0.000100563
2017-10-11T14:54:16.594035: step 2220, loss 0.200436, acc 0.921875, learning_rate 0.00010056
2017-10-11T14:54:16.888345: step 2221, loss 0.154695, acc 0.921875, learning_rate 0.000100558
2017-10-11T14:54:17.169685: step 2222, loss 0.0925769, acc 0.953125, learning_rate 0.000100556
2017-10-11T14:54:17.464168: step 2223, loss 0.149916, acc 0.9375, learning_rate 0.000100554
2017-10-11T14:54:17.737337: step 2224, loss 0.129537, acc 0.96875, learning_rate 0.000100551
2017-10-11T14:54:18.002493: step 2225, loss 0.147806, acc 0.9375, learning_rate 0.000100549
2017-10-11T14:54:18.232685: step 2226, loss 0.128858, acc 0.953125, learning_rate 0.000100547
2017-10-11T14:54:18.476852: step 2227, loss 0.0728338, acc 0.984375, learning_rate 0.000100545
2017-10-11T14:54:18.790458: step 2228, loss 0.0936432, acc 0.953125, learning_rate 0.000100542
2017-10-11T14:54:19.088034: step 2229, loss 0.11757, acc 0.96875, learning_rate 0.00010054
2017-10-11T14:54:19.381300: step 2230, loss 0.131783, acc 0.953125, learning_rate 0.000100538
2017-10-11T14:54:19.684524: step 2231, loss 0.0874376, acc 0.96875, learning_rate 0.000100536
2017-10-11T14:54:19.998766: step 2232, loss 0.0290293, acc 1, learning_rate 0.000100534
2017-10-11T14:54:20.281794: step 2233, loss 0.126038, acc 0.96875, learning_rate 0.000100531
2017-10-11T14:54:20.562631: step 2234, loss 0.186944, acc 0.921875, learning_rate 0.000100529
2017-10-11T14:54:20.839682: step 2235, loss 0.0547876, acc 0.984375, learning_rate 0.000100527
2017-10-11T14:54:21.166777: step 2236, loss 0.190137, acc 0.9375, learning_rate 0.000100525
2017-10-11T14:54:21.445175: step 2237, loss 0.123686, acc 0.953125, learning_rate 0.000100523
2017-10-11T14:54:21.762725: step 2238, loss 0.178138, acc 0.9375, learning_rate 0.000100521
2017-10-11T14:54:22.040667: step 2239, loss 0.0826105, acc 0.96875, learning_rate 0.000100519
2017-10-11T14:54:22.309258: step 2240, loss 0.0934177, acc 0.953125, learning_rate 0.000100516

Evaluation:
2017-10-11T14:54:22.555459: step 2240, loss 0.226502, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2240

2017-10-11T14:54:25.135201: step 2241, loss 0.12172, acc 0.96875, learning_rate 0.000100514
2017-10-11T14:54:25.465781: step 2242, loss 0.034177, acc 1, learning_rate 0.000100512
2017-10-11T14:54:25.703658: step 2243, loss 0.152531, acc 0.9375, learning_rate 0.00010051
2017-10-11T14:54:26.000411: step 2244, loss 0.0811013, acc 0.96875, learning_rate 0.000100508
2017-10-11T14:54:26.310506: step 2245, loss 0.135936, acc 0.96875, learning_rate 0.000100506
2017-10-11T14:54:26.593223: step 2246, loss 0.103342, acc 0.96875, learning_rate 0.000100504
2017-10-11T14:54:26.915491: step 2247, loss 0.100144, acc 0.96875, learning_rate 0.000100502
2017-10-11T14:54:27.225776: step 2248, loss 0.165841, acc 0.921875, learning_rate 0.0001005
2017-10-11T14:54:27.514803: step 2249, loss 0.105004, acc 0.984375, learning_rate 0.000100498
2017-10-11T14:54:27.825111: step 2250, loss 0.146155, acc 0.96875, learning_rate 0.000100496
2017-10-11T14:54:28.142160: step 2251, loss 0.137938, acc 0.9375, learning_rate 0.000100494
2017-10-11T14:54:28.433452: step 2252, loss 0.118159, acc 0.953125, learning_rate 0.000100492
2017-10-11T14:54:28.764444: step 2253, loss 0.103894, acc 0.96875, learning_rate 0.00010049
2017-10-11T14:54:29.035619: step 2254, loss 0.100924, acc 0.980392, learning_rate 0.000100488
2017-10-11T14:54:29.354399: step 2255, loss 0.105492, acc 0.953125, learning_rate 0.000100486
2017-10-11T14:54:29.650243: step 2256, loss 0.123073, acc 0.96875, learning_rate 0.000100484
2017-10-11T14:54:29.944211: step 2257, loss 0.133787, acc 0.953125, learning_rate 0.000100482
2017-10-11T14:54:30.150324: step 2258, loss 0.148954, acc 0.9375, learning_rate 0.00010048
2017-10-11T14:54:30.430595: step 2259, loss 0.0707465, acc 0.984375, learning_rate 0.000100478
2017-10-11T14:54:30.710498: step 2260, loss 0.0587865, acc 0.96875, learning_rate 0.000100476
2017-10-11T14:54:30.969892: step 2261, loss 0.0404612, acc 0.984375, learning_rate 0.000100474
2017-10-11T14:54:31.225117: step 2262, loss 0.155454, acc 0.96875, learning_rate 0.000100472
2017-10-11T14:54:31.466249: step 2263, loss 0.0628602, acc 0.96875, learning_rate 0.00010047
2017-10-11T14:54:31.747499: step 2264, loss 0.0588995, acc 0.984375, learning_rate 0.000100468
2017-10-11T14:54:32.036496: step 2265, loss 0.0779705, acc 0.984375, learning_rate 0.000100466
2017-10-11T14:54:32.306210: step 2266, loss 0.0369235, acc 0.984375, learning_rate 0.000100464
2017-10-11T14:54:32.643048: step 2267, loss 0.0846424, acc 0.953125, learning_rate 0.000100462
2017-10-11T14:54:32.971421: step 2268, loss 0.0906132, acc 0.953125, learning_rate 0.000100461
2017-10-11T14:54:33.230796: step 2269, loss 0.116406, acc 0.96875, learning_rate 0.000100459
2017-10-11T14:54:33.505470: step 2270, loss 0.10107, acc 0.96875, learning_rate 0.000100457
2017-10-11T14:54:33.819674: step 2271, loss 0.143416, acc 0.953125, learning_rate 0.000100455
2017-10-11T14:54:34.125112: step 2272, loss 0.146765, acc 0.921875, learning_rate 0.000100453
2017-10-11T14:54:34.402357: step 2273, loss 0.0905145, acc 0.984375, learning_rate 0.000100451
2017-10-11T14:54:34.658885: step 2274, loss 0.111944, acc 0.953125, learning_rate 0.000100449
2017-10-11T14:54:34.961885: step 2275, loss 0.0755496, acc 0.984375, learning_rate 0.000100448
2017-10-11T14:54:35.266434: step 2276, loss 0.0662809, acc 0.96875, learning_rate 0.000100446
2017-10-11T14:54:35.587157: step 2277, loss 0.0771132, acc 0.953125, learning_rate 0.000100444
2017-10-11T14:54:35.884358: step 2278, loss 0.137624, acc 0.96875, learning_rate 0.000100442
2017-10-11T14:54:36.159778: step 2279, loss 0.102361, acc 0.9375, learning_rate 0.00010044
2017-10-11T14:54:36.426251: step 2280, loss 0.142875, acc 0.953125, learning_rate 0.000100439

Evaluation:
2017-10-11T14:54:36.696476: step 2280, loss 0.227444, acc 0.909353

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2280

2017-10-11T14:54:39.514760: step 2281, loss 0.221386, acc 0.953125, learning_rate 0.000100437
2017-10-11T14:54:39.785572: step 2282, loss 0.133687, acc 0.953125, learning_rate 0.000100435
2017-10-11T14:54:40.075569: step 2283, loss 0.13787, acc 0.953125, learning_rate 0.000100433
2017-10-11T14:54:40.364245: step 2284, loss 0.0224313, acc 1, learning_rate 0.000100431
2017-10-11T14:54:40.660524: step 2285, loss 0.0751052, acc 0.984375, learning_rate 0.00010043
2017-10-11T14:54:40.940545: step 2286, loss 0.0913767, acc 0.984375, learning_rate 0.000100428
2017-10-11T14:54:41.211574: step 2287, loss 0.115342, acc 0.96875, learning_rate 0.000100426
2017-10-11T14:54:41.486675: step 2288, loss 0.137083, acc 0.953125, learning_rate 0.000100424
2017-10-11T14:54:41.744022: step 2289, loss 0.216041, acc 0.921875, learning_rate 0.000100423
2017-10-11T14:54:42.010073: step 2290, loss 0.0862662, acc 0.96875, learning_rate 0.000100421
2017-10-11T14:54:42.268475: step 2291, loss 0.0835465, acc 0.984375, learning_rate 0.000100419
2017-10-11T14:54:42.530571: step 2292, loss 0.0478551, acc 0.984375, learning_rate 0.000100418
2017-10-11T14:54:42.781053: step 2293, loss 0.176878, acc 0.921875, learning_rate 0.000100416
2017-10-11T14:54:43.039978: step 2294, loss 0.129235, acc 0.96875, learning_rate 0.000100414
2017-10-11T14:54:43.294082: step 2295, loss 0.0248338, acc 1, learning_rate 0.000100412
2017-10-11T14:54:43.535471: step 2296, loss 0.0947103, acc 0.953125, learning_rate 0.000100411
2017-10-11T14:54:43.772095: step 2297, loss 0.0561049, acc 0.984375, learning_rate 0.000100409
2017-10-11T14:54:44.030085: step 2298, loss 0.218264, acc 0.90625, learning_rate 0.000100407
2017-10-11T14:54:44.276670: step 2299, loss 0.0709151, acc 0.984375, learning_rate 0.000100406
2017-10-11T14:54:44.553847: step 2300, loss 0.0936104, acc 0.96875, learning_rate 0.000100404
2017-10-11T14:54:44.830337: step 2301, loss 0.145215, acc 0.96875, learning_rate 0.000100402
2017-10-11T14:54:45.110077: step 2302, loss 0.184474, acc 0.921875, learning_rate 0.000100401
2017-10-11T14:54:45.373758: step 2303, loss 0.0493214, acc 0.984375, learning_rate 0.000100399
2017-10-11T14:54:45.616306: step 2304, loss 0.0849113, acc 0.96875, learning_rate 0.000100398
2017-10-11T14:54:45.930581: step 2305, loss 0.03655, acc 1, learning_rate 0.000100396
2017-10-11T14:54:46.219815: step 2306, loss 0.13641, acc 0.9375, learning_rate 0.000100394
2017-10-11T14:54:46.534676: step 2307, loss 0.118139, acc 0.953125, learning_rate 0.000100393
2017-10-11T14:54:46.817421: step 2308, loss 0.0613043, acc 0.96875, learning_rate 0.000100391
2017-10-11T14:54:47.105398: step 2309, loss 0.136932, acc 0.96875, learning_rate 0.000100389
2017-10-11T14:54:47.421846: step 2310, loss 0.167824, acc 0.9375, learning_rate 0.000100388
2017-10-11T14:54:47.727515: step 2311, loss 0.0606243, acc 0.96875, learning_rate 0.000100386
2017-10-11T14:54:48.026371: step 2312, loss 0.0208562, acc 1, learning_rate 0.000100385
2017-10-11T14:54:48.300583: step 2313, loss 0.0839228, acc 0.96875, learning_rate 0.000100383
2017-10-11T14:54:48.632689: step 2314, loss 0.0950694, acc 0.953125, learning_rate 0.000100382
2017-10-11T14:54:48.937272: step 2315, loss 0.173862, acc 0.921875, learning_rate 0.00010038
2017-10-11T14:54:49.223102: step 2316, loss 0.164883, acc 0.921875, learning_rate 0.000100378
2017-10-11T14:54:49.526423: step 2317, loss 0.0348717, acc 0.984375, learning_rate 0.000100377
2017-10-11T14:54:49.831157: step 2318, loss 0.26641, acc 0.921875, learning_rate 0.000100375
2017-10-11T14:54:50.129744: step 2319, loss 0.104829, acc 0.96875, learning_rate 0.000100374
2017-10-11T14:54:50.399428: step 2320, loss 0.163118, acc 0.953125, learning_rate 0.000100372

Evaluation:
2017-10-11T14:54:50.654141: step 2320, loss 0.228316, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2320

2017-10-11T14:54:52.458079: step 2321, loss 0.0730263, acc 0.96875, learning_rate 0.000100371
2017-10-11T14:54:52.771913: step 2322, loss 0.100276, acc 0.96875, learning_rate 0.000100369
2017-10-11T14:54:53.091327: step 2323, loss 0.0966507, acc 0.9375, learning_rate 0.000100368
2017-10-11T14:54:53.372927: step 2324, loss 0.0938498, acc 0.984375, learning_rate 0.000100366
2017-10-11T14:54:53.683064: step 2325, loss 0.18076, acc 0.9375, learning_rate 0.000100365
2017-10-11T14:54:53.992340: step 2326, loss 0.0482622, acc 1, learning_rate 0.000100363
2017-10-11T14:54:54.316742: step 2327, loss 0.159537, acc 0.96875, learning_rate 0.000100362
2017-10-11T14:54:54.631702: step 2328, loss 0.0704062, acc 0.984375, learning_rate 0.00010036
2017-10-11T14:54:54.940392: step 2329, loss 0.109079, acc 0.984375, learning_rate 0.000100359
2017-10-11T14:54:55.238812: step 2330, loss 0.115042, acc 0.9375, learning_rate 0.000100357
2017-10-11T14:54:55.528903: step 2331, loss 0.145258, acc 0.9375, learning_rate 0.000100356
2017-10-11T14:54:55.818493: step 2332, loss 0.0516276, acc 0.984375, learning_rate 0.000100354
2017-10-11T14:54:56.091957: step 2333, loss 0.163392, acc 0.90625, learning_rate 0.000100353
2017-10-11T14:54:56.331797: step 2334, loss 0.121859, acc 0.96875, learning_rate 0.000100352
2017-10-11T14:54:56.589468: step 2335, loss 0.0940833, acc 0.953125, learning_rate 0.00010035
2017-10-11T14:54:56.850896: step 2336, loss 0.132769, acc 0.953125, learning_rate 0.000100349
2017-10-11T14:54:57.117481: step 2337, loss 0.149361, acc 0.953125, learning_rate 0.000100347
2017-10-11T14:54:57.384744: step 2338, loss 0.112033, acc 0.96875, learning_rate 0.000100346
2017-10-11T14:54:57.615230: step 2339, loss 0.197229, acc 0.90625, learning_rate 0.000100344
2017-10-11T14:54:57.852509: step 2340, loss 0.0966963, acc 0.953125, learning_rate 0.000100343
2017-10-11T14:54:58.092432: step 2341, loss 0.0816156, acc 0.953125, learning_rate 0.000100342
2017-10-11T14:54:58.313163: step 2342, loss 0.0331036, acc 0.984375, learning_rate 0.00010034
2017-10-11T14:54:58.553968: step 2343, loss 0.0574848, acc 0.984375, learning_rate 0.000100339
2017-10-11T14:54:58.834808: step 2344, loss 0.0966595, acc 0.984375, learning_rate 0.000100338
2017-10-11T14:54:59.187891: step 2345, loss 0.100051, acc 0.96875, learning_rate 0.000100336
2017-10-11T14:54:59.485669: step 2346, loss 0.174818, acc 0.9375, learning_rate 0.000100335
2017-10-11T14:54:59.802142: step 2347, loss 0.0574044, acc 0.984375, learning_rate 0.000100333
2017-10-11T14:55:00.121160: step 2348, loss 0.154242, acc 0.9375, learning_rate 0.000100332
2017-10-11T14:55:00.441431: step 2349, loss 0.105801, acc 0.96875, learning_rate 0.000100331
2017-10-11T14:55:00.746128: step 2350, loss 0.0693851, acc 0.984375, learning_rate 0.000100329
2017-10-11T14:55:01.047386: step 2351, loss 0.123519, acc 0.984375, learning_rate 0.000100328
2017-10-11T14:55:01.348116: step 2352, loss 0.0826954, acc 0.960784, learning_rate 0.000100327
2017-10-11T14:55:01.627452: step 2353, loss 0.129907, acc 0.953125, learning_rate 0.000100325
2017-10-11T14:55:01.937853: step 2354, loss 0.121051, acc 0.96875, learning_rate 0.000100324
2017-10-11T14:55:02.271337: step 2355, loss 0.0833135, acc 0.96875, learning_rate 0.000100323
2017-10-11T14:55:02.623378: step 2356, loss 0.156444, acc 0.921875, learning_rate 0.000100321
2017-10-11T14:55:02.887888: step 2357, loss 0.0860375, acc 0.953125, learning_rate 0.00010032
2017-10-11T14:55:03.356762: step 2358, loss 0.0876679, acc 0.953125, learning_rate 0.000100319
2017-10-11T14:55:03.621267: step 2359, loss 0.0860851, acc 0.96875, learning_rate 0.000100317
2017-10-11T14:55:03.926824: step 2360, loss 0.0982017, acc 0.96875, learning_rate 0.000100316

Evaluation:
2017-10-11T14:55:04.150186: step 2360, loss 0.224747, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2360

2017-10-11T14:55:06.325905: step 2361, loss 0.0442869, acc 1, learning_rate 0.000100315
2017-10-11T14:55:06.571958: step 2362, loss 0.159816, acc 0.96875, learning_rate 0.000100314
2017-10-11T14:55:06.888453: step 2363, loss 0.175244, acc 0.953125, learning_rate 0.000100312
2017-10-11T14:55:07.206339: step 2364, loss 0.19201, acc 0.953125, learning_rate 0.000100311
2017-10-11T14:55:07.518042: step 2365, loss 0.0888624, acc 0.96875, learning_rate 0.00010031
2017-10-11T14:55:07.793306: step 2366, loss 0.171849, acc 0.9375, learning_rate 0.000100308
2017-10-11T14:55:08.089721: step 2367, loss 0.0428333, acc 1, learning_rate 0.000100307
2017-10-11T14:55:08.391267: step 2368, loss 0.112394, acc 0.96875, learning_rate 0.000100306
2017-10-11T14:55:08.696844: step 2369, loss 0.0401848, acc 1, learning_rate 0.000100305
2017-10-11T14:55:09.019572: step 2370, loss 0.0599376, acc 0.984375, learning_rate 0.000100303
2017-10-11T14:55:09.319764: step 2371, loss 0.0864236, acc 0.984375, learning_rate 0.000100302
2017-10-11T14:55:09.596808: step 2372, loss 0.0220403, acc 1, learning_rate 0.000100301
2017-10-11T14:55:09.859086: step 2373, loss 0.159022, acc 0.953125, learning_rate 0.0001003
2017-10-11T14:55:10.136501: step 2374, loss 0.059726, acc 0.984375, learning_rate 0.000100299
2017-10-11T14:55:10.435144: step 2375, loss 0.171461, acc 0.921875, learning_rate 0.000100297
2017-10-11T14:55:10.719569: step 2376, loss 0.102717, acc 0.953125, learning_rate 0.000100296
2017-10-11T14:55:10.950497: step 2377, loss 0.137087, acc 0.953125, learning_rate 0.000100295
2017-10-11T14:55:11.191080: step 2378, loss 0.108834, acc 0.953125, learning_rate 0.000100294
2017-10-11T14:55:11.406776: step 2379, loss 0.105732, acc 0.9375, learning_rate 0.000100292
2017-10-11T14:55:11.631496: step 2380, loss 0.0628764, acc 0.984375, learning_rate 0.000100291
2017-10-11T14:55:11.858021: step 2381, loss 0.153453, acc 0.9375, learning_rate 0.00010029
2017-10-11T14:55:12.101995: step 2382, loss 0.0381416, acc 0.984375, learning_rate 0.000100289
2017-10-11T14:55:12.369137: step 2383, loss 0.0863268, acc 1, learning_rate 0.000100288
2017-10-11T14:55:12.606130: step 2384, loss 0.0924348, acc 0.96875, learning_rate 0.000100287
2017-10-11T14:55:12.868534: step 2385, loss 0.0536114, acc 0.984375, learning_rate 0.000100285
2017-10-11T14:55:13.187185: step 2386, loss 0.0588583, acc 0.984375, learning_rate 0.000100284
2017-10-11T14:55:13.449411: step 2387, loss 0.114735, acc 0.96875, learning_rate 0.000100283
2017-10-11T14:55:13.765666: step 2388, loss 0.168474, acc 0.921875, learning_rate 0.000100282
2017-10-11T14:55:14.054986: step 2389, loss 0.0744296, acc 0.984375, learning_rate 0.000100281
2017-10-11T14:55:14.387229: step 2390, loss 0.0992602, acc 0.96875, learning_rate 0.00010028
2017-10-11T14:55:14.683433: step 2391, loss 0.0557263, acc 0.984375, learning_rate 0.000100278
2017-10-11T14:55:15.000951: step 2392, loss 0.0429105, acc 0.984375, learning_rate 0.000100277
2017-10-11T14:55:15.295828: step 2393, loss 0.049956, acc 0.984375, learning_rate 0.000100276
2017-10-11T14:55:15.617903: step 2394, loss 0.10257, acc 0.96875, learning_rate 0.000100275
2017-10-11T14:55:15.944572: step 2395, loss 0.155098, acc 0.953125, learning_rate 0.000100274
2017-10-11T14:55:16.279303: step 2396, loss 0.126953, acc 0.953125, learning_rate 0.000100273
2017-10-11T14:55:16.583060: step 2397, loss 0.0920229, acc 0.96875, learning_rate 0.000100272
2017-10-11T14:55:16.875505: step 2398, loss 0.0376114, acc 0.984375, learning_rate 0.000100271
2017-10-11T14:55:17.223158: step 2399, loss 0.0848162, acc 0.96875, learning_rate 0.00010027
2017-10-11T14:55:17.483226: step 2400, loss 0.0583864, acc 0.984375, learning_rate 0.000100268

Evaluation:
2017-10-11T14:55:17.715457: step 2400, loss 0.222285, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2400

2017-10-11T14:55:21.009764: step 2401, loss 0.0736553, acc 0.96875, learning_rate 0.000100267
2017-10-11T14:55:21.352297: step 2402, loss 0.0448591, acc 0.984375, learning_rate 0.000100266
2017-10-11T14:55:21.661102: step 2403, loss 0.0321683, acc 1, learning_rate 0.000100265
2017-10-11T14:55:21.964176: step 2404, loss 0.137541, acc 0.953125, learning_rate 0.000100264
2017-10-11T14:55:22.297142: step 2405, loss 0.0788955, acc 0.984375, learning_rate 0.000100263
2017-10-11T14:55:22.590333: step 2406, loss 0.111777, acc 0.9375, learning_rate 0.000100262
2017-10-11T14:55:22.898058: step 2407, loss 0.0992501, acc 0.953125, learning_rate 0.000100261
2017-10-11T14:55:23.186676: step 2408, loss 0.109812, acc 0.96875, learning_rate 0.00010026
2017-10-11T14:55:23.522314: step 2409, loss 0.0451887, acc 0.96875, learning_rate 0.000100259
2017-10-11T14:55:23.797535: step 2410, loss 0.144684, acc 0.9375, learning_rate 0.000100258
2017-10-11T14:55:24.081984: step 2411, loss 0.286017, acc 0.890625, learning_rate 0.000100257
2017-10-11T14:55:24.332972: step 2412, loss 0.103984, acc 0.96875, learning_rate 0.000100256
2017-10-11T14:55:24.554153: step 2413, loss 0.187652, acc 0.921875, learning_rate 0.000100255
2017-10-11T14:55:24.787542: step 2414, loss 0.0962556, acc 0.96875, learning_rate 0.000100253
2017-10-11T14:55:25.026005: step 2415, loss 0.0259311, acc 1, learning_rate 0.000100252
2017-10-11T14:55:25.252396: step 2416, loss 0.211117, acc 0.953125, learning_rate 0.000100251
2017-10-11T14:55:25.474036: step 2417, loss 0.0711224, acc 0.96875, learning_rate 0.00010025
2017-10-11T14:55:25.764824: step 2418, loss 0.146802, acc 0.96875, learning_rate 0.000100249
2017-10-11T14:55:26.102122: step 2419, loss 0.0939759, acc 0.953125, learning_rate 0.000100248
2017-10-11T14:55:26.440001: step 2420, loss 0.0955412, acc 0.96875, learning_rate 0.000100247
2017-10-11T14:55:26.737138: step 2421, loss 0.0736261, acc 0.96875, learning_rate 0.000100246
2017-10-11T14:55:27.060452: step 2422, loss 0.0654135, acc 0.96875, learning_rate 0.000100245
2017-10-11T14:55:27.394363: step 2423, loss 0.130812, acc 0.953125, learning_rate 0.000100244
2017-10-11T14:55:27.655263: step 2424, loss 0.0929357, acc 0.984375, learning_rate 0.000100243
2017-10-11T14:55:27.969292: step 2425, loss 0.224081, acc 0.90625, learning_rate 0.000100242
2017-10-11T14:55:28.283391: step 2426, loss 0.0619235, acc 0.984375, learning_rate 0.000100241
2017-10-11T14:55:28.605975: step 2427, loss 0.186761, acc 0.9375, learning_rate 0.00010024
2017-10-11T14:55:28.896312: step 2428, loss 0.102868, acc 0.953125, learning_rate 0.000100239
2017-10-11T14:55:29.214879: step 2429, loss 0.122212, acc 0.96875, learning_rate 0.000100238
2017-10-11T14:55:29.511165: step 2430, loss 0.0875236, acc 0.953125, learning_rate 0.000100237
2017-10-11T14:55:29.802068: step 2431, loss 0.043125, acc 0.984375, learning_rate 0.000100236
2017-10-11T14:55:30.098469: step 2432, loss 0.046711, acc 0.984375, learning_rate 0.000100235
2017-10-11T14:55:30.402919: step 2433, loss 0.0268662, acc 1, learning_rate 0.000100235
2017-10-11T14:55:30.700192: step 2434, loss 0.0574031, acc 0.984375, learning_rate 0.000100234
2017-10-11T14:55:30.996408: step 2435, loss 0.0776703, acc 0.96875, learning_rate 0.000100233
2017-10-11T14:55:31.330038: step 2436, loss 0.0768179, acc 0.984375, learning_rate 0.000100232
2017-10-11T14:55:31.603674: step 2437, loss 0.140209, acc 0.9375, learning_rate 0.000100231
2017-10-11T14:55:31.931029: step 2438, loss 0.0806985, acc 0.9375, learning_rate 0.00010023
2017-10-11T14:55:32.198059: step 2439, loss 0.0984069, acc 0.96875, learning_rate 0.000100229
2017-10-11T14:55:32.469295: step 2440, loss 0.243762, acc 0.921875, learning_rate 0.000100228

Evaluation:
2017-10-11T14:55:32.771112: step 2440, loss 0.226107, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2440

2017-10-11T14:55:34.913020: step 2441, loss 0.112081, acc 0.953125, learning_rate 0.000100227
2017-10-11T14:55:35.187251: step 2442, loss 0.17348, acc 0.921875, learning_rate 0.000100226
2017-10-11T14:55:35.469727: step 2443, loss 0.0884719, acc 0.96875, learning_rate 0.000100225
2017-10-11T14:55:35.750577: step 2444, loss 0.11649, acc 0.953125, learning_rate 0.000100224
2017-10-11T14:55:36.041300: step 2445, loss 0.127852, acc 0.953125, learning_rate 0.000100223
2017-10-11T14:55:36.361720: step 2446, loss 0.0669177, acc 0.984375, learning_rate 0.000100222
2017-10-11T14:55:36.652759: step 2447, loss 0.127088, acc 0.953125, learning_rate 0.000100221
2017-10-11T14:55:36.924713: step 2448, loss 0.0942905, acc 0.96875, learning_rate 0.000100221
2017-10-11T14:55:37.274483: step 2449, loss 0.179332, acc 0.9375, learning_rate 0.00010022
2017-10-11T14:55:37.519242: step 2450, loss 0.234055, acc 0.882353, learning_rate 0.000100219
2017-10-11T14:55:37.822840: step 2451, loss 0.0977574, acc 0.953125, learning_rate 0.000100218
2017-10-11T14:55:38.047690: step 2452, loss 0.197721, acc 0.96875, learning_rate 0.000100217
2017-10-11T14:55:38.261565: step 2453, loss 0.211395, acc 0.921875, learning_rate 0.000100216
2017-10-11T14:55:38.508819: step 2454, loss 0.0253081, acc 1, learning_rate 0.000100215
2017-10-11T14:55:38.750442: step 2455, loss 0.118395, acc 0.953125, learning_rate 0.000100214
2017-10-11T14:55:38.985902: step 2456, loss 0.0808243, acc 0.96875, learning_rate 0.000100213
2017-10-11T14:55:39.232584: step 2457, loss 0.106639, acc 0.953125, learning_rate 0.000100213
2017-10-11T14:55:39.521062: step 2458, loss 0.0613251, acc 0.984375, learning_rate 0.000100212
2017-10-11T14:55:39.819141: step 2459, loss 0.0710462, acc 0.96875, learning_rate 0.000100211
2017-10-11T14:55:40.091417: step 2460, loss 0.0953653, acc 0.96875, learning_rate 0.00010021
2017-10-11T14:55:40.395025: step 2461, loss 0.102763, acc 0.96875, learning_rate 0.000100209
2017-10-11T14:55:40.686956: step 2462, loss 0.185372, acc 0.90625, learning_rate 0.000100208
2017-10-11T14:55:40.974863: step 2463, loss 0.0896422, acc 0.96875, learning_rate 0.000100207
2017-10-11T14:55:41.292897: step 2464, loss 0.132022, acc 0.953125, learning_rate 0.000100207
2017-10-11T14:55:41.610669: step 2465, loss 0.26208, acc 0.890625, learning_rate 0.000100206
2017-10-11T14:55:41.909247: step 2466, loss 0.161425, acc 0.953125, learning_rate 0.000100205
2017-10-11T14:55:42.211912: step 2467, loss 0.101047, acc 0.96875, learning_rate 0.000100204
2017-10-11T14:55:42.524559: step 2468, loss 0.0581163, acc 0.984375, learning_rate 0.000100203
2017-10-11T14:55:42.849778: step 2469, loss 0.109217, acc 0.96875, learning_rate 0.000100202
2017-10-11T14:55:43.152953: step 2470, loss 0.172572, acc 0.921875, learning_rate 0.000100202
2017-10-11T14:55:43.480058: step 2471, loss 0.0993438, acc 0.96875, learning_rate 0.000100201
2017-10-11T14:55:43.784725: step 2472, loss 0.143525, acc 0.953125, learning_rate 0.0001002
2017-10-11T14:55:44.087796: step 2473, loss 0.104977, acc 0.9375, learning_rate 0.000100199
2017-10-11T14:55:44.368159: step 2474, loss 0.116202, acc 0.953125, learning_rate 0.000100198
2017-10-11T14:55:44.689817: step 2475, loss 0.0915977, acc 0.953125, learning_rate 0.000100198
2017-10-11T14:55:44.989988: step 2476, loss 0.0822578, acc 0.984375, learning_rate 0.000100197
2017-10-11T14:55:45.267433: step 2477, loss 0.143927, acc 0.921875, learning_rate 0.000100196
2017-10-11T14:55:45.549750: step 2478, loss 0.137116, acc 0.96875, learning_rate 0.000100195
2017-10-11T14:55:45.844443: step 2479, loss 0.0460424, acc 1, learning_rate 0.000100194
2017-10-11T14:55:46.132104: step 2480, loss 0.108493, acc 0.9375, learning_rate 0.000100194

Evaluation:
2017-10-11T14:55:46.374567: step 2480, loss 0.226159, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2480

2017-10-11T14:55:48.865220: step 2481, loss 0.131328, acc 0.96875, learning_rate 0.000100193
2017-10-11T14:55:49.183615: step 2482, loss 0.0747092, acc 0.984375, learning_rate 0.000100192
2017-10-11T14:55:49.456432: step 2483, loss 0.0723425, acc 0.984375, learning_rate 0.000100191
2017-10-11T14:55:49.737156: step 2484, loss 0.0524551, acc 0.984375, learning_rate 0.00010019
2017-10-11T14:55:50.058091: step 2485, loss 0.0910502, acc 0.953125, learning_rate 0.00010019
2017-10-11T14:55:50.344547: step 2486, loss 0.126416, acc 0.953125, learning_rate 0.000100189
2017-10-11T14:55:50.650506: step 2487, loss 0.225531, acc 0.90625, learning_rate 0.000100188
2017-10-11T14:55:50.930393: step 2488, loss 0.134458, acc 0.96875, learning_rate 0.000100187
2017-10-11T14:55:51.238061: step 2489, loss 0.0988705, acc 0.953125, learning_rate 0.000100187
2017-10-11T14:55:51.519524: step 2490, loss 0.180326, acc 0.9375, learning_rate 0.000100186
2017-10-11T14:55:51.773855: step 2491, loss 0.0965506, acc 0.953125, learning_rate 0.000100185
2017-10-11T14:55:52.053427: step 2492, loss 0.0511604, acc 0.984375, learning_rate 0.000100184
2017-10-11T14:55:52.274409: step 2493, loss 0.0273839, acc 1, learning_rate 0.000100183
2017-10-11T14:55:52.479327: step 2494, loss 0.168425, acc 0.9375, learning_rate 0.000100183
2017-10-11T14:55:52.683264: step 2495, loss 0.0795482, acc 0.96875, learning_rate 0.000100182
2017-10-11T14:55:52.892112: step 2496, loss 0.0452439, acc 1, learning_rate 0.000100181
2017-10-11T14:55:53.126556: step 2497, loss 0.0426138, acc 0.984375, learning_rate 0.000100181
2017-10-11T14:55:53.344811: step 2498, loss 0.185268, acc 0.96875, learning_rate 0.00010018
2017-10-11T14:55:53.562383: step 2499, loss 0.129791, acc 0.953125, learning_rate 0.000100179
2017-10-11T14:55:53.826360: step 2500, loss 0.129327, acc 0.953125, learning_rate 0.000100178
2017-10-11T14:55:54.105663: step 2501, loss 0.0578015, acc 1, learning_rate 0.000100178
2017-10-11T14:55:54.431742: step 2502, loss 0.0872404, acc 0.96875, learning_rate 0.000100177
2017-10-11T14:55:54.747298: step 2503, loss 0.0832374, acc 0.96875, learning_rate 0.000100176
2017-10-11T14:55:55.011838: step 2504, loss 0.108784, acc 0.96875, learning_rate 0.000100175
2017-10-11T14:55:55.324395: step 2505, loss 0.0477527, acc 0.984375, learning_rate 0.000100175
2017-10-11T14:55:55.622225: step 2506, loss 0.0451121, acc 0.984375, learning_rate 0.000100174
2017-10-11T14:55:55.899777: step 2507, loss 0.101422, acc 0.96875, learning_rate 0.000100173
2017-10-11T14:55:56.172081: step 2508, loss 0.125343, acc 0.9375, learning_rate 0.000100173
2017-10-11T14:55:56.493244: step 2509, loss 0.127953, acc 0.953125, learning_rate 0.000100172
2017-10-11T14:55:56.767293: step 2510, loss 0.172402, acc 0.96875, learning_rate 0.000100171
2017-10-11T14:55:57.014799: step 2511, loss 0.0877414, acc 0.953125, learning_rate 0.00010017
2017-10-11T14:55:57.299030: step 2512, loss 0.0990921, acc 0.96875, learning_rate 0.00010017
2017-10-11T14:55:57.611032: step 2513, loss 0.0498336, acc 0.96875, learning_rate 0.000100169
2017-10-11T14:55:57.909699: step 2514, loss 0.0909918, acc 0.984375, learning_rate 0.000100168
2017-10-11T14:55:58.164917: step 2515, loss 0.124673, acc 0.96875, learning_rate 0.000100168
2017-10-11T14:55:58.462009: step 2516, loss 0.11186, acc 0.9375, learning_rate 0.000100167
2017-10-11T14:55:58.774116: step 2517, loss 0.200335, acc 0.921875, learning_rate 0.000100166
2017-10-11T14:55:59.035547: step 2518, loss 0.0475357, acc 0.984375, learning_rate 0.000100166
2017-10-11T14:55:59.307434: step 2519, loss 0.223088, acc 0.953125, learning_rate 0.000100165
2017-10-11T14:55:59.569025: step 2520, loss 0.167398, acc 0.953125, learning_rate 0.000100164

Evaluation:
2017-10-11T14:55:59.814626: step 2520, loss 0.229023, acc 0.910791

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2520

2017-10-11T14:56:01.624069: step 2521, loss 0.0688905, acc 0.96875, learning_rate 0.000100164
2017-10-11T14:56:01.934904: step 2522, loss 0.27137, acc 0.9375, learning_rate 0.000100163
2017-10-11T14:56:02.261146: step 2523, loss 0.0927261, acc 0.96875, learning_rate 0.000100162
2017-10-11T14:56:02.535741: step 2524, loss 0.0635617, acc 0.96875, learning_rate 0.000100162
2017-10-11T14:56:02.818845: step 2525, loss 0.120207, acc 0.9375, learning_rate 0.000100161
2017-10-11T14:56:03.131552: step 2526, loss 0.107854, acc 0.96875, learning_rate 0.00010016
2017-10-11T14:56:03.387720: step 2527, loss 0.175615, acc 0.953125, learning_rate 0.00010016
2017-10-11T14:56:03.654480: step 2528, loss 0.0188067, acc 1, learning_rate 0.000100159
2017-10-11T14:56:03.974568: step 2529, loss 0.0942235, acc 0.984375, learning_rate 0.000100158
2017-10-11T14:56:04.302939: step 2530, loss 0.182772, acc 0.9375, learning_rate 0.000100158
2017-10-11T14:56:04.607005: step 2531, loss 0.124999, acc 0.96875, learning_rate 0.000100157
2017-10-11T14:56:04.922106: step 2532, loss 0.0880551, acc 0.953125, learning_rate 0.000100156
2017-10-11T14:56:05.185449: step 2533, loss 0.0947676, acc 0.953125, learning_rate 0.000100156
2017-10-11T14:56:05.435914: step 2534, loss 0.0960789, acc 0.953125, learning_rate 0.000100155
2017-10-11T14:56:05.646386: step 2535, loss 0.06705, acc 0.953125, learning_rate 0.000100155
2017-10-11T14:56:05.845178: step 2536, loss 0.115524, acc 0.953125, learning_rate 0.000100154
2017-10-11T14:56:06.060665: step 2537, loss 0.0674468, acc 0.984375, learning_rate 0.000100153
2017-10-11T14:56:06.306787: step 2538, loss 0.0873446, acc 0.96875, learning_rate 0.000100153
2017-10-11T14:56:06.548271: step 2539, loss 0.027672, acc 1, learning_rate 0.000100152
2017-10-11T14:56:06.823675: step 2540, loss 0.183008, acc 0.921875, learning_rate 0.000100151
2017-10-11T14:56:07.124844: step 2541, loss 0.0567423, acc 0.984375, learning_rate 0.000100151
2017-10-11T14:56:07.412984: step 2542, loss 0.0907565, acc 0.96875, learning_rate 0.00010015
2017-10-11T14:56:07.664857: step 2543, loss 0.239323, acc 0.921875, learning_rate 0.00010015
2017-10-11T14:56:08.002611: step 2544, loss 0.0792034, acc 0.96875, learning_rate 0.000100149
2017-10-11T14:56:08.284417: step 2545, loss 0.198616, acc 0.953125, learning_rate 0.000100148
2017-10-11T14:56:08.582459: step 2546, loss 0.234782, acc 0.9375, learning_rate 0.000100148
2017-10-11T14:56:08.881247: step 2547, loss 0.0581571, acc 0.984375, learning_rate 0.000100147
2017-10-11T14:56:09.166289: step 2548, loss 0.075825, acc 0.980392, learning_rate 0.000100147
2017-10-11T14:56:09.508319: step 2549, loss 0.121233, acc 0.953125, learning_rate 0.000100146
2017-10-11T14:56:09.812803: step 2550, loss 0.0591258, acc 0.96875, learning_rate 0.000100145
2017-10-11T14:56:10.116269: step 2551, loss 0.142867, acc 0.953125, learning_rate 0.000100145
2017-10-11T14:56:10.447154: step 2552, loss 0.0657422, acc 1, learning_rate 0.000100144
2017-10-11T14:56:10.779611: step 2553, loss 0.0467894, acc 0.984375, learning_rate 0.000100144
2017-10-11T14:56:11.102554: step 2554, loss 0.155346, acc 0.953125, learning_rate 0.000100143
2017-10-11T14:56:11.371561: step 2555, loss 0.111127, acc 0.953125, learning_rate 0.000100142
2017-10-11T14:56:11.677966: step 2556, loss 0.0584542, acc 0.984375, learning_rate 0.000100142
2017-10-11T14:56:12.002730: step 2557, loss 0.132168, acc 0.953125, learning_rate 0.000100141
2017-10-11T14:56:12.264705: step 2558, loss 0.104802, acc 0.96875, learning_rate 0.000100141
2017-10-11T14:56:12.512716: step 2559, loss 0.113838, acc 0.953125, learning_rate 0.00010014
2017-10-11T14:56:12.799334: step 2560, loss 0.169721, acc 0.953125, learning_rate 0.00010014

Evaluation:
2017-10-11T14:56:13.053765: step 2560, loss 0.228228, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2560

2017-10-11T14:56:15.655566: step 2561, loss 0.163205, acc 0.9375, learning_rate 0.000100139
2017-10-11T14:56:15.999490: step 2562, loss 0.0409164, acc 0.984375, learning_rate 0.000100138
2017-10-11T14:56:16.309523: step 2563, loss 0.0706269, acc 0.96875, learning_rate 0.000100138
2017-10-11T14:56:16.601268: step 2564, loss 0.184086, acc 0.96875, learning_rate 0.000100137
2017-10-11T14:56:16.896144: step 2565, loss 0.0453102, acc 0.984375, learning_rate 0.000100137
2017-10-11T14:56:17.223173: step 2566, loss 0.11143, acc 0.953125, learning_rate 0.000100136
2017-10-11T14:56:17.553576: step 2567, loss 0.115168, acc 0.9375, learning_rate 0.000100136
2017-10-11T14:56:17.836803: step 2568, loss 0.0974149, acc 0.96875, learning_rate 0.000100135
2017-10-11T14:56:18.159410: step 2569, loss 0.113176, acc 0.96875, learning_rate 0.000100134
2017-10-11T14:56:18.462604: step 2570, loss 0.0368921, acc 1, learning_rate 0.000100134
2017-10-11T14:56:18.772403: step 2571, loss 0.0361407, acc 1, learning_rate 0.000100133
2017-10-11T14:56:19.045165: step 2572, loss 0.0952132, acc 0.953125, learning_rate 0.000100133
2017-10-11T14:56:19.301149: step 2573, loss 0.102896, acc 0.9375, learning_rate 0.000100132
2017-10-11T14:56:19.598288: step 2574, loss 0.100721, acc 0.953125, learning_rate 0.000100132
2017-10-11T14:56:19.829326: step 2575, loss 0.119932, acc 0.96875, learning_rate 0.000100131
2017-10-11T14:56:20.040219: step 2576, loss 0.129068, acc 0.96875, learning_rate 0.000100131
2017-10-11T14:56:20.248790: step 2577, loss 0.105671, acc 0.96875, learning_rate 0.00010013
2017-10-11T14:56:20.551595: step 2578, loss 0.171105, acc 0.953125, learning_rate 0.00010013
2017-10-11T14:56:20.849039: step 2579, loss 0.0516534, acc 0.984375, learning_rate 0.000100129
2017-10-11T14:56:21.178944: step 2580, loss 0.0546308, acc 0.984375, learning_rate 0.000100129
2017-10-11T14:56:21.481738: step 2581, loss 0.123813, acc 0.953125, learning_rate 0.000100128
2017-10-11T14:56:21.797885: step 2582, loss 0.0286678, acc 1, learning_rate 0.000100128
2017-10-11T14:56:22.101610: step 2583, loss 0.294362, acc 0.9375, learning_rate 0.000100127
2017-10-11T14:56:22.399863: step 2584, loss 0.144563, acc 0.9375, learning_rate 0.000100126
2017-10-11T14:56:22.738151: step 2585, loss 0.0342726, acc 0.984375, learning_rate 0.000100126
2017-10-11T14:56:23.045659: step 2586, loss 0.028729, acc 1, learning_rate 0.000100125
2017-10-11T14:56:23.315888: step 2587, loss 0.0274585, acc 1, learning_rate 0.000100125
2017-10-11T14:56:23.615908: step 2588, loss 0.0567335, acc 0.984375, learning_rate 0.000100124
2017-10-11T14:56:23.926894: step 2589, loss 0.0887715, acc 0.96875, learning_rate 0.000100124
2017-10-11T14:56:24.227225: step 2590, loss 0.0452359, acc 0.984375, learning_rate 0.000100123
2017-10-11T14:56:24.543652: step 2591, loss 0.0926354, acc 0.96875, learning_rate 0.000100123
2017-10-11T14:56:24.863614: step 2592, loss 0.0913277, acc 0.96875, learning_rate 0.000100122
2017-10-11T14:56:25.173764: step 2593, loss 0.207238, acc 0.9375, learning_rate 0.000100122
2017-10-11T14:56:25.482424: step 2594, loss 0.0455312, acc 1, learning_rate 0.000100121
2017-10-11T14:56:25.767904: step 2595, loss 0.0949793, acc 0.9375, learning_rate 0.000100121
2017-10-11T14:56:26.075741: step 2596, loss 0.143889, acc 0.953125, learning_rate 0.00010012
2017-10-11T14:56:26.361264: step 2597, loss 0.0493781, acc 1, learning_rate 0.00010012
2017-10-11T14:56:26.652746: step 2598, loss 0.0834597, acc 0.953125, learning_rate 0.000100119
2017-10-11T14:56:26.909371: step 2599, loss 0.0928505, acc 0.953125, learning_rate 0.000100119
2017-10-11T14:56:27.201506: step 2600, loss 0.07394, acc 0.96875, learning_rate 0.000100118

Evaluation:
2017-10-11T14:56:27.466279: step 2600, loss 0.229025, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2600

2017-10-11T14:56:30.374183: step 2601, loss 0.0987896, acc 0.953125, learning_rate 0.000100118
2017-10-11T14:56:30.671771: step 2602, loss 0.0365537, acc 1, learning_rate 0.000100117
2017-10-11T14:56:30.953227: step 2603, loss 0.0677456, acc 0.984375, learning_rate 0.000100117
2017-10-11T14:56:31.247843: step 2604, loss 0.148026, acc 0.9375, learning_rate 0.000100117
2017-10-11T14:56:31.567309: step 2605, loss 0.0419189, acc 0.984375, learning_rate 0.000100116
2017-10-11T14:56:31.855703: step 2606, loss 0.0443379, acc 1, learning_rate 0.000100116
2017-10-11T14:56:32.152016: step 2607, loss 0.120269, acc 0.953125, learning_rate 0.000100115
2017-10-11T14:56:32.417240: step 2608, loss 0.13242, acc 0.953125, learning_rate 0.000100115
2017-10-11T14:56:32.669943: step 2609, loss 0.151757, acc 0.96875, learning_rate 0.000100114
2017-10-11T14:56:32.910179: step 2610, loss 0.0844272, acc 0.984375, learning_rate 0.000100114
2017-10-11T14:56:33.169538: step 2611, loss 0.294914, acc 0.890625, learning_rate 0.000100113
2017-10-11T14:56:33.406093: step 2612, loss 0.0293998, acc 1, learning_rate 0.000100113
2017-10-11T14:56:33.658167: step 2613, loss 0.123208, acc 0.9375, learning_rate 0.000100112
2017-10-11T14:56:33.900581: step 2614, loss 0.218406, acc 0.921875, learning_rate 0.000100112
2017-10-11T14:56:34.150435: step 2615, loss 0.11647, acc 0.921875, learning_rate 0.000100111
2017-10-11T14:56:34.376922: step 2616, loss 0.125688, acc 0.953125, learning_rate 0.000100111
2017-10-11T14:56:34.652512: step 2617, loss 0.141146, acc 0.921875, learning_rate 0.000100111
2017-10-11T14:56:34.926877: step 2618, loss 0.0545687, acc 0.984375, learning_rate 0.00010011
2017-10-11T14:56:35.211891: step 2619, loss 0.149174, acc 0.953125, learning_rate 0.00010011
2017-10-11T14:56:35.509089: step 2620, loss 0.116321, acc 0.953125, learning_rate 0.000100109
2017-10-11T14:56:35.816765: step 2621, loss 0.0747998, acc 0.984375, learning_rate 0.000100109
2017-10-11T14:56:36.128259: step 2622, loss 0.101409, acc 0.96875, learning_rate 0.000100108
2017-10-11T14:56:36.456770: step 2623, loss 0.111249, acc 0.953125, learning_rate 0.000100108
2017-10-11T14:56:36.763089: step 2624, loss 0.105109, acc 0.953125, learning_rate 0.000100107
2017-10-11T14:56:37.052072: step 2625, loss 0.0578367, acc 0.984375, learning_rate 0.000100107
2017-10-11T14:56:37.356706: step 2626, loss 0.0480317, acc 0.984375, learning_rate 0.000100107
2017-10-11T14:56:37.647492: step 2627, loss 0.0848833, acc 0.96875, learning_rate 0.000100106
2017-10-11T14:56:37.948019: step 2628, loss 0.088844, acc 0.984375, learning_rate 0.000100106
2017-10-11T14:56:38.235634: step 2629, loss 0.111887, acc 0.9375, learning_rate 0.000100105
2017-10-11T14:56:38.553239: step 2630, loss 0.162449, acc 0.9375, learning_rate 0.000100105
2017-10-11T14:56:38.869072: step 2631, loss 0.0985643, acc 0.96875, learning_rate 0.000100104
2017-10-11T14:56:39.210113: step 2632, loss 0.0663376, acc 0.984375, learning_rate 0.000100104
2017-10-11T14:56:39.544353: step 2633, loss 0.037174, acc 1, learning_rate 0.000100104
2017-10-11T14:56:39.853487: step 2634, loss 0.0930958, acc 0.9375, learning_rate 0.000100103
2017-10-11T14:56:40.138181: step 2635, loss 0.062079, acc 0.96875, learning_rate 0.000100103
2017-10-11T14:56:40.362868: step 2636, loss 0.153865, acc 0.96875, learning_rate 0.000100102
2017-10-11T14:56:40.649617: step 2637, loss 0.0658035, acc 0.984375, learning_rate 0.000100102
2017-10-11T14:56:40.951596: step 2638, loss 0.0532535, acc 0.984375, learning_rate 0.000100101
2017-10-11T14:56:41.228406: step 2639, loss 0.119358, acc 0.9375, learning_rate 0.000100101
2017-10-11T14:56:41.497606: step 2640, loss 0.139966, acc 0.953125, learning_rate 0.000100101

Evaluation:
2017-10-11T14:56:41.741051: step 2640, loss 0.225929, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2640

2017-10-11T14:56:43.488871: step 2641, loss 0.201938, acc 0.9375, learning_rate 0.0001001
2017-10-11T14:56:43.833938: step 2642, loss 0.0874183, acc 0.984375, learning_rate 0.0001001
2017-10-11T14:56:44.141685: step 2643, loss 0.102673, acc 0.9375, learning_rate 0.000100099
2017-10-11T14:56:44.472426: step 2644, loss 0.107917, acc 0.96875, learning_rate 0.000100099
2017-10-11T14:56:44.796194: step 2645, loss 0.102991, acc 0.953125, learning_rate 0.000100099
2017-10-11T14:56:45.108039: step 2646, loss 0.0840834, acc 0.960784, learning_rate 0.000100098
2017-10-11T14:56:45.416156: step 2647, loss 0.133413, acc 0.9375, learning_rate 0.000100098
2017-10-11T14:56:45.755906: step 2648, loss 0.0370051, acc 0.984375, learning_rate 0.000100097
2017-10-11T14:56:46.073852: step 2649, loss 0.115931, acc 0.953125, learning_rate 0.000100097
2017-10-11T14:56:46.387995: step 2650, loss 0.0908134, acc 0.984375, learning_rate 0.000100097
2017-10-11T14:56:46.689785: step 2651, loss 0.103926, acc 0.953125, learning_rate 0.000100096
2017-10-11T14:56:46.957826: step 2652, loss 0.170972, acc 0.921875, learning_rate 0.000100096
2017-10-11T14:56:47.233250: step 2653, loss 0.153559, acc 0.9375, learning_rate 0.000100095
2017-10-11T14:56:47.498216: step 2654, loss 0.0696147, acc 0.96875, learning_rate 0.000100095
2017-10-11T14:56:47.718420: step 2655, loss 0.0804249, acc 0.984375, learning_rate 0.000100095
2017-10-11T14:56:47.950204: step 2656, loss 0.0780872, acc 0.984375, learning_rate 0.000100094
2017-10-11T14:56:48.180151: step 2657, loss 0.179348, acc 0.953125, learning_rate 0.000100094
2017-10-11T14:56:48.385803: step 2658, loss 0.304881, acc 0.890625, learning_rate 0.000100093
2017-10-11T14:56:48.665367: step 2659, loss 0.0653333, acc 0.96875, learning_rate 0.000100093
2017-10-11T14:56:48.957708: step 2660, loss 0.0576401, acc 0.984375, learning_rate 0.000100093
2017-10-11T14:56:49.262479: step 2661, loss 0.0615532, acc 0.984375, learning_rate 0.000100092
2017-10-11T14:56:49.564930: step 2662, loss 0.130412, acc 0.96875, learning_rate 0.000100092
2017-10-11T14:56:49.875511: step 2663, loss 0.0436417, acc 0.984375, learning_rate 0.000100092
2017-10-11T14:56:50.204496: step 2664, loss 0.19248, acc 0.9375, learning_rate 0.000100091
2017-10-11T14:56:50.496400: step 2665, loss 0.105598, acc 0.96875, learning_rate 0.000100091
2017-10-11T14:56:50.783562: step 2666, loss 0.113616, acc 0.9375, learning_rate 0.00010009
2017-10-11T14:56:51.088018: step 2667, loss 0.0951495, acc 0.984375, learning_rate 0.00010009
2017-10-11T14:56:51.385556: step 2668, loss 0.178262, acc 0.9375, learning_rate 0.00010009
2017-10-11T14:56:51.660412: step 2669, loss 0.152712, acc 0.9375, learning_rate 0.000100089
2017-10-11T14:56:51.971903: step 2670, loss 0.0913408, acc 0.96875, learning_rate 0.000100089
2017-10-11T14:56:52.274773: step 2671, loss 0.146954, acc 0.953125, learning_rate 0.000100089
2017-10-11T14:56:52.537470: step 2672, loss 0.0866892, acc 0.96875, learning_rate 0.000100088
2017-10-11T14:56:52.866766: step 2673, loss 0.0604981, acc 0.96875, learning_rate 0.000100088
2017-10-11T14:56:53.172688: step 2674, loss 0.021743, acc 1, learning_rate 0.000100088
2017-10-11T14:56:53.477545: step 2675, loss 0.105549, acc 0.953125, learning_rate 0.000100087
2017-10-11T14:56:53.776864: step 2676, loss 0.155497, acc 0.984375, learning_rate 0.000100087
2017-10-11T14:56:54.084687: step 2677, loss 0.125101, acc 0.953125, learning_rate 0.000100086
2017-10-11T14:56:54.401994: step 2678, loss 0.0949086, acc 0.953125, learning_rate 0.000100086
2017-10-11T14:56:54.686375: step 2679, loss 0.0377388, acc 0.984375, learning_rate 0.000100086
2017-10-11T14:56:54.974561: step 2680, loss 0.108569, acc 0.953125, learning_rate 0.000100085

Evaluation:
2017-10-11T14:56:55.257965: step 2680, loss 0.225492, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2680

2017-10-11T14:56:58.021258: step 2681, loss 0.0563454, acc 0.984375, learning_rate 0.000100085
2017-10-11T14:56:58.314807: step 2682, loss 0.0549447, acc 0.984375, learning_rate 0.000100085
2017-10-11T14:56:58.579065: step 2683, loss 0.0390369, acc 0.984375, learning_rate 0.000100084
2017-10-11T14:56:58.869216: step 2684, loss 0.0394694, acc 0.984375, learning_rate 0.000100084
2017-10-11T14:56:59.120681: step 2685, loss 0.150446, acc 0.953125, learning_rate 0.000100084
2017-10-11T14:56:59.433115: step 2686, loss 0.1164, acc 0.96875, learning_rate 0.000100083
2017-10-11T14:56:59.726109: step 2687, loss 0.0642691, acc 0.984375, learning_rate 0.000100083
2017-10-11T14:57:00.053626: step 2688, loss 0.0496898, acc 1, learning_rate 0.000100083
2017-10-11T14:57:00.348278: step 2689, loss 0.0546555, acc 0.984375, learning_rate 0.000100082
2017-10-11T14:57:00.591127: step 2690, loss 0.125831, acc 0.96875, learning_rate 0.000100082
2017-10-11T14:57:00.805396: step 2691, loss 0.0584501, acc 0.984375, learning_rate 0.000100082
2017-10-11T14:57:01.017517: step 2692, loss 0.0830514, acc 0.953125, learning_rate 0.000100081
2017-10-11T14:57:01.252064: step 2693, loss 0.154347, acc 0.921875, learning_rate 0.000100081
2017-10-11T14:57:01.474802: step 2694, loss 0.100361, acc 0.96875, learning_rate 0.000100081
2017-10-11T14:57:01.704182: step 2695, loss 0.10512, acc 0.96875, learning_rate 0.00010008
2017-10-11T14:57:01.929715: step 2696, loss 0.0847159, acc 0.96875, learning_rate 0.00010008
2017-10-11T14:57:02.205574: step 2697, loss 0.0966528, acc 0.96875, learning_rate 0.00010008
2017-10-11T14:57:02.553108: step 2698, loss 0.199123, acc 0.90625, learning_rate 0.000100079
2017-10-11T14:57:02.850885: step 2699, loss 0.0499221, acc 0.984375, learning_rate 0.000100079
2017-10-11T14:57:03.145147: step 2700, loss 0.0665078, acc 0.984375, learning_rate 0.000100079
2017-10-11T14:57:03.455678: step 2701, loss 0.039694, acc 0.984375, learning_rate 0.000100078
2017-10-11T14:57:03.747710: step 2702, loss 0.171364, acc 0.9375, learning_rate 0.000100078
2017-10-11T14:57:04.037603: step 2703, loss 0.117243, acc 0.953125, learning_rate 0.000100078
2017-10-11T14:57:04.356102: step 2704, loss 0.104344, acc 0.96875, learning_rate 0.000100077
2017-10-11T14:57:04.635391: step 2705, loss 0.0569945, acc 1, learning_rate 0.000100077
2017-10-11T14:57:04.909192: step 2706, loss 0.228493, acc 0.921875, learning_rate 0.000100077
2017-10-11T14:57:05.177593: step 2707, loss 0.099817, acc 0.953125, learning_rate 0.000100076
2017-10-11T14:57:05.491945: step 2708, loss 0.0865568, acc 0.96875, learning_rate 0.000100076
2017-10-11T14:57:05.778574: step 2709, loss 0.0473215, acc 0.96875, learning_rate 0.000100076
2017-10-11T14:57:06.106483: step 2710, loss 0.0317326, acc 1, learning_rate 0.000100076
2017-10-11T14:57:06.403492: step 2711, loss 0.142108, acc 0.9375, learning_rate 0.000100075
2017-10-11T14:57:06.718918: step 2712, loss 0.0565611, acc 0.96875, learning_rate 0.000100075
2017-10-11T14:57:07.053081: step 2713, loss 0.165648, acc 0.9375, learning_rate 0.000100075
2017-10-11T14:57:07.345098: step 2714, loss 0.120157, acc 0.953125, learning_rate 0.000100074
2017-10-11T14:57:07.670031: step 2715, loss 0.0639974, acc 0.96875, learning_rate 0.000100074
2017-10-11T14:57:07.962620: step 2716, loss 0.0962148, acc 0.96875, learning_rate 0.000100074
2017-10-11T14:57:08.269086: step 2717, loss 0.122935, acc 0.96875, learning_rate 0.000100073
2017-10-11T14:57:08.507702: step 2718, loss 0.0630838, acc 0.984375, learning_rate 0.000100073
2017-10-11T14:57:08.763355: step 2719, loss 0.287748, acc 0.84375, learning_rate 0.000100073
2017-10-11T14:57:09.022005: step 2720, loss 0.0958398, acc 0.953125, learning_rate 0.000100073

Evaluation:
2017-10-11T14:57:09.263455: step 2720, loss 0.226223, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2720

2017-10-11T14:57:12.109288: step 2721, loss 0.081582, acc 0.953125, learning_rate 0.000100072
2017-10-11T14:57:12.416018: step 2722, loss 0.0504236, acc 0.984375, learning_rate 0.000100072
2017-10-11T14:57:12.688289: step 2723, loss 0.0605421, acc 0.984375, learning_rate 0.000100072
2017-10-11T14:57:12.975684: step 2724, loss 0.0828126, acc 0.984375, learning_rate 0.000100071
2017-10-11T14:57:13.278334: step 2725, loss 0.0464591, acc 1, learning_rate 0.000100071
2017-10-11T14:57:13.572435: step 2726, loss 0.0917728, acc 0.953125, learning_rate 0.000100071
2017-10-11T14:57:13.859222: step 2727, loss 0.0280893, acc 1, learning_rate 0.00010007
2017-10-11T14:57:14.102766: step 2728, loss 0.0333012, acc 0.984375, learning_rate 0.00010007
2017-10-11T14:57:14.325154: step 2729, loss 0.194313, acc 0.953125, learning_rate 0.00010007
2017-10-11T14:57:14.520109: step 2730, loss 0.147401, acc 0.953125, learning_rate 0.00010007
2017-10-11T14:57:14.728812: step 2731, loss 0.130611, acc 0.9375, learning_rate 0.000100069
2017-10-11T14:57:14.946000: step 2732, loss 0.104341, acc 0.96875, learning_rate 0.000100069
2017-10-11T14:57:15.203102: step 2733, loss 0.0992211, acc 0.9375, learning_rate 0.000100069
2017-10-11T14:57:15.444110: step 2734, loss 0.256042, acc 0.921875, learning_rate 0.000100068
2017-10-11T14:57:15.719509: step 2735, loss 0.0756632, acc 0.96875, learning_rate 0.000100068
2017-10-11T14:57:15.983132: step 2736, loss 0.166107, acc 0.953125, learning_rate 0.000100068
2017-10-11T14:57:16.311433: step 2737, loss 0.0860778, acc 0.96875, learning_rate 0.000100068
2017-10-11T14:57:16.670540: step 2738, loss 0.121422, acc 0.953125, learning_rate 0.000100067
2017-10-11T14:57:17.036913: step 2739, loss 0.142617, acc 0.96875, learning_rate 0.000100067
2017-10-11T14:57:17.372235: step 2740, loss 0.101749, acc 0.96875, learning_rate 0.000100067
2017-10-11T14:57:17.685307: step 2741, loss 0.203481, acc 0.953125, learning_rate 0.000100067
2017-10-11T14:57:18.010527: step 2742, loss 0.126952, acc 0.953125, learning_rate 0.000100066
2017-10-11T14:57:18.340196: step 2743, loss 0.117752, acc 0.96875, learning_rate 0.000100066
2017-10-11T14:57:18.656225: step 2744, loss 0.098314, acc 0.980392, learning_rate 0.000100066
2017-10-11T14:57:18.985484: step 2745, loss 0.114142, acc 0.96875, learning_rate 0.000100065
2017-10-11T14:57:19.301278: step 2746, loss 0.121187, acc 0.953125, learning_rate 0.000100065
2017-10-11T14:57:19.601347: step 2747, loss 0.183347, acc 0.90625, learning_rate 0.000100065
2017-10-11T14:57:19.907381: step 2748, loss 0.0993368, acc 0.96875, learning_rate 0.000100065
2017-10-11T14:57:20.250426: step 2749, loss 0.0487991, acc 0.984375, learning_rate 0.000100064
2017-10-11T14:57:20.580844: step 2750, loss 0.113574, acc 0.984375, learning_rate 0.000100064
2017-10-11T14:57:20.897362: step 2751, loss 0.146785, acc 0.96875, learning_rate 0.000100064
2017-10-11T14:57:21.190611: step 2752, loss 0.143391, acc 0.9375, learning_rate 0.000100064
2017-10-11T14:57:21.524814: step 2753, loss 0.069588, acc 0.984375, learning_rate 0.000100063
2017-10-11T14:57:21.830730: step 2754, loss 0.072848, acc 0.953125, learning_rate 0.000100063
2017-10-11T14:57:22.139845: step 2755, loss 0.0724253, acc 0.984375, learning_rate 0.000100063
2017-10-11T14:57:22.413535: step 2756, loss 0.103466, acc 0.96875, learning_rate 0.000100063
2017-10-11T14:57:22.667904: step 2757, loss 0.197078, acc 0.9375, learning_rate 0.000100062
2017-10-11T14:57:22.935616: step 2758, loss 0.0610265, acc 0.96875, learning_rate 0.000100062
2017-10-11T14:57:23.181933: step 2759, loss 0.158979, acc 0.921875, learning_rate 0.000100062
2017-10-11T14:57:23.441850: step 2760, loss 0.0402881, acc 0.984375, learning_rate 0.000100062

Evaluation:
2017-10-11T14:57:23.732305: step 2760, loss 0.227064, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2760

2017-10-11T14:57:25.707198: step 2761, loss 0.0307714, acc 1, learning_rate 0.000100061
2017-10-11T14:57:26.025880: step 2762, loss 0.105357, acc 0.984375, learning_rate 0.000100061
2017-10-11T14:57:26.339312: step 2763, loss 0.148263, acc 0.921875, learning_rate 0.000100061
2017-10-11T14:57:26.644733: step 2764, loss 0.0841129, acc 0.96875, learning_rate 0.000100061
2017-10-11T14:57:26.937842: step 2765, loss 0.129685, acc 0.953125, learning_rate 0.00010006
2017-10-11T14:57:27.226443: step 2766, loss 0.0714926, acc 0.96875, learning_rate 0.00010006
2017-10-11T14:57:27.486421: step 2767, loss 0.182919, acc 0.921875, learning_rate 0.00010006
2017-10-11T14:57:27.759195: step 2768, loss 0.117354, acc 0.953125, learning_rate 0.00010006
2017-10-11T14:57:28.006272: step 2769, loss 0.0898052, acc 0.96875, learning_rate 0.000100059
2017-10-11T14:57:28.281678: step 2770, loss 0.156346, acc 0.9375, learning_rate 0.000100059
2017-10-11T14:57:28.517765: step 2771, loss 0.199389, acc 0.953125, learning_rate 0.000100059
2017-10-11T14:57:28.745476: step 2772, loss 0.0166383, acc 1, learning_rate 0.000100059
2017-10-11T14:57:28.978697: step 2773, loss 0.0813579, acc 0.953125, learning_rate 0.000100058
2017-10-11T14:57:29.187313: step 2774, loss 0.137428, acc 0.953125, learning_rate 0.000100058
2017-10-11T14:57:29.495520: step 2775, loss 0.124869, acc 0.953125, learning_rate 0.000100058
2017-10-11T14:57:29.792123: step 2776, loss 0.171692, acc 0.9375, learning_rate 0.000100058
2017-10-11T14:57:30.071503: step 2777, loss 0.11204, acc 0.96875, learning_rate 0.000100057
2017-10-11T14:57:30.347461: step 2778, loss 0.112436, acc 0.96875, learning_rate 0.000100057
2017-10-11T14:57:30.647003: step 2779, loss 0.0749465, acc 0.96875, learning_rate 0.000100057
2017-10-11T14:57:31.020508: step 2780, loss 0.166652, acc 0.90625, learning_rate 0.000100057
2017-10-11T14:57:31.385935: step 2781, loss 0.0784332, acc 0.984375, learning_rate 0.000100056
2017-10-11T14:57:31.702949: step 2782, loss 0.162518, acc 0.9375, learning_rate 0.000100056
2017-10-11T14:57:32.002475: step 2783, loss 0.0824453, acc 0.96875, learning_rate 0.000100056
2017-10-11T14:57:32.281171: step 2784, loss 0.0646515, acc 0.984375, learning_rate 0.000100056
2017-10-11T14:57:32.608916: step 2785, loss 0.108357, acc 0.953125, learning_rate 0.000100056
2017-10-11T14:57:32.907041: step 2786, loss 0.15293, acc 0.9375, learning_rate 0.000100055
2017-10-11T14:57:33.215099: step 2787, loss 0.10397, acc 0.984375, learning_rate 0.000100055
2017-10-11T14:57:33.533401: step 2788, loss 0.106375, acc 0.953125, learning_rate 0.000100055
2017-10-11T14:57:33.831694: step 2789, loss 0.0968527, acc 0.96875, learning_rate 0.000100055
2017-10-11T14:57:34.129530: step 2790, loss 0.0711901, acc 0.96875, learning_rate 0.000100054
2017-10-11T14:57:34.427748: step 2791, loss 0.0401977, acc 0.984375, learning_rate 0.000100054
2017-10-11T14:57:34.752473: step 2792, loss 0.0561202, acc 0.984375, learning_rate 0.000100054
2017-10-11T14:57:35.047432: step 2793, loss 0.181987, acc 0.953125, learning_rate 0.000100054
2017-10-11T14:57:35.346456: step 2794, loss 0.0853463, acc 0.984375, learning_rate 0.000100054
2017-10-11T14:57:35.582563: step 2795, loss 0.158831, acc 0.9375, learning_rate 0.000100053
2017-10-11T14:57:35.844903: step 2796, loss 0.125545, acc 0.9375, learning_rate 0.000100053
2017-10-11T14:57:36.108283: step 2797, loss 0.0223868, acc 1, learning_rate 0.000100053
2017-10-11T14:57:36.379507: step 2798, loss 0.103142, acc 0.96875, learning_rate 0.000100053
2017-10-11T14:57:36.633568: step 2799, loss 0.159354, acc 0.921875, learning_rate 0.000100052
2017-10-11T14:57:36.890254: step 2800, loss 0.0816665, acc 0.96875, learning_rate 0.000100052

Evaluation:
2017-10-11T14:57:37.158317: step 2800, loss 0.226355, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2800

2017-10-11T14:57:40.480192: step 2801, loss 0.0432391, acc 1, learning_rate 0.000100052
2017-10-11T14:57:40.762016: step 2802, loss 0.0892326, acc 0.96875, learning_rate 0.000100052
2017-10-11T14:57:41.105190: step 2803, loss 0.0660373, acc 0.96875, learning_rate 0.000100052
2017-10-11T14:57:41.366904: step 2804, loss 0.0623589, acc 0.984375, learning_rate 0.000100051
2017-10-11T14:57:41.635953: step 2805, loss 0.0776915, acc 0.96875, learning_rate 0.000100051
2017-10-11T14:57:41.888819: step 2806, loss 0.165901, acc 0.953125, learning_rate 0.000100051
2017-10-11T14:57:42.151979: step 2807, loss 0.0650707, acc 0.984375, learning_rate 0.000100051
2017-10-11T14:57:42.409941: step 2808, loss 0.0642921, acc 0.96875, learning_rate 0.000100051
2017-10-11T14:57:42.616311: step 2809, loss 0.114749, acc 0.96875, learning_rate 0.00010005
2017-10-11T14:57:42.856603: step 2810, loss 0.0808438, acc 0.96875, learning_rate 0.00010005
2017-10-11T14:57:43.081991: step 2811, loss 0.108495, acc 0.953125, learning_rate 0.00010005
2017-10-11T14:57:43.280261: step 2812, loss 0.18714, acc 0.9375, learning_rate 0.00010005
2017-10-11T14:57:43.582868: step 2813, loss 0.0903667, acc 0.953125, learning_rate 0.00010005
2017-10-11T14:57:43.867324: step 2814, loss 0.105234, acc 0.953125, learning_rate 0.000100049
2017-10-11T14:57:44.198587: step 2815, loss 0.171496, acc 0.953125, learning_rate 0.000100049
2017-10-11T14:57:44.514260: step 2816, loss 0.0709911, acc 0.984375, learning_rate 0.000100049
2017-10-11T14:57:44.798440: step 2817, loss 0.0663527, acc 0.96875, learning_rate 0.000100049
2017-10-11T14:57:45.134279: step 2818, loss 0.0803996, acc 0.96875, learning_rate 0.000100049
2017-10-11T14:57:45.428772: step 2819, loss 0.0244002, acc 1, learning_rate 0.000100048
2017-10-11T14:57:45.733938: step 2820, loss 0.049205, acc 1, learning_rate 0.000100048
2017-10-11T14:57:46.056494: step 2821, loss 0.164629, acc 0.96875, learning_rate 0.000100048
2017-10-11T14:57:46.379485: step 2822, loss 0.077425, acc 0.96875, learning_rate 0.000100048
2017-10-11T14:57:46.730735: step 2823, loss 0.0569982, acc 0.984375, learning_rate 0.000100048
2017-10-11T14:57:47.044385: step 2824, loss 0.0970014, acc 0.96875, learning_rate 0.000100047
2017-10-11T14:57:47.304390: step 2825, loss 0.117273, acc 0.953125, learning_rate 0.000100047
2017-10-11T14:57:47.637420: step 2826, loss 0.0480469, acc 1, learning_rate 0.000100047
2017-10-11T14:57:47.954123: step 2827, loss 0.0550396, acc 0.984375, learning_rate 0.000100047
2017-10-11T14:57:48.288483: step 2828, loss 0.0935556, acc 0.984375, learning_rate 0.000100047
2017-10-11T14:57:48.617346: step 2829, loss 0.0979338, acc 0.96875, learning_rate 0.000100046
2017-10-11T14:57:48.885543: step 2830, loss 0.0972972, acc 0.953125, learning_rate 0.000100046
2017-10-11T14:57:49.163118: step 2831, loss 0.086259, acc 0.953125, learning_rate 0.000100046
2017-10-11T14:57:49.430995: step 2832, loss 0.0969242, acc 0.96875, learning_rate 0.000100046
2017-10-11T14:57:49.711040: step 2833, loss 0.11657, acc 0.953125, learning_rate 0.000100046
2017-10-11T14:57:50.016162: step 2834, loss 0.10531, acc 0.953125, learning_rate 0.000100045
2017-10-11T14:57:50.311333: step 2835, loss 0.0933665, acc 0.984375, learning_rate 0.000100045
2017-10-11T14:57:50.587049: step 2836, loss 0.113253, acc 0.96875, learning_rate 0.000100045
2017-10-11T14:57:50.899549: step 2837, loss 0.155674, acc 0.921875, learning_rate 0.000100045
2017-10-11T14:57:51.225192: step 2838, loss 0.153791, acc 0.921875, learning_rate 0.000100045
2017-10-11T14:57:51.536320: step 2839, loss 0.106584, acc 0.953125, learning_rate 0.000100045
2017-10-11T14:57:51.838687: step 2840, loss 0.042059, acc 0.984375, learning_rate 0.000100044

Evaluation:
2017-10-11T14:57:52.154210: step 2840, loss 0.22507, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2840

2017-10-11T14:57:54.335010: step 2841, loss 0.123817, acc 0.9375, learning_rate 0.000100044
2017-10-11T14:57:54.625767: step 2842, loss 0.171495, acc 0.921569, learning_rate 0.000100044
2017-10-11T14:57:54.951578: step 2843, loss 0.106311, acc 0.953125, learning_rate 0.000100044
2017-10-11T14:57:55.304398: step 2844, loss 0.116938, acc 0.953125, learning_rate 0.000100044
2017-10-11T14:57:55.623785: step 2845, loss 0.130115, acc 0.96875, learning_rate 0.000100043
2017-10-11T14:57:55.890134: step 2846, loss 0.0447855, acc 0.984375, learning_rate 0.000100043
2017-10-11T14:57:56.161940: step 2847, loss 0.123325, acc 0.921875, learning_rate 0.000100043
2017-10-11T14:57:56.378198: step 2848, loss 0.0746637, acc 0.984375, learning_rate 0.000100043
2017-10-11T14:57:56.616388: step 2849, loss 0.109334, acc 0.9375, learning_rate 0.000100043
2017-10-11T14:57:56.838194: step 2850, loss 0.121952, acc 0.953125, learning_rate 0.000100043
2017-10-11T14:57:57.071644: step 2851, loss 0.0823518, acc 0.984375, learning_rate 0.000100042
2017-10-11T14:57:57.352174: step 2852, loss 0.139166, acc 0.953125, learning_rate 0.000100042
2017-10-11T14:57:57.630044: step 2853, loss 0.0973129, acc 0.96875, learning_rate 0.000100042
2017-10-11T14:57:57.866342: step 2854, loss 0.0658084, acc 0.984375, learning_rate 0.000100042
2017-10-11T14:57:58.144741: step 2855, loss 0.0987785, acc 0.984375, learning_rate 0.000100042
2017-10-11T14:57:58.435735: step 2856, loss 0.0883511, acc 0.96875, learning_rate 0.000100042
2017-10-11T14:57:58.789574: step 2857, loss 0.0924311, acc 0.953125, learning_rate 0.000100041
2017-10-11T14:57:59.079684: step 2858, loss 0.158454, acc 0.953125, learning_rate 0.000100041
2017-10-11T14:57:59.393747: step 2859, loss 0.0618916, acc 0.96875, learning_rate 0.000100041
2017-10-11T14:57:59.676880: step 2860, loss 0.0501759, acc 0.984375, learning_rate 0.000100041
2017-10-11T14:57:59.955642: step 2861, loss 0.0663342, acc 1, learning_rate 0.000100041
2017-10-11T14:58:00.241981: step 2862, loss 0.0989328, acc 0.96875, learning_rate 0.000100041
2017-10-11T14:58:00.560427: step 2863, loss 0.114111, acc 0.96875, learning_rate 0.00010004
2017-10-11T14:58:00.876897: step 2864, loss 0.145504, acc 0.96875, learning_rate 0.00010004
2017-10-11T14:58:01.140111: step 2865, loss 0.0456932, acc 0.984375, learning_rate 0.00010004
2017-10-11T14:58:01.440355: step 2866, loss 0.0518625, acc 0.984375, learning_rate 0.00010004
2017-10-11T14:58:01.732757: step 2867, loss 0.0571611, acc 0.984375, learning_rate 0.00010004
2017-10-11T14:58:02.037013: step 2868, loss 0.0491205, acc 1, learning_rate 0.00010004
2017-10-11T14:58:02.315410: step 2869, loss 0.143821, acc 0.953125, learning_rate 0.000100039
2017-10-11T14:58:02.625476: step 2870, loss 0.0466884, acc 0.984375, learning_rate 0.000100039
2017-10-11T14:58:02.907629: step 2871, loss 0.059014, acc 0.984375, learning_rate 0.000100039
2017-10-11T14:58:03.166724: step 2872, loss 0.0458958, acc 1, learning_rate 0.000100039
2017-10-11T14:58:03.429271: step 2873, loss 0.139719, acc 0.9375, learning_rate 0.000100039
2017-10-11T14:58:03.705437: step 2874, loss 0.14198, acc 0.96875, learning_rate 0.000100039
2017-10-11T14:58:03.950931: step 2875, loss 0.162662, acc 0.9375, learning_rate 0.000100038
2017-10-11T14:58:04.233356: step 2876, loss 0.0577154, acc 0.984375, learning_rate 0.000100038
2017-10-11T14:58:04.574825: step 2877, loss 0.0260399, acc 1, learning_rate 0.000100038
2017-10-11T14:58:04.852129: step 2878, loss 0.0592147, acc 0.984375, learning_rate 0.000100038
2017-10-11T14:58:05.158619: step 2879, loss 0.0745276, acc 1, learning_rate 0.000100038
2017-10-11T14:58:05.473488: step 2880, loss 0.0822166, acc 0.96875, learning_rate 0.000100038

Evaluation:
2017-10-11T14:58:05.783920: step 2880, loss 0.22491, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2880

2017-10-11T14:58:08.012242: step 2881, loss 0.063731, acc 0.984375, learning_rate 0.000100038
2017-10-11T14:58:08.313925: step 2882, loss 0.0357534, acc 1, learning_rate 0.000100037
2017-10-11T14:58:08.631119: step 2883, loss 0.178899, acc 0.953125, learning_rate 0.000100037
2017-10-11T14:58:08.920532: step 2884, loss 0.0626033, acc 0.953125, learning_rate 0.000100037
2017-10-11T14:58:09.182407: step 2885, loss 0.0489067, acc 0.984375, learning_rate 0.000100037
2017-10-11T14:58:09.469326: step 2886, loss 0.0202246, acc 1, learning_rate 0.000100037
2017-10-11T14:58:09.733665: step 2887, loss 0.166018, acc 0.96875, learning_rate 0.000100037
2017-10-11T14:58:10.007322: step 2888, loss 0.113481, acc 0.9375, learning_rate 0.000100036
2017-10-11T14:58:10.256609: step 2889, loss 0.0799813, acc 0.984375, learning_rate 0.000100036
2017-10-11T14:58:10.459603: step 2890, loss 0.057063, acc 0.984375, learning_rate 0.000100036
2017-10-11T14:58:10.688441: step 2891, loss 0.215889, acc 0.9375, learning_rate 0.000100036
2017-10-11T14:58:10.965704: step 2892, loss 0.0261628, acc 1, learning_rate 0.000100036
2017-10-11T14:58:11.245627: step 2893, loss 0.0671904, acc 0.96875, learning_rate 0.000100036
2017-10-11T14:58:11.577239: step 2894, loss 0.0841506, acc 0.96875, learning_rate 0.000100036
2017-10-11T14:58:11.909714: step 2895, loss 0.0663842, acc 0.96875, learning_rate 0.000100035
2017-10-11T14:58:12.218860: step 2896, loss 0.164427, acc 0.9375, learning_rate 0.000100035
2017-10-11T14:58:12.498325: step 2897, loss 0.116658, acc 0.953125, learning_rate 0.000100035
2017-10-11T14:58:12.806574: step 2898, loss 0.157776, acc 0.96875, learning_rate 0.000100035
2017-10-11T14:58:13.092407: step 2899, loss 0.0592838, acc 0.96875, learning_rate 0.000100035
2017-10-11T14:58:13.409191: step 2900, loss 0.127351, acc 0.96875, learning_rate 0.000100035
2017-10-11T14:58:13.704011: step 2901, loss 0.0396698, acc 1, learning_rate 0.000100035
2017-10-11T14:58:13.963849: step 2902, loss 0.0696722, acc 0.96875, learning_rate 0.000100034
2017-10-11T14:58:14.299179: step 2903, loss 0.0612129, acc 0.96875, learning_rate 0.000100034
2017-10-11T14:58:14.622075: step 2904, loss 0.0929132, acc 0.96875, learning_rate 0.000100034
2017-10-11T14:58:14.893497: step 2905, loss 0.177704, acc 0.9375, learning_rate 0.000100034
2017-10-11T14:58:15.167921: step 2906, loss 0.119253, acc 0.96875, learning_rate 0.000100034
2017-10-11T14:58:15.474241: step 2907, loss 0.144001, acc 0.9375, learning_rate 0.000100034
2017-10-11T14:58:15.790904: step 2908, loss 0.101766, acc 0.984375, learning_rate 0.000100034
2017-10-11T14:58:16.056459: step 2909, loss 0.146927, acc 0.96875, learning_rate 0.000100033
2017-10-11T14:58:16.301315: step 2910, loss 0.135745, acc 0.96875, learning_rate 0.000100033
2017-10-11T14:58:16.579713: step 2911, loss 0.104421, acc 0.9375, learning_rate 0.000100033
2017-10-11T14:58:16.889494: step 2912, loss 0.071373, acc 0.984375, learning_rate 0.000100033
2017-10-11T14:58:17.160454: step 2913, loss 0.114686, acc 0.9375, learning_rate 0.000100033
2017-10-11T14:58:17.418970: step 2914, loss 0.0385747, acc 1, learning_rate 0.000100033
2017-10-11T14:58:17.656401: step 2915, loss 0.117355, acc 0.953125, learning_rate 0.000100033
2017-10-11T14:58:17.918250: step 2916, loss 0.0599737, acc 0.984375, learning_rate 0.000100033
2017-10-11T14:58:18.192452: step 2917, loss 0.0771566, acc 0.984375, learning_rate 0.000100032
2017-10-11T14:58:18.559520: step 2918, loss 0.050694, acc 0.984375, learning_rate 0.000100032
2017-10-11T14:58:18.831877: step 2919, loss 0.135077, acc 0.953125, learning_rate 0.000100032
2017-10-11T14:58:19.193205: step 2920, loss 0.0988352, acc 0.984375, learning_rate 0.000100032

Evaluation:
2017-10-11T14:58:19.490140: step 2920, loss 0.226507, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2920

2017-10-11T14:58:22.503744: step 2921, loss 0.0891845, acc 0.984375, learning_rate 0.000100032
2017-10-11T14:58:22.775137: step 2922, loss 0.116123, acc 0.9375, learning_rate 0.000100032
2017-10-11T14:58:23.057322: step 2923, loss 0.139701, acc 0.9375, learning_rate 0.000100032
2017-10-11T14:58:23.291384: step 2924, loss 0.0224675, acc 1, learning_rate 0.000100031
2017-10-11T14:58:23.512452: step 2925, loss 0.10978, acc 0.96875, learning_rate 0.000100031
2017-10-11T14:58:23.735317: step 2926, loss 0.119193, acc 0.984375, learning_rate 0.000100031
2017-10-11T14:58:23.965735: step 2927, loss 0.0918961, acc 0.953125, learning_rate 0.000100031
2017-10-11T14:58:24.218539: step 2928, loss 0.164885, acc 0.921875, learning_rate 0.000100031
2017-10-11T14:58:24.496144: step 2929, loss 0.105662, acc 0.953125, learning_rate 0.000100031
2017-10-11T14:58:24.748386: step 2930, loss 0.145375, acc 0.921875, learning_rate 0.000100031
2017-10-11T14:58:25.011906: step 2931, loss 0.0749897, acc 0.984375, learning_rate 0.000100031
2017-10-11T14:58:25.293746: step 2932, loss 0.122983, acc 0.9375, learning_rate 0.00010003
2017-10-11T14:58:25.539769: step 2933, loss 0.0912051, acc 0.96875, learning_rate 0.00010003
2017-10-11T14:58:25.822361: step 2934, loss 0.0937556, acc 0.96875, learning_rate 0.00010003
2017-10-11T14:58:26.134185: step 2935, loss 0.122341, acc 0.921875, learning_rate 0.00010003
2017-10-11T14:58:26.443658: step 2936, loss 0.0898573, acc 0.953125, learning_rate 0.00010003
2017-10-11T14:58:26.785763: step 2937, loss 0.108959, acc 0.953125, learning_rate 0.00010003
2017-10-11T14:58:27.087027: step 2938, loss 0.0647486, acc 0.984375, learning_rate 0.00010003
2017-10-11T14:58:27.401665: step 2939, loss 0.208669, acc 0.953125, learning_rate 0.00010003
2017-10-11T14:58:27.684050: step 2940, loss 0.0568585, acc 0.980392, learning_rate 0.000100029
2017-10-11T14:58:28.022171: step 2941, loss 0.133594, acc 0.953125, learning_rate 0.000100029
2017-10-11T14:58:28.341576: step 2942, loss 0.180787, acc 0.953125, learning_rate 0.000100029
2017-10-11T14:58:28.666313: step 2943, loss 0.15008, acc 0.9375, learning_rate 0.000100029
2017-10-11T14:58:28.974636: step 2944, loss 0.161415, acc 0.9375, learning_rate 0.000100029
2017-10-11T14:58:29.264640: step 2945, loss 0.0858922, acc 0.9375, learning_rate 0.000100029
2017-10-11T14:58:29.590406: step 2946, loss 0.124613, acc 0.984375, learning_rate 0.000100029
2017-10-11T14:58:29.949702: step 2947, loss 0.0440388, acc 0.984375, learning_rate 0.000100029
2017-10-11T14:58:30.195741: step 2948, loss 0.111154, acc 0.96875, learning_rate 0.000100029
2017-10-11T14:58:30.494900: step 2949, loss 0.0933174, acc 0.96875, learning_rate 0.000100028
2017-10-11T14:58:30.770752: step 2950, loss 0.0911587, acc 0.96875, learning_rate 0.000100028
2017-10-11T14:58:31.039217: step 2951, loss 0.105658, acc 0.953125, learning_rate 0.000100028
2017-10-11T14:58:31.296628: step 2952, loss 0.21189, acc 0.921875, learning_rate 0.000100028
2017-10-11T14:58:31.570766: step 2953, loss 0.087437, acc 0.96875, learning_rate 0.000100028
2017-10-11T14:58:31.888000: step 2954, loss 0.0577696, acc 0.96875, learning_rate 0.000100028
2017-10-11T14:58:32.190890: step 2955, loss 0.0359203, acc 1, learning_rate 0.000100028
2017-10-11T14:58:32.526109: step 2956, loss 0.0621865, acc 0.984375, learning_rate 0.000100028
2017-10-11T14:58:32.811754: step 2957, loss 0.0696957, acc 0.96875, learning_rate 0.000100028
2017-10-11T14:58:33.129132: step 2958, loss 0.0797073, acc 0.96875, learning_rate 0.000100027
2017-10-11T14:58:33.457853: step 2959, loss 0.0906138, acc 0.984375, learning_rate 0.000100027
2017-10-11T14:58:33.755149: step 2960, loss 0.0678494, acc 0.96875, learning_rate 0.000100027

Evaluation:
2017-10-11T14:58:34.011837: step 2960, loss 0.224981, acc 0.910791

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-2960

2017-10-11T14:58:35.703634: step 2961, loss 0.0732726, acc 0.984375, learning_rate 0.000100027
2017-10-11T14:58:36.030712: step 2962, loss 0.043252, acc 0.984375, learning_rate 0.000100027
2017-10-11T14:58:36.340151: step 2963, loss 0.0428129, acc 0.984375, learning_rate 0.000100027
2017-10-11T14:58:36.639120: step 2964, loss 0.113121, acc 0.96875, learning_rate 0.000100027
2017-10-11T14:58:36.911159: step 2965, loss 0.0326742, acc 1, learning_rate 0.000100027
2017-10-11T14:58:37.179577: step 2966, loss 0.128179, acc 0.96875, learning_rate 0.000100027
2017-10-11T14:58:37.444269: step 2967, loss 0.11992, acc 0.96875, learning_rate 0.000100026
2017-10-11T14:58:37.745327: step 2968, loss 0.0265581, acc 1, learning_rate 0.000100026
2017-10-11T14:58:37.987656: step 2969, loss 0.0698288, acc 0.984375, learning_rate 0.000100026
2017-10-11T14:58:38.243720: step 2970, loss 0.0807757, acc 0.984375, learning_rate 0.000100026
2017-10-11T14:58:38.463611: step 2971, loss 0.083769, acc 0.96875, learning_rate 0.000100026
2017-10-11T14:58:38.736057: step 2972, loss 0.213219, acc 0.921875, learning_rate 0.000100026
2017-10-11T14:58:39.000713: step 2973, loss 0.0125684, acc 1, learning_rate 0.000100026
2017-10-11T14:58:39.276675: step 2974, loss 0.0464754, acc 0.984375, learning_rate 0.000100026
2017-10-11T14:58:39.537980: step 2975, loss 0.0711656, acc 0.984375, learning_rate 0.000100026
2017-10-11T14:58:39.796063: step 2976, loss 0.0254948, acc 1, learning_rate 0.000100025
2017-10-11T14:58:40.037404: step 2977, loss 0.111583, acc 0.984375, learning_rate 0.000100025
2017-10-11T14:58:40.411447: step 2978, loss 0.163816, acc 0.9375, learning_rate 0.000100025
2017-10-11T14:58:40.754954: step 2979, loss 0.094414, acc 0.96875, learning_rate 0.000100025
2017-10-11T14:58:41.057660: step 2980, loss 0.0346577, acc 1, learning_rate 0.000100025
2017-10-11T14:58:41.377757: step 2981, loss 0.141643, acc 0.953125, learning_rate 0.000100025
2017-10-11T14:58:41.705206: step 2982, loss 0.0798996, acc 0.984375, learning_rate 0.000100025
2017-10-11T14:58:42.021364: step 2983, loss 0.0882204, acc 0.984375, learning_rate 0.000100025
2017-10-11T14:58:42.351577: step 2984, loss 0.0619024, acc 0.96875, learning_rate 0.000100025
2017-10-11T14:58:42.690148: step 2985, loss 0.0880975, acc 0.9375, learning_rate 0.000100025
2017-10-11T14:58:43.014287: step 2986, loss 0.0937428, acc 0.96875, learning_rate 0.000100024
2017-10-11T14:58:43.314037: step 2987, loss 0.0910391, acc 0.96875, learning_rate 0.000100024
2017-10-11T14:58:43.595235: step 2988, loss 0.23472, acc 0.921875, learning_rate 0.000100024
2017-10-11T14:58:43.851865: step 2989, loss 0.167225, acc 0.96875, learning_rate 0.000100024
2017-10-11T14:58:44.094564: step 2990, loss 0.143139, acc 0.953125, learning_rate 0.000100024
2017-10-11T14:58:44.366096: step 2991, loss 0.0624855, acc 0.984375, learning_rate 0.000100024
2017-10-11T14:58:44.608902: step 2992, loss 0.207178, acc 0.9375, learning_rate 0.000100024
2017-10-11T14:58:44.840032: step 2993, loss 0.0516302, acc 0.96875, learning_rate 0.000100024
2017-10-11T14:58:45.148417: step 2994, loss 0.0232062, acc 1, learning_rate 0.000100024
2017-10-11T14:58:45.493502: step 2995, loss 0.0206879, acc 1, learning_rate 0.000100024
2017-10-11T14:58:45.805294: step 2996, loss 0.0543917, acc 0.984375, learning_rate 0.000100023
2017-10-11T14:58:46.114920: step 2997, loss 0.0583503, acc 0.96875, learning_rate 0.000100023
2017-10-11T14:58:46.413103: step 2998, loss 0.0423639, acc 0.984375, learning_rate 0.000100023
2017-10-11T14:58:46.698124: step 2999, loss 0.0902169, acc 0.96875, learning_rate 0.000100023
2017-10-11T14:58:47.008246: step 3000, loss 0.0496754, acc 0.984375, learning_rate 0.000100023

Evaluation:
2017-10-11T14:58:47.305155: step 3000, loss 0.224988, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3000

2017-10-11T14:58:49.805034: step 3001, loss 0.0611183, acc 0.96875, learning_rate 0.000100023
2017-10-11T14:58:50.053873: step 3002, loss 0.0863935, acc 0.96875, learning_rate 0.000100023
2017-10-11T14:58:50.313720: step 3003, loss 0.1735, acc 0.953125, learning_rate 0.000100023
2017-10-11T14:58:50.555476: step 3004, loss 0.0479169, acc 0.984375, learning_rate 0.000100023
2017-10-11T14:58:50.827009: step 3005, loss 0.0367487, acc 0.984375, learning_rate 0.000100023
2017-10-11T14:58:51.119531: step 3006, loss 0.0720986, acc 0.96875, learning_rate 0.000100023
2017-10-11T14:58:51.397607: step 3007, loss 0.131048, acc 0.953125, learning_rate 0.000100022
2017-10-11T14:58:51.681425: step 3008, loss 0.0900155, acc 0.96875, learning_rate 0.000100022
2017-10-11T14:58:51.912097: step 3009, loss 0.0615988, acc 0.984375, learning_rate 0.000100022
2017-10-11T14:58:52.115535: step 3010, loss 0.146395, acc 0.953125, learning_rate 0.000100022
2017-10-11T14:58:52.329524: step 3011, loss 0.0903502, acc 0.953125, learning_rate 0.000100022
2017-10-11T14:58:52.525135: step 3012, loss 0.0382666, acc 1, learning_rate 0.000100022
2017-10-11T14:58:52.824820: step 3013, loss 0.0663545, acc 0.96875, learning_rate 0.000100022
2017-10-11T14:58:53.169499: step 3014, loss 0.0348854, acc 0.984375, learning_rate 0.000100022
2017-10-11T14:58:53.480084: step 3015, loss 0.124475, acc 0.9375, learning_rate 0.000100022
2017-10-11T14:58:53.751143: step 3016, loss 0.116932, acc 0.96875, learning_rate 0.000100022
2017-10-11T14:58:54.061877: step 3017, loss 0.165151, acc 0.953125, learning_rate 0.000100022
2017-10-11T14:58:54.360095: step 3018, loss 0.0291326, acc 1, learning_rate 0.000100021
2017-10-11T14:58:54.651393: step 3019, loss 0.187021, acc 0.921875, learning_rate 0.000100021
2017-10-11T14:58:54.990326: step 3020, loss 0.0407099, acc 0.984375, learning_rate 0.000100021
2017-10-11T14:58:55.297895: step 3021, loss 0.0521443, acc 0.96875, learning_rate 0.000100021
2017-10-11T14:58:55.589716: step 3022, loss 0.12057, acc 0.96875, learning_rate 0.000100021
2017-10-11T14:58:55.902830: step 3023, loss 0.106919, acc 0.953125, learning_rate 0.000100021
2017-10-11T14:58:56.206926: step 3024, loss 0.121752, acc 0.9375, learning_rate 0.000100021
2017-10-11T14:58:56.534717: step 3025, loss 0.101894, acc 0.96875, learning_rate 0.000100021
2017-10-11T14:58:56.814736: step 3026, loss 0.122756, acc 0.984375, learning_rate 0.000100021
2017-10-11T14:58:57.090053: step 3027, loss 0.0840798, acc 0.984375, learning_rate 0.000100021
2017-10-11T14:58:57.350886: step 3028, loss 0.101778, acc 0.96875, learning_rate 0.000100021
2017-10-11T14:58:57.608545: step 3029, loss 0.0580428, acc 0.96875, learning_rate 0.00010002
2017-10-11T14:58:57.889320: step 3030, loss 0.0899401, acc 0.96875, learning_rate 0.00010002
2017-10-11T14:58:58.138048: step 3031, loss 0.0508841, acc 1, learning_rate 0.00010002
2017-10-11T14:58:58.431273: step 3032, loss 0.13432, acc 0.9375, learning_rate 0.00010002
2017-10-11T14:58:58.719617: step 3033, loss 0.0692458, acc 0.984375, learning_rate 0.00010002
2017-10-11T14:58:59.039043: step 3034, loss 0.190007, acc 0.953125, learning_rate 0.00010002
2017-10-11T14:58:59.381179: step 3035, loss 0.143489, acc 0.96875, learning_rate 0.00010002
2017-10-11T14:58:59.726273: step 3036, loss 0.140003, acc 0.96875, learning_rate 0.00010002
2017-10-11T14:59:00.036187: step 3037, loss 0.0475208, acc 0.984375, learning_rate 0.00010002
2017-10-11T14:59:00.315590: step 3038, loss 0.0908473, acc 0.941176, learning_rate 0.00010002
2017-10-11T14:59:00.622760: step 3039, loss 0.0917204, acc 0.953125, learning_rate 0.00010002
2017-10-11T14:59:00.906805: step 3040, loss 0.0928489, acc 0.96875, learning_rate 0.00010002

Evaluation:
2017-10-11T14:59:01.184846: step 3040, loss 0.222088, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3040

2017-10-11T14:59:04.028438: step 3041, loss 0.0979603, acc 0.96875, learning_rate 0.00010002
2017-10-11T14:59:04.345136: step 3042, loss 0.0795776, acc 0.96875, learning_rate 0.000100019
2017-10-11T14:59:04.675199: step 3043, loss 0.0606399, acc 0.96875, learning_rate 0.000100019
2017-10-11T14:59:04.954874: step 3044, loss 0.145772, acc 0.96875, learning_rate 0.000100019
2017-10-11T14:59:05.179274: step 3045, loss 0.037576, acc 0.984375, learning_rate 0.000100019
2017-10-11T14:59:05.424247: step 3046, loss 0.040014, acc 0.984375, learning_rate 0.000100019
2017-10-11T14:59:05.671949: step 3047, loss 0.120997, acc 0.953125, learning_rate 0.000100019
2017-10-11T14:59:05.886319: step 3048, loss 0.0367246, acc 1, learning_rate 0.000100019
2017-10-11T14:59:06.146784: step 3049, loss 0.104436, acc 0.9375, learning_rate 0.000100019
2017-10-11T14:59:06.371618: step 3050, loss 0.0693439, acc 0.984375, learning_rate 0.000100019
2017-10-11T14:59:06.668013: step 3051, loss 0.0732756, acc 0.984375, learning_rate 0.000100019
2017-10-11T14:59:06.962681: step 3052, loss 0.0342746, acc 1, learning_rate 0.000100019
2017-10-11T14:59:07.194895: step 3053, loss 0.015209, acc 1, learning_rate 0.000100019
2017-10-11T14:59:07.532710: step 3054, loss 0.156258, acc 0.90625, learning_rate 0.000100018
2017-10-11T14:59:07.795491: step 3055, loss 0.0405285, acc 0.984375, learning_rate 0.000100018
2017-10-11T14:59:08.104938: step 3056, loss 0.111187, acc 0.953125, learning_rate 0.000100018
2017-10-11T14:59:08.398172: step 3057, loss 0.0542833, acc 0.984375, learning_rate 0.000100018
2017-10-11T14:59:08.683661: step 3058, loss 0.176945, acc 0.9375, learning_rate 0.000100018
2017-10-11T14:59:08.953356: step 3059, loss 0.0477889, acc 0.984375, learning_rate 0.000100018
2017-10-11T14:59:09.251150: step 3060, loss 0.104297, acc 0.9375, learning_rate 0.000100018
2017-10-11T14:59:09.584778: step 3061, loss 0.0657743, acc 0.984375, learning_rate 0.000100018
2017-10-11T14:59:09.889078: step 3062, loss 0.175191, acc 0.9375, learning_rate 0.000100018
2017-10-11T14:59:10.178644: step 3063, loss 0.114386, acc 0.953125, learning_rate 0.000100018
2017-10-11T14:59:10.444910: step 3064, loss 0.109664, acc 0.96875, learning_rate 0.000100018
2017-10-11T14:59:10.720020: step 3065, loss 0.0553227, acc 1, learning_rate 0.000100018
2017-10-11T14:59:10.994576: step 3066, loss 0.07712, acc 0.984375, learning_rate 0.000100018
2017-10-11T14:59:11.274357: step 3067, loss 0.0646766, acc 0.984375, learning_rate 0.000100018
2017-10-11T14:59:11.546944: step 3068, loss 0.129693, acc 0.96875, learning_rate 0.000100017
2017-10-11T14:59:11.846249: step 3069, loss 0.112115, acc 0.96875, learning_rate 0.000100017
2017-10-11T14:59:12.167267: step 3070, loss 0.128766, acc 0.953125, learning_rate 0.000100017
2017-10-11T14:59:12.499958: step 3071, loss 0.0750423, acc 0.984375, learning_rate 0.000100017
2017-10-11T14:59:12.786438: step 3072, loss 0.129774, acc 0.984375, learning_rate 0.000100017
2017-10-11T14:59:13.138900: step 3073, loss 0.0980483, acc 0.96875, learning_rate 0.000100017
2017-10-11T14:59:13.447125: step 3074, loss 0.18848, acc 0.9375, learning_rate 0.000100017
2017-10-11T14:59:13.742580: step 3075, loss 0.0660475, acc 0.96875, learning_rate 0.000100017
2017-10-11T14:59:14.025464: step 3076, loss 0.0797052, acc 0.984375, learning_rate 0.000100017
2017-10-11T14:59:14.332683: step 3077, loss 0.0578317, acc 0.984375, learning_rate 0.000100017
2017-10-11T14:59:14.659094: step 3078, loss 0.125134, acc 0.96875, learning_rate 0.000100017
2017-10-11T14:59:14.991607: step 3079, loss 0.104474, acc 0.953125, learning_rate 0.000100017
2017-10-11T14:59:15.307500: step 3080, loss 0.0493091, acc 0.984375, learning_rate 0.000100017

Evaluation:
2017-10-11T14:59:15.618212: step 3080, loss 0.22332, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3080

2017-10-11T14:59:17.269699: step 3081, loss 0.173876, acc 0.921875, learning_rate 0.000100017
2017-10-11T14:59:17.569556: step 3082, loss 0.0824448, acc 0.953125, learning_rate 0.000100016
2017-10-11T14:59:17.830350: step 3083, loss 0.0591208, acc 0.96875, learning_rate 0.000100016
2017-10-11T14:59:18.130026: step 3084, loss 0.102349, acc 0.9375, learning_rate 0.000100016
2017-10-11T14:59:18.428885: step 3085, loss 0.175132, acc 0.953125, learning_rate 0.000100016
2017-10-11T14:59:18.704716: step 3086, loss 0.0990479, acc 0.984375, learning_rate 0.000100016
2017-10-11T14:59:18.941031: step 3087, loss 0.12374, acc 0.984375, learning_rate 0.000100016
2017-10-11T14:59:19.219122: step 3088, loss 0.107221, acc 0.953125, learning_rate 0.000100016
2017-10-11T14:59:19.493680: step 3089, loss 0.153907, acc 0.90625, learning_rate 0.000100016
2017-10-11T14:59:19.733368: step 3090, loss 0.104784, acc 0.9375, learning_rate 0.000100016
2017-10-11T14:59:19.961933: step 3091, loss 0.161006, acc 0.953125, learning_rate 0.000100016
2017-10-11T14:59:20.211388: step 3092, loss 0.0999907, acc 0.96875, learning_rate 0.000100016
2017-10-11T14:59:20.437223: step 3093, loss 0.11585, acc 0.96875, learning_rate 0.000100016
2017-10-11T14:59:20.753001: step 3094, loss 0.142001, acc 0.96875, learning_rate 0.000100016
2017-10-11T14:59:21.069931: step 3095, loss 0.156008, acc 0.9375, learning_rate 0.000100016
2017-10-11T14:59:21.330644: step 3096, loss 0.0791185, acc 0.96875, learning_rate 0.000100016
2017-10-11T14:59:21.599915: step 3097, loss 0.0349453, acc 0.984375, learning_rate 0.000100016
2017-10-11T14:59:21.851171: step 3098, loss 0.114856, acc 0.953125, learning_rate 0.000100015
2017-10-11T14:59:22.117935: step 3099, loss 0.124265, acc 0.96875, learning_rate 0.000100015
2017-10-11T14:59:22.406486: step 3100, loss 0.161446, acc 0.96875, learning_rate 0.000100015
2017-10-11T14:59:22.698501: step 3101, loss 0.101652, acc 0.953125, learning_rate 0.000100015
2017-10-11T14:59:23.041328: step 3102, loss 0.0664498, acc 0.984375, learning_rate 0.000100015
2017-10-11T14:59:23.380017: step 3103, loss 0.175031, acc 0.953125, learning_rate 0.000100015
2017-10-11T14:59:23.632168: step 3104, loss 0.0360409, acc 1, learning_rate 0.000100015
2017-10-11T14:59:23.932502: step 3105, loss 0.0679801, acc 0.96875, learning_rate 0.000100015
2017-10-11T14:59:24.211859: step 3106, loss 0.0874087, acc 0.953125, learning_rate 0.000100015
2017-10-11T14:59:24.517592: step 3107, loss 0.0814374, acc 0.984375, learning_rate 0.000100015
2017-10-11T14:59:24.778578: step 3108, loss 0.0337118, acc 0.984375, learning_rate 0.000100015
2017-10-11T14:59:25.037776: step 3109, loss 0.094649, acc 0.9375, learning_rate 0.000100015
2017-10-11T14:59:25.265178: step 3110, loss 0.11332, acc 0.96875, learning_rate 0.000100015
2017-10-11T14:59:25.525551: step 3111, loss 0.056423, acc 0.984375, learning_rate 0.000100015
2017-10-11T14:59:25.804482: step 3112, loss 0.0292754, acc 1, learning_rate 0.000100015
2017-10-11T14:59:26.069561: step 3113, loss 0.0505443, acc 0.984375, learning_rate 0.000100015
2017-10-11T14:59:26.309214: step 3114, loss 0.123392, acc 0.96875, learning_rate 0.000100014
2017-10-11T14:59:26.599053: step 3115, loss 0.029126, acc 1, learning_rate 0.000100014
2017-10-11T14:59:26.919700: step 3116, loss 0.101262, acc 0.984375, learning_rate 0.000100014
2017-10-11T14:59:27.220212: step 3117, loss 0.200591, acc 0.9375, learning_rate 0.000100014
2017-10-11T14:59:27.546702: step 3118, loss 0.0691337, acc 0.984375, learning_rate 0.000100014
2017-10-11T14:59:27.843074: step 3119, loss 0.0802126, acc 0.96875, learning_rate 0.000100014
2017-10-11T14:59:28.135734: step 3120, loss 0.115762, acc 0.96875, learning_rate 0.000100014

Evaluation:
2017-10-11T14:59:28.433470: step 3120, loss 0.223862, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3120

2017-10-11T14:59:31.396766: step 3121, loss 0.0341273, acc 0.984375, learning_rate 0.000100014
2017-10-11T14:59:31.654567: step 3122, loss 0.117284, acc 0.953125, learning_rate 0.000100014
2017-10-11T14:59:31.939061: step 3123, loss 0.187052, acc 0.953125, learning_rate 0.000100014
2017-10-11T14:59:32.202916: step 3124, loss 0.105159, acc 0.96875, learning_rate 0.000100014
2017-10-11T14:59:32.469071: step 3125, loss 0.0643796, acc 0.984375, learning_rate 0.000100014
2017-10-11T14:59:32.763453: step 3126, loss 0.128821, acc 0.921875, learning_rate 0.000100014
2017-10-11T14:59:33.033661: step 3127, loss 0.0616736, acc 0.984375, learning_rate 0.000100014
2017-10-11T14:59:33.315245: step 3128, loss 0.0857797, acc 0.984375, learning_rate 0.000100014
2017-10-11T14:59:33.586711: step 3129, loss 0.0663654, acc 0.96875, learning_rate 0.000100014
2017-10-11T14:59:33.788959: step 3130, loss 0.0596828, acc 0.96875, learning_rate 0.000100014
2017-10-11T14:59:34.090507: step 3131, loss 0.0268767, acc 1, learning_rate 0.000100014
2017-10-11T14:59:34.349388: step 3132, loss 0.0216024, acc 1, learning_rate 0.000100013
2017-10-11T14:59:34.639301: step 3133, loss 0.103597, acc 0.96875, learning_rate 0.000100013
2017-10-11T14:59:34.929122: step 3134, loss 0.0629892, acc 0.96875, learning_rate 0.000100013
2017-10-11T14:59:35.250635: step 3135, loss 0.0991909, acc 0.953125, learning_rate 0.000100013
2017-10-11T14:59:35.505620: step 3136, loss 0.122616, acc 0.960784, learning_rate 0.000100013
2017-10-11T14:59:35.843806: step 3137, loss 0.0469318, acc 0.984375, learning_rate 0.000100013
2017-10-11T14:59:36.161306: step 3138, loss 0.0480959, acc 1, learning_rate 0.000100013
2017-10-11T14:59:36.456110: step 3139, loss 0.118988, acc 0.96875, learning_rate 0.000100013
2017-10-11T14:59:36.791099: step 3140, loss 0.133449, acc 0.96875, learning_rate 0.000100013
2017-10-11T14:59:37.091245: step 3141, loss 0.0779483, acc 0.96875, learning_rate 0.000100013
2017-10-11T14:59:37.440804: step 3142, loss 0.0717004, acc 0.984375, learning_rate 0.000100013
2017-10-11T14:59:37.757744: step 3143, loss 0.0662147, acc 0.984375, learning_rate 0.000100013
2017-10-11T14:59:38.074791: step 3144, loss 0.0494552, acc 0.984375, learning_rate 0.000100013
2017-10-11T14:59:38.355946: step 3145, loss 0.0120474, acc 1, learning_rate 0.000100013
2017-10-11T14:59:38.626043: step 3146, loss 0.077258, acc 0.96875, learning_rate 0.000100013
2017-10-11T14:59:38.891426: step 3147, loss 0.122502, acc 0.953125, learning_rate 0.000100013
2017-10-11T14:59:39.179313: step 3148, loss 0.0833323, acc 0.9375, learning_rate 0.000100013
2017-10-11T14:59:39.456731: step 3149, loss 0.0857271, acc 0.96875, learning_rate 0.000100013
2017-10-11T14:59:39.733202: step 3150, loss 0.0441473, acc 1, learning_rate 0.000100012
2017-10-11T14:59:40.072886: step 3151, loss 0.0385572, acc 0.984375, learning_rate 0.000100012
2017-10-11T14:59:40.383797: step 3152, loss 0.0679324, acc 0.96875, learning_rate 0.000100012
2017-10-11T14:59:40.664727: step 3153, loss 0.0576523, acc 1, learning_rate 0.000100012
2017-10-11T14:59:40.982299: step 3154, loss 0.0707855, acc 0.96875, learning_rate 0.000100012
2017-10-11T14:59:41.325626: step 3155, loss 0.109567, acc 0.96875, learning_rate 0.000100012
2017-10-11T14:59:41.628302: step 3156, loss 0.0958603, acc 0.9375, learning_rate 0.000100012
2017-10-11T14:59:41.920535: step 3157, loss 0.0686553, acc 0.984375, learning_rate 0.000100012
2017-10-11T14:59:42.203482: step 3158, loss 0.0902484, acc 0.984375, learning_rate 0.000100012
2017-10-11T14:59:42.470727: step 3159, loss 0.0845116, acc 0.984375, learning_rate 0.000100012
2017-10-11T14:59:42.785680: step 3160, loss 0.0829546, acc 0.953125, learning_rate 0.000100012

Evaluation:
2017-10-11T14:59:43.069325: step 3160, loss 0.22593, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3160

2017-10-11T14:59:45.083033: step 3161, loss 0.149927, acc 0.921875, learning_rate 0.000100012
2017-10-11T14:59:45.388584: step 3162, loss 0.101906, acc 0.953125, learning_rate 0.000100012
2017-10-11T14:59:45.742838: step 3163, loss 0.106883, acc 0.953125, learning_rate 0.000100012
2017-10-11T14:59:46.033265: step 3164, loss 0.08071, acc 0.96875, learning_rate 0.000100012
2017-10-11T14:59:46.285188: step 3165, loss 0.128426, acc 0.96875, learning_rate 0.000100012
2017-10-11T14:59:46.557568: step 3166, loss 0.179645, acc 0.921875, learning_rate 0.000100012
2017-10-11T14:59:46.824109: step 3167, loss 0.174429, acc 0.9375, learning_rate 0.000100012
2017-10-11T14:59:47.099514: step 3168, loss 0.0821394, acc 0.96875, learning_rate 0.000100012
2017-10-11T14:59:47.316607: step 3169, loss 0.124532, acc 0.9375, learning_rate 0.000100012
2017-10-11T14:59:47.550284: step 3170, loss 0.0998916, acc 0.96875, learning_rate 0.000100012
2017-10-11T14:59:47.802210: step 3171, loss 0.0797506, acc 0.96875, learning_rate 0.000100011
2017-10-11T14:59:48.091009: step 3172, loss 0.0220349, acc 1, learning_rate 0.000100011
2017-10-11T14:59:48.358734: step 3173, loss 0.0675956, acc 0.984375, learning_rate 0.000100011
2017-10-11T14:59:48.645388: step 3174, loss 0.191356, acc 0.90625, learning_rate 0.000100011
2017-10-11T14:59:48.886204: step 3175, loss 0.0393953, acc 1, learning_rate 0.000100011
2017-10-11T14:59:49.115194: step 3176, loss 0.102052, acc 0.9375, learning_rate 0.000100011
2017-10-11T14:59:49.347788: step 3177, loss 0.0726256, acc 0.984375, learning_rate 0.000100011
2017-10-11T14:59:49.608925: step 3178, loss 0.0575325, acc 0.984375, learning_rate 0.000100011
2017-10-11T14:59:49.829978: step 3179, loss 0.196866, acc 0.90625, learning_rate 0.000100011
2017-10-11T14:59:50.127440: step 3180, loss 0.0854512, acc 0.96875, learning_rate 0.000100011
2017-10-11T14:59:50.439058: step 3181, loss 0.0681196, acc 0.96875, learning_rate 0.000100011
2017-10-11T14:59:50.689327: step 3182, loss 0.083175, acc 0.96875, learning_rate 0.000100011
2017-10-11T14:59:51.053076: step 3183, loss 0.0293778, acc 1, learning_rate 0.000100011
2017-10-11T14:59:51.309025: step 3184, loss 0.123118, acc 0.9375, learning_rate 0.000100011
2017-10-11T14:59:51.567851: step 3185, loss 0.0526421, acc 1, learning_rate 0.000100011
2017-10-11T14:59:51.805500: step 3186, loss 0.0804454, acc 0.96875, learning_rate 0.000100011
2017-10-11T14:59:52.055937: step 3187, loss 0.0706063, acc 0.96875, learning_rate 0.000100011
2017-10-11T14:59:52.306875: step 3188, loss 0.144408, acc 0.9375, learning_rate 0.000100011
2017-10-11T14:59:52.551037: step 3189, loss 0.0564024, acc 1, learning_rate 0.000100011
2017-10-11T14:59:52.816726: step 3190, loss 0.0959849, acc 0.984375, learning_rate 0.000100011
2017-10-11T14:59:53.126823: step 3191, loss 0.189224, acc 0.984375, learning_rate 0.000100011
2017-10-11T14:59:53.456742: step 3192, loss 0.11181, acc 0.96875, learning_rate 0.000100011
2017-10-11T14:59:53.794339: step 3193, loss 0.0368716, acc 1, learning_rate 0.00010001
2017-10-11T14:59:54.075463: step 3194, loss 0.0737321, acc 0.984375, learning_rate 0.00010001
2017-10-11T14:59:54.363179: step 3195, loss 0.0841459, acc 0.96875, learning_rate 0.00010001
2017-10-11T14:59:54.683973: step 3196, loss 0.0708788, acc 0.953125, learning_rate 0.00010001
2017-10-11T14:59:55.007121: step 3197, loss 0.0619055, acc 0.96875, learning_rate 0.00010001
2017-10-11T14:59:55.317520: step 3198, loss 0.0559225, acc 0.96875, learning_rate 0.00010001
2017-10-11T14:59:55.653645: step 3199, loss 0.0183435, acc 1, learning_rate 0.00010001
2017-10-11T14:59:55.968197: step 3200, loss 0.071154, acc 0.96875, learning_rate 0.00010001

Evaluation:
2017-10-11T14:59:56.268058: step 3200, loss 0.223773, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3200

2017-10-11T14:59:58.275837: step 3201, loss 0.0478446, acc 1, learning_rate 0.00010001
2017-10-11T14:59:58.595476: step 3202, loss 0.0755912, acc 0.96875, learning_rate 0.00010001
2017-10-11T14:59:58.913447: step 3203, loss 0.021831, acc 1, learning_rate 0.00010001
2017-10-11T14:59:59.196706: step 3204, loss 0.152037, acc 0.953125, learning_rate 0.00010001
2017-10-11T14:59:59.477192: step 3205, loss 0.0219471, acc 1, learning_rate 0.00010001
2017-10-11T14:59:59.760810: step 3206, loss 0.11397, acc 0.96875, learning_rate 0.00010001
2017-10-11T15:00:00.017199: step 3207, loss 0.129495, acc 0.953125, learning_rate 0.00010001
2017-10-11T15:00:00.279243: step 3208, loss 0.079716, acc 0.984375, learning_rate 0.00010001
2017-10-11T15:00:00.553635: step 3209, loss 0.151781, acc 0.953125, learning_rate 0.00010001
2017-10-11T15:00:00.829371: step 3210, loss 0.163614, acc 0.953125, learning_rate 0.00010001
2017-10-11T15:00:01.095793: step 3211, loss 0.0883341, acc 0.96875, learning_rate 0.00010001
2017-10-11T15:00:01.446465: step 3212, loss 0.0702206, acc 0.984375, learning_rate 0.00010001
2017-10-11T15:00:01.777320: step 3213, loss 0.121077, acc 0.96875, learning_rate 0.00010001
2017-10-11T15:00:02.067791: step 3214, loss 0.0930815, acc 0.96875, learning_rate 0.00010001
2017-10-11T15:00:02.367938: step 3215, loss 0.101395, acc 0.96875, learning_rate 0.00010001
2017-10-11T15:00:02.655229: step 3216, loss 0.0421138, acc 1, learning_rate 0.00010001
2017-10-11T15:00:02.917005: step 3217, loss 0.209315, acc 0.921875, learning_rate 0.000100009
2017-10-11T15:00:03.200692: step 3218, loss 0.116201, acc 0.953125, learning_rate 0.000100009
2017-10-11T15:00:03.461233: step 3219, loss 0.0794065, acc 0.984375, learning_rate 0.000100009
2017-10-11T15:00:03.782653: step 3220, loss 0.160677, acc 0.953125, learning_rate 0.000100009
2017-10-11T15:00:04.105887: step 3221, loss 0.182604, acc 0.9375, learning_rate 0.000100009
2017-10-11T15:00:04.424378: step 3222, loss 0.0853208, acc 0.96875, learning_rate 0.000100009
2017-10-11T15:00:04.769674: step 3223, loss 0.0418834, acc 1, learning_rate 0.000100009
2017-10-11T15:00:05.094817: step 3224, loss 0.0513316, acc 0.984375, learning_rate 0.000100009
2017-10-11T15:00:05.358619: step 3225, loss 0.114457, acc 0.953125, learning_rate 0.000100009
2017-10-11T15:00:05.629408: step 3226, loss 0.0143071, acc 1, learning_rate 0.000100009
2017-10-11T15:00:05.899105: step 3227, loss 0.144163, acc 0.953125, learning_rate 0.000100009
2017-10-11T15:00:06.191622: step 3228, loss 0.0560342, acc 0.984375, learning_rate 0.000100009
2017-10-11T15:00:06.461093: step 3229, loss 0.0668022, acc 0.96875, learning_rate 0.000100009
2017-10-11T15:00:06.703367: step 3230, loss 0.111426, acc 0.9375, learning_rate 0.000100009
2017-10-11T15:00:06.956453: step 3231, loss 0.104178, acc 0.96875, learning_rate 0.000100009
2017-10-11T15:00:07.239326: step 3232, loss 0.154251, acc 0.9375, learning_rate 0.000100009
2017-10-11T15:00:07.511277: step 3233, loss 0.0860129, acc 0.984375, learning_rate 0.000100009
2017-10-11T15:00:07.765561: step 3234, loss 0.0996705, acc 0.980392, learning_rate 0.000100009
2017-10-11T15:00:08.061639: step 3235, loss 0.0463287, acc 1, learning_rate 0.000100009
2017-10-11T15:00:08.358796: step 3236, loss 0.0704705, acc 0.96875, learning_rate 0.000100009
2017-10-11T15:00:08.659008: step 3237, loss 0.0876551, acc 0.96875, learning_rate 0.000100009
2017-10-11T15:00:08.967069: step 3238, loss 0.0320702, acc 0.984375, learning_rate 0.000100009
2017-10-11T15:00:09.290567: step 3239, loss 0.0406451, acc 0.984375, learning_rate 0.000100009
2017-10-11T15:00:09.616856: step 3240, loss 0.0670574, acc 0.96875, learning_rate 0.000100009

Evaluation:
2017-10-11T15:00:09.932078: step 3240, loss 0.222113, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3240

2017-10-11T15:00:13.029732: step 3241, loss 0.0898664, acc 0.984375, learning_rate 0.000100009
2017-10-11T15:00:13.299153: step 3242, loss 0.0715469, acc 0.984375, learning_rate 0.000100009
2017-10-11T15:00:13.585590: step 3243, loss 0.119272, acc 0.96875, learning_rate 0.000100009
2017-10-11T15:00:13.882611: step 3244, loss 0.0787221, acc 0.953125, learning_rate 0.000100009
2017-10-11T15:00:14.171265: step 3245, loss 0.0875091, acc 0.96875, learning_rate 0.000100008
2017-10-11T15:00:14.466270: step 3246, loss 0.0888292, acc 0.96875, learning_rate 0.000100008
2017-10-11T15:00:14.726046: step 3247, loss 0.168302, acc 0.921875, learning_rate 0.000100008
2017-10-11T15:00:15.008286: step 3248, loss 0.0569468, acc 0.96875, learning_rate 0.000100008
2017-10-11T15:00:15.279044: step 3249, loss 0.140737, acc 0.953125, learning_rate 0.000100008
2017-10-11T15:00:15.509790: step 3250, loss 0.0697013, acc 0.984375, learning_rate 0.000100008
2017-10-11T15:00:15.732825: step 3251, loss 0.0524997, acc 0.984375, learning_rate 0.000100008
2017-10-11T15:00:15.952343: step 3252, loss 0.0636039, acc 0.984375, learning_rate 0.000100008
2017-10-11T15:00:16.217080: step 3253, loss 0.158543, acc 0.953125, learning_rate 0.000100008
2017-10-11T15:00:16.501762: step 3254, loss 0.15763, acc 0.96875, learning_rate 0.000100008
2017-10-11T15:00:16.738653: step 3255, loss 0.0798177, acc 0.984375, learning_rate 0.000100008
2017-10-11T15:00:17.028601: step 3256, loss 0.0600305, acc 0.984375, learning_rate 0.000100008
2017-10-11T15:00:17.325427: step 3257, loss 0.0373536, acc 1, learning_rate 0.000100008
2017-10-11T15:00:17.667803: step 3258, loss 0.0902118, acc 0.96875, learning_rate 0.000100008
2017-10-11T15:00:17.984145: step 3259, loss 0.198714, acc 0.9375, learning_rate 0.000100008
2017-10-11T15:00:18.328819: step 3260, loss 0.114006, acc 0.9375, learning_rate 0.000100008
2017-10-11T15:00:18.628005: step 3261, loss 0.05249, acc 0.984375, learning_rate 0.000100008
2017-10-11T15:00:18.927245: step 3262, loss 0.0800534, acc 0.96875, learning_rate 0.000100008
2017-10-11T15:00:19.246852: step 3263, loss 0.142169, acc 0.96875, learning_rate 0.000100008
2017-10-11T15:00:19.533592: step 3264, loss 0.117948, acc 0.9375, learning_rate 0.000100008
2017-10-11T15:00:19.883944: step 3265, loss 0.0895971, acc 0.96875, learning_rate 0.000100008
2017-10-11T15:00:20.153882: step 3266, loss 0.0241593, acc 0.984375, learning_rate 0.000100008
2017-10-11T15:00:20.440427: step 3267, loss 0.154206, acc 0.9375, learning_rate 0.000100008
2017-10-11T15:00:20.685536: step 3268, loss 0.0322863, acc 0.984375, learning_rate 0.000100008
2017-10-11T15:00:20.947182: step 3269, loss 0.150279, acc 0.921875, learning_rate 0.000100008
2017-10-11T15:00:21.204060: step 3270, loss 0.0662683, acc 0.984375, learning_rate 0.000100008
2017-10-11T15:00:21.523096: step 3271, loss 0.0807383, acc 0.96875, learning_rate 0.000100008
2017-10-11T15:00:21.838530: step 3272, loss 0.052531, acc 0.984375, learning_rate 0.000100008
2017-10-11T15:00:22.147011: step 3273, loss 0.108362, acc 0.96875, learning_rate 0.000100008
2017-10-11T15:00:22.451066: step 3274, loss 0.0272093, acc 1, learning_rate 0.000100008
2017-10-11T15:00:22.734729: step 3275, loss 0.128381, acc 0.96875, learning_rate 0.000100007
2017-10-11T15:00:23.019452: step 3276, loss 0.0711234, acc 0.96875, learning_rate 0.000100007
2017-10-11T15:00:23.302190: step 3277, loss 0.0679319, acc 0.984375, learning_rate 0.000100007
2017-10-11T15:00:23.640855: step 3278, loss 0.125931, acc 0.953125, learning_rate 0.000100007
2017-10-11T15:00:23.927406: step 3279, loss 0.104425, acc 0.96875, learning_rate 0.000100007
2017-10-11T15:00:24.223593: step 3280, loss 0.0672583, acc 0.984375, learning_rate 0.000100007

Evaluation:
2017-10-11T15:00:24.497049: step 3280, loss 0.226596, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3280

2017-10-11T15:00:26.398876: step 3281, loss 0.0441555, acc 0.984375, learning_rate 0.000100007
2017-10-11T15:00:26.719136: step 3282, loss 0.0572521, acc 0.984375, learning_rate 0.000100007
2017-10-11T15:00:27.042456: step 3283, loss 0.146675, acc 0.921875, learning_rate 0.000100007
2017-10-11T15:00:27.335982: step 3284, loss 0.0528793, acc 0.96875, learning_rate 0.000100007
2017-10-11T15:00:27.655338: step 3285, loss 0.109445, acc 0.953125, learning_rate 0.000100007
2017-10-11T15:00:28.003328: step 3286, loss 0.117714, acc 0.953125, learning_rate 0.000100007
2017-10-11T15:00:28.285069: step 3287, loss 0.0677801, acc 0.984375, learning_rate 0.000100007
2017-10-11T15:00:28.559087: step 3288, loss 0.0328559, acc 1, learning_rate 0.000100007
2017-10-11T15:00:28.845590: step 3289, loss 0.105445, acc 0.96875, learning_rate 0.000100007
2017-10-11T15:00:29.090629: step 3290, loss 0.0535865, acc 0.984375, learning_rate 0.000100007
2017-10-11T15:00:29.324281: step 3291, loss 0.0774835, acc 0.984375, learning_rate 0.000100007
2017-10-11T15:00:29.562952: step 3292, loss 0.10875, acc 0.9375, learning_rate 0.000100007
2017-10-11T15:00:29.830097: step 3293, loss 0.093323, acc 0.96875, learning_rate 0.000100007
2017-10-11T15:00:30.106098: step 3294, loss 0.0401759, acc 1, learning_rate 0.000100007
2017-10-11T15:00:30.402116: step 3295, loss 0.0296449, acc 1, learning_rate 0.000100007
2017-10-11T15:00:30.701148: step 3296, loss 0.071033, acc 0.984375, learning_rate 0.000100007
2017-10-11T15:00:30.962241: step 3297, loss 0.138939, acc 0.9375, learning_rate 0.000100007
2017-10-11T15:00:31.192303: step 3298, loss 0.132575, acc 0.953125, learning_rate 0.000100007
2017-10-11T15:00:31.497386: step 3299, loss 0.0469578, acc 1, learning_rate 0.000100007
2017-10-11T15:00:31.807573: step 3300, loss 0.120698, acc 0.953125, learning_rate 0.000100007
2017-10-11T15:00:32.129055: step 3301, loss 0.0576095, acc 0.984375, learning_rate 0.000100007
2017-10-11T15:00:32.444033: step 3302, loss 0.11274, acc 0.96875, learning_rate 0.000100007
2017-10-11T15:00:32.735987: step 3303, loss 0.0497316, acc 0.984375, learning_rate 0.000100007
2017-10-11T15:00:33.053391: step 3304, loss 0.0796027, acc 0.96875, learning_rate 0.000100007
2017-10-11T15:00:33.332443: step 3305, loss 0.102232, acc 0.953125, learning_rate 0.000100007
2017-10-11T15:00:33.622048: step 3306, loss 0.131999, acc 0.9375, learning_rate 0.000100007
2017-10-11T15:00:33.902967: step 3307, loss 0.0319311, acc 0.984375, learning_rate 0.000100007
2017-10-11T15:00:34.171937: step 3308, loss 0.0553403, acc 0.96875, learning_rate 0.000100007
2017-10-11T15:00:34.451913: step 3309, loss 0.223005, acc 0.90625, learning_rate 0.000100007
2017-10-11T15:00:34.721134: step 3310, loss 0.0832947, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:35.000825: step 3311, loss 0.109434, acc 0.953125, learning_rate 0.000100006
2017-10-11T15:00:35.310352: step 3312, loss 0.080903, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:35.576629: step 3313, loss 0.0567829, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:35.911018: step 3314, loss 0.056875, acc 0.984375, learning_rate 0.000100006
2017-10-11T15:00:36.250501: step 3315, loss 0.100098, acc 0.953125, learning_rate 0.000100006
2017-10-11T15:00:36.577962: step 3316, loss 0.265485, acc 0.9375, learning_rate 0.000100006
2017-10-11T15:00:36.921490: step 3317, loss 0.0905758, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:37.235764: step 3318, loss 0.08384, acc 0.984375, learning_rate 0.000100006
2017-10-11T15:00:37.536077: step 3319, loss 0.0755051, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:37.855045: step 3320, loss 0.148284, acc 0.9375, learning_rate 0.000100006

Evaluation:
2017-10-11T15:00:38.175239: step 3320, loss 0.225349, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3320

2017-10-11T15:00:40.074188: step 3321, loss 0.0862089, acc 0.953125, learning_rate 0.000100006
2017-10-11T15:00:40.355134: step 3322, loss 0.141387, acc 0.9375, learning_rate 0.000100006
2017-10-11T15:00:40.656921: step 3323, loss 0.101968, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:40.985685: step 3324, loss 0.0481032, acc 0.984375, learning_rate 0.000100006
2017-10-11T15:00:41.308279: step 3325, loss 0.0625359, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:41.626542: step 3326, loss 0.0563797, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:41.897662: step 3327, loss 0.136805, acc 0.953125, learning_rate 0.000100006
2017-10-11T15:00:42.156816: step 3328, loss 0.0784142, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:42.445230: step 3329, loss 0.193684, acc 0.953125, learning_rate 0.000100006
2017-10-11T15:00:42.749253: step 3330, loss 0.196623, acc 0.921875, learning_rate 0.000100006
2017-10-11T15:00:43.021191: step 3331, loss 0.277235, acc 0.90625, learning_rate 0.000100006
2017-10-11T15:00:43.236073: step 3332, loss 0.281245, acc 0.901961, learning_rate 0.000100006
2017-10-11T15:00:43.486287: step 3333, loss 0.0856319, acc 0.953125, learning_rate 0.000100006
2017-10-11T15:00:43.713964: step 3334, loss 0.0720249, acc 0.953125, learning_rate 0.000100006
2017-10-11T15:00:43.944534: step 3335, loss 0.127404, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:44.163817: step 3336, loss 0.0916217, acc 0.984375, learning_rate 0.000100006
2017-10-11T15:00:44.389646: step 3337, loss 0.19958, acc 0.953125, learning_rate 0.000100006
2017-10-11T15:00:44.633049: step 3338, loss 0.0505442, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:44.901195: step 3339, loss 0.047119, acc 0.984375, learning_rate 0.000100006
2017-10-11T15:00:45.258511: step 3340, loss 0.107685, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:45.599189: step 3341, loss 0.0497318, acc 1, learning_rate 0.000100006
2017-10-11T15:00:45.941365: step 3342, loss 0.131673, acc 0.9375, learning_rate 0.000100006
2017-10-11T15:00:46.289452: step 3343, loss 0.129473, acc 0.9375, learning_rate 0.000100006
2017-10-11T15:00:46.618719: step 3344, loss 0.0421469, acc 0.984375, learning_rate 0.000100006
2017-10-11T15:00:46.958086: step 3345, loss 0.0624288, acc 0.984375, learning_rate 0.000100006
2017-10-11T15:00:47.219292: step 3346, loss 0.173381, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:47.491177: step 3347, loss 0.0939878, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:47.775848: step 3348, loss 0.0800742, acc 0.984375, learning_rate 0.000100006
2017-10-11T15:00:48.020253: step 3349, loss 0.0708297, acc 0.96875, learning_rate 0.000100006
2017-10-11T15:00:48.299090: step 3350, loss 0.0274619, acc 0.984375, learning_rate 0.000100006
2017-10-11T15:00:48.564250: step 3351, loss 0.205024, acc 0.9375, learning_rate 0.000100005
2017-10-11T15:00:48.854632: step 3352, loss 0.126253, acc 0.953125, learning_rate 0.000100005
2017-10-11T15:00:49.135970: step 3353, loss 0.0300763, acc 1, learning_rate 0.000100005
2017-10-11T15:00:49.402677: step 3354, loss 0.0350253, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:00:49.651786: step 3355, loss 0.0530255, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:00:49.949712: step 3356, loss 0.161267, acc 0.96875, learning_rate 0.000100005
2017-10-11T15:00:50.274662: step 3357, loss 0.0488708, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:00:50.584324: step 3358, loss 0.108542, acc 0.9375, learning_rate 0.000100005
2017-10-11T15:00:50.877980: step 3359, loss 0.0824155, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:00:51.211744: step 3360, loss 0.0971617, acc 0.96875, learning_rate 0.000100005

Evaluation:
2017-10-11T15:00:51.510711: step 3360, loss 0.227666, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3360

2017-10-11T15:00:54.525979: step 3361, loss 0.163183, acc 0.9375, learning_rate 0.000100005
2017-10-11T15:00:54.870891: step 3362, loss 0.0568986, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:00:55.184236: step 3363, loss 0.0916069, acc 0.96875, learning_rate 0.000100005
2017-10-11T15:00:55.506571: step 3364, loss 0.157304, acc 0.953125, learning_rate 0.000100005
2017-10-11T15:00:55.835658: step 3365, loss 0.0701838, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:00:56.133421: step 3366, loss 0.145357, acc 0.9375, learning_rate 0.000100005
2017-10-11T15:00:56.406335: step 3367, loss 0.119157, acc 0.953125, learning_rate 0.000100005
2017-10-11T15:00:56.685269: step 3368, loss 0.0585418, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:00:56.962938: step 3369, loss 0.0346653, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:00:57.204580: step 3370, loss 0.0559055, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:00:57.466112: step 3371, loss 0.175405, acc 0.9375, learning_rate 0.000100005
2017-10-11T15:00:57.717073: step 3372, loss 0.0822263, acc 0.96875, learning_rate 0.000100005
2017-10-11T15:00:57.962506: step 3373, loss 0.123225, acc 0.953125, learning_rate 0.000100005
2017-10-11T15:00:58.177786: step 3374, loss 0.105493, acc 0.953125, learning_rate 0.000100005
2017-10-11T15:00:58.476458: step 3375, loss 0.134373, acc 0.9375, learning_rate 0.000100005
2017-10-11T15:00:58.790202: step 3376, loss 0.109214, acc 0.953125, learning_rate 0.000100005
2017-10-11T15:00:59.025655: step 3377, loss 0.0663419, acc 0.96875, learning_rate 0.000100005
2017-10-11T15:00:59.282928: step 3378, loss 0.134902, acc 0.96875, learning_rate 0.000100005
2017-10-11T15:00:59.510996: step 3379, loss 0.0970621, acc 0.96875, learning_rate 0.000100005
2017-10-11T15:00:59.823855: step 3380, loss 0.104323, acc 0.96875, learning_rate 0.000100005
2017-10-11T15:01:00.144261: step 3381, loss 0.0847185, acc 0.96875, learning_rate 0.000100005
2017-10-11T15:01:00.482436: step 3382, loss 0.154433, acc 0.953125, learning_rate 0.000100005
2017-10-11T15:01:00.799819: step 3383, loss 0.0620518, acc 0.96875, learning_rate 0.000100005
2017-10-11T15:01:01.093553: step 3384, loss 0.100223, acc 0.9375, learning_rate 0.000100005
2017-10-11T15:01:01.381242: step 3385, loss 0.103416, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:01:01.678808: step 3386, loss 0.150866, acc 0.953125, learning_rate 0.000100005
2017-10-11T15:01:01.960011: step 3387, loss 0.0404597, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:01:02.205712: step 3388, loss 0.117676, acc 0.953125, learning_rate 0.000100005
2017-10-11T15:01:02.471408: step 3389, loss 0.0651272, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:01:02.744349: step 3390, loss 0.0477685, acc 1, learning_rate 0.000100005
2017-10-11T15:01:03.076298: step 3391, loss 0.076656, acc 0.96875, learning_rate 0.000100005
2017-10-11T15:01:03.384532: step 3392, loss 0.101062, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:01:03.706124: step 3393, loss 0.19861, acc 0.96875, learning_rate 0.000100005
2017-10-11T15:01:04.002424: step 3394, loss 0.0768959, acc 0.96875, learning_rate 0.000100005
2017-10-11T15:01:04.332586: step 3395, loss 0.0358676, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:01:04.632430: step 3396, loss 0.118683, acc 0.96875, learning_rate 0.000100005
2017-10-11T15:01:04.937508: step 3397, loss 0.137121, acc 0.953125, learning_rate 0.000100005
2017-10-11T15:01:05.259802: step 3398, loss 0.0478788, acc 0.984375, learning_rate 0.000100005
2017-10-11T15:01:05.557562: step 3399, loss 0.122999, acc 0.953125, learning_rate 0.000100005
2017-10-11T15:01:05.878507: step 3400, loss 0.184213, acc 0.953125, learning_rate 0.000100004

Evaluation:
2017-10-11T15:01:06.168168: step 3400, loss 0.226607, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3400

2017-10-11T15:01:08.102650: step 3401, loss 0.0545203, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:08.389631: step 3402, loss 0.114672, acc 0.953125, learning_rate 0.000100004
2017-10-11T15:01:08.707184: step 3403, loss 0.086059, acc 0.96875, learning_rate 0.000100004
2017-10-11T15:01:09.003747: step 3404, loss 0.0587358, acc 0.96875, learning_rate 0.000100004
2017-10-11T15:01:09.257844: step 3405, loss 0.114466, acc 0.953125, learning_rate 0.000100004
2017-10-11T15:01:09.585352: step 3406, loss 0.220883, acc 0.90625, learning_rate 0.000100004
2017-10-11T15:01:09.906585: step 3407, loss 0.0890631, acc 0.96875, learning_rate 0.000100004
2017-10-11T15:01:10.212151: step 3408, loss 0.0262149, acc 1, learning_rate 0.000100004
2017-10-11T15:01:10.495667: step 3409, loss 0.0694005, acc 0.96875, learning_rate 0.000100004
2017-10-11T15:01:10.764366: step 3410, loss 0.176273, acc 0.921875, learning_rate 0.000100004
2017-10-11T15:01:11.002919: step 3411, loss 0.0985494, acc 0.953125, learning_rate 0.000100004
2017-10-11T15:01:11.295403: step 3412, loss 0.0782107, acc 0.96875, learning_rate 0.000100004
2017-10-11T15:01:11.568835: step 3413, loss 0.064957, acc 0.96875, learning_rate 0.000100004
2017-10-11T15:01:11.815259: step 3414, loss 0.0685481, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:12.061853: step 3415, loss 0.0786291, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:12.301409: step 3416, loss 0.144975, acc 0.96875, learning_rate 0.000100004
2017-10-11T15:01:12.580502: step 3417, loss 0.136671, acc 0.9375, learning_rate 0.000100004
2017-10-11T15:01:12.888264: step 3418, loss 0.119353, acc 0.9375, learning_rate 0.000100004
2017-10-11T15:01:13.138577: step 3419, loss 0.0894302, acc 0.96875, learning_rate 0.000100004
2017-10-11T15:01:13.408943: step 3420, loss 0.220374, acc 0.953125, learning_rate 0.000100004
2017-10-11T15:01:13.681459: step 3421, loss 0.103683, acc 0.96875, learning_rate 0.000100004
2017-10-11T15:01:13.999752: step 3422, loss 0.0393497, acc 1, learning_rate 0.000100004
2017-10-11T15:01:14.342609: step 3423, loss 0.103487, acc 0.96875, learning_rate 0.000100004
2017-10-11T15:01:14.701321: step 3424, loss 0.118621, acc 0.953125, learning_rate 0.000100004
2017-10-11T15:01:14.957341: step 3425, loss 0.0892795, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:15.226205: step 3426, loss 0.1914, acc 0.921875, learning_rate 0.000100004
2017-10-11T15:01:15.529873: step 3427, loss 0.0806499, acc 0.96875, learning_rate 0.000100004
2017-10-11T15:01:15.811840: step 3428, loss 0.0617559, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:16.099102: step 3429, loss 0.111132, acc 0.953125, learning_rate 0.000100004
2017-10-11T15:01:16.345289: step 3430, loss 0.213172, acc 0.901961, learning_rate 0.000100004
2017-10-11T15:01:16.586742: step 3431, loss 0.0332116, acc 1, learning_rate 0.000100004
2017-10-11T15:01:16.862928: step 3432, loss 0.0636267, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:17.111619: step 3433, loss 0.0324838, acc 1, learning_rate 0.000100004
2017-10-11T15:01:17.355119: step 3434, loss 0.0268113, acc 1, learning_rate 0.000100004
2017-10-11T15:01:17.642065: step 3435, loss 0.111132, acc 0.96875, learning_rate 0.000100004
2017-10-11T15:01:17.972776: step 3436, loss 0.0394985, acc 1, learning_rate 0.000100004
2017-10-11T15:01:18.279503: step 3437, loss 0.0893872, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:18.579845: step 3438, loss 0.0560012, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:18.875210: step 3439, loss 0.0642965, acc 0.96875, learning_rate 0.000100004
2017-10-11T15:01:19.182852: step 3440, loss 0.0666429, acc 0.984375, learning_rate 0.000100004

Evaluation:
2017-10-11T15:01:19.482607: step 3440, loss 0.221862, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3440

2017-10-11T15:01:22.852880: step 3441, loss 0.134199, acc 0.953125, learning_rate 0.000100004
2017-10-11T15:01:23.168885: step 3442, loss 0.0844894, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:23.479021: step 3443, loss 0.0350117, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:23.801805: step 3444, loss 0.103067, acc 0.953125, learning_rate 0.000100004
2017-10-11T15:01:24.114328: step 3445, loss 0.0555821, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:24.395678: step 3446, loss 0.202782, acc 0.921875, learning_rate 0.000100004
2017-10-11T15:01:24.672098: step 3447, loss 0.16621, acc 0.953125, learning_rate 0.000100004
2017-10-11T15:01:24.947317: step 3448, loss 0.133108, acc 0.953125, learning_rate 0.000100004
2017-10-11T15:01:25.232244: step 3449, loss 0.0732834, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:25.512808: step 3450, loss 0.0467701, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:25.765232: step 3451, loss 0.0325928, acc 1, learning_rate 0.000100004
2017-10-11T15:01:26.002764: step 3452, loss 0.120894, acc 0.953125, learning_rate 0.000100004
2017-10-11T15:01:26.249610: step 3453, loss 0.0343112, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:26.489718: step 3454, loss 0.0389895, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:26.770348: step 3455, loss 0.0427546, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:27.050069: step 3456, loss 0.0719842, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:27.375573: step 3457, loss 0.126269, acc 0.96875, learning_rate 0.000100004
2017-10-11T15:01:27.705875: step 3458, loss 0.086707, acc 0.953125, learning_rate 0.000100004
2017-10-11T15:01:28.020928: step 3459, loss 0.037072, acc 1, learning_rate 0.000100004
2017-10-11T15:01:28.342252: step 3460, loss 0.133252, acc 0.9375, learning_rate 0.000100004
2017-10-11T15:01:28.651190: step 3461, loss 0.0702624, acc 0.984375, learning_rate 0.000100004
2017-10-11T15:01:28.953089: step 3462, loss 0.123319, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:29.237316: step 3463, loss 0.0982849, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:29.516000: step 3464, loss 0.141535, acc 0.921875, learning_rate 0.000100003
2017-10-11T15:01:29.767555: step 3465, loss 0.138012, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:30.047803: step 3466, loss 0.128534, acc 0.9375, learning_rate 0.000100003
2017-10-11T15:01:30.325046: step 3467, loss 0.116129, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:30.604611: step 3468, loss 0.0762214, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:30.865867: step 3469, loss 0.0305541, acc 1, learning_rate 0.000100003
2017-10-11T15:01:31.196075: step 3470, loss 0.0970418, acc 0.9375, learning_rate 0.000100003
2017-10-11T15:01:31.511187: step 3471, loss 0.0527485, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:31.802083: step 3472, loss 0.0588153, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:32.103726: step 3473, loss 0.0272522, acc 1, learning_rate 0.000100003
2017-10-11T15:01:32.350032: step 3474, loss 0.18434, acc 0.921875, learning_rate 0.000100003
2017-10-11T15:01:32.655169: step 3475, loss 0.0211855, acc 1, learning_rate 0.000100003
2017-10-11T15:01:32.920585: step 3476, loss 0.0572671, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:33.197765: step 3477, loss 0.0781468, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:33.515701: step 3478, loss 0.0652652, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:33.846522: step 3479, loss 0.0579562, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:34.195459: step 3480, loss 0.123629, acc 0.953125, learning_rate 0.000100003

Evaluation:
2017-10-11T15:01:34.488926: step 3480, loss 0.221415, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3480

2017-10-11T15:01:36.175893: step 3481, loss 0.0554625, acc 1, learning_rate 0.000100003
2017-10-11T15:01:36.421499: step 3482, loss 0.0446529, acc 1, learning_rate 0.000100003
2017-10-11T15:01:36.709252: step 3483, loss 0.0947378, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:37.018564: step 3484, loss 0.133756, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:37.344421: step 3485, loss 0.194733, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:37.652572: step 3486, loss 0.042531, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:37.918302: step 3487, loss 0.0173555, acc 1, learning_rate 0.000100003
2017-10-11T15:01:38.225384: step 3488, loss 0.0891427, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:38.516602: step 3489, loss 0.0233722, acc 1, learning_rate 0.000100003
2017-10-11T15:01:38.789029: step 3490, loss 0.0404653, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:39.027904: step 3491, loss 0.0538645, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:39.301964: step 3492, loss 0.12907, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:39.539725: step 3493, loss 0.0390951, acc 1, learning_rate 0.000100003
2017-10-11T15:01:39.769840: step 3494, loss 0.0886523, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:40.034003: step 3495, loss 0.157372, acc 0.90625, learning_rate 0.000100003
2017-10-11T15:01:40.337069: step 3496, loss 0.0391254, acc 1, learning_rate 0.000100003
2017-10-11T15:01:40.612198: step 3497, loss 0.0366429, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:40.884046: step 3498, loss 0.263744, acc 0.890625, learning_rate 0.000100003
2017-10-11T15:01:41.170467: step 3499, loss 0.0388628, acc 1, learning_rate 0.000100003
2017-10-11T15:01:41.421221: step 3500, loss 0.151963, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:41.726436: step 3501, loss 0.0827982, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:42.021627: step 3502, loss 0.13002, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:42.323044: step 3503, loss 0.0898206, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:42.616941: step 3504, loss 0.0487876, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:42.919639: step 3505, loss 0.0979355, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:43.207619: step 3506, loss 0.119131, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:43.490136: step 3507, loss 0.081342, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:43.761703: step 3508, loss 0.018871, acc 1, learning_rate 0.000100003
2017-10-11T15:01:44.015588: step 3509, loss 0.15874, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:44.278980: step 3510, loss 0.106517, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:44.557097: step 3511, loss 0.102095, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:44.797612: step 3512, loss 0.0934733, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:45.048771: step 3513, loss 0.0384575, acc 1, learning_rate 0.000100003
2017-10-11T15:01:45.354104: step 3514, loss 0.129268, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:45.623137: step 3515, loss 0.123387, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:45.911557: step 3516, loss 0.0535173, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:46.242685: step 3517, loss 0.0745058, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:46.544790: step 3518, loss 0.0245805, acc 1, learning_rate 0.000100003
2017-10-11T15:01:46.844133: step 3519, loss 0.116916, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:47.136572: step 3520, loss 0.20669, acc 0.9375, learning_rate 0.000100003

Evaluation:
2017-10-11T15:01:47.458350: step 3520, loss 0.22419, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3520

2017-10-11T15:01:49.566884: step 3521, loss 0.129616, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:49.892641: step 3522, loss 0.131596, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:50.177997: step 3523, loss 0.102255, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:50.432548: step 3524, loss 0.105763, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:50.705744: step 3525, loss 0.0702243, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:50.987604: step 3526, loss 0.111082, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:51.319260: step 3527, loss 0.0584212, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:51.588085: step 3528, loss 0.028311, acc 1, learning_rate 0.000100003
2017-10-11T15:01:51.848165: step 3529, loss 0.0538632, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:52.131143: step 3530, loss 0.0679919, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:52.360645: step 3531, loss 0.113862, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:52.593422: step 3532, loss 0.0271841, acc 1, learning_rate 0.000100003
2017-10-11T15:01:52.851309: step 3533, loss 0.154295, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:53.141311: step 3534, loss 0.116834, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:53.380976: step 3535, loss 0.0317643, acc 1, learning_rate 0.000100003
2017-10-11T15:01:53.680865: step 3536, loss 0.0405059, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:53.972479: step 3537, loss 0.041529, acc 0.984375, learning_rate 0.000100003
2017-10-11T15:01:54.263919: step 3538, loss 0.148682, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:54.528848: step 3539, loss 0.101398, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:54.819401: step 3540, loss 0.108071, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:55.108745: step 3541, loss 0.121775, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:55.387603: step 3542, loss 0.122286, acc 0.953125, learning_rate 0.000100003
2017-10-11T15:01:55.703750: step 3543, loss 0.0691578, acc 0.96875, learning_rate 0.000100003
2017-10-11T15:01:56.050337: step 3544, loss 0.106992, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:01:56.373539: step 3545, loss 0.0468198, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:01:56.671812: step 3546, loss 0.168414, acc 0.921875, learning_rate 0.000100002
2017-10-11T15:01:56.981620: step 3547, loss 0.0796476, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:01:57.257085: step 3548, loss 0.0556744, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:01:57.535937: step 3549, loss 0.0862368, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:01:57.787370: step 3550, loss 0.093555, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:01:58.082488: step 3551, loss 0.0846148, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:01:58.345096: step 3552, loss 0.0753738, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:01:58.638034: step 3553, loss 0.116481, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:01:58.911505: step 3554, loss 0.0251654, acc 1, learning_rate 0.000100002
2017-10-11T15:01:59.206900: step 3555, loss 0.0772863, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:01:59.495198: step 3556, loss 0.0717408, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:01:59.813324: step 3557, loss 0.200383, acc 0.890625, learning_rate 0.000100002
2017-10-11T15:02:00.119168: step 3558, loss 0.0472617, acc 1, learning_rate 0.000100002
2017-10-11T15:02:00.419659: step 3559, loss 0.0645268, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:00.724230: step 3560, loss 0.0701775, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-10-11T15:02:01.003148: step 3560, loss 0.225062, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3560

2017-10-11T15:02:04.002954: step 3561, loss 0.0620828, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:04.315947: step 3562, loss 0.0772765, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:04.602786: step 3563, loss 0.122973, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:04.906525: step 3564, loss 0.0344229, acc 1, learning_rate 0.000100002
2017-10-11T15:02:05.209107: step 3565, loss 0.0719646, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:05.487391: step 3566, loss 0.0392785, acc 1, learning_rate 0.000100002
2017-10-11T15:02:05.753697: step 3567, loss 0.0737442, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:06.012305: step 3568, loss 0.151196, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:06.268348: step 3569, loss 0.243026, acc 0.9375, learning_rate 0.000100002
2017-10-11T15:02:06.543053: step 3570, loss 0.0862805, acc 0.9375, learning_rate 0.000100002
2017-10-11T15:02:06.828485: step 3571, loss 0.0753536, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:07.071130: step 3572, loss 0.0559265, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:07.330006: step 3573, loss 0.106399, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:07.564026: step 3574, loss 0.1681, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:07.803035: step 3575, loss 0.0410828, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:08.105150: step 3576, loss 0.0893786, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:08.392515: step 3577, loss 0.0679642, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:08.662969: step 3578, loss 0.0506487, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:08.938209: step 3579, loss 0.109038, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:09.260818: step 3580, loss 0.0431047, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:09.563322: step 3581, loss 0.152885, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:09.862988: step 3582, loss 0.0211754, acc 1, learning_rate 0.000100002
2017-10-11T15:02:10.145040: step 3583, loss 0.115512, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:10.429221: step 3584, loss 0.06979, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:10.731899: step 3585, loss 0.0199609, acc 1, learning_rate 0.000100002
2017-10-11T15:02:10.993770: step 3586, loss 0.179843, acc 0.9375, learning_rate 0.000100002
2017-10-11T15:02:11.262329: step 3587, loss 0.09471, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:11.522953: step 3588, loss 0.126621, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:11.808799: step 3589, loss 0.14026, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:12.083302: step 3590, loss 0.09238, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:12.373462: step 3591, loss 0.0305517, acc 1, learning_rate 0.000100002
2017-10-11T15:02:12.671418: step 3592, loss 0.116131, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:12.975655: step 3593, loss 0.0991355, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:13.259896: step 3594, loss 0.0198305, acc 1, learning_rate 0.000100002
2017-10-11T15:02:13.572843: step 3595, loss 0.0345381, acc 1, learning_rate 0.000100002
2017-10-11T15:02:13.901982: step 3596, loss 0.105216, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:14.204552: step 3597, loss 0.139577, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:14.508686: step 3598, loss 0.0360635, acc 1, learning_rate 0.000100002
2017-10-11T15:02:14.800473: step 3599, loss 0.0876617, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:15.115859: step 3600, loss 0.109498, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-10-11T15:02:15.423019: step 3600, loss 0.222337, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3600

2017-10-11T15:02:17.308323: step 3601, loss 0.0946389, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:17.641751: step 3602, loss 0.0666224, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:17.958199: step 3603, loss 0.0648785, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:18.257494: step 3604, loss 0.0658229, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:18.560423: step 3605, loss 0.0490471, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:18.826275: step 3606, loss 0.103925, acc 0.9375, learning_rate 0.000100002
2017-10-11T15:02:19.120566: step 3607, loss 0.134876, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:19.432153: step 3608, loss 0.105186, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:19.741133: step 3609, loss 0.118738, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:20.009557: step 3610, loss 0.0637227, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:20.250693: step 3611, loss 0.0652235, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:20.500165: step 3612, loss 0.0491775, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:20.737965: step 3613, loss 0.133118, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:20.973195: step 3614, loss 0.0973078, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:21.228911: step 3615, loss 0.20039, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:21.511698: step 3616, loss 0.0904568, acc 0.9375, learning_rate 0.000100002
2017-10-11T15:02:21.792870: step 3617, loss 0.0494771, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:22.081112: step 3618, loss 0.0559142, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:22.364027: step 3619, loss 0.0980459, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:22.656217: step 3620, loss 0.104035, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:22.917019: step 3621, loss 0.0396644, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:23.222249: step 3622, loss 0.0494933, acc 1, learning_rate 0.000100002
2017-10-11T15:02:23.529436: step 3623, loss 0.0620417, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:23.848797: step 3624, loss 0.129753, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:24.116227: step 3625, loss 0.139525, acc 0.9375, learning_rate 0.000100002
2017-10-11T15:02:24.375296: step 3626, loss 0.246044, acc 0.921569, learning_rate 0.000100002
2017-10-11T15:02:24.673125: step 3627, loss 0.150249, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:24.954272: step 3628, loss 0.0424549, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:25.197834: step 3629, loss 0.0742444, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:25.435164: step 3630, loss 0.193147, acc 0.890625, learning_rate 0.000100002
2017-10-11T15:02:25.680649: step 3631, loss 0.0673549, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:25.929851: step 3632, loss 0.0936819, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:26.276671: step 3633, loss 0.0390018, acc 1, learning_rate 0.000100002
2017-10-11T15:02:26.586435: step 3634, loss 0.112246, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:26.897444: step 3635, loss 0.072708, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:27.172651: step 3636, loss 0.120248, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:27.487506: step 3637, loss 0.0123071, acc 1, learning_rate 0.000100002
2017-10-11T15:02:27.793117: step 3638, loss 0.105771, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:28.097272: step 3639, loss 0.0584134, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:28.387352: step 3640, loss 0.0483231, acc 1, learning_rate 0.000100002

Evaluation:
2017-10-11T15:02:28.676401: step 3640, loss 0.221047, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3640

2017-10-11T15:02:31.097839: step 3641, loss 0.110789, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:31.432033: step 3642, loss 0.178536, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:31.741617: step 3643, loss 0.0707362, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:32.033377: step 3644, loss 0.0852303, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:32.335962: step 3645, loss 0.118951, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:32.644166: step 3646, loss 0.0348873, acc 1, learning_rate 0.000100002
2017-10-11T15:02:32.988681: step 3647, loss 0.0725585, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:33.261536: step 3648, loss 0.0474037, acc 1, learning_rate 0.000100002
2017-10-11T15:02:33.545688: step 3649, loss 0.0947296, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:33.847770: step 3650, loss 0.0635158, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:34.105263: step 3651, loss 0.0707424, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:34.371105: step 3652, loss 0.0713138, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:34.654078: step 3653, loss 0.0180469, acc 1, learning_rate 0.000100002
2017-10-11T15:02:34.850947: step 3654, loss 0.191141, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:35.077765: step 3655, loss 0.0543771, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:35.360600: step 3656, loss 0.0884113, acc 0.953125, learning_rate 0.000100002
2017-10-11T15:02:35.630004: step 3657, loss 0.0764765, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:35.894526: step 3658, loss 0.0894335, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:36.216609: step 3659, loss 0.0472104, acc 1, learning_rate 0.000100002
2017-10-11T15:02:36.473609: step 3660, loss 0.0667278, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:36.745011: step 3661, loss 0.0558385, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:37.062334: step 3662, loss 0.164504, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:37.384329: step 3663, loss 0.0457889, acc 0.984375, learning_rate 0.000100002
2017-10-11T15:02:37.703963: step 3664, loss 0.116495, acc 0.9375, learning_rate 0.000100002
2017-10-11T15:02:37.972969: step 3665, loss 0.0733727, acc 0.96875, learning_rate 0.000100002
2017-10-11T15:02:38.277492: step 3666, loss 0.156399, acc 0.90625, learning_rate 0.000100002
2017-10-11T15:02:38.568224: step 3667, loss 0.0469903, acc 1, learning_rate 0.000100002
2017-10-11T15:02:38.845553: step 3668, loss 0.151268, acc 0.9375, learning_rate 0.000100002
2017-10-11T15:02:39.158097: step 3669, loss 0.0239251, acc 1, learning_rate 0.000100001
2017-10-11T15:02:39.421817: step 3670, loss 0.053649, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:02:39.717572: step 3671, loss 0.059374, acc 1, learning_rate 0.000100001
2017-10-11T15:02:39.982730: step 3672, loss 0.0845209, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:02:40.272404: step 3673, loss 0.124944, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:02:40.590655: step 3674, loss 0.112544, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:02:40.896963: step 3675, loss 0.02011, acc 1, learning_rate 0.000100001
2017-10-11T15:02:41.196596: step 3676, loss 0.0197246, acc 1, learning_rate 0.000100001
2017-10-11T15:02:41.464287: step 3677, loss 0.0812427, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:02:41.742344: step 3678, loss 0.105698, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:02:42.064184: step 3679, loss 0.0552818, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:02:42.359246: step 3680, loss 0.0252555, acc 1, learning_rate 0.000100001

Evaluation:
2017-10-11T15:02:42.638617: step 3680, loss 0.221874, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3680

2017-10-11T15:02:45.508718: step 3681, loss 0.0362543, acc 1, learning_rate 0.000100001
2017-10-11T15:02:45.795573: step 3682, loss 0.0376813, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:02:46.104905: step 3683, loss 0.0746904, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:02:46.431782: step 3684, loss 0.0625281, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:02:46.725001: step 3685, loss 0.165399, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:02:47.037433: step 3686, loss 0.0936334, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:02:47.336034: step 3687, loss 0.109655, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:02:47.605923: step 3688, loss 0.0865368, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:02:47.874648: step 3689, loss 0.0811149, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:02:48.132282: step 3690, loss 0.0762104, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:02:48.347465: step 3691, loss 0.0932905, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:02:48.588553: step 3692, loss 0.111687, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:02:48.861786: step 3693, loss 0.126771, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:02:49.161327: step 3694, loss 0.146617, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:02:49.430854: step 3695, loss 0.0800063, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:02:49.759566: step 3696, loss 0.0913092, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:02:50.086054: step 3697, loss 0.160599, acc 0.921875, learning_rate 0.000100001
2017-10-11T15:02:50.420687: step 3698, loss 0.0906215, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:02:50.737421: step 3699, loss 0.0836353, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:02:51.031707: step 3700, loss 0.040992, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:02:51.332496: step 3701, loss 0.128958, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:02:51.596715: step 3702, loss 0.134527, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:02:51.891540: step 3703, loss 0.0685804, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:02:52.158637: step 3704, loss 0.121094, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:02:52.446301: step 3705, loss 0.0536432, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:02:52.743612: step 3706, loss 0.10469, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:02:52.994914: step 3707, loss 0.0893572, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:02:53.283588: step 3708, loss 0.0744829, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:02:53.583149: step 3709, loss 0.0716684, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:02:53.920205: step 3710, loss 0.0316015, acc 1, learning_rate 0.000100001
2017-10-11T15:02:54.205099: step 3711, loss 0.102445, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:02:54.507149: step 3712, loss 0.080918, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:02:54.832863: step 3713, loss 0.0137118, acc 1, learning_rate 0.000100001
2017-10-11T15:02:55.141121: step 3714, loss 0.055556, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:02:55.436051: step 3715, loss 0.112312, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:02:55.723140: step 3716, loss 0.0560554, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:02:56.021486: step 3717, loss 0.127046, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:02:56.353619: step 3718, loss 0.0176807, acc 1, learning_rate 0.000100001
2017-10-11T15:02:56.666081: step 3719, loss 0.031371, acc 1, learning_rate 0.000100001
2017-10-11T15:02:56.993773: step 3720, loss 0.112132, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-10-11T15:02:57.288636: step 3720, loss 0.223067, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3720

2017-10-11T15:02:59.244558: step 3721, loss 0.032756, acc 1, learning_rate 0.000100001
2017-10-11T15:02:59.549189: step 3722, loss 0.102095, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:02:59.868423: step 3723, loss 0.0762246, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:00.179593: step 3724, loss 0.0987917, acc 0.960784, learning_rate 0.000100001
2017-10-11T15:03:00.465410: step 3725, loss 0.0498953, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:00.729534: step 3726, loss 0.0602767, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:01.023930: step 3727, loss 0.0925296, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:01.285402: step 3728, loss 0.0353604, acc 1, learning_rate 0.000100001
2017-10-11T15:03:01.517515: step 3729, loss 0.013937, acc 1, learning_rate 0.000100001
2017-10-11T15:03:01.796486: step 3730, loss 0.168275, acc 0.921875, learning_rate 0.000100001
2017-10-11T15:03:02.044032: step 3731, loss 0.151106, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:03:02.289034: step 3732, loss 0.0985622, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:02.526636: step 3733, loss 0.129022, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:02.760030: step 3734, loss 0.0548802, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:03.023129: step 3735, loss 0.0810605, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:03.310325: step 3736, loss 0.0880622, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:03.611152: step 3737, loss 0.072109, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:03.924526: step 3738, loss 0.0583436, acc 1, learning_rate 0.000100001
2017-10-11T15:03:04.240129: step 3739, loss 0.0649178, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:04.550294: step 3740, loss 0.0844968, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:04.891600: step 3741, loss 0.170237, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:05.222003: step 3742, loss 0.0412853, acc 1, learning_rate 0.000100001
2017-10-11T15:03:05.532152: step 3743, loss 0.108701, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:03:05.792865: step 3744, loss 0.0826423, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:06.065045: step 3745, loss 0.085776, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:06.332618: step 3746, loss 0.0337698, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:06.616396: step 3747, loss 0.10328, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:06.900261: step 3748, loss 0.0839609, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:07.281882: step 3749, loss 0.0589564, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:07.586105: step 3750, loss 0.0577261, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:07.896876: step 3751, loss 0.0543703, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:08.172951: step 3752, loss 0.0877969, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:08.457621: step 3753, loss 0.0445295, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:08.769005: step 3754, loss 0.0787875, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:09.086046: step 3755, loss 0.07241, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:09.408509: step 3756, loss 0.0626584, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:09.706110: step 3757, loss 0.0520849, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:10.052244: step 3758, loss 0.0592028, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:10.367016: step 3759, loss 0.116699, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:10.687721: step 3760, loss 0.0961152, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-11T15:03:10.977176: step 3760, loss 0.225951, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3760

2017-10-11T15:03:13.592396: step 3761, loss 0.0752276, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:13.829359: step 3762, loss 0.0793873, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:14.141868: step 3763, loss 0.0750159, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:14.445781: step 3764, loss 0.179923, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:03:14.779884: step 3765, loss 0.0643071, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:15.037548: step 3766, loss 0.102662, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:15.306945: step 3767, loss 0.102101, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:15.571013: step 3768, loss 0.099834, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:15.843869: step 3769, loss 0.106497, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:16.084602: step 3770, loss 0.128181, acc 0.921875, learning_rate 0.000100001
2017-10-11T15:03:16.335469: step 3771, loss 0.0866092, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:16.583229: step 3772, loss 0.142386, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:16.807786: step 3773, loss 0.108581, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:03:17.061620: step 3774, loss 0.101073, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:17.320460: step 3775, loss 0.108393, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:03:17.647852: step 3776, loss 0.107614, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:17.969273: step 3777, loss 0.0553315, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:18.287711: step 3778, loss 0.0830883, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:18.571069: step 3779, loss 0.0817659, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:18.863075: step 3780, loss 0.113645, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:19.144154: step 3781, loss 0.0666543, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:19.456489: step 3782, loss 0.0575249, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:19.763545: step 3783, loss 0.0765605, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:20.052117: step 3784, loss 0.0349877, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:20.394084: step 3785, loss 0.0664043, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:20.654230: step 3786, loss 0.0537168, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:20.924073: step 3787, loss 0.0950417, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:21.204848: step 3788, loss 0.0527248, acc 1, learning_rate 0.000100001
2017-10-11T15:03:21.498209: step 3789, loss 0.210379, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:21.766528: step 3790, loss 0.0955899, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:22.037666: step 3791, loss 0.0235027, acc 1, learning_rate 0.000100001
2017-10-11T15:03:22.339606: step 3792, loss 0.125025, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:03:22.638642: step 3793, loss 0.195611, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:03:22.909987: step 3794, loss 0.0399778, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:23.223345: step 3795, loss 0.060754, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:23.532094: step 3796, loss 0.233191, acc 0.921875, learning_rate 0.000100001
2017-10-11T15:03:23.838555: step 3797, loss 0.110422, acc 0.921875, learning_rate 0.000100001
2017-10-11T15:03:24.186253: step 3798, loss 0.0282671, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:24.490189: step 3799, loss 0.0805798, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:24.851055: step 3800, loss 0.046481, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-11T15:03:25.178458: step 3800, loss 0.220536, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3800

2017-10-11T15:03:27.046928: step 3801, loss 0.0763115, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:27.318853: step 3802, loss 0.131803, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:03:27.604680: step 3803, loss 0.0788043, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:27.904293: step 3804, loss 0.0796477, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:28.254839: step 3805, loss 0.0674443, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:28.575993: step 3806, loss 0.06973, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:28.883854: step 3807, loss 0.0915357, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:29.172695: step 3808, loss 0.0774063, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:29.437438: step 3809, loss 0.139708, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:29.676197: step 3810, loss 0.0463778, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:29.913990: step 3811, loss 0.0488706, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:30.157659: step 3812, loss 0.111304, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:30.388304: step 3813, loss 0.0903934, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:30.611233: step 3814, loss 0.067914, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:30.895101: step 3815, loss 0.0581842, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:31.139465: step 3816, loss 0.113988, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:31.335824: step 3817, loss 0.0827832, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:31.611767: step 3818, loss 0.164421, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:31.908562: step 3819, loss 0.149961, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:32.213688: step 3820, loss 0.072573, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:32.513114: step 3821, loss 0.147964, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:32.782616: step 3822, loss 0.0448152, acc 0.980392, learning_rate 0.000100001
2017-10-11T15:03:33.110857: step 3823, loss 0.072398, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:33.427460: step 3824, loss 0.0906236, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:33.738297: step 3825, loss 0.0235672, acc 1, learning_rate 0.000100001
2017-10-11T15:03:34.055603: step 3826, loss 0.0526435, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:34.321922: step 3827, loss 0.0710656, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:34.591568: step 3828, loss 0.0986837, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:34.871125: step 3829, loss 0.0379024, acc 1, learning_rate 0.000100001
2017-10-11T15:03:35.137289: step 3830, loss 0.0609867, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:35.418144: step 3831, loss 0.0113069, acc 1, learning_rate 0.000100001
2017-10-11T15:03:35.672018: step 3832, loss 0.047614, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:35.983819: step 3833, loss 0.0516402, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:36.224799: step 3834, loss 0.0475275, acc 1, learning_rate 0.000100001
2017-10-11T15:03:36.509492: step 3835, loss 0.127339, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:36.814844: step 3836, loss 0.143555, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:37.113014: step 3837, loss 0.101582, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:37.428753: step 3838, loss 0.107189, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:37.778785: step 3839, loss 0.031919, acc 1, learning_rate 0.000100001
2017-10-11T15:03:38.067734: step 3840, loss 0.0334897, acc 1, learning_rate 0.000100001

Evaluation:
2017-10-11T15:03:38.333077: step 3840, loss 0.227705, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3840

2017-10-11T15:03:40.819049: step 3841, loss 0.0984056, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:41.104027: step 3842, loss 0.178515, acc 0.921875, learning_rate 0.000100001
2017-10-11T15:03:41.411295: step 3843, loss 0.0991193, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:41.671657: step 3844, loss 0.0540891, acc 1, learning_rate 0.000100001
2017-10-11T15:03:41.965708: step 3845, loss 0.0867986, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:42.292560: step 3846, loss 0.125413, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:42.628277: step 3847, loss 0.0820204, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:42.891117: step 3848, loss 0.0604358, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:43.159679: step 3849, loss 0.123877, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:43.444565: step 3850, loss 0.0982443, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:43.662208: step 3851, loss 0.0821472, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:43.891197: step 3852, loss 0.0867142, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:44.114106: step 3853, loss 0.0465722, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:44.311060: step 3854, loss 0.0703066, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:44.557065: step 3855, loss 0.0530129, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:44.823030: step 3856, loss 0.127296, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:45.109955: step 3857, loss 0.0892733, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:45.415360: step 3858, loss 0.0631973, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:45.718120: step 3859, loss 0.0406046, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:46.076426: step 3860, loss 0.0230688, acc 1, learning_rate 0.000100001
2017-10-11T15:03:46.379527: step 3861, loss 0.101679, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:46.702740: step 3862, loss 0.0363972, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:47.027154: step 3863, loss 0.0447487, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:47.350107: step 3864, loss 0.270165, acc 0.890625, learning_rate 0.000100001
2017-10-11T15:03:47.677171: step 3865, loss 0.0374277, acc 1, learning_rate 0.000100001
2017-10-11T15:03:48.004265: step 3866, loss 0.0416479, acc 1, learning_rate 0.000100001
2017-10-11T15:03:48.266279: step 3867, loss 0.144276, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:03:48.528618: step 3868, loss 0.0798423, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:48.772278: step 3869, loss 0.0718942, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:49.075261: step 3870, loss 0.0803989, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:49.351593: step 3871, loss 0.0552076, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:49.662692: step 3872, loss 0.0574251, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:49.955124: step 3873, loss 0.0311401, acc 1, learning_rate 0.000100001
2017-10-11T15:03:50.218796: step 3874, loss 0.102879, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:50.475525: step 3875, loss 0.0786469, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:50.747424: step 3876, loss 0.130483, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:51.023771: step 3877, loss 0.0281288, acc 1, learning_rate 0.000100001
2017-10-11T15:03:51.319811: step 3878, loss 0.0895699, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:51.657442: step 3879, loss 0.176521, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:03:51.974340: step 3880, loss 0.0623575, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-11T15:03:52.272985: step 3880, loss 0.2245, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3880

2017-10-11T15:03:55.451990: step 3881, loss 0.0458615, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:55.686251: step 3882, loss 0.164889, acc 0.90625, learning_rate 0.000100001
2017-10-11T15:03:55.975683: step 3883, loss 0.0858611, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:56.289733: step 3884, loss 0.0949737, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:56.600051: step 3885, loss 0.106749, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:56.869047: step 3886, loss 0.044274, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:57.102742: step 3887, loss 0.0471005, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:57.345284: step 3888, loss 0.0488586, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:57.548401: step 3889, loss 0.0829612, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:57.779830: step 3890, loss 0.044771, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:58.030584: step 3891, loss 0.0236546, acc 1, learning_rate 0.000100001
2017-10-11T15:03:58.255158: step 3892, loss 0.126779, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:03:58.487789: step 3893, loss 0.10544, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:58.700626: step 3894, loss 0.0829228, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:58.957916: step 3895, loss 0.0956796, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:03:59.236412: step 3896, loss 0.1369, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:59.580399: step 3897, loss 0.101157, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:03:59.891624: step 3898, loss 0.098031, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:00.197529: step 3899, loss 0.0294896, acc 1, learning_rate 0.000100001
2017-10-11T15:04:00.493650: step 3900, loss 0.101262, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:04:00.782192: step 3901, loss 0.126638, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:01.058206: step 3902, loss 0.0343332, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:04:01.324813: step 3903, loss 0.153879, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:01.586971: step 3904, loss 0.107686, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:01.862824: step 3905, loss 0.103051, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:02.187389: step 3906, loss 0.0968748, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:02.489878: step 3907, loss 0.104638, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:02.786026: step 3908, loss 0.0828189, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:04:03.018576: step 3909, loss 0.0135248, acc 1, learning_rate 0.000100001
2017-10-11T15:04:03.287222: step 3910, loss 0.0291707, acc 1, learning_rate 0.000100001
2017-10-11T15:04:03.559367: step 3911, loss 0.151488, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:03.835200: step 3912, loss 0.0395562, acc 1, learning_rate 0.000100001
2017-10-11T15:04:04.154377: step 3913, loss 0.094173, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:04.504496: step 3914, loss 0.126731, acc 0.9375, learning_rate 0.000100001
2017-10-11T15:04:04.834437: step 3915, loss 0.0910793, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:04:05.144579: step 3916, loss 0.0257757, acc 1, learning_rate 0.000100001
2017-10-11T15:04:05.478700: step 3917, loss 0.139666, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:04:05.810810: step 3918, loss 0.0216203, acc 1, learning_rate 0.000100001
2017-10-11T15:04:06.102980: step 3919, loss 0.084667, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:04:06.378251: step 3920, loss 0.0352908, acc 0.980392, learning_rate 0.000100001

Evaluation:
2017-10-11T15:04:06.673009: step 3920, loss 0.220086, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3920

2017-10-11T15:04:08.404858: step 3921, loss 0.0785492, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:08.707076: step 3922, loss 0.156249, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:09.009212: step 3923, loss 0.0434461, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:04:09.357080: step 3924, loss 0.145315, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:09.679748: step 3925, loss 0.086351, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:04:10.007354: step 3926, loss 0.127931, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:10.384327: step 3927, loss 0.155351, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:10.686974: step 3928, loss 0.0936516, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:04:10.996737: step 3929, loss 0.0547968, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:04:11.261488: step 3930, loss 0.0567673, acc 0.984375, learning_rate 0.000100001
2017-10-11T15:04:11.537824: step 3931, loss 0.146504, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:11.818013: step 3932, loss 0.106689, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:04:12.054125: step 3933, loss 0.0383795, acc 1, learning_rate 0.000100001
2017-10-11T15:04:12.264861: step 3934, loss 0.128384, acc 0.96875, learning_rate 0.000100001
2017-10-11T15:04:12.496744: step 3935, loss 0.157669, acc 0.921875, learning_rate 0.000100001
2017-10-11T15:04:12.715065: step 3936, loss 0.0906161, acc 0.953125, learning_rate 0.000100001
2017-10-11T15:04:12.941233: step 3937, loss 0.0513107, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:13.259751: step 3938, loss 0.144598, acc 0.9375, learning_rate 0.0001
2017-10-11T15:04:13.555673: step 3939, loss 0.101842, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:13.858537: step 3940, loss 0.0258201, acc 1, learning_rate 0.0001
2017-10-11T15:04:14.188757: step 3941, loss 0.0434206, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:14.496421: step 3942, loss 0.129842, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:14.776983: step 3943, loss 0.138733, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:15.066254: step 3944, loss 0.0239253, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:15.350431: step 3945, loss 0.0924419, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:15.668068: step 3946, loss 0.111514, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:15.958421: step 3947, loss 0.0709395, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:16.263119: step 3948, loss 0.0719433, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:16.545055: step 3949, loss 0.0669047, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:16.790182: step 3950, loss 0.061686, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:17.031355: step 3951, loss 0.0584464, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:17.295967: step 3952, loss 0.145194, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:17.573716: step 3953, loss 0.0127354, acc 1, learning_rate 0.0001
2017-10-11T15:04:17.816733: step 3954, loss 0.0564839, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:18.113767: step 3955, loss 0.137068, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:18.390263: step 3956, loss 0.0340703, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:18.666873: step 3957, loss 0.129592, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:18.968743: step 3958, loss 0.0828732, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:19.285590: step 3959, loss 0.0468684, acc 1, learning_rate 0.0001
2017-10-11T15:04:19.595054: step 3960, loss 0.0605404, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:04:19.880103: step 3960, loss 0.22136, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-3960

2017-10-11T15:04:22.527298: step 3961, loss 0.14793, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:22.854621: step 3962, loss 0.0344952, acc 1, learning_rate 0.0001
2017-10-11T15:04:23.177723: step 3963, loss 0.0861443, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:23.448108: step 3964, loss 0.151853, acc 0.9375, learning_rate 0.0001
2017-10-11T15:04:23.756269: step 3965, loss 0.0753494, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:24.068936: step 3966, loss 0.0838531, acc 0.9375, learning_rate 0.0001
2017-10-11T15:04:24.383999: step 3967, loss 0.0761646, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:24.683921: step 3968, loss 0.0317767, acc 1, learning_rate 0.0001
2017-10-11T15:04:24.947604: step 3969, loss 0.075462, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:25.149613: step 3970, loss 0.0734929, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:25.387013: step 3971, loss 0.0852169, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:25.629055: step 3972, loss 0.160137, acc 0.921875, learning_rate 0.0001
2017-10-11T15:04:25.863880: step 3973, loss 0.0509466, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:26.075671: step 3974, loss 0.155948, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:26.303767: step 3975, loss 0.0408607, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:26.561093: step 3976, loss 0.0815583, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:26.826242: step 3977, loss 0.0249254, acc 1, learning_rate 0.0001
2017-10-11T15:04:27.090811: step 3978, loss 0.0879539, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:27.358153: step 3979, loss 0.0400197, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:27.665804: step 3980, loss 0.0984354, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:27.923890: step 3981, loss 0.00869348, acc 1, learning_rate 0.0001
2017-10-11T15:04:28.255782: step 3982, loss 0.0607664, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:28.546496: step 3983, loss 0.132434, acc 0.9375, learning_rate 0.0001
2017-10-11T15:04:28.871159: step 3984, loss 0.116068, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:29.176635: step 3985, loss 0.0606245, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:29.468941: step 3986, loss 0.0872071, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:29.783996: step 3987, loss 0.0279125, acc 1, learning_rate 0.0001
2017-10-11T15:04:30.066681: step 3988, loss 0.09815, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:30.352190: step 3989, loss 0.15692, acc 0.9375, learning_rate 0.0001
2017-10-11T15:04:30.629270: step 3990, loss 0.042651, acc 1, learning_rate 0.0001
2017-10-11T15:04:30.910791: step 3991, loss 0.0790681, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:31.169962: step 3992, loss 0.0570874, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:31.428713: step 3993, loss 0.140789, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:31.708101: step 3994, loss 0.132205, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:31.984883: step 3995, loss 0.0820121, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:32.263706: step 3996, loss 0.126041, acc 0.9375, learning_rate 0.0001
2017-10-11T15:04:32.532614: step 3997, loss 0.101261, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:32.829864: step 3998, loss 0.0560659, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:33.192608: step 3999, loss 0.158827, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:33.485263: step 4000, loss 0.046977, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:04:33.784933: step 4000, loss 0.225129, acc 0.910791

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4000

2017-10-11T15:04:36.497504: step 4001, loss 0.133252, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:36.790581: step 4002, loss 0.114723, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:37.085944: step 4003, loss 0.0835546, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:37.355437: step 4004, loss 0.0606836, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:37.675846: step 4005, loss 0.0674749, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:37.988995: step 4006, loss 0.043783, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:38.236718: step 4007, loss 0.0936699, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:38.520520: step 4008, loss 0.101712, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:38.785280: step 4009, loss 0.102499, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:39.100909: step 4010, loss 0.173767, acc 0.9375, learning_rate 0.0001
2017-10-11T15:04:39.341895: step 4011, loss 0.150851, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:39.623176: step 4012, loss 0.0839955, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:39.878803: step 4013, loss 0.117038, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:40.186825: step 4014, loss 0.0843199, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:40.465098: step 4015, loss 0.127617, acc 0.9375, learning_rate 0.0001
2017-10-11T15:04:40.736182: step 4016, loss 0.105757, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:41.002381: step 4017, loss 0.078332, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:41.255659: step 4018, loss 0.0844661, acc 0.960784, learning_rate 0.0001
2017-10-11T15:04:41.575887: step 4019, loss 0.0293471, acc 1, learning_rate 0.0001
2017-10-11T15:04:41.905422: step 4020, loss 0.0225171, acc 1, learning_rate 0.0001
2017-10-11T15:04:42.209614: step 4021, loss 0.0823789, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:42.503158: step 4022, loss 0.227378, acc 0.9375, learning_rate 0.0001
2017-10-11T15:04:42.782428: step 4023, loss 0.0591259, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:43.057898: step 4024, loss 0.0367944, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:43.353646: step 4025, loss 0.129253, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:43.664973: step 4026, loss 0.116421, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:43.990123: step 4027, loss 0.103007, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:44.276275: step 4028, loss 0.0714951, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:44.553279: step 4029, loss 0.0679636, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:44.777117: step 4030, loss 0.178153, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:45.059127: step 4031, loss 0.042397, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:45.327848: step 4032, loss 0.0602245, acc 1, learning_rate 0.0001
2017-10-11T15:04:45.600030: step 4033, loss 0.0437573, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:45.854759: step 4034, loss 0.0521664, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:46.154910: step 4035, loss 0.110159, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:46.487977: step 4036, loss 0.0483349, acc 1, learning_rate 0.0001
2017-10-11T15:04:46.774624: step 4037, loss 0.0439054, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:47.054246: step 4038, loss 0.030736, acc 1, learning_rate 0.0001
2017-10-11T15:04:47.367072: step 4039, loss 0.0640411, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:47.670742: step 4040, loss 0.0517913, acc 1, learning_rate 0.0001

Evaluation:
2017-10-11T15:04:47.951113: step 4040, loss 0.221851, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4040

2017-10-11T15:04:49.841741: step 4041, loss 0.03599, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:50.140911: step 4042, loss 0.104464, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:50.418000: step 4043, loss 0.129533, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:50.759767: step 4044, loss 0.0604711, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:51.082553: step 4045, loss 0.0489239, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:51.392601: step 4046, loss 0.135932, acc 0.921875, learning_rate 0.0001
2017-10-11T15:04:51.655967: step 4047, loss 0.198734, acc 0.9375, learning_rate 0.0001
2017-10-11T15:04:51.934502: step 4048, loss 0.166522, acc 0.9375, learning_rate 0.0001
2017-10-11T15:04:52.230912: step 4049, loss 0.0245255, acc 1, learning_rate 0.0001
2017-10-11T15:04:52.527346: step 4050, loss 0.0663241, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:52.769842: step 4051, loss 0.0516205, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:53.016080: step 4052, loss 0.113362, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:53.252815: step 4053, loss 0.0992975, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:53.482273: step 4054, loss 0.0282212, acc 1, learning_rate 0.0001
2017-10-11T15:04:53.727564: step 4055, loss 0.108185, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:53.957063: step 4056, loss 0.0757874, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:54.177039: step 4057, loss 0.131138, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:54.440991: step 4058, loss 0.109418, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:54.681559: step 4059, loss 0.128966, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:54.945829: step 4060, loss 0.0368527, acc 1, learning_rate 0.0001
2017-10-11T15:04:55.212342: step 4061, loss 0.0839624, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:55.514752: step 4062, loss 0.0950962, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:55.781520: step 4063, loss 0.0595066, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:56.097045: step 4064, loss 0.154211, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:56.408877: step 4065, loss 0.101888, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:56.709960: step 4066, loss 0.0652816, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:57.012127: step 4067, loss 0.0872365, acc 0.953125, learning_rate 0.0001
2017-10-11T15:04:57.313691: step 4068, loss 0.0187157, acc 1, learning_rate 0.0001
2017-10-11T15:04:57.565999: step 4069, loss 0.0282352, acc 1, learning_rate 0.0001
2017-10-11T15:04:57.823126: step 4070, loss 0.089779, acc 0.96875, learning_rate 0.0001
2017-10-11T15:04:58.059120: step 4071, loss 0.151356, acc 0.9375, learning_rate 0.0001
2017-10-11T15:04:58.333166: step 4072, loss 0.0572455, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:58.633296: step 4073, loss 0.0298192, acc 1, learning_rate 0.0001
2017-10-11T15:04:58.883601: step 4074, loss 0.0607164, acc 0.984375, learning_rate 0.0001
2017-10-11T15:04:59.136012: step 4075, loss 0.246832, acc 0.90625, learning_rate 0.0001
2017-10-11T15:04:59.412554: step 4076, loss 0.100373, acc 0.921875, learning_rate 0.0001
2017-10-11T15:04:59.690882: step 4077, loss 0.0656915, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:00.017411: step 4078, loss 0.0570613, acc 1, learning_rate 0.0001
2017-10-11T15:05:00.372745: step 4079, loss 0.167234, acc 0.9375, learning_rate 0.0001
2017-10-11T15:05:00.696911: step 4080, loss 0.122352, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:05:01.001146: step 4080, loss 0.221796, acc 0.910791

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4080

2017-10-11T15:05:04.478148: step 4081, loss 0.0944662, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:04.759631: step 4082, loss 0.0303015, acc 1, learning_rate 0.0001
2017-10-11T15:05:05.073187: step 4083, loss 0.0692142, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:05.392324: step 4084, loss 0.0361515, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:05.689870: step 4085, loss 0.0895099, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:05.969765: step 4086, loss 0.0398256, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:06.244667: step 4087, loss 0.0229949, acc 1, learning_rate 0.0001
2017-10-11T15:05:06.514657: step 4088, loss 0.0587251, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:06.769850: step 4089, loss 0.0229708, acc 1, learning_rate 0.0001
2017-10-11T15:05:07.019417: step 4090, loss 0.119532, acc 0.9375, learning_rate 0.0001
2017-10-11T15:05:07.278158: step 4091, loss 0.159149, acc 0.9375, learning_rate 0.0001
2017-10-11T15:05:07.561311: step 4092, loss 0.0368822, acc 1, learning_rate 0.0001
2017-10-11T15:05:07.847683: step 4093, loss 0.128885, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:08.151084: step 4094, loss 0.0356789, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:08.457660: step 4095, loss 0.0928053, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:08.714625: step 4096, loss 0.0461461, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:08.966818: step 4097, loss 0.0398216, acc 1, learning_rate 0.0001
2017-10-11T15:05:09.203532: step 4098, loss 0.0658665, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:09.478631: step 4099, loss 0.0236354, acc 1, learning_rate 0.0001
2017-10-11T15:05:09.774098: step 4100, loss 0.0388161, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:10.055811: step 4101, loss 0.115781, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:10.357300: step 4102, loss 0.021114, acc 1, learning_rate 0.0001
2017-10-11T15:05:10.665993: step 4103, loss 0.0890415, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:10.997624: step 4104, loss 0.190431, acc 0.921875, learning_rate 0.0001
2017-10-11T15:05:11.316579: step 4105, loss 0.171837, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:11.603535: step 4106, loss 0.125596, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:11.866348: step 4107, loss 0.0601273, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:12.104820: step 4108, loss 0.0690005, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:12.386786: step 4109, loss 0.107985, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:12.658748: step 4110, loss 0.0587337, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:12.944554: step 4111, loss 0.0895669, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:13.222540: step 4112, loss 0.101479, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:13.535181: step 4113, loss 0.0713802, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:13.832287: step 4114, loss 0.0505933, acc 1, learning_rate 0.0001
2017-10-11T15:05:14.127870: step 4115, loss 0.195067, acc 0.921875, learning_rate 0.0001
2017-10-11T15:05:14.438166: step 4116, loss 0.0364307, acc 1, learning_rate 0.0001
2017-10-11T15:05:14.736463: step 4117, loss 0.0305047, acc 1, learning_rate 0.0001
2017-10-11T15:05:15.055592: step 4118, loss 0.0465134, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:15.384978: step 4119, loss 0.0193055, acc 1, learning_rate 0.0001
2017-10-11T15:05:15.656451: step 4120, loss 0.0486256, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:05:15.958653: step 4120, loss 0.217994, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4120

2017-10-11T15:05:18.117234: step 4121, loss 0.088512, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:18.382034: step 4122, loss 0.0817269, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:18.688650: step 4123, loss 0.0718779, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:19.023202: step 4124, loss 0.104678, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:19.286535: step 4125, loss 0.140495, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:19.549756: step 4126, loss 0.088039, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:19.839347: step 4127, loss 0.168277, acc 0.9375, learning_rate 0.0001
2017-10-11T15:05:20.110468: step 4128, loss 0.166969, acc 0.9375, learning_rate 0.0001
2017-10-11T15:05:20.394173: step 4129, loss 0.0461932, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:20.676954: step 4130, loss 0.149723, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:20.981218: step 4131, loss 0.0526048, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:21.224773: step 4132, loss 0.0831233, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:21.477767: step 4133, loss 0.10164, acc 0.9375, learning_rate 0.0001
2017-10-11T15:05:21.770242: step 4134, loss 0.0826502, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:22.051592: step 4135, loss 0.0494298, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:22.332447: step 4136, loss 0.0584269, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:22.619280: step 4137, loss 0.030919, acc 1, learning_rate 0.0001
2017-10-11T15:05:22.975999: step 4138, loss 0.0480876, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:23.300218: step 4139, loss 0.0390661, acc 1, learning_rate 0.0001
2017-10-11T15:05:23.636110: step 4140, loss 0.120036, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:23.962185: step 4141, loss 0.179545, acc 0.921875, learning_rate 0.0001
2017-10-11T15:05:24.261361: step 4142, loss 0.142437, acc 0.9375, learning_rate 0.0001
2017-10-11T15:05:24.552068: step 4143, loss 0.0150397, acc 1, learning_rate 0.0001
2017-10-11T15:05:24.885218: step 4144, loss 0.0142485, acc 1, learning_rate 0.0001
2017-10-11T15:05:25.191623: step 4145, loss 0.0757539, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:25.449861: step 4146, loss 0.0942193, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:25.738645: step 4147, loss 0.0252172, acc 1, learning_rate 0.0001
2017-10-11T15:05:26.001401: step 4148, loss 0.0770439, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:26.258890: step 4149, loss 0.0614295, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:26.512048: step 4150, loss 0.0411737, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:26.777698: step 4151, loss 0.0309576, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:27.083922: step 4152, loss 0.0734825, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:27.393923: step 4153, loss 0.0977624, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:27.732819: step 4154, loss 0.0694955, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:28.016665: step 4155, loss 0.0542346, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:28.359233: step 4156, loss 0.0222958, acc 1, learning_rate 0.0001
2017-10-11T15:05:28.664017: step 4157, loss 0.0830631, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:28.954257: step 4158, loss 0.0380587, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:29.253389: step 4159, loss 0.0899049, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:29.559499: step 4160, loss 0.129972, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T15:05:29.852982: step 4160, loss 0.224886, acc 0.909353

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4160

2017-10-11T15:05:32.083262: step 4161, loss 0.0662842, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:32.377528: step 4162, loss 0.0828145, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:32.682415: step 4163, loss 0.0523154, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:32.977589: step 4164, loss 0.118112, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:33.288659: step 4165, loss 0.038829, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:33.592274: step 4166, loss 0.0557852, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:33.862369: step 4167, loss 0.0435922, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:34.160609: step 4168, loss 0.0923876, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:34.463902: step 4169, loss 0.148431, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:34.742554: step 4170, loss 0.0241808, acc 1, learning_rate 0.0001
2017-10-11T15:05:35.028504: step 4171, loss 0.122516, acc 0.9375, learning_rate 0.0001
2017-10-11T15:05:35.336210: step 4172, loss 0.0762519, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:35.601246: step 4173, loss 0.0822708, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:35.864660: step 4174, loss 0.104675, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:36.144227: step 4175, loss 0.149155, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:36.384154: step 4176, loss 0.154716, acc 0.9375, learning_rate 0.0001
2017-10-11T15:05:36.685866: step 4177, loss 0.108041, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:36.997257: step 4178, loss 0.0450182, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:37.285654: step 4179, loss 0.108942, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:37.575818: step 4180, loss 0.0982344, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:37.920526: step 4181, loss 0.0585706, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:38.255798: step 4182, loss 0.113499, acc 0.9375, learning_rate 0.0001
2017-10-11T15:05:38.552889: step 4183, loss 0.0533207, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:38.852659: step 4184, loss 0.131959, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:39.158365: step 4185, loss 0.0708136, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:39.398277: step 4186, loss 0.0642866, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:39.660508: step 4187, loss 0.166958, acc 0.921875, learning_rate 0.0001
2017-10-11T15:05:39.896785: step 4188, loss 0.107042, acc 0.921875, learning_rate 0.0001
2017-10-11T15:05:40.162185: step 4189, loss 0.0434872, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:40.396281: step 4190, loss 0.161698, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:40.675638: step 4191, loss 0.0588551, acc 1, learning_rate 0.0001
2017-10-11T15:05:40.925428: step 4192, loss 0.236762, acc 0.9375, learning_rate 0.0001
2017-10-11T15:05:41.200768: step 4193, loss 0.0941768, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:41.477608: step 4194, loss 0.0577252, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:41.802188: step 4195, loss 0.0900969, acc 1, learning_rate 0.0001
2017-10-11T15:05:42.148708: step 4196, loss 0.0416806, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:42.459018: step 4197, loss 0.105788, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:42.760039: step 4198, loss 0.0279805, acc 1, learning_rate 0.0001
2017-10-11T15:05:43.077448: step 4199, loss 0.137869, acc 0.90625, learning_rate 0.0001
2017-10-11T15:05:43.383753: step 4200, loss 0.0576354, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:05:43.674376: step 4200, loss 0.215774, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4200

2017-10-11T15:05:46.673926: step 4201, loss 0.0153276, acc 1, learning_rate 0.0001
2017-10-11T15:05:46.948682: step 4202, loss 0.167121, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:47.194906: step 4203, loss 0.093405, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:47.443181: step 4204, loss 0.0456658, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:47.697076: step 4205, loss 0.165016, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:47.941284: step 4206, loss 0.0797566, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:48.203050: step 4207, loss 0.108563, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:48.456902: step 4208, loss 0.0450631, acc 1, learning_rate 0.0001
2017-10-11T15:05:48.751579: step 4209, loss 0.111167, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:49.035972: step 4210, loss 0.106642, acc 0.9375, learning_rate 0.0001
2017-10-11T15:05:49.330293: step 4211, loss 0.0504866, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:49.633733: step 4212, loss 0.0417078, acc 1, learning_rate 0.0001
2017-10-11T15:05:49.879229: step 4213, loss 0.0361111, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:50.107627: step 4214, loss 0.177862, acc 0.941176, learning_rate 0.0001
2017-10-11T15:05:50.354792: step 4215, loss 0.0555459, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:50.594439: step 4216, loss 0.0521427, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:50.911278: step 4217, loss 0.068128, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:51.194931: step 4218, loss 0.0721681, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:51.500724: step 4219, loss 0.135417, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:51.805108: step 4220, loss 0.0390291, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:52.092372: step 4221, loss 0.066311, acc 1, learning_rate 0.0001
2017-10-11T15:05:52.406168: step 4222, loss 0.118586, acc 0.9375, learning_rate 0.0001
2017-10-11T15:05:52.704560: step 4223, loss 0.0563855, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:52.993800: step 4224, loss 0.118874, acc 0.96875, learning_rate 0.0001
2017-10-11T15:05:53.291263: step 4225, loss 0.110728, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:53.552336: step 4226, loss 0.0913991, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:53.804389: step 4227, loss 0.115765, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:54.067088: step 4228, loss 0.153144, acc 0.9375, learning_rate 0.0001
2017-10-11T15:05:54.315790: step 4229, loss 0.0339003, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:54.588237: step 4230, loss 0.0920804, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:54.876986: step 4231, loss 0.0627375, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:55.154954: step 4232, loss 0.0718048, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:55.474316: step 4233, loss 0.0284261, acc 1, learning_rate 0.0001
2017-10-11T15:05:55.785022: step 4234, loss 0.0487241, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:56.095307: step 4235, loss 0.0348221, acc 1, learning_rate 0.0001
2017-10-11T15:05:56.395146: step 4236, loss 0.100837, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:56.718038: step 4237, loss 0.0410617, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:57.040099: step 4238, loss 0.132475, acc 0.953125, learning_rate 0.0001
2017-10-11T15:05:57.361540: step 4239, loss 0.0470322, acc 0.984375, learning_rate 0.0001
2017-10-11T15:05:57.703660: step 4240, loss 0.0453971, acc 1, learning_rate 0.0001

Evaluation:
2017-10-11T15:05:58.003092: step 4240, loss 0.216854, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4240

2017-10-11T15:05:59.916434: step 4241, loss 0.0681985, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:00.228520: step 4242, loss 0.0445735, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:00.526911: step 4243, loss 0.108564, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:00.818006: step 4244, loss 0.121943, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:01.135051: step 4245, loss 0.0385275, acc 1, learning_rate 0.0001
2017-10-11T15:06:01.421698: step 4246, loss 0.0923304, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:01.709203: step 4247, loss 0.0657659, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:02.034504: step 4248, loss 0.0875589, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:02.300418: step 4249, loss 0.150012, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:02.576205: step 4250, loss 0.108632, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:02.795174: step 4251, loss 0.0748131, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:03.046654: step 4252, loss 0.0260651, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:03.318203: step 4253, loss 0.114633, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:03.582574: step 4254, loss 0.188845, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:03.834169: step 4255, loss 0.069244, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:04.165454: step 4256, loss 0.12668, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:04.457434: step 4257, loss 0.113323, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:04.758645: step 4258, loss 0.0872497, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:05.071504: step 4259, loss 0.0818236, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:05.372163: step 4260, loss 0.138073, acc 0.9375, learning_rate 0.0001
2017-10-11T15:06:05.677445: step 4261, loss 0.0354414, acc 1, learning_rate 0.0001
2017-10-11T15:06:05.987434: step 4262, loss 0.157387, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:06.293288: step 4263, loss 0.120187, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:06.592060: step 4264, loss 0.135287, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:06.855378: step 4265, loss 0.0123605, acc 1, learning_rate 0.0001
2017-10-11T15:06:07.131793: step 4266, loss 0.0549389, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:07.415640: step 4267, loss 0.158735, acc 0.9375, learning_rate 0.0001
2017-10-11T15:06:07.687940: step 4268, loss 0.0788406, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:07.961967: step 4269, loss 0.0438447, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:08.220258: step 4270, loss 0.0765218, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:08.506011: step 4271, loss 0.0632667, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:08.802911: step 4272, loss 0.118891, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:09.130242: step 4273, loss 0.169021, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:09.430116: step 4274, loss 0.0624416, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:09.740832: step 4275, loss 0.0797955, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:10.053982: step 4276, loss 0.147247, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:10.361135: step 4277, loss 0.122037, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:10.680853: step 4278, loss 0.067365, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:10.975312: step 4279, loss 0.094462, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:11.282750: step 4280, loss 0.133135, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T15:06:11.594767: step 4280, loss 0.223283, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4280

2017-10-11T15:06:14.092416: step 4281, loss 0.0643455, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:14.416847: step 4282, loss 0.0606267, acc 1, learning_rate 0.0001
2017-10-11T15:06:14.749193: step 4283, loss 0.0509069, acc 1, learning_rate 0.0001
2017-10-11T15:06:15.012642: step 4284, loss 0.147708, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:15.282624: step 4285, loss 0.079941, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:15.574000: step 4286, loss 0.066039, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:15.835022: step 4287, loss 0.125608, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:16.055409: step 4288, loss 0.0787262, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:16.270695: step 4289, loss 0.0416819, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:16.610480: step 4290, loss 0.0727028, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:16.900155: step 4291, loss 0.0731078, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:17.200016: step 4292, loss 0.0284805, acc 1, learning_rate 0.0001
2017-10-11T15:06:17.465076: step 4293, loss 0.0394273, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:17.746929: step 4294, loss 0.0403945, acc 1, learning_rate 0.0001
2017-10-11T15:06:18.014103: step 4295, loss 0.158027, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:18.267100: step 4296, loss 0.0259534, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:18.583740: step 4297, loss 0.0513392, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:18.880526: step 4298, loss 0.0280831, acc 1, learning_rate 0.0001
2017-10-11T15:06:19.193453: step 4299, loss 0.0808244, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:19.503993: step 4300, loss 0.0448697, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:19.799084: step 4301, loss 0.134309, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:20.128719: step 4302, loss 0.0415068, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:20.442525: step 4303, loss 0.0160667, acc 1, learning_rate 0.0001
2017-10-11T15:06:20.713855: step 4304, loss 0.172451, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:21.003228: step 4305, loss 0.0237862, acc 1, learning_rate 0.0001
2017-10-11T15:06:21.284097: step 4306, loss 0.049301, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:21.533742: step 4307, loss 0.0437132, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:21.837366: step 4308, loss 0.155633, acc 0.9375, learning_rate 0.0001
2017-10-11T15:06:22.097210: step 4309, loss 0.127874, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:22.346837: step 4310, loss 0.0296358, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:22.627986: step 4311, loss 0.0620986, acc 1, learning_rate 0.0001
2017-10-11T15:06:22.884550: step 4312, loss 0.171876, acc 0.960784, learning_rate 0.0001
2017-10-11T15:06:23.205071: step 4313, loss 0.0922606, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:23.521437: step 4314, loss 0.116084, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:23.838928: step 4315, loss 0.0718791, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:24.128212: step 4316, loss 0.013496, acc 1, learning_rate 0.0001
2017-10-11T15:06:24.430289: step 4317, loss 0.0832011, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:24.732283: step 4318, loss 0.0428586, acc 1, learning_rate 0.0001
2017-10-11T15:06:25.014890: step 4319, loss 0.0673795, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:25.288633: step 4320, loss 0.112667, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:06:25.546696: step 4320, loss 0.220447, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4320

2017-10-11T15:06:28.514831: step 4321, loss 0.0503785, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:28.791534: step 4322, loss 0.0717976, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:29.070537: step 4323, loss 0.0611111, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:29.358695: step 4324, loss 0.162098, acc 0.9375, learning_rate 0.0001
2017-10-11T15:06:29.674607: step 4325, loss 0.148237, acc 0.9375, learning_rate 0.0001
2017-10-11T15:06:29.956993: step 4326, loss 0.0641709, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:30.221478: step 4327, loss 0.0246016, acc 1, learning_rate 0.0001
2017-10-11T15:06:30.508126: step 4328, loss 0.0447395, acc 1, learning_rate 0.0001
2017-10-11T15:06:30.753825: step 4329, loss 0.0445583, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:31.012489: step 4330, loss 0.0805668, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:31.284897: step 4331, loss 0.0749474, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:31.623228: step 4332, loss 0.0944494, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:31.947488: step 4333, loss 0.0923431, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:32.253172: step 4334, loss 0.0174534, acc 1, learning_rate 0.0001
2017-10-11T15:06:32.555851: step 4335, loss 0.14905, acc 0.90625, learning_rate 0.0001
2017-10-11T15:06:32.871558: step 4336, loss 0.118576, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:33.164501: step 4337, loss 0.0297755, acc 1, learning_rate 0.0001
2017-10-11T15:06:33.460490: step 4338, loss 0.0935755, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:33.734230: step 4339, loss 0.0143708, acc 1, learning_rate 0.0001
2017-10-11T15:06:34.050433: step 4340, loss 0.0152758, acc 1, learning_rate 0.0001
2017-10-11T15:06:34.295332: step 4341, loss 0.0790704, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:34.542692: step 4342, loss 0.0709459, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:34.838708: step 4343, loss 0.013308, acc 1, learning_rate 0.0001
2017-10-11T15:06:35.073699: step 4344, loss 0.0621396, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:35.344534: step 4345, loss 0.0329987, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:35.625445: step 4346, loss 0.0179429, acc 1, learning_rate 0.0001
2017-10-11T15:06:35.902653: step 4347, loss 0.0995026, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:36.219406: step 4348, loss 0.165528, acc 0.921875, learning_rate 0.0001
2017-10-11T15:06:36.497886: step 4349, loss 0.104976, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:36.815150: step 4350, loss 0.102264, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:37.106721: step 4351, loss 0.0510659, acc 1, learning_rate 0.0001
2017-10-11T15:06:37.415054: step 4352, loss 0.0866641, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:37.698114: step 4353, loss 0.0951651, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:37.981932: step 4354, loss 0.028585, acc 1, learning_rate 0.0001
2017-10-11T15:06:38.271913: step 4355, loss 0.159246, acc 0.921875, learning_rate 0.0001
2017-10-11T15:06:38.597001: step 4356, loss 0.133588, acc 0.9375, learning_rate 0.0001
2017-10-11T15:06:38.887110: step 4357, loss 0.110504, acc 0.9375, learning_rate 0.0001
2017-10-11T15:06:39.173994: step 4358, loss 0.110782, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:39.492062: step 4359, loss 0.0937202, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:39.780000: step 4360, loss 0.0459431, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:06:40.076957: step 4360, loss 0.217439, acc 0.920863

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4360

2017-10-11T15:06:42.214326: step 4361, loss 0.0612521, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:42.507814: step 4362, loss 0.0251728, acc 1, learning_rate 0.0001
2017-10-11T15:06:42.772886: step 4363, loss 0.0929411, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:43.070496: step 4364, loss 0.0524141, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:43.291483: step 4365, loss 0.170944, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:43.515813: step 4366, loss 0.0768432, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:43.768816: step 4367, loss 0.0390209, acc 1, learning_rate 0.0001
2017-10-11T15:06:44.061746: step 4368, loss 0.0515012, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:44.344852: step 4369, loss 0.0150746, acc 1, learning_rate 0.0001
2017-10-11T15:06:44.656545: step 4370, loss 0.0905802, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:44.935469: step 4371, loss 0.0588275, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:45.182719: step 4372, loss 0.111019, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:45.487192: step 4373, loss 0.110631, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:45.810927: step 4374, loss 0.0747716, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:46.095680: step 4375, loss 0.0883801, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:46.395400: step 4376, loss 0.0763046, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:46.692592: step 4377, loss 0.086811, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:47.029455: step 4378, loss 0.0568788, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:47.369369: step 4379, loss 0.0721136, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:47.618988: step 4380, loss 0.0693067, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:47.892445: step 4381, loss 0.0762821, acc 0.9375, learning_rate 0.0001
2017-10-11T15:06:48.134302: step 4382, loss 0.0694316, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:48.405746: step 4383, loss 0.0267314, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:48.655966: step 4384, loss 0.0276865, acc 1, learning_rate 0.0001
2017-10-11T15:06:48.907556: step 4385, loss 0.0362145, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:49.180301: step 4386, loss 0.0368644, acc 1, learning_rate 0.0001
2017-10-11T15:06:49.489747: step 4387, loss 0.0825148, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:49.801350: step 4388, loss 0.088732, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:50.139411: step 4389, loss 0.0369004, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:50.429603: step 4390, loss 0.0418325, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:50.725584: step 4391, loss 0.0843911, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:50.999554: step 4392, loss 0.104547, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:51.300068: step 4393, loss 0.0637192, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:51.561268: step 4394, loss 0.0294191, acc 1, learning_rate 0.0001
2017-10-11T15:06:51.858678: step 4395, loss 0.138624, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:52.141686: step 4396, loss 0.0912645, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:52.430520: step 4397, loss 0.0655986, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:52.744275: step 4398, loss 0.141829, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:53.028781: step 4399, loss 0.0747919, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:53.342011: step 4400, loss 0.0942425, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T15:06:53.600628: step 4400, loss 0.223603, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4400

2017-10-11T15:06:56.832792: step 4401, loss 0.0398868, acc 1, learning_rate 0.0001
2017-10-11T15:06:57.146093: step 4402, loss 0.0396162, acc 0.96875, learning_rate 0.0001
2017-10-11T15:06:57.428130: step 4403, loss 0.0359861, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:57.682297: step 4404, loss 0.0812656, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:57.963196: step 4405, loss 0.0698788, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:58.235776: step 4406, loss 0.0408688, acc 1, learning_rate 0.0001
2017-10-11T15:06:58.507958: step 4407, loss 0.106713, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:58.745093: step 4408, loss 0.0510542, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:59.013470: step 4409, loss 0.10302, acc 0.953125, learning_rate 0.0001
2017-10-11T15:06:59.258541: step 4410, loss 0.165061, acc 0.960784, learning_rate 0.0001
2017-10-11T15:06:59.560554: step 4411, loss 0.0729525, acc 0.984375, learning_rate 0.0001
2017-10-11T15:06:59.866076: step 4412, loss 0.0405574, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:00.195024: step 4413, loss 0.0680407, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:00.485199: step 4414, loss 0.0281376, acc 1, learning_rate 0.0001
2017-10-11T15:07:00.748941: step 4415, loss 0.0290234, acc 1, learning_rate 0.0001
2017-10-11T15:07:01.004467: step 4416, loss 0.070031, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:01.287622: step 4417, loss 0.0234561, acc 1, learning_rate 0.0001
2017-10-11T15:07:01.562431: step 4418, loss 0.0857982, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:01.779870: step 4419, loss 0.0781565, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:02.034068: step 4420, loss 0.0249237, acc 1, learning_rate 0.0001
2017-10-11T15:07:02.329057: step 4421, loss 0.0545437, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:02.640741: step 4422, loss 0.125478, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:02.940950: step 4423, loss 0.0961029, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:03.240294: step 4424, loss 0.0716567, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:03.552199: step 4425, loss 0.0642085, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:03.854565: step 4426, loss 0.0433277, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:04.147243: step 4427, loss 0.0609372, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:04.399550: step 4428, loss 0.0869721, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:04.747766: step 4429, loss 0.0409256, acc 1, learning_rate 0.0001
2017-10-11T15:07:05.070505: step 4430, loss 0.146162, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:05.405262: step 4431, loss 0.173513, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:05.717497: step 4432, loss 0.12681, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:05.968412: step 4433, loss 0.0235184, acc 1, learning_rate 0.0001
2017-10-11T15:07:06.266173: step 4434, loss 0.0245026, acc 1, learning_rate 0.0001
2017-10-11T15:07:06.690370: step 4435, loss 0.0381585, acc 1, learning_rate 0.0001
2017-10-11T15:07:07.013857: step 4436, loss 0.0579569, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:07.327569: step 4437, loss 0.068152, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:07.635900: step 4438, loss 0.0189277, acc 1, learning_rate 0.0001
2017-10-11T15:07:07.956233: step 4439, loss 0.0579025, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:08.240149: step 4440, loss 0.131305, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:07:08.505132: step 4440, loss 0.222516, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4440

2017-10-11T15:07:10.448068: step 4441, loss 0.0216277, acc 1, learning_rate 0.0001
2017-10-11T15:07:10.735027: step 4442, loss 0.0169366, acc 1, learning_rate 0.0001
2017-10-11T15:07:11.052489: step 4443, loss 0.0385589, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:11.362509: step 4444, loss 0.077856, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:11.656878: step 4445, loss 0.11778, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:11.946573: step 4446, loss 0.0392269, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:12.229818: step 4447, loss 0.137369, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:12.472252: step 4448, loss 0.13601, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:12.747798: step 4449, loss 0.120831, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:13.067835: step 4450, loss 0.0275486, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:13.358702: step 4451, loss 0.0852662, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:13.663500: step 4452, loss 0.144872, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:13.990667: step 4453, loss 0.0408848, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:14.254860: step 4454, loss 0.146377, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:14.509871: step 4455, loss 0.0454655, acc 1, learning_rate 0.0001
2017-10-11T15:07:14.784294: step 4456, loss 0.0595952, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:15.089191: step 4457, loss 0.0955432, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:15.338490: step 4458, loss 0.0444142, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:15.599255: step 4459, loss 0.0797407, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:15.857232: step 4460, loss 0.0162635, acc 1, learning_rate 0.0001
2017-10-11T15:07:16.164595: step 4461, loss 0.0646848, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:16.459910: step 4462, loss 0.0495606, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:16.776483: step 4463, loss 0.0406392, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:17.095713: step 4464, loss 0.0484334, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:17.405425: step 4465, loss 0.0423509, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:17.688133: step 4466, loss 0.0730663, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:17.979375: step 4467, loss 0.0337952, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:18.303780: step 4468, loss 0.0375719, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:18.598623: step 4469, loss 0.0831707, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:18.889860: step 4470, loss 0.0911189, acc 0.9375, learning_rate 0.0001
2017-10-11T15:07:19.181890: step 4471, loss 0.0368251, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:19.514210: step 4472, loss 0.0804596, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:19.818359: step 4473, loss 0.0481666, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:20.151547: step 4474, loss 0.0448167, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:20.494093: step 4475, loss 0.0422877, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:20.827066: step 4476, loss 0.0542652, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:21.148428: step 4477, loss 0.0868317, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:21.443183: step 4478, loss 0.111199, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:21.745275: step 4479, loss 0.0517076, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:22.038430: step 4480, loss 0.0797849, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T15:07:22.329694: step 4480, loss 0.218059, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4480

2017-10-11T15:07:24.133491: step 4481, loss 0.0891419, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:24.455846: step 4482, loss 0.0894372, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:24.775724: step 4483, loss 0.062023, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:25.035722: step 4484, loss 0.124765, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:25.331002: step 4485, loss 0.0605814, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:25.608601: step 4486, loss 0.0890069, acc 0.9375, learning_rate 0.0001
2017-10-11T15:07:25.903665: step 4487, loss 0.16687, acc 0.9375, learning_rate 0.0001
2017-10-11T15:07:26.190979: step 4488, loss 0.10306, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:26.444216: step 4489, loss 0.0652212, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:26.727767: step 4490, loss 0.0835525, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:27.020746: step 4491, loss 0.118686, acc 0.9375, learning_rate 0.0001
2017-10-11T15:07:27.319045: step 4492, loss 0.0684198, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:27.613219: step 4493, loss 0.110594, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:27.923179: step 4494, loss 0.0794097, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:28.179292: step 4495, loss 0.0932074, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:28.449830: step 4496, loss 0.0414493, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:28.698017: step 4497, loss 0.117869, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:29.002548: step 4498, loss 0.0602847, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:29.271645: step 4499, loss 0.0434507, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:29.528374: step 4500, loss 0.0234675, acc 1, learning_rate 0.0001
2017-10-11T15:07:29.765289: step 4501, loss 0.159075, acc 0.921875, learning_rate 0.0001
2017-10-11T15:07:30.040586: step 4502, loss 0.132232, acc 0.921875, learning_rate 0.0001
2017-10-11T15:07:30.323931: step 4503, loss 0.114556, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:30.593735: step 4504, loss 0.0365941, acc 1, learning_rate 0.0001
2017-10-11T15:07:30.893916: step 4505, loss 0.00872687, acc 1, learning_rate 0.0001
2017-10-11T15:07:31.216211: step 4506, loss 0.0939131, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:31.494859: step 4507, loss 0.115994, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:31.803407: step 4508, loss 0.0552438, acc 0.980392, learning_rate 0.0001
2017-10-11T15:07:32.116892: step 4509, loss 0.159428, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:32.415818: step 4510, loss 0.207043, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:32.725557: step 4511, loss 0.0348155, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:33.046594: step 4512, loss 0.0256443, acc 1, learning_rate 0.0001
2017-10-11T15:07:33.397395: step 4513, loss 0.0796313, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:33.691429: step 4514, loss 0.141522, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:33.970794: step 4515, loss 0.0332308, acc 1, learning_rate 0.0001
2017-10-11T15:07:34.252480: step 4516, loss 0.0459516, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:34.555747: step 4517, loss 0.162136, acc 0.9375, learning_rate 0.0001
2017-10-11T15:07:34.859498: step 4518, loss 0.135264, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:35.151434: step 4519, loss 0.0805517, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:35.432407: step 4520, loss 0.0454097, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:07:35.692006: step 4520, loss 0.218833, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4520

2017-10-11T15:07:37.900829: step 4521, loss 0.0355725, acc 1, learning_rate 0.0001
2017-10-11T15:07:38.230020: step 4522, loss 0.11275, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:38.517116: step 4523, loss 0.139159, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:38.798480: step 4524, loss 0.0106227, acc 1, learning_rate 0.0001
2017-10-11T15:07:39.076243: step 4525, loss 0.0780187, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:39.346378: step 4526, loss 0.0933768, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:39.574056: step 4527, loss 0.0743181, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:39.852992: step 4528, loss 0.0450765, acc 1, learning_rate 0.0001
2017-10-11T15:07:40.135698: step 4529, loss 0.0460736, acc 1, learning_rate 0.0001
2017-10-11T15:07:40.408958: step 4530, loss 0.0571628, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:40.663918: step 4531, loss 0.0759251, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:40.933700: step 4532, loss 0.125242, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:41.225626: step 4533, loss 0.0608253, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:41.499413: step 4534, loss 0.122795, acc 0.9375, learning_rate 0.0001
2017-10-11T15:07:41.814226: step 4535, loss 0.0971605, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:42.109992: step 4536, loss 0.0921122, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:42.404828: step 4537, loss 0.0144125, acc 1, learning_rate 0.0001
2017-10-11T15:07:42.648532: step 4538, loss 0.0432435, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:42.914916: step 4539, loss 0.0604055, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:43.178878: step 4540, loss 0.117703, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:43.437388: step 4541, loss 0.0333036, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:43.666168: step 4542, loss 0.0788649, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:43.965456: step 4543, loss 0.0480453, acc 1, learning_rate 0.0001
2017-10-11T15:07:44.272591: step 4544, loss 0.0483837, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:44.545417: step 4545, loss 0.0539989, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:44.841867: step 4546, loss 0.0525663, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:45.118931: step 4547, loss 0.0686353, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:45.388439: step 4548, loss 0.0720284, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:45.683458: step 4549, loss 0.0580324, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:45.986278: step 4550, loss 0.0201086, acc 1, learning_rate 0.0001
2017-10-11T15:07:46.281222: step 4551, loss 0.0376327, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:46.634681: step 4552, loss 0.144068, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:46.928932: step 4553, loss 0.0527533, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:47.232206: step 4554, loss 0.0339782, acc 1, learning_rate 0.0001
2017-10-11T15:07:47.541845: step 4555, loss 0.0300236, acc 1, learning_rate 0.0001
2017-10-11T15:07:47.839642: step 4556, loss 0.137331, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:48.135046: step 4557, loss 0.155793, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:48.436451: step 4558, loss 0.0557347, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:48.735805: step 4559, loss 0.14484, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:49.010136: step 4560, loss 0.11223, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:07:49.329553: step 4560, loss 0.221158, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4560

2017-10-11T15:07:51.437990: step 4561, loss 0.10233, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:51.750187: step 4562, loss 0.0708822, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:52.057440: step 4563, loss 0.128397, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:52.309744: step 4564, loss 0.0446207, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:52.587842: step 4565, loss 0.104751, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:52.852971: step 4566, loss 0.0302491, acc 1, learning_rate 0.0001
2017-10-11T15:07:53.120385: step 4567, loss 0.0202318, acc 1, learning_rate 0.0001
2017-10-11T15:07:53.419375: step 4568, loss 0.0314321, acc 1, learning_rate 0.0001
2017-10-11T15:07:53.661625: step 4569, loss 0.0916234, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:53.978402: step 4570, loss 0.0355711, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:54.296896: step 4571, loss 0.0742604, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:54.616729: step 4572, loss 0.0267479, acc 1, learning_rate 0.0001
2017-10-11T15:07:54.923448: step 4573, loss 0.0397564, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:55.239590: step 4574, loss 0.0633487, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:55.504331: step 4575, loss 0.0808578, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:55.783183: step 4576, loss 0.146116, acc 0.9375, learning_rate 0.0001
2017-10-11T15:07:56.069120: step 4577, loss 0.155937, acc 0.9375, learning_rate 0.0001
2017-10-11T15:07:56.373397: step 4578, loss 0.127939, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:56.622794: step 4579, loss 0.166309, acc 0.921875, learning_rate 0.0001
2017-10-11T15:07:56.885880: step 4580, loss 0.0242644, acc 1, learning_rate 0.0001
2017-10-11T15:07:57.142221: step 4581, loss 0.113856, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:57.445820: step 4582, loss 0.103047, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:57.721909: step 4583, loss 0.100645, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:58.059535: step 4584, loss 0.0362207, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:58.343705: step 4585, loss 0.0654415, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:58.653106: step 4586, loss 0.079487, acc 0.984375, learning_rate 0.0001
2017-10-11T15:07:58.944727: step 4587, loss 0.127924, acc 0.96875, learning_rate 0.0001
2017-10-11T15:07:59.231946: step 4588, loss 0.120536, acc 0.953125, learning_rate 0.0001
2017-10-11T15:07:59.502534: step 4589, loss 0.0377361, acc 1, learning_rate 0.0001
2017-10-11T15:07:59.791685: step 4590, loss 0.0261113, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:00.078358: step 4591, loss 0.0241806, acc 1, learning_rate 0.0001
2017-10-11T15:08:00.341579: step 4592, loss 0.0239332, acc 1, learning_rate 0.0001
2017-10-11T15:08:00.628797: step 4593, loss 0.0708824, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:00.916958: step 4594, loss 0.0704867, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:01.225533: step 4595, loss 0.101402, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:01.521722: step 4596, loss 0.223864, acc 0.921875, learning_rate 0.0001
2017-10-11T15:08:01.838217: step 4597, loss 0.0812202, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:02.140355: step 4598, loss 0.0771931, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:02.429044: step 4599, loss 0.0768429, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:02.718623: step 4600, loss 0.0595305, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:08:03.023902: step 4600, loss 0.220041, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4600

2017-10-11T15:08:05.601491: step 4601, loss 0.0691835, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:05.888830: step 4602, loss 0.0235846, acc 1, learning_rate 0.0001
2017-10-11T15:08:06.139768: step 4603, loss 0.090155, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:06.421264: step 4604, loss 0.0780576, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:06.680249: step 4605, loss 0.0350425, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:06.896628: step 4606, loss 0.046825, acc 0.980392, learning_rate 0.0001
2017-10-11T15:08:07.161284: step 4607, loss 0.0299134, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:07.402672: step 4608, loss 0.0773503, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:07.714512: step 4609, loss 0.11195, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:08.008171: step 4610, loss 0.0788562, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:08.317602: step 4611, loss 0.103292, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:08.606775: step 4612, loss 0.0494891, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:08.913029: step 4613, loss 0.0832677, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:09.220915: step 4614, loss 0.115147, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:09.518313: step 4615, loss 0.090559, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:09.792793: step 4616, loss 0.0958752, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:10.080891: step 4617, loss 0.0397646, acc 1, learning_rate 0.0001
2017-10-11T15:08:10.352616: step 4618, loss 0.0645146, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:10.641017: step 4619, loss 0.0349436, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:10.934987: step 4620, loss 0.126874, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:11.213473: step 4621, loss 0.192425, acc 0.921875, learning_rate 0.0001
2017-10-11T15:08:11.459691: step 4622, loss 0.0749378, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:11.741523: step 4623, loss 0.0883181, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:12.072357: step 4624, loss 0.105751, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:12.388279: step 4625, loss 0.061639, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:12.684144: step 4626, loss 0.139322, acc 0.9375, learning_rate 0.0001
2017-10-11T15:08:13.012738: step 4627, loss 0.0608022, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:13.358337: step 4628, loss 0.0508056, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:13.671460: step 4629, loss 0.026628, acc 1, learning_rate 0.0001
2017-10-11T15:08:13.944754: step 4630, loss 0.0967599, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:14.220295: step 4631, loss 0.165196, acc 0.9375, learning_rate 0.0001
2017-10-11T15:08:14.517001: step 4632, loss 0.0565448, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:14.816218: step 4633, loss 0.0999841, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:15.148422: step 4634, loss 0.119578, acc 0.9375, learning_rate 0.0001
2017-10-11T15:08:15.447588: step 4635, loss 0.0575398, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:15.730189: step 4636, loss 0.0669441, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:16.036142: step 4637, loss 0.0218714, acc 1, learning_rate 0.0001
2017-10-11T15:08:16.351719: step 4638, loss 0.0556039, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:16.613195: step 4639, loss 0.0926656, acc 0.9375, learning_rate 0.0001
2017-10-11T15:08:16.887856: step 4640, loss 0.0567405, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:08:17.144595: step 4640, loss 0.219639, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4640

2017-10-11T15:08:19.979508: step 4641, loss 0.111445, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:20.262436: step 4642, loss 0.0507729, acc 1, learning_rate 0.0001
2017-10-11T15:08:20.529046: step 4643, loss 0.0293066, acc 1, learning_rate 0.0001
2017-10-11T15:08:20.792082: step 4644, loss 0.0518956, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:21.098083: step 4645, loss 0.0669645, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:21.429390: step 4646, loss 0.0590743, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:21.727673: step 4647, loss 0.0518679, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:22.043186: step 4648, loss 0.0779825, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:22.342829: step 4649, loss 0.0318599, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:22.608613: step 4650, loss 0.049149, acc 1, learning_rate 0.0001
2017-10-11T15:08:22.934912: step 4651, loss 0.15425, acc 0.9375, learning_rate 0.0001
2017-10-11T15:08:23.205611: step 4652, loss 0.051368, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:23.522492: step 4653, loss 0.100722, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:23.804859: step 4654, loss 0.0636486, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:24.057859: step 4655, loss 0.0488187, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:24.345328: step 4656, loss 0.0257695, acc 1, learning_rate 0.0001
2017-10-11T15:08:24.618937: step 4657, loss 0.0663426, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:24.904882: step 4658, loss 0.0426773, acc 1, learning_rate 0.0001
2017-10-11T15:08:25.250376: step 4659, loss 0.112881, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:25.576425: step 4660, loss 0.0166557, acc 1, learning_rate 0.0001
2017-10-11T15:08:25.917581: step 4661, loss 0.0773514, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:26.201004: step 4662, loss 0.0666809, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:26.521731: step 4663, loss 0.0614676, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:26.862944: step 4664, loss 0.175284, acc 0.921875, learning_rate 0.0001
2017-10-11T15:08:27.187262: step 4665, loss 0.0353894, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:27.480380: step 4666, loss 0.0722416, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:27.774790: step 4667, loss 0.143211, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:28.096735: step 4668, loss 0.187396, acc 0.90625, learning_rate 0.0001
2017-10-11T15:08:28.416268: step 4669, loss 0.0328939, acc 1, learning_rate 0.0001
2017-10-11T15:08:28.696129: step 4670, loss 0.0601129, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:28.998408: step 4671, loss 0.0877114, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:29.327282: step 4672, loss 0.0989154, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:29.617893: step 4673, loss 0.0540049, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:29.917658: step 4674, loss 0.0450437, acc 1, learning_rate 0.0001
2017-10-11T15:08:30.200932: step 4675, loss 0.0551419, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:30.464244: step 4676, loss 0.0282778, acc 1, learning_rate 0.0001
2017-10-11T15:08:30.731117: step 4677, loss 0.0248709, acc 1, learning_rate 0.0001
2017-10-11T15:08:30.990774: step 4678, loss 0.11874, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:31.248419: step 4679, loss 0.0633301, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:31.520703: step 4680, loss 0.0579649, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:08:31.773875: step 4680, loss 0.224164, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4680

2017-10-11T15:08:33.768605: step 4681, loss 0.0494284, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:34.029319: step 4682, loss 0.0922189, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:34.296990: step 4683, loss 0.0262028, acc 1, learning_rate 0.0001
2017-10-11T15:08:34.553295: step 4684, loss 0.0758151, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:34.813304: step 4685, loss 0.0887895, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:35.083942: step 4686, loss 0.0421718, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:35.409666: step 4687, loss 0.171752, acc 0.9375, learning_rate 0.0001
2017-10-11T15:08:35.705872: step 4688, loss 0.055863, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:35.997201: step 4689, loss 0.0624417, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:36.330451: step 4690, loss 0.0599568, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:36.646907: step 4691, loss 0.0564286, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:36.919022: step 4692, loss 0.0751177, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:37.192987: step 4693, loss 0.111475, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:37.454141: step 4694, loss 0.124935, acc 0.9375, learning_rate 0.0001
2017-10-11T15:08:37.714974: step 4695, loss 0.0924634, acc 0.9375, learning_rate 0.0001
2017-10-11T15:08:38.013390: step 4696, loss 0.0894684, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:38.295435: step 4697, loss 0.0523996, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:38.578503: step 4698, loss 0.125627, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:38.834087: step 4699, loss 0.0869678, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:39.076716: step 4700, loss 0.068734, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:39.430148: step 4701, loss 0.0149576, acc 1, learning_rate 0.0001
2017-10-11T15:08:39.733747: step 4702, loss 0.0290205, acc 1, learning_rate 0.0001
2017-10-11T15:08:40.037186: step 4703, loss 0.0370567, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:40.305501: step 4704, loss 0.0542618, acc 0.960784, learning_rate 0.0001
2017-10-11T15:08:40.616374: step 4705, loss 0.127911, acc 0.9375, learning_rate 0.0001
2017-10-11T15:08:40.916958: step 4706, loss 0.0843071, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:41.214152: step 4707, loss 0.0333589, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:41.525612: step 4708, loss 0.0399075, acc 1, learning_rate 0.0001
2017-10-11T15:08:41.797592: step 4709, loss 0.089722, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:42.069394: step 4710, loss 0.0548509, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:42.385542: step 4711, loss 0.0207862, acc 1, learning_rate 0.0001
2017-10-11T15:08:42.738607: step 4712, loss 0.0467588, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:43.081326: step 4713, loss 0.0893334, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:43.423205: step 4714, loss 0.116804, acc 0.9375, learning_rate 0.0001
2017-10-11T15:08:43.738655: step 4715, loss 0.0804222, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:44.059034: step 4716, loss 0.0761543, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:44.351071: step 4717, loss 0.0845118, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:44.625086: step 4718, loss 0.0519519, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:44.893068: step 4719, loss 0.12195, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:45.148468: step 4720, loss 0.0847797, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:08:45.403142: step 4720, loss 0.220343, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4720

2017-10-11T15:08:48.524184: step 4721, loss 0.0737894, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:48.834431: step 4722, loss 0.0400358, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:49.179598: step 4723, loss 0.0783417, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:49.566483: step 4724, loss 0.0902222, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:49.902515: step 4725, loss 0.0764481, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:50.221614: step 4726, loss 0.0149409, acc 1, learning_rate 0.0001
2017-10-11T15:08:50.550809: step 4727, loss 0.0846884, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:50.829297: step 4728, loss 0.0271983, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:51.128936: step 4729, loss 0.0885895, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:51.413351: step 4730, loss 0.0968711, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:51.681787: step 4731, loss 0.0652208, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:51.949727: step 4732, loss 0.0342082, acc 1, learning_rate 0.0001
2017-10-11T15:08:52.190208: step 4733, loss 0.0259457, acc 1, learning_rate 0.0001
2017-10-11T15:08:52.463929: step 4734, loss 0.0575755, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:52.776871: step 4735, loss 0.27835, acc 0.890625, learning_rate 0.0001
2017-10-11T15:08:53.054905: step 4736, loss 0.0737287, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:53.386818: step 4737, loss 0.11005, acc 0.9375, learning_rate 0.0001
2017-10-11T15:08:53.684135: step 4738, loss 0.0440687, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:53.997291: step 4739, loss 0.0549922, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:54.249726: step 4740, loss 0.0234646, acc 1, learning_rate 0.0001
2017-10-11T15:08:54.565933: step 4741, loss 0.0197288, acc 1, learning_rate 0.0001
2017-10-11T15:08:54.885459: step 4742, loss 0.123378, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:55.196265: step 4743, loss 0.0902726, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:55.525331: step 4744, loss 0.0316129, acc 1, learning_rate 0.0001
2017-10-11T15:08:55.814228: step 4745, loss 0.0466121, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:56.121704: step 4746, loss 0.106737, acc 0.953125, learning_rate 0.0001
2017-10-11T15:08:56.421332: step 4747, loss 0.0829023, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:56.744549: step 4748, loss 0.0384365, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:57.049711: step 4749, loss 0.089174, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:57.376313: step 4750, loss 0.14258, acc 0.9375, learning_rate 0.0001
2017-10-11T15:08:57.700739: step 4751, loss 0.0338271, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:57.984154: step 4752, loss 0.0510024, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:58.269040: step 4753, loss 0.0357133, acc 1, learning_rate 0.0001
2017-10-11T15:08:58.527170: step 4754, loss 0.138576, acc 0.9375, learning_rate 0.0001
2017-10-11T15:08:58.852946: step 4755, loss 0.0487968, acc 1, learning_rate 0.0001
2017-10-11T15:08:59.120197: step 4756, loss 0.0782242, acc 0.96875, learning_rate 0.0001
2017-10-11T15:08:59.362636: step 4757, loss 0.0216121, acc 1, learning_rate 0.0001
2017-10-11T15:08:59.669916: step 4758, loss 0.0656417, acc 0.984375, learning_rate 0.0001
2017-10-11T15:08:59.950186: step 4759, loss 0.17117, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:00.265231: step 4760, loss 0.15635, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:09:00.586670: step 4760, loss 0.222207, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4760

2017-10-11T15:09:02.258719: step 4761, loss 0.0399746, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:02.465245: step 4762, loss 0.0721851, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:02.711722: step 4763, loss 0.045296, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:02.994385: step 4764, loss 0.121426, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:03.350142: step 4765, loss 0.0391643, acc 1, learning_rate 0.0001
2017-10-11T15:09:03.628889: step 4766, loss 0.0324752, acc 1, learning_rate 0.0001
2017-10-11T15:09:03.972224: step 4767, loss 0.0924761, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:04.313693: step 4768, loss 0.0560134, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:04.665655: step 4769, loss 0.131732, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:04.936002: step 4770, loss 0.0866972, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:05.232550: step 4771, loss 0.146463, acc 0.9375, learning_rate 0.0001
2017-10-11T15:09:05.499168: step 4772, loss 0.0760364, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:05.785385: step 4773, loss 0.0493873, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:06.056892: step 4774, loss 0.0289881, acc 1, learning_rate 0.0001
2017-10-11T15:09:06.331673: step 4775, loss 0.0166443, acc 1, learning_rate 0.0001
2017-10-11T15:09:06.588373: step 4776, loss 0.0655705, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:06.851041: step 4777, loss 0.0365664, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:07.189257: step 4778, loss 0.105092, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:07.515267: step 4779, loss 0.0542716, acc 1, learning_rate 0.0001
2017-10-11T15:09:07.867171: step 4780, loss 0.0499164, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:08.195790: step 4781, loss 0.0688389, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:08.513074: step 4782, loss 0.125786, acc 0.9375, learning_rate 0.0001
2017-10-11T15:09:08.823019: step 4783, loss 0.0492648, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:09.133313: step 4784, loss 0.0708113, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:09.423005: step 4785, loss 0.0812804, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:09.715698: step 4786, loss 0.019555, acc 1, learning_rate 0.0001
2017-10-11T15:09:10.017684: step 4787, loss 0.0184766, acc 1, learning_rate 0.0001
2017-10-11T15:09:10.275733: step 4788, loss 0.0771753, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:10.583601: step 4789, loss 0.158532, acc 0.9375, learning_rate 0.0001
2017-10-11T15:09:10.881670: step 4790, loss 0.0259989, acc 1, learning_rate 0.0001
2017-10-11T15:09:11.168069: step 4791, loss 0.0224083, acc 1, learning_rate 0.0001
2017-10-11T15:09:11.426776: step 4792, loss 0.0525951, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:11.723503: step 4793, loss 0.039634, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:12.044080: step 4794, loss 0.0649523, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:12.344950: step 4795, loss 0.0675397, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:12.657472: step 4796, loss 0.0367099, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:12.938112: step 4797, loss 0.11284, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:13.189415: step 4798, loss 0.0319428, acc 1, learning_rate 0.0001
2017-10-11T15:09:13.456785: step 4799, loss 0.0393548, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:13.706159: step 4800, loss 0.0515936, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:09:13.930163: step 4800, loss 0.218319, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4800

2017-10-11T15:09:16.071234: step 4801, loss 0.0899891, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:16.278189: step 4802, loss 0.0801389, acc 0.960784, learning_rate 0.0001
2017-10-11T15:09:16.540835: step 4803, loss 0.0354804, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:16.826097: step 4804, loss 0.146431, acc 0.9375, learning_rate 0.0001
2017-10-11T15:09:17.161651: step 4805, loss 0.0757255, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:17.508997: step 4806, loss 0.0133463, acc 1, learning_rate 0.0001
2017-10-11T15:09:17.825343: step 4807, loss 0.0493959, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:18.169482: step 4808, loss 0.0614771, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:18.485894: step 4809, loss 0.117123, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:18.819722: step 4810, loss 0.0556754, acc 1, learning_rate 0.0001
2017-10-11T15:09:19.083312: step 4811, loss 0.0176502, acc 1, learning_rate 0.0001
2017-10-11T15:09:19.368416: step 4812, loss 0.0996638, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:19.648462: step 4813, loss 0.0636953, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:19.913995: step 4814, loss 0.082172, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:20.202678: step 4815, loss 0.0657132, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:20.462774: step 4816, loss 0.104541, acc 0.9375, learning_rate 0.0001
2017-10-11T15:09:20.747196: step 4817, loss 0.0803637, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:21.008655: step 4818, loss 0.088131, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:21.285696: step 4819, loss 0.0588644, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:21.603422: step 4820, loss 0.0525908, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:21.914233: step 4821, loss 0.0310662, acc 1, learning_rate 0.0001
2017-10-11T15:09:22.254088: step 4822, loss 0.0769636, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:22.544518: step 4823, loss 0.0601116, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:22.851996: step 4824, loss 0.0674206, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:23.144407: step 4825, loss 0.0750542, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:23.484248: step 4826, loss 0.0389006, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:23.804576: step 4827, loss 0.13344, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:24.128182: step 4828, loss 0.0927091, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:24.438145: step 4829, loss 0.0582696, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:24.769044: step 4830, loss 0.0404007, acc 1, learning_rate 0.0001
2017-10-11T15:09:25.027978: step 4831, loss 0.121541, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:25.306530: step 4832, loss 0.0557458, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:25.600736: step 4833, loss 0.0851676, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:25.914309: step 4834, loss 0.0896331, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:26.191243: step 4835, loss 0.0362013, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:26.446475: step 4836, loss 0.180078, acc 0.9375, learning_rate 0.0001
2017-10-11T15:09:26.739628: step 4837, loss 0.0458601, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:27.008443: step 4838, loss 0.0660758, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:27.288708: step 4839, loss 0.132149, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:27.567815: step 4840, loss 0.0622337, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:09:27.829475: step 4840, loss 0.22436, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4840

2017-10-11T15:09:31.312913: step 4841, loss 0.251277, acc 0.921875, learning_rate 0.0001
2017-10-11T15:09:31.580079: step 4842, loss 0.0399558, acc 1, learning_rate 0.0001
2017-10-11T15:09:31.827698: step 4843, loss 0.198057, acc 0.9375, learning_rate 0.0001
2017-10-11T15:09:32.130426: step 4844, loss 0.0452805, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:32.438755: step 4845, loss 0.0798471, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:32.749485: step 4846, loss 0.115611, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:33.056071: step 4847, loss 0.0676717, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:33.391988: step 4848, loss 0.0510856, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:33.665759: step 4849, loss 0.0697961, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:33.947675: step 4850, loss 0.100905, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:34.299463: step 4851, loss 0.0394997, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:34.591075: step 4852, loss 0.0949988, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:34.890548: step 4853, loss 0.154935, acc 0.9375, learning_rate 0.0001
2017-10-11T15:09:35.219440: step 4854, loss 0.0629561, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:35.528365: step 4855, loss 0.0467646, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:35.881518: step 4856, loss 0.0536943, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:36.216824: step 4857, loss 0.0814245, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:36.535631: step 4858, loss 0.0698833, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:36.849384: step 4859, loss 0.0224929, acc 1, learning_rate 0.0001
2017-10-11T15:09:37.180272: step 4860, loss 0.053704, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:37.475455: step 4861, loss 0.0314477, acc 1, learning_rate 0.0001
2017-10-11T15:09:37.769725: step 4862, loss 0.0263472, acc 1, learning_rate 0.0001
2017-10-11T15:09:38.111769: step 4863, loss 0.0225704, acc 1, learning_rate 0.0001
2017-10-11T15:09:38.408477: step 4864, loss 0.05221, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:38.710826: step 4865, loss 0.107677, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:39.012666: step 4866, loss 0.101798, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:39.322490: step 4867, loss 0.0562846, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:39.631237: step 4868, loss 0.0638036, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:39.878036: step 4869, loss 0.0100805, acc 1, learning_rate 0.0001
2017-10-11T15:09:40.160243: step 4870, loss 0.0426821, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:40.422228: step 4871, loss 0.0857627, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:40.722078: step 4872, loss 0.0395654, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:41.017664: step 4873, loss 0.0373506, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:41.308031: step 4874, loss 0.194925, acc 0.9375, learning_rate 0.0001
2017-10-11T15:09:41.597579: step 4875, loss 0.107649, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:41.896934: step 4876, loss 0.054075, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:42.228167: step 4877, loss 0.0272782, acc 1, learning_rate 0.0001
2017-10-11T15:09:42.548256: step 4878, loss 0.0752645, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:42.874004: step 4879, loss 0.044821, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:43.198981: step 4880, loss 0.0688126, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:09:43.499801: step 4880, loss 0.221552, acc 0.91223

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4880

2017-10-11T15:09:45.393239: step 4881, loss 0.0291382, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:45.666508: step 4882, loss 0.0645138, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:45.938041: step 4883, loss 0.0754776, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:46.225921: step 4884, loss 0.0969778, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:46.540797: step 4885, loss 0.0969278, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:46.902730: step 4886, loss 0.062192, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:47.199786: step 4887, loss 0.133975, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:47.489372: step 4888, loss 0.0484266, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:47.775118: step 4889, loss 0.0464239, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:48.053401: step 4890, loss 0.0647948, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:48.326791: step 4891, loss 0.0263051, acc 1, learning_rate 0.0001
2017-10-11T15:09:48.588959: step 4892, loss 0.0548044, acc 1, learning_rate 0.0001
2017-10-11T15:09:48.876813: step 4893, loss 0.0513, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:49.153031: step 4894, loss 0.116466, acc 0.9375, learning_rate 0.0001
2017-10-11T15:09:49.515312: step 4895, loss 0.0576618, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:49.848574: step 4896, loss 0.0328181, acc 1, learning_rate 0.0001
2017-10-11T15:09:50.171599: step 4897, loss 0.0694583, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:50.451369: step 4898, loss 0.0213164, acc 1, learning_rate 0.0001
2017-10-11T15:09:50.757669: step 4899, loss 0.0827011, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:51.031204: step 4900, loss 0.0333468, acc 1, learning_rate 0.0001
2017-10-11T15:09:51.355940: step 4901, loss 0.0678834, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:51.682717: step 4902, loss 0.0643313, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:51.973412: step 4903, loss 0.0652158, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:52.283655: step 4904, loss 0.0170396, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:52.625484: step 4905, loss 0.117593, acc 0.9375, learning_rate 0.0001
2017-10-11T15:09:52.910981: step 4906, loss 0.0307088, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:53.221564: step 4907, loss 0.123566, acc 0.9375, learning_rate 0.0001
2017-10-11T15:09:53.568284: step 4908, loss 0.0414239, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:53.905986: step 4909, loss 0.099198, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:54.215805: step 4910, loss 0.0329923, acc 1, learning_rate 0.0001
2017-10-11T15:09:54.484768: step 4911, loss 0.0417199, acc 1, learning_rate 0.0001
2017-10-11T15:09:54.735434: step 4912, loss 0.0283305, acc 1, learning_rate 0.0001
2017-10-11T15:09:54.997006: step 4913, loss 0.130899, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:55.257058: step 4914, loss 0.0555427, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:55.520060: step 4915, loss 0.0502257, acc 1, learning_rate 0.0001
2017-10-11T15:09:55.787962: step 4916, loss 0.194185, acc 0.953125, learning_rate 0.0001
2017-10-11T15:09:56.119337: step 4917, loss 0.0622281, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:56.457964: step 4918, loss 0.0482319, acc 0.96875, learning_rate 0.0001
2017-10-11T15:09:56.830813: step 4919, loss 0.020093, acc 1, learning_rate 0.0001
2017-10-11T15:09:57.104018: step 4920, loss 0.0922895, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:09:57.428136: step 4920, loss 0.219698, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4920

2017-10-11T15:09:59.519496: step 4921, loss 0.0496851, acc 0.984375, learning_rate 0.0001
2017-10-11T15:09:59.803508: step 4922, loss 0.0581627, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:00.092162: step 4923, loss 0.084165, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:00.366533: step 4924, loss 0.0826618, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:00.636007: step 4925, loss 0.0824566, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:00.894227: step 4926, loss 0.13442, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:01.227446: step 4927, loss 0.0622864, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:01.501976: step 4928, loss 0.0350592, acc 1, learning_rate 0.0001
2017-10-11T15:10:01.760387: step 4929, loss 0.133257, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:02.039916: step 4930, loss 0.0792682, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:02.310221: step 4931, loss 0.0973261, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:02.583456: step 4932, loss 0.120527, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:02.861563: step 4933, loss 0.0187422, acc 1, learning_rate 0.0001
2017-10-11T15:10:03.118783: step 4934, loss 0.0552337, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:03.376983: step 4935, loss 0.117576, acc 0.9375, learning_rate 0.0001
2017-10-11T15:10:03.677788: step 4936, loss 0.041747, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:04.033623: step 4937, loss 0.10522, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:04.306440: step 4938, loss 0.105771, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:04.625875: step 4939, loss 0.0802273, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:04.926736: step 4940, loss 0.0424425, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:05.245824: step 4941, loss 0.103543, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:05.534170: step 4942, loss 0.0968665, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:05.838502: step 4943, loss 0.0765944, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:06.163808: step 4944, loss 0.0943263, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:06.480776: step 4945, loss 0.0910283, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:06.763622: step 4946, loss 0.108409, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:07.033592: step 4947, loss 0.0761738, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:07.351958: step 4948, loss 0.048098, acc 1, learning_rate 0.0001
2017-10-11T15:10:07.701577: step 4949, loss 0.0363911, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:08.023159: step 4950, loss 0.0665491, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:08.305085: step 4951, loss 0.0864487, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:08.605862: step 4952, loss 0.0906181, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:08.869060: step 4953, loss 0.0582699, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:09.140973: step 4954, loss 0.0452396, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:09.394539: step 4955, loss 0.0518785, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:09.711439: step 4956, loss 0.106024, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:10.040728: step 4957, loss 0.00714295, acc 1, learning_rate 0.0001
2017-10-11T15:10:10.357895: step 4958, loss 0.0401957, acc 1, learning_rate 0.0001
2017-10-11T15:10:10.670814: step 4959, loss 0.0871828, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:10.963298: step 4960, loss 0.146836, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T15:10:11.290018: step 4960, loss 0.218013, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-4960

2017-10-11T15:10:14.219591: step 4961, loss 0.0510442, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:14.525184: step 4962, loss 0.065197, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:14.850536: step 4963, loss 0.0638199, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:15.166382: step 4964, loss 0.0370863, acc 1, learning_rate 0.0001
2017-10-11T15:10:15.446459: step 4965, loss 0.0599235, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:15.746846: step 4966, loss 0.087972, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:15.967171: step 4967, loss 0.0855821, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:16.264584: step 4968, loss 0.0120867, acc 1, learning_rate 0.0001
2017-10-11T15:10:16.529966: step 4969, loss 0.0406571, acc 1, learning_rate 0.0001
2017-10-11T15:10:16.807734: step 4970, loss 0.173308, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:17.128364: step 4971, loss 0.119905, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:17.446294: step 4972, loss 0.0175382, acc 1, learning_rate 0.0001
2017-10-11T15:10:17.733717: step 4973, loss 0.0255279, acc 1, learning_rate 0.0001
2017-10-11T15:10:18.055564: step 4974, loss 0.0754396, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:18.363038: step 4975, loss 0.100037, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:18.665935: step 4976, loss 0.085649, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:18.993262: step 4977, loss 0.196972, acc 0.90625, learning_rate 0.0001
2017-10-11T15:10:19.299830: step 4978, loss 0.0150992, acc 1, learning_rate 0.0001
2017-10-11T15:10:19.625463: step 4979, loss 0.0971557, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:19.904405: step 4980, loss 0.0453074, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:20.218802: step 4981, loss 0.0591612, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:20.557417: step 4982, loss 0.157514, acc 0.921875, learning_rate 0.0001
2017-10-11T15:10:20.884342: step 4983, loss 0.110246, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:21.198522: step 4984, loss 0.0160133, acc 1, learning_rate 0.0001
2017-10-11T15:10:21.468599: step 4985, loss 0.0787274, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:21.785062: step 4986, loss 0.0733013, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:22.060986: step 4987, loss 0.0671123, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:22.337442: step 4988, loss 0.0647185, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:22.608799: step 4989, loss 0.0420692, acc 1, learning_rate 0.0001
2017-10-11T15:10:22.900410: step 4990, loss 0.0796169, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:23.183267: step 4991, loss 0.069335, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:23.489280: step 4992, loss 0.214956, acc 0.890625, learning_rate 0.0001
2017-10-11T15:10:23.771587: step 4993, loss 0.0393058, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:24.047610: step 4994, loss 0.0488496, acc 1, learning_rate 0.0001
2017-10-11T15:10:24.341487: step 4995, loss 0.100145, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:24.603002: step 4996, loss 0.0850812, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:24.930322: step 4997, loss 0.135655, acc 0.921875, learning_rate 0.0001
2017-10-11T15:10:25.228626: step 4998, loss 0.115944, acc 0.960784, learning_rate 0.0001
2017-10-11T15:10:25.549259: step 4999, loss 0.137462, acc 0.9375, learning_rate 0.0001
2017-10-11T15:10:25.834000: step 5000, loss 0.0911103, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:10:26.158835: step 5000, loss 0.219148, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5000

2017-10-11T15:10:27.850157: step 5001, loss 0.113907, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:28.137293: step 5002, loss 0.0483427, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:28.434410: step 5003, loss 0.0234902, acc 1, learning_rate 0.0001
2017-10-11T15:10:28.714960: step 5004, loss 0.0912723, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:28.956988: step 5005, loss 0.0602965, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:29.244848: step 5006, loss 0.108822, acc 0.921875, learning_rate 0.0001
2017-10-11T15:10:29.581125: step 5007, loss 0.0697047, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:29.868952: step 5008, loss 0.0472721, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:30.122882: step 5009, loss 0.0369965, acc 1, learning_rate 0.0001
2017-10-11T15:10:30.401851: step 5010, loss 0.0503054, acc 1, learning_rate 0.0001
2017-10-11T15:10:30.603840: step 5011, loss 0.0528173, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:30.816873: step 5012, loss 0.0298188, acc 1, learning_rate 0.0001
2017-10-11T15:10:31.079922: step 5013, loss 0.0160148, acc 1, learning_rate 0.0001
2017-10-11T15:10:31.321169: step 5014, loss 0.0549382, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:31.606206: step 5015, loss 0.0281804, acc 1, learning_rate 0.0001
2017-10-11T15:10:31.903879: step 5016, loss 0.0895389, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:32.218768: step 5017, loss 0.105306, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:32.519836: step 5018, loss 0.120497, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:32.803364: step 5019, loss 0.0832169, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:33.054600: step 5020, loss 0.0950828, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:33.348907: step 5021, loss 0.0851044, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:33.652115: step 5022, loss 0.0483327, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:33.946081: step 5023, loss 0.0381338, acc 1, learning_rate 0.0001
2017-10-11T15:10:34.219676: step 5024, loss 0.0221252, acc 1, learning_rate 0.0001
2017-10-11T15:10:34.524936: step 5025, loss 0.0472274, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:34.857375: step 5026, loss 0.0310874, acc 1, learning_rate 0.0001
2017-10-11T15:10:35.178325: step 5027, loss 0.0772018, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:35.502243: step 5028, loss 0.0400081, acc 1, learning_rate 0.0001
2017-10-11T15:10:35.802149: step 5029, loss 0.107122, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:36.121727: step 5030, loss 0.076902, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:36.450306: step 5031, loss 0.122206, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:36.696276: step 5032, loss 0.0656248, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:36.950842: step 5033, loss 0.077185, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:37.195430: step 5034, loss 0.0940726, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:37.497008: step 5035, loss 0.0959359, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:37.782567: step 5036, loss 0.0435166, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:38.128273: step 5037, loss 0.049944, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:38.451761: step 5038, loss 0.0583878, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:38.762093: step 5039, loss 0.0609836, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:39.051932: step 5040, loss 0.129445, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T15:10:39.334726: step 5040, loss 0.221925, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5040

2017-10-11T15:10:42.830277: step 5041, loss 0.0920194, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:43.162714: step 5042, loss 0.125595, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:43.420356: step 5043, loss 0.0747942, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:43.701976: step 5044, loss 0.0996278, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:43.975468: step 5045, loss 0.0446548, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:44.238627: step 5046, loss 0.0339451, acc 1, learning_rate 0.0001
2017-10-11T15:10:44.537375: step 5047, loss 0.0390657, acc 1, learning_rate 0.0001
2017-10-11T15:10:44.883212: step 5048, loss 0.0361743, acc 1, learning_rate 0.0001
2017-10-11T15:10:45.187484: step 5049, loss 0.112301, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:45.483927: step 5050, loss 0.0348297, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:45.813308: step 5051, loss 0.0862119, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:46.115100: step 5052, loss 0.0591269, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:46.395566: step 5053, loss 0.0875243, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:46.683364: step 5054, loss 0.0577387, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:46.979449: step 5055, loss 0.0442277, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:47.321581: step 5056, loss 0.0190299, acc 1, learning_rate 0.0001
2017-10-11T15:10:47.648798: step 5057, loss 0.0503317, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:47.937116: step 5058, loss 0.0745657, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:48.226130: step 5059, loss 0.0410702, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:48.479149: step 5060, loss 0.0787438, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:48.725925: step 5061, loss 0.0470157, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:49.063421: step 5062, loss 0.0194309, acc 1, learning_rate 0.0001
2017-10-11T15:10:49.366779: step 5063, loss 0.0529003, acc 1, learning_rate 0.0001
2017-10-11T15:10:49.664185: step 5064, loss 0.142267, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:49.960158: step 5065, loss 0.0421721, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:50.266307: step 5066, loss 0.115778, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:50.571727: step 5067, loss 0.107356, acc 0.9375, learning_rate 0.0001
2017-10-11T15:10:50.876744: step 5068, loss 0.0245193, acc 1, learning_rate 0.0001
2017-10-11T15:10:51.109667: step 5069, loss 0.0398391, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:51.395350: step 5070, loss 0.0681622, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:51.685472: step 5071, loss 0.10193, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:51.956077: step 5072, loss 0.0468205, acc 1, learning_rate 0.0001
2017-10-11T15:10:52.251966: step 5073, loss 0.073463, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:52.573464: step 5074, loss 0.0687819, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:52.872258: step 5075, loss 0.173519, acc 0.921875, learning_rate 0.0001
2017-10-11T15:10:53.197539: step 5076, loss 0.0714536, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:53.491068: step 5077, loss 0.107445, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:53.775209: step 5078, loss 0.0451196, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:54.042764: step 5079, loss 0.0438544, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:54.337653: step 5080, loss 0.11595, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T15:10:54.623647: step 5080, loss 0.221794, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5080

2017-10-11T15:10:56.277708: step 5081, loss 0.0416811, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:56.551491: step 5082, loss 0.0755469, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:56.831124: step 5083, loss 0.0535549, acc 1, learning_rate 0.0001
2017-10-11T15:10:57.136260: step 5084, loss 0.0971042, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:57.417394: step 5085, loss 0.0785536, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:57.739778: step 5086, loss 0.056308, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:58.101496: step 5087, loss 0.0626525, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:58.387973: step 5088, loss 0.057669, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:58.697868: step 5089, loss 0.067861, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:58.984355: step 5090, loss 0.103858, acc 0.953125, learning_rate 0.0001
2017-10-11T15:10:59.270486: step 5091, loss 0.0784047, acc 0.984375, learning_rate 0.0001
2017-10-11T15:10:59.576376: step 5092, loss 0.100372, acc 0.96875, learning_rate 0.0001
2017-10-11T15:10:59.902830: step 5093, loss 0.0649528, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:00.205415: step 5094, loss 0.0955731, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:00.506646: step 5095, loss 0.0931023, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:00.786925: step 5096, loss 0.129685, acc 0.941176, learning_rate 0.0001
2017-10-11T15:11:01.085752: step 5097, loss 0.0629593, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:01.384668: step 5098, loss 0.0420575, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:01.662695: step 5099, loss 0.115356, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:01.989325: step 5100, loss 0.0657802, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:02.313941: step 5101, loss 0.0216157, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:02.629001: step 5102, loss 0.120474, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:02.933451: step 5103, loss 0.0823798, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:03.255315: step 5104, loss 0.0474839, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:03.552087: step 5105, loss 0.0388304, acc 1, learning_rate 0.0001
2017-10-11T15:11:03.859267: step 5106, loss 0.0998423, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:04.208563: step 5107, loss 0.0726701, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:04.489800: step 5108, loss 0.0761477, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:04.763397: step 5109, loss 0.072809, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:05.048637: step 5110, loss 0.127368, acc 0.9375, learning_rate 0.0001
2017-10-11T15:11:05.283854: step 5111, loss 0.110641, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:05.600514: step 5112, loss 0.00935171, acc 1, learning_rate 0.0001
2017-10-11T15:11:05.967333: step 5113, loss 0.10574, acc 0.9375, learning_rate 0.0001
2017-10-11T15:11:06.314662: step 5114, loss 0.0682586, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:06.652295: step 5115, loss 0.0668934, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:06.928874: step 5116, loss 0.0525321, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:07.237989: step 5117, loss 0.0642762, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:07.557919: step 5118, loss 0.0979082, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:07.820049: step 5119, loss 0.179792, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:08.085543: step 5120, loss 0.0740949, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:11:08.377056: step 5120, loss 0.220195, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5120

2017-10-11T15:11:10.871232: step 5121, loss 0.0394812, acc 1, learning_rate 0.0001
2017-10-11T15:11:11.180196: step 5122, loss 0.0735538, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:11.437921: step 5123, loss 0.0106412, acc 1, learning_rate 0.0001
2017-10-11T15:11:11.667413: step 5124, loss 0.109813, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:11.906062: step 5125, loss 0.121499, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:12.179720: step 5126, loss 0.1085, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:12.463954: step 5127, loss 0.0621485, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:12.732203: step 5128, loss 0.0476077, acc 1, learning_rate 0.0001
2017-10-11T15:11:13.004337: step 5129, loss 0.075098, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:13.265374: step 5130, loss 0.0254451, acc 1, learning_rate 0.0001
2017-10-11T15:11:13.600060: step 5131, loss 0.036523, acc 1, learning_rate 0.0001
2017-10-11T15:11:13.923493: step 5132, loss 0.168469, acc 0.9375, learning_rate 0.0001
2017-10-11T15:11:14.251271: step 5133, loss 0.0361416, acc 1, learning_rate 0.0001
2017-10-11T15:11:14.574518: step 5134, loss 0.0672598, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:14.901700: step 5135, loss 0.0819342, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:15.200865: step 5136, loss 0.03878, acc 1, learning_rate 0.0001
2017-10-11T15:11:15.541728: step 5137, loss 0.0424191, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:15.844219: step 5138, loss 0.0500986, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:16.149234: step 5139, loss 0.0553722, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:16.434674: step 5140, loss 0.00896654, acc 1, learning_rate 0.0001
2017-10-11T15:11:16.734807: step 5141, loss 0.100065, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:17.050220: step 5142, loss 0.0294739, acc 1, learning_rate 0.0001
2017-10-11T15:11:17.333833: step 5143, loss 0.0466518, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:17.634516: step 5144, loss 0.102996, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:17.943311: step 5145, loss 0.0216188, acc 1, learning_rate 0.0001
2017-10-11T15:11:18.212263: step 5146, loss 0.0461868, acc 1, learning_rate 0.0001
2017-10-11T15:11:18.450753: step 5147, loss 0.0984538, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:18.735341: step 5148, loss 0.0170335, acc 1, learning_rate 0.0001
2017-10-11T15:11:18.980931: step 5149, loss 0.038338, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:19.295086: step 5150, loss 0.0566333, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:19.615729: step 5151, loss 0.0300109, acc 1, learning_rate 0.0001
2017-10-11T15:11:19.933744: step 5152, loss 0.104243, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:20.265963: step 5153, loss 0.0811991, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:20.560566: step 5154, loss 0.0568459, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:20.878980: step 5155, loss 0.0913617, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:21.193123: step 5156, loss 0.0622257, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:21.502310: step 5157, loss 0.0272801, acc 1, learning_rate 0.0001
2017-10-11T15:11:21.808301: step 5158, loss 0.0509086, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:22.128856: step 5159, loss 0.0230524, acc 1, learning_rate 0.0001
2017-10-11T15:11:22.439019: step 5160, loss 0.0625933, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:11:22.669254: step 5160, loss 0.218763, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5160

2017-10-11T15:11:25.152699: step 5161, loss 0.0973899, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:25.430415: step 5162, loss 0.030908, acc 1, learning_rate 0.0001
2017-10-11T15:11:25.690587: step 5163, loss 0.0127189, acc 1, learning_rate 0.0001
2017-10-11T15:11:25.952355: step 5164, loss 0.0250367, acc 1, learning_rate 0.0001
2017-10-11T15:11:26.231204: step 5165, loss 0.086533, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:26.514908: step 5166, loss 0.0219269, acc 1, learning_rate 0.0001
2017-10-11T15:11:26.884891: step 5167, loss 0.058268, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:27.198501: step 5168, loss 0.124381, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:27.514200: step 5169, loss 0.0228249, acc 1, learning_rate 0.0001
2017-10-11T15:11:27.810603: step 5170, loss 0.121462, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:28.122504: step 5171, loss 0.0457336, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:28.433881: step 5172, loss 0.0344049, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:28.785666: step 5173, loss 0.158983, acc 0.921875, learning_rate 0.0001
2017-10-11T15:11:29.095030: step 5174, loss 0.0542766, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:29.411911: step 5175, loss 0.168623, acc 0.9375, learning_rate 0.0001
2017-10-11T15:11:29.688003: step 5176, loss 0.0784877, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:29.993322: step 5177, loss 0.0860024, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:30.313674: step 5178, loss 0.0171384, acc 1, learning_rate 0.0001
2017-10-11T15:11:30.622701: step 5179, loss 0.123466, acc 0.9375, learning_rate 0.0001
2017-10-11T15:11:30.897280: step 5180, loss 0.0496403, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:31.135977: step 5181, loss 0.0338968, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:31.396606: step 5182, loss 0.0864573, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:31.713555: step 5183, loss 0.110802, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:31.980314: step 5184, loss 0.0800247, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:32.225960: step 5185, loss 0.0636195, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:32.510020: step 5186, loss 0.056763, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:32.786768: step 5187, loss 0.111847, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:33.031115: step 5188, loss 0.0484838, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:33.315870: step 5189, loss 0.0145601, acc 1, learning_rate 0.0001
2017-10-11T15:11:33.624528: step 5190, loss 0.0693067, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:33.944317: step 5191, loss 0.106665, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:34.296363: step 5192, loss 0.069799, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:34.577639: step 5193, loss 0.0481915, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:34.874783: step 5194, loss 0.0539537, acc 0.980392, learning_rate 0.0001
2017-10-11T15:11:35.211516: step 5195, loss 0.051446, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:35.511946: step 5196, loss 0.170052, acc 0.9375, learning_rate 0.0001
2017-10-11T15:11:35.783167: step 5197, loss 0.117363, acc 0.9375, learning_rate 0.0001
2017-10-11T15:11:36.092680: step 5198, loss 0.0761745, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:36.401452: step 5199, loss 0.107983, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:36.721803: step 5200, loss 0.0459636, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:11:37.012516: step 5200, loss 0.22236, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5200

2017-10-11T15:11:38.839779: step 5201, loss 0.0304981, acc 1, learning_rate 0.0001
2017-10-11T15:11:39.149252: step 5202, loss 0.10808, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:39.388568: step 5203, loss 0.0554339, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:39.645912: step 5204, loss 0.139569, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:39.934405: step 5205, loss 0.0363622, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:40.245615: step 5206, loss 0.0767374, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:40.501404: step 5207, loss 0.0302438, acc 1, learning_rate 0.0001
2017-10-11T15:11:40.816297: step 5208, loss 0.0289495, acc 1, learning_rate 0.0001
2017-10-11T15:11:41.132216: step 5209, loss 0.0895154, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:41.439594: step 5210, loss 0.0582497, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:41.744758: step 5211, loss 0.13851, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:42.053534: step 5212, loss 0.0576859, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:42.341386: step 5213, loss 0.0504996, acc 1, learning_rate 0.0001
2017-10-11T15:11:42.614823: step 5214, loss 0.0826805, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:42.946963: step 5215, loss 0.116715, acc 0.9375, learning_rate 0.0001
2017-10-11T15:11:43.238085: step 5216, loss 0.15435, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:43.537400: step 5217, loss 0.048203, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:43.850635: step 5218, loss 0.080031, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:44.163410: step 5219, loss 0.0253795, acc 1, learning_rate 0.0001
2017-10-11T15:11:44.458816: step 5220, loss 0.122935, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:44.800357: step 5221, loss 0.14662, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:45.177861: step 5222, loss 0.0204721, acc 1, learning_rate 0.0001
2017-10-11T15:11:45.516321: step 5223, loss 0.12269, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:45.797793: step 5224, loss 0.0444641, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:46.054602: step 5225, loss 0.10965, acc 0.9375, learning_rate 0.0001
2017-10-11T15:11:46.287081: step 5226, loss 0.163241, acc 0.9375, learning_rate 0.0001
2017-10-11T15:11:46.545848: step 5227, loss 0.0664381, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:46.814496: step 5228, loss 0.0272935, acc 1, learning_rate 0.0001
2017-10-11T15:11:47.090490: step 5229, loss 0.149837, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:47.366033: step 5230, loss 0.113605, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:47.664821: step 5231, loss 0.0444026, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:47.950397: step 5232, loss 0.0787887, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:48.268231: step 5233, loss 0.02639, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:48.628166: step 5234, loss 0.0729806, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:48.936286: step 5235, loss 0.0402473, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:49.281575: step 5236, loss 0.0805338, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:49.603706: step 5237, loss 0.032304, acc 1, learning_rate 0.0001
2017-10-11T15:11:49.888286: step 5238, loss 0.0347048, acc 1, learning_rate 0.0001
2017-10-11T15:11:50.133114: step 5239, loss 0.0437003, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:50.440200: step 5240, loss 0.103777, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:11:50.677342: step 5240, loss 0.217533, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5240

2017-10-11T15:11:52.789551: step 5241, loss 0.133798, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:53.071928: step 5242, loss 0.0797295, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:53.324321: step 5243, loss 0.0462057, acc 1, learning_rate 0.0001
2017-10-11T15:11:53.602384: step 5244, loss 0.0465025, acc 1, learning_rate 0.0001
2017-10-11T15:11:53.850119: step 5245, loss 0.183187, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:54.118674: step 5246, loss 0.0959566, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:54.353715: step 5247, loss 0.0474889, acc 1, learning_rate 0.0001
2017-10-11T15:11:54.638082: step 5248, loss 0.11653, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:54.915915: step 5249, loss 0.0224755, acc 1, learning_rate 0.0001
2017-10-11T15:11:55.243731: step 5250, loss 0.0394693, acc 1, learning_rate 0.0001
2017-10-11T15:11:55.570667: step 5251, loss 0.0714822, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:55.887630: step 5252, loss 0.0769683, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:56.178393: step 5253, loss 0.0488788, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:56.446189: step 5254, loss 0.0552036, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:56.728178: step 5255, loss 0.0990573, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:57.031192: step 5256, loss 0.255148, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:57.353101: step 5257, loss 0.0308099, acc 1, learning_rate 0.0001
2017-10-11T15:11:57.672700: step 5258, loss 0.0503874, acc 0.96875, learning_rate 0.0001
2017-10-11T15:11:57.995040: step 5259, loss 0.02705, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:58.274792: step 5260, loss 0.0185143, acc 1, learning_rate 0.0001
2017-10-11T15:11:58.563305: step 5261, loss 0.0508522, acc 0.984375, learning_rate 0.0001
2017-10-11T15:11:58.868869: step 5262, loss 0.123153, acc 0.9375, learning_rate 0.0001
2017-10-11T15:11:59.172762: step 5263, loss 0.16108, acc 0.9375, learning_rate 0.0001
2017-10-11T15:11:59.451845: step 5264, loss 0.126928, acc 0.953125, learning_rate 0.0001
2017-10-11T15:11:59.725344: step 5265, loss 0.196348, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:00.027862: step 5266, loss 0.228437, acc 0.9375, learning_rate 0.0001
2017-10-11T15:12:00.333383: step 5267, loss 0.0313027, acc 1, learning_rate 0.0001
2017-10-11T15:12:00.613772: step 5268, loss 0.0806297, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:00.880399: step 5269, loss 0.044907, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:01.142740: step 5270, loss 0.218496, acc 0.9375, learning_rate 0.0001
2017-10-11T15:12:01.428126: step 5271, loss 0.0787769, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:01.741984: step 5272, loss 0.0666568, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:02.049503: step 5273, loss 0.0714736, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:02.362389: step 5274, loss 0.069983, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:02.623850: step 5275, loss 0.119186, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:02.939694: step 5276, loss 0.127729, acc 0.9375, learning_rate 0.0001
2017-10-11T15:12:03.265019: step 5277, loss 0.115689, acc 0.9375, learning_rate 0.0001
2017-10-11T15:12:03.573934: step 5278, loss 0.149206, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:03.864245: step 5279, loss 0.0316118, acc 1, learning_rate 0.0001
2017-10-11T15:12:04.186910: step 5280, loss 0.0761276, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:12:04.479148: step 5280, loss 0.224558, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5280

2017-10-11T15:12:07.526940: step 5281, loss 0.0314369, acc 1, learning_rate 0.0001
2017-10-11T15:12:07.826320: step 5282, loss 0.181606, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:08.106867: step 5283, loss 0.0708125, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:08.388415: step 5284, loss 0.115989, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:08.732486: step 5285, loss 0.022075, acc 1, learning_rate 0.0001
2017-10-11T15:12:09.073675: step 5286, loss 0.0627189, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:09.432097: step 5287, loss 0.086607, acc 0.9375, learning_rate 0.0001
2017-10-11T15:12:09.708961: step 5288, loss 0.126819, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:10.016918: step 5289, loss 0.0579505, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:10.334509: step 5290, loss 0.0509586, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:10.611743: step 5291, loss 0.0134014, acc 1, learning_rate 0.0001
2017-10-11T15:12:10.905842: step 5292, loss 0.141449, acc 0.941176, learning_rate 0.0001
2017-10-11T15:12:11.205636: step 5293, loss 0.181672, acc 0.90625, learning_rate 0.0001
2017-10-11T15:12:11.524320: step 5294, loss 0.0210129, acc 1, learning_rate 0.0001
2017-10-11T15:12:11.861129: step 5295, loss 0.0183931, acc 1, learning_rate 0.0001
2017-10-11T15:12:12.153568: step 5296, loss 0.0401091, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:12.456340: step 5297, loss 0.0402499, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:12.746797: step 5298, loss 0.123565, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:13.034188: step 5299, loss 0.0738639, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:13.292200: step 5300, loss 0.0393686, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:13.536366: step 5301, loss 0.0205115, acc 1, learning_rate 0.0001
2017-10-11T15:12:13.812342: step 5302, loss 0.0684332, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:14.076040: step 5303, loss 0.085693, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:14.375371: step 5304, loss 0.121471, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:14.664520: step 5305, loss 0.0455038, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:14.918869: step 5306, loss 0.0395527, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:15.200626: step 5307, loss 0.0361961, acc 1, learning_rate 0.0001
2017-10-11T15:12:15.495565: step 5308, loss 0.047456, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:15.852717: step 5309, loss 0.106765, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:16.160630: step 5310, loss 0.0748953, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:16.494770: step 5311, loss 0.0394428, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:16.814152: step 5312, loss 0.027361, acc 1, learning_rate 0.0001
2017-10-11T15:12:17.148344: step 5313, loss 0.0434172, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:17.485773: step 5314, loss 0.056293, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:17.766916: step 5315, loss 0.0127503, acc 1, learning_rate 0.0001
2017-10-11T15:12:18.041511: step 5316, loss 0.0208334, acc 1, learning_rate 0.0001
2017-10-11T15:12:18.300320: step 5317, loss 0.086799, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:18.598061: step 5318, loss 0.0751214, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:18.887468: step 5319, loss 0.0655619, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:19.157665: step 5320, loss 0.0488098, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:12:19.427739: step 5320, loss 0.222475, acc 0.913669

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5320

2017-10-11T15:12:21.156877: step 5321, loss 0.0948227, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:21.390492: step 5322, loss 0.181132, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:21.646569: step 5323, loss 0.0752156, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:21.916210: step 5324, loss 0.0786437, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:22.251580: step 5325, loss 0.105178, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:22.510311: step 5326, loss 0.0523113, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:22.769203: step 5327, loss 0.0759229, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:23.098783: step 5328, loss 0.0644692, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:23.443446: step 5329, loss 0.14565, acc 0.921875, learning_rate 0.0001
2017-10-11T15:12:23.743710: step 5330, loss 0.0524439, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:24.084623: step 5331, loss 0.0272851, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:24.417265: step 5332, loss 0.0141847, acc 1, learning_rate 0.0001
2017-10-11T15:12:24.707837: step 5333, loss 0.0893722, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:25.000447: step 5334, loss 0.0553644, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:25.324688: step 5335, loss 0.0675588, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:25.639227: step 5336, loss 0.0630256, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:25.938475: step 5337, loss 0.132162, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:26.258149: step 5338, loss 0.0581511, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:26.549403: step 5339, loss 0.0299861, acc 1, learning_rate 0.0001
2017-10-11T15:12:26.858202: step 5340, loss 0.0690367, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:27.171905: step 5341, loss 0.108486, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:27.491693: step 5342, loss 0.0599354, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:27.772117: step 5343, loss 0.0604766, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:28.049454: step 5344, loss 0.0637138, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:28.329234: step 5345, loss 0.0711163, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:28.581987: step 5346, loss 0.0902566, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:28.855886: step 5347, loss 0.0359489, acc 1, learning_rate 0.0001
2017-10-11T15:12:29.137501: step 5348, loss 0.0167849, acc 1, learning_rate 0.0001
2017-10-11T15:12:29.462712: step 5349, loss 0.0136176, acc 1, learning_rate 0.0001
2017-10-11T15:12:29.843377: step 5350, loss 0.0719589, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:30.155297: step 5351, loss 0.0928631, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:30.455425: step 5352, loss 0.0869562, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:30.775261: step 5353, loss 0.0383541, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:31.113308: step 5354, loss 0.0682785, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:31.393421: step 5355, loss 0.0498083, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:31.682492: step 5356, loss 0.12877, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:32.005245: step 5357, loss 0.0430584, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:32.290125: step 5358, loss 0.0948919, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:32.611032: step 5359, loss 0.0520872, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:32.893623: step 5360, loss 0.136924, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T15:12:33.128911: step 5360, loss 0.219332, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5360

2017-10-11T15:12:35.993170: step 5361, loss 0.0890121, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:36.294995: step 5362, loss 0.0512999, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:36.650850: step 5363, loss 0.0837855, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:36.980720: step 5364, loss 0.0912824, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:37.299031: step 5365, loss 0.0691279, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:37.610992: step 5366, loss 0.0324349, acc 1, learning_rate 0.0001
2017-10-11T15:12:37.949205: step 5367, loss 0.023336, acc 1, learning_rate 0.0001
2017-10-11T15:12:38.270762: step 5368, loss 0.123402, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:38.588992: step 5369, loss 0.0783314, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:38.899437: step 5370, loss 0.176601, acc 0.921875, learning_rate 0.0001
2017-10-11T15:12:39.213148: step 5371, loss 0.0446734, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:39.540391: step 5372, loss 0.107314, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:39.833675: step 5373, loss 0.0873066, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:40.127530: step 5374, loss 0.118358, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:40.429269: step 5375, loss 0.0807644, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:40.781430: step 5376, loss 0.0583408, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:41.099404: step 5377, loss 0.0898322, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:41.370273: step 5378, loss 0.0787454, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:41.631219: step 5379, loss 0.100648, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:41.915849: step 5380, loss 0.105677, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:42.173987: step 5381, loss 0.204867, acc 0.90625, learning_rate 0.0001
2017-10-11T15:12:42.440386: step 5382, loss 0.100603, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:42.700863: step 5383, loss 0.0499107, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:42.994382: step 5384, loss 0.15885, acc 0.9375, learning_rate 0.0001
2017-10-11T15:12:43.278106: step 5385, loss 0.058936, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:43.591082: step 5386, loss 0.0185544, acc 1, learning_rate 0.0001
2017-10-11T15:12:43.849540: step 5387, loss 0.0447643, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:44.154755: step 5388, loss 0.0846841, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:44.429632: step 5389, loss 0.0287707, acc 1, learning_rate 0.0001
2017-10-11T15:12:44.712327: step 5390, loss 0.0485473, acc 1, learning_rate 0.0001
2017-10-11T15:12:45.026579: step 5391, loss 0.0323155, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:45.333250: step 5392, loss 0.0824171, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:45.612143: step 5393, loss 0.00908464, acc 1, learning_rate 0.0001
2017-10-11T15:12:45.923666: step 5394, loss 0.0794106, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:46.238773: step 5395, loss 0.0974604, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:46.541637: step 5396, loss 0.0302926, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:46.819014: step 5397, loss 0.0783353, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:47.131703: step 5398, loss 0.0651747, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:47.400115: step 5399, loss 0.093326, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:47.661935: step 5400, loss 0.0507683, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:12:47.902690: step 5400, loss 0.218982, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5400

2017-10-11T15:12:49.609088: step 5401, loss 0.0341452, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:49.848106: step 5402, loss 0.0433795, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:50.103160: step 5403, loss 0.0840451, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:50.364355: step 5404, loss 0.0526163, acc 1, learning_rate 0.0001
2017-10-11T15:12:50.657604: step 5405, loss 0.039463, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:50.993944: step 5406, loss 0.0193271, acc 1, learning_rate 0.0001
2017-10-11T15:12:51.318401: step 5407, loss 0.0673851, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:51.608757: step 5408, loss 0.108443, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:51.908146: step 5409, loss 0.0926454, acc 0.9375, learning_rate 0.0001
2017-10-11T15:12:52.204900: step 5410, loss 0.0348566, acc 1, learning_rate 0.0001
2017-10-11T15:12:52.549275: step 5411, loss 0.144327, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:52.831472: step 5412, loss 0.047848, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:53.160846: step 5413, loss 0.119324, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:53.466527: step 5414, loss 0.114639, acc 0.9375, learning_rate 0.0001
2017-10-11T15:12:53.790126: step 5415, loss 0.0280178, acc 1, learning_rate 0.0001
2017-10-11T15:12:54.129800: step 5416, loss 0.0437578, acc 1, learning_rate 0.0001
2017-10-11T15:12:54.413459: step 5417, loss 0.0188378, acc 1, learning_rate 0.0001
2017-10-11T15:12:54.687977: step 5418, loss 0.070365, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:54.961306: step 5419, loss 0.014624, acc 1, learning_rate 0.0001
2017-10-11T15:12:55.217281: step 5420, loss 0.0125908, acc 1, learning_rate 0.0001
2017-10-11T15:12:55.459306: step 5421, loss 0.0186947, acc 1, learning_rate 0.0001
2017-10-11T15:12:55.740099: step 5422, loss 0.128033, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:56.030965: step 5423, loss 0.0450046, acc 0.984375, learning_rate 0.0001
2017-10-11T15:12:56.300402: step 5424, loss 0.012142, acc 1, learning_rate 0.0001
2017-10-11T15:12:56.625240: step 5425, loss 0.069921, acc 0.96875, learning_rate 0.0001
2017-10-11T15:12:56.947654: step 5426, loss 0.0179839, acc 1, learning_rate 0.0001
2017-10-11T15:12:57.259491: step 5427, loss 0.0379138, acc 1, learning_rate 0.0001
2017-10-11T15:12:57.532558: step 5428, loss 0.176227, acc 0.921875, learning_rate 0.0001
2017-10-11T15:12:57.826582: step 5429, loss 0.0224141, acc 1, learning_rate 0.0001
2017-10-11T15:12:58.137041: step 5430, loss 0.0264888, acc 1, learning_rate 0.0001
2017-10-11T15:12:58.451323: step 5431, loss 0.0912366, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:58.754628: step 5432, loss 0.157825, acc 0.9375, learning_rate 0.0001
2017-10-11T15:12:59.051287: step 5433, loss 0.0915765, acc 0.953125, learning_rate 0.0001
2017-10-11T15:12:59.360612: step 5434, loss 0.0209304, acc 1, learning_rate 0.0001
2017-10-11T15:12:59.673090: step 5435, loss 0.0232767, acc 1, learning_rate 0.0001
2017-10-11T15:12:59.965991: step 5436, loss 0.120652, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:00.274515: step 5437, loss 0.0459054, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:00.599613: step 5438, loss 0.0659024, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:00.892949: step 5439, loss 0.0193311, acc 1, learning_rate 0.0001
2017-10-11T15:13:01.155341: step 5440, loss 0.0420823, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:13:01.447260: step 5440, loss 0.215303, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5440

2017-10-11T15:13:03.629395: step 5441, loss 0.0603053, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:03.935096: step 5442, loss 0.0410127, acc 1, learning_rate 0.0001
2017-10-11T15:13:04.207711: step 5443, loss 0.0406508, acc 1, learning_rate 0.0001
2017-10-11T15:13:04.469745: step 5444, loss 0.0788496, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:04.716118: step 5445, loss 0.0580278, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:04.945662: step 5446, loss 0.0702691, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:05.267574: step 5447, loss 0.0217527, acc 1, learning_rate 0.0001
2017-10-11T15:13:05.587207: step 5448, loss 0.0496078, acc 1, learning_rate 0.0001
2017-10-11T15:13:05.917580: step 5449, loss 0.0608454, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:06.245318: step 5450, loss 0.0721272, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:06.584577: step 5451, loss 0.10499, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:06.872213: step 5452, loss 0.0759774, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:07.222013: step 5453, loss 0.0617832, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:07.507548: step 5454, loss 0.0686202, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:07.809551: step 5455, loss 0.0386395, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:08.102225: step 5456, loss 0.0757272, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:08.376829: step 5457, loss 0.149543, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:08.666893: step 5458, loss 0.137929, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:08.931819: step 5459, loss 0.0266685, acc 1, learning_rate 0.0001
2017-10-11T15:13:09.226266: step 5460, loss 0.0952208, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:09.503842: step 5461, loss 0.0416987, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:09.793392: step 5462, loss 0.0977734, acc 0.9375, learning_rate 0.0001
2017-10-11T15:13:10.075364: step 5463, loss 0.0778983, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:10.413079: step 5464, loss 0.0244677, acc 1, learning_rate 0.0001
2017-10-11T15:13:10.763393: step 5465, loss 0.0463915, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:11.064768: step 5466, loss 0.0407631, acc 1, learning_rate 0.0001
2017-10-11T15:13:11.423050: step 5467, loss 0.080755, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:11.741978: step 5468, loss 0.0502089, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:12.037747: step 5469, loss 0.046478, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:12.345696: step 5470, loss 0.0819808, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:12.673353: step 5471, loss 0.101303, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:12.955899: step 5472, loss 0.0421271, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:13.274070: step 5473, loss 0.0495757, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:13.613311: step 5474, loss 0.0745151, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:13.911434: step 5475, loss 0.0856459, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:14.228462: step 5476, loss 0.0888347, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:14.556172: step 5477, loss 0.0175257, acc 1, learning_rate 0.0001
2017-10-11T15:13:14.899859: step 5478, loss 0.0285072, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:15.180467: step 5479, loss 0.0590002, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:15.455363: step 5480, loss 0.07907, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T15:13:15.706020: step 5480, loss 0.217439, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5480

2017-10-11T15:13:18.443085: step 5481, loss 0.0377386, acc 1, learning_rate 0.0001
2017-10-11T15:13:18.748526: step 5482, loss 0.0646533, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:19.103161: step 5483, loss 0.0161134, acc 1, learning_rate 0.0001
2017-10-11T15:13:19.435074: step 5484, loss 0.047918, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:19.780516: step 5485, loss 0.0306212, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:20.090407: step 5486, loss 0.0237025, acc 1, learning_rate 0.0001
2017-10-11T15:13:20.408403: step 5487, loss 0.0432635, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:20.714253: step 5488, loss 0.0724988, acc 0.980392, learning_rate 0.0001
2017-10-11T15:13:21.082032: step 5489, loss 0.0806033, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:21.400157: step 5490, loss 0.0702867, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:21.728731: step 5491, loss 0.0887976, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:22.027129: step 5492, loss 0.0302887, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:22.292436: step 5493, loss 0.0474035, acc 1, learning_rate 0.0001
2017-10-11T15:13:22.535425: step 5494, loss 0.0969314, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:22.816013: step 5495, loss 0.129806, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:23.062587: step 5496, loss 0.0819543, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:23.315192: step 5497, loss 0.024208, acc 1, learning_rate 0.0001
2017-10-11T15:13:23.605213: step 5498, loss 0.0564568, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:23.863315: step 5499, loss 0.0552493, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:24.165686: step 5500, loss 0.0317783, acc 1, learning_rate 0.0001
2017-10-11T15:13:24.478779: step 5501, loss 0.105751, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:24.779065: step 5502, loss 0.0194574, acc 1, learning_rate 0.0001
2017-10-11T15:13:25.049827: step 5503, loss 0.0842039, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:25.324985: step 5504, loss 0.164525, acc 0.921875, learning_rate 0.0001
2017-10-11T15:13:25.624058: step 5505, loss 0.0287093, acc 1, learning_rate 0.0001
2017-10-11T15:13:25.937562: step 5506, loss 0.138087, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:26.226171: step 5507, loss 0.0449192, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:26.505611: step 5508, loss 0.0103876, acc 1, learning_rate 0.0001
2017-10-11T15:13:26.807767: step 5509, loss 0.0136142, acc 1, learning_rate 0.0001
2017-10-11T15:13:27.070947: step 5510, loss 0.0786214, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:27.381649: step 5511, loss 0.12399, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:27.674336: step 5512, loss 0.0336047, acc 1, learning_rate 0.0001
2017-10-11T15:13:27.984283: step 5513, loss 0.0267916, acc 1, learning_rate 0.0001
2017-10-11T15:13:28.297724: step 5514, loss 0.0299603, acc 1, learning_rate 0.0001
2017-10-11T15:13:28.583228: step 5515, loss 0.0663717, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:28.899788: step 5516, loss 0.069171, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:29.227086: step 5517, loss 0.046332, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:29.487435: step 5518, loss 0.084655, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:29.768929: step 5519, loss 0.104365, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:30.058734: step 5520, loss 0.129304, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T15:13:30.303792: step 5520, loss 0.216312, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5520

2017-10-11T15:13:32.134663: step 5521, loss 0.0531668, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:32.397239: step 5522, loss 0.103629, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:32.663170: step 5523, loss 0.0385694, acc 1, learning_rate 0.0001
2017-10-11T15:13:32.995281: step 5524, loss 0.03002, acc 1, learning_rate 0.0001
2017-10-11T15:13:33.286824: step 5525, loss 0.0668852, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:33.610040: step 5526, loss 0.0473838, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:33.937258: step 5527, loss 0.0688666, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:34.240024: step 5528, loss 0.0478926, acc 1, learning_rate 0.0001
2017-10-11T15:13:34.570426: step 5529, loss 0.0488005, acc 1, learning_rate 0.0001
2017-10-11T15:13:34.867610: step 5530, loss 0.0685042, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:35.174231: step 5531, loss 0.0552394, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:35.477995: step 5532, loss 0.0923038, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:35.755234: step 5533, loss 0.109261, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:36.090435: step 5534, loss 0.0330277, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:36.331807: step 5535, loss 0.122551, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:36.591286: step 5536, loss 0.0154006, acc 1, learning_rate 0.0001
2017-10-11T15:13:36.811862: step 5537, loss 0.0394865, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:37.076666: step 5538, loss 0.0191533, acc 1, learning_rate 0.0001
2017-10-11T15:13:37.338341: step 5539, loss 0.0238654, acc 1, learning_rate 0.0001
2017-10-11T15:13:37.612213: step 5540, loss 0.0373719, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:37.942727: step 5541, loss 0.0254809, acc 1, learning_rate 0.0001
2017-10-11T15:13:38.289766: step 5542, loss 0.0218399, acc 1, learning_rate 0.0001
2017-10-11T15:13:38.567563: step 5543, loss 0.0180604, acc 1, learning_rate 0.0001
2017-10-11T15:13:38.852170: step 5544, loss 0.0810493, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:39.161258: step 5545, loss 0.0917941, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:39.464200: step 5546, loss 0.0472149, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:39.784207: step 5547, loss 0.0730224, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:40.070574: step 5548, loss 0.0313764, acc 1, learning_rate 0.0001
2017-10-11T15:13:40.373552: step 5549, loss 0.0627749, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:40.698350: step 5550, loss 0.100136, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:41.023481: step 5551, loss 0.0623086, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:41.334921: step 5552, loss 0.0665598, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:41.657407: step 5553, loss 0.0676151, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:41.998780: step 5554, loss 0.0419895, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:42.300070: step 5555, loss 0.00909092, acc 1, learning_rate 0.0001
2017-10-11T15:13:42.567865: step 5556, loss 0.0534083, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:42.803577: step 5557, loss 0.0261726, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:43.094545: step 5558, loss 0.047008, acc 1, learning_rate 0.0001
2017-10-11T15:13:43.353091: step 5559, loss 0.0722011, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:43.616514: step 5560, loss 0.0492275, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:13:43.873586: step 5560, loss 0.216602, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5560

2017-10-11T15:13:46.088986: step 5561, loss 0.0444637, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:46.381589: step 5562, loss 0.0979243, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:46.649218: step 5563, loss 0.0854483, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:46.920791: step 5564, loss 0.0901871, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:47.163024: step 5565, loss 0.0554047, acc 1, learning_rate 0.0001
2017-10-11T15:13:47.493294: step 5566, loss 0.0702786, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:47.836856: step 5567, loss 0.0570735, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:48.140405: step 5568, loss 0.0595662, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:48.448547: step 5569, loss 0.0994645, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:48.747342: step 5570, loss 0.0697688, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:49.089731: step 5571, loss 0.0626205, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:49.342375: step 5572, loss 0.0547898, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:49.617276: step 5573, loss 0.0282517, acc 1, learning_rate 0.0001
2017-10-11T15:13:49.890432: step 5574, loss 0.0279818, acc 1, learning_rate 0.0001
2017-10-11T15:13:50.161322: step 5575, loss 0.051135, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:50.429923: step 5576, loss 0.133645, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:50.700135: step 5577, loss 0.0683719, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:50.946100: step 5578, loss 0.189244, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:51.245395: step 5579, loss 0.0854681, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:51.554434: step 5580, loss 0.053467, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:51.843360: step 5581, loss 0.241094, acc 0.90625, learning_rate 0.0001
2017-10-11T15:13:52.177963: step 5582, loss 0.0290258, acc 1, learning_rate 0.0001
2017-10-11T15:13:52.493162: step 5583, loss 0.0400252, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:52.801266: step 5584, loss 0.0674183, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:53.115865: step 5585, loss 0.0869139, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:53.404991: step 5586, loss 0.0593627, acc 0.960784, learning_rate 0.0001
2017-10-11T15:13:53.711779: step 5587, loss 0.0494821, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:54.007506: step 5588, loss 0.115336, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:54.330761: step 5589, loss 0.0636855, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:54.667207: step 5590, loss 0.0809732, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:54.976191: step 5591, loss 0.0403973, acc 1, learning_rate 0.0001
2017-10-11T15:13:55.299330: step 5592, loss 0.0348743, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:55.595865: step 5593, loss 0.129998, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:55.918752: step 5594, loss 0.0187875, acc 1, learning_rate 0.0001
2017-10-11T15:13:56.236257: step 5595, loss 0.0929864, acc 0.953125, learning_rate 0.0001
2017-10-11T15:13:56.544185: step 5596, loss 0.0639711, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:56.869124: step 5597, loss 0.0706088, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:57.217395: step 5598, loss 0.0990998, acc 0.96875, learning_rate 0.0001
2017-10-11T15:13:57.457300: step 5599, loss 0.0410769, acc 0.984375, learning_rate 0.0001
2017-10-11T15:13:57.718295: step 5600, loss 0.0755097, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:13:57.956630: step 5600, loss 0.219243, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5600

2017-10-11T15:14:00.943887: step 5601, loss 0.0534993, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:01.261595: step 5602, loss 0.0956987, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:01.595125: step 5603, loss 0.0468872, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:01.920156: step 5604, loss 0.0183705, acc 1, learning_rate 0.0001
2017-10-11T15:14:02.274106: step 5605, loss 0.0980694, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:02.592210: step 5606, loss 0.0945913, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:02.883831: step 5607, loss 0.0616701, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:03.173828: step 5608, loss 0.0468013, acc 1, learning_rate 0.0001
2017-10-11T15:14:03.456638: step 5609, loss 0.0887555, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:03.729497: step 5610, loss 0.0937136, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:04.024340: step 5611, loss 0.0596974, acc 1, learning_rate 0.0001
2017-10-11T15:14:04.312599: step 5612, loss 0.0203302, acc 1, learning_rate 0.0001
2017-10-11T15:14:04.566574: step 5613, loss 0.165798, acc 0.921875, learning_rate 0.0001
2017-10-11T15:14:04.819257: step 5614, loss 0.0535142, acc 1, learning_rate 0.0001
2017-10-11T15:14:05.104738: step 5615, loss 0.017808, acc 1, learning_rate 0.0001
2017-10-11T15:14:05.380844: step 5616, loss 0.0404571, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:05.672294: step 5617, loss 0.163731, acc 0.921875, learning_rate 0.0001
2017-10-11T15:14:05.941875: step 5618, loss 0.0334002, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:06.255860: step 5619, loss 0.0413982, acc 1, learning_rate 0.0001
2017-10-11T15:14:06.570029: step 5620, loss 0.102726, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:06.891833: step 5621, loss 0.0752642, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:07.216031: step 5622, loss 0.0227536, acc 1, learning_rate 0.0001
2017-10-11T15:14:07.531498: step 5623, loss 0.0992135, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:07.865602: step 5624, loss 0.0879945, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:08.192859: step 5625, loss 0.0599163, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:08.511244: step 5626, loss 0.0307096, acc 1, learning_rate 0.0001
2017-10-11T15:14:08.803072: step 5627, loss 0.0211156, acc 1, learning_rate 0.0001
2017-10-11T15:14:09.104441: step 5628, loss 0.0628218, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:09.386845: step 5629, loss 0.107249, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:09.680212: step 5630, loss 0.0460495, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:09.985677: step 5631, loss 0.130987, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:10.310841: step 5632, loss 0.0318868, acc 1, learning_rate 0.0001
2017-10-11T15:14:10.610734: step 5633, loss 0.0124807, acc 1, learning_rate 0.0001
2017-10-11T15:14:10.952529: step 5634, loss 0.0939183, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:11.221916: step 5635, loss 0.0870278, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:11.502424: step 5636, loss 0.0569701, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:11.799779: step 5637, loss 0.045081, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:12.067774: step 5638, loss 0.0502841, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:12.338658: step 5639, loss 0.0267415, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:12.591612: step 5640, loss 0.0603987, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:14:12.827751: step 5640, loss 0.217975, acc 0.920863

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5640

2017-10-11T15:14:14.689054: step 5641, loss 0.114137, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:14.953264: step 5642, loss 0.0208985, acc 1, learning_rate 0.0001
2017-10-11T15:14:15.227786: step 5643, loss 0.0196582, acc 1, learning_rate 0.0001
2017-10-11T15:14:15.479030: step 5644, loss 0.0777522, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:15.718085: step 5645, loss 0.0692387, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:15.991787: step 5646, loss 0.0619221, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:16.233268: step 5647, loss 0.050645, acc 1, learning_rate 0.0001
2017-10-11T15:14:16.541831: step 5648, loss 0.0350697, acc 1, learning_rate 0.0001
2017-10-11T15:14:16.826364: step 5649, loss 0.0884552, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:17.122984: step 5650, loss 0.150868, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:17.462983: step 5651, loss 0.0219923, acc 1, learning_rate 0.0001
2017-10-11T15:14:17.722337: step 5652, loss 0.0968302, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:18.006472: step 5653, loss 0.0459179, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:18.282179: step 5654, loss 0.0406002, acc 1, learning_rate 0.0001
2017-10-11T15:14:18.564133: step 5655, loss 0.0270005, acc 1, learning_rate 0.0001
2017-10-11T15:14:18.814337: step 5656, loss 0.057673, acc 1, learning_rate 0.0001
2017-10-11T15:14:19.114842: step 5657, loss 0.0506022, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:19.425491: step 5658, loss 0.0537049, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:19.745934: step 5659, loss 0.0428815, acc 1, learning_rate 0.0001
2017-10-11T15:14:20.022469: step 5660, loss 0.0982176, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:20.333836: step 5661, loss 0.015529, acc 1, learning_rate 0.0001
2017-10-11T15:14:20.643872: step 5662, loss 0.020584, acc 1, learning_rate 0.0001
2017-10-11T15:14:20.944831: step 5663, loss 0.0483853, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:21.253114: step 5664, loss 0.0638014, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:21.599151: step 5665, loss 0.0670356, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:21.914014: step 5666, loss 0.0330483, acc 1, learning_rate 0.0001
2017-10-11T15:14:22.240767: step 5667, loss 0.106618, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:22.521908: step 5668, loss 0.0532286, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:22.829637: step 5669, loss 0.0559194, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:23.127143: step 5670, loss 0.0842829, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:23.435462: step 5671, loss 0.0235714, acc 1, learning_rate 0.0001
2017-10-11T15:14:23.738356: step 5672, loss 0.095182, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:24.020240: step 5673, loss 0.0423766, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:24.351553: step 5674, loss 0.0534572, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:24.636478: step 5675, loss 0.0474642, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:24.891007: step 5676, loss 0.0753195, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:25.167715: step 5677, loss 0.0668005, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:25.446347: step 5678, loss 0.0148855, acc 1, learning_rate 0.0001
2017-10-11T15:14:25.731811: step 5679, loss 0.0294321, acc 1, learning_rate 0.0001
2017-10-11T15:14:25.982022: step 5680, loss 0.0558509, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:14:26.246207: step 5680, loss 0.217147, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5680

2017-10-11T15:14:28.748883: step 5681, loss 0.0554054, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:29.036334: step 5682, loss 0.0277671, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:29.323246: step 5683, loss 0.0238096, acc 1, learning_rate 0.0001
2017-10-11T15:14:29.574633: step 5684, loss 0.0277386, acc 1, learning_rate 0.0001
2017-10-11T15:14:29.883320: step 5685, loss 0.11084, acc 0.9375, learning_rate 0.0001
2017-10-11T15:14:30.208385: step 5686, loss 0.0595334, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:30.595564: step 5687, loss 0.0714305, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:30.876461: step 5688, loss 0.0779655, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:31.165368: step 5689, loss 0.126324, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:31.380758: step 5690, loss 0.0514162, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:31.679994: step 5691, loss 0.0423701, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:31.967787: step 5692, loss 0.103429, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:32.240787: step 5693, loss 0.0857064, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:32.477758: step 5694, loss 0.021392, acc 1, learning_rate 0.0001
2017-10-11T15:14:32.740701: step 5695, loss 0.0435028, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:33.018898: step 5696, loss 0.0811293, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:33.320713: step 5697, loss 0.0917526, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:33.614061: step 5698, loss 0.0692034, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:33.916302: step 5699, loss 0.050383, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:34.254830: step 5700, loss 0.0638038, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:34.564112: step 5701, loss 0.0942822, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:34.945399: step 5702, loss 0.0697547, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:35.237542: step 5703, loss 0.115024, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:35.574469: step 5704, loss 0.025317, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:35.928142: step 5705, loss 0.0210533, acc 1, learning_rate 0.0001
2017-10-11T15:14:36.221991: step 5706, loss 0.0413894, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:36.520610: step 5707, loss 0.0988534, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:36.832578: step 5708, loss 0.0269299, acc 1, learning_rate 0.0001
2017-10-11T15:14:37.135210: step 5709, loss 0.108643, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:37.452698: step 5710, loss 0.0341081, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:37.761420: step 5711, loss 0.0285472, acc 1, learning_rate 0.0001
2017-10-11T15:14:38.028605: step 5712, loss 0.0697532, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:38.361387: step 5713, loss 0.0574083, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:38.670552: step 5714, loss 0.134324, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:38.971218: step 5715, loss 0.043705, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:39.254600: step 5716, loss 0.0605061, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:39.516724: step 5717, loss 0.0883072, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:39.782278: step 5718, loss 0.0512878, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:40.067750: step 5719, loss 0.0820754, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:40.309465: step 5720, loss 0.0959803, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T15:14:40.571091: step 5720, loss 0.22189, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5720

2017-10-11T15:14:42.593835: step 5721, loss 0.0611321, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:42.833334: step 5722, loss 0.0377129, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:43.103381: step 5723, loss 0.0417752, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:43.391406: step 5724, loss 0.0301424, acc 1, learning_rate 0.0001
2017-10-11T15:14:43.668067: step 5725, loss 0.0931338, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:43.969729: step 5726, loss 0.0318013, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:44.267987: step 5727, loss 0.0379842, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:44.585673: step 5728, loss 0.0201633, acc 1, learning_rate 0.0001
2017-10-11T15:14:44.909832: step 5729, loss 0.0216288, acc 1, learning_rate 0.0001
2017-10-11T15:14:45.193801: step 5730, loss 0.0604173, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:45.453194: step 5731, loss 0.100074, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:45.695757: step 5732, loss 0.0687807, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:45.917105: step 5733, loss 0.0443363, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:46.175055: step 5734, loss 0.0232129, acc 1, learning_rate 0.0001
2017-10-11T15:14:46.456204: step 5735, loss 0.0241515, acc 1, learning_rate 0.0001
2017-10-11T15:14:46.749796: step 5736, loss 0.060561, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:47.059782: step 5737, loss 0.0722598, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:47.380309: step 5738, loss 0.0865607, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:47.696443: step 5739, loss 0.0220088, acc 1, learning_rate 0.0001
2017-10-11T15:14:48.007662: step 5740, loss 0.132711, acc 0.9375, learning_rate 0.0001
2017-10-11T15:14:48.303591: step 5741, loss 0.0203745, acc 1, learning_rate 0.0001
2017-10-11T15:14:48.587929: step 5742, loss 0.116772, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:48.878033: step 5743, loss 0.0721466, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:49.186665: step 5744, loss 0.158184, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:49.483562: step 5745, loss 0.104718, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:49.799283: step 5746, loss 0.0894608, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:50.080991: step 5747, loss 0.0755346, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:50.372653: step 5748, loss 0.148875, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:50.666110: step 5749, loss 0.0748529, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:50.927346: step 5750, loss 0.0228355, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:51.244099: step 5751, loss 0.07961, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:51.513515: step 5752, loss 0.0430793, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:51.807563: step 5753, loss 0.0445349, acc 1, learning_rate 0.0001
2017-10-11T15:14:52.101661: step 5754, loss 0.0288083, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:52.374189: step 5755, loss 0.0793245, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:52.636134: step 5756, loss 0.164526, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:52.878107: step 5757, loss 0.0672812, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:53.121453: step 5758, loss 0.0470333, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:53.362170: step 5759, loss 0.117138, acc 0.953125, learning_rate 0.0001
2017-10-11T15:14:53.612677: step 5760, loss 0.0590543, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:14:53.868153: step 5760, loss 0.217774, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5760

2017-10-11T15:14:55.940949: step 5761, loss 0.0702543, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:56.161801: step 5762, loss 0.040387, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:56.420624: step 5763, loss 0.0909756, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:56.696500: step 5764, loss 0.15808, acc 0.9375, learning_rate 0.0001
2017-10-11T15:14:56.970862: step 5765, loss 0.077421, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:57.230425: step 5766, loss 0.0557394, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:57.492102: step 5767, loss 0.0575618, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:57.744629: step 5768, loss 0.0858743, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:57.995370: step 5769, loss 0.0273626, acc 1, learning_rate 0.0001
2017-10-11T15:14:58.254476: step 5770, loss 0.0944392, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:58.487330: step 5771, loss 0.0446747, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:58.699971: step 5772, loss 0.0620809, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:58.965687: step 5773, loss 0.0818415, acc 0.96875, learning_rate 0.0001
2017-10-11T15:14:59.275303: step 5774, loss 0.0651894, acc 0.984375, learning_rate 0.0001
2017-10-11T15:14:59.524972: step 5775, loss 0.0153147, acc 1, learning_rate 0.0001
2017-10-11T15:14:59.794765: step 5776, loss 0.0389775, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:00.069326: step 5777, loss 0.0746832, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:00.433466: step 5778, loss 0.051165, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:00.718275: step 5779, loss 0.0526415, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:01.080058: step 5780, loss 0.0414401, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:01.422745: step 5781, loss 0.0650967, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:01.722409: step 5782, loss 0.0209978, acc 1, learning_rate 0.0001
2017-10-11T15:15:02.022974: step 5783, loss 0.0260193, acc 1, learning_rate 0.0001
2017-10-11T15:15:02.276689: step 5784, loss 0.0611394, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:02.571067: step 5785, loss 0.0444141, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:02.858269: step 5786, loss 0.193559, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:03.150206: step 5787, loss 0.042175, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:03.462025: step 5788, loss 0.0371171, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:03.781213: step 5789, loss 0.0943147, acc 0.9375, learning_rate 0.0001
2017-10-11T15:15:04.085630: step 5790, loss 0.0591078, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:04.394525: step 5791, loss 0.0486598, acc 1, learning_rate 0.0001
2017-10-11T15:15:04.717658: step 5792, loss 0.0953371, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:05.012134: step 5793, loss 0.058663, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:05.365682: step 5794, loss 0.0740012, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:05.675609: step 5795, loss 0.0631248, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:05.982019: step 5796, loss 0.0237145, acc 1, learning_rate 0.0001
2017-10-11T15:15:06.255731: step 5797, loss 0.045297, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:06.544397: step 5798, loss 0.0554794, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:06.805918: step 5799, loss 0.0499793, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:07.111950: step 5800, loss 0.0603793, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:15:07.392281: step 5800, loss 0.220454, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5800

2017-10-11T15:15:10.831439: step 5801, loss 0.0744043, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:11.124856: step 5802, loss 0.0807337, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:11.399862: step 5803, loss 0.142152, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:11.682594: step 5804, loss 0.038729, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:11.920773: step 5805, loss 0.0131206, acc 1, learning_rate 0.0001
2017-10-11T15:15:12.180245: step 5806, loss 0.0185041, acc 1, learning_rate 0.0001
2017-10-11T15:15:12.463194: step 5807, loss 0.0695783, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:12.813919: step 5808, loss 0.0874748, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:13.154437: step 5809, loss 0.0852243, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:13.423059: step 5810, loss 0.085525, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:13.710815: step 5811, loss 0.0576761, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:13.994077: step 5812, loss 0.0434977, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:14.272286: step 5813, loss 0.0664603, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:14.528200: step 5814, loss 0.0655976, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:14.843717: step 5815, loss 0.0482585, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:15.136445: step 5816, loss 0.0779305, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:15.451592: step 5817, loss 0.0304685, acc 1, learning_rate 0.0001
2017-10-11T15:15:15.788454: step 5818, loss 0.073666, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:16.121843: step 5819, loss 0.0455735, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:16.445077: step 5820, loss 0.0971268, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:16.762982: step 5821, loss 0.0404386, acc 1, learning_rate 0.0001
2017-10-11T15:15:17.074772: step 5822, loss 0.0916956, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:17.360581: step 5823, loss 0.0609199, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:17.706027: step 5824, loss 0.0385748, acc 1, learning_rate 0.0001
2017-10-11T15:15:18.029824: step 5825, loss 0.0999132, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:18.342441: step 5826, loss 0.0441582, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:18.665803: step 5827, loss 0.0539401, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:18.972453: step 5828, loss 0.0766731, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:19.282126: step 5829, loss 0.0727109, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:19.598498: step 5830, loss 0.0825291, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:19.919757: step 5831, loss 0.0427404, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:20.244560: step 5832, loss 0.112493, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:20.565309: step 5833, loss 0.12878, acc 0.9375, learning_rate 0.0001
2017-10-11T15:15:20.826126: step 5834, loss 0.155437, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:21.050490: step 5835, loss 0.0744222, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:21.328032: step 5836, loss 0.0671018, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:21.569028: step 5837, loss 0.0814812, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:21.840354: step 5838, loss 0.0116362, acc 1, learning_rate 0.0001
2017-10-11T15:15:22.131779: step 5839, loss 0.0265031, acc 1, learning_rate 0.0001
2017-10-11T15:15:22.487587: step 5840, loss 0.133463, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T15:15:22.788538: step 5840, loss 0.21662, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5840

2017-10-11T15:15:24.658385: step 5841, loss 0.0490062, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:24.928766: step 5842, loss 0.0100785, acc 1, learning_rate 0.0001
2017-10-11T15:15:25.238933: step 5843, loss 0.0968169, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:25.514088: step 5844, loss 0.16103, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:25.796925: step 5845, loss 0.0498726, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:26.063856: step 5846, loss 0.0584794, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:26.364366: step 5847, loss 0.0554897, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:26.654644: step 5848, loss 0.0909068, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:26.970651: step 5849, loss 0.0550451, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:27.248170: step 5850, loss 0.0700435, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:27.510653: step 5851, loss 0.124589, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:27.817457: step 5852, loss 0.0648498, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:28.123024: step 5853, loss 0.0788027, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:28.461438: step 5854, loss 0.0350084, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:28.770874: step 5855, loss 0.0594209, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:29.066352: step 5856, loss 0.0592894, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:29.358526: step 5857, loss 0.0495361, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:29.657350: step 5858, loss 0.0754107, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:29.988472: step 5859, loss 0.0201587, acc 1, learning_rate 0.0001
2017-10-11T15:15:30.310469: step 5860, loss 0.120398, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:30.618279: step 5861, loss 0.0372281, acc 1, learning_rate 0.0001
2017-10-11T15:15:30.905258: step 5862, loss 0.0810068, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:31.221747: step 5863, loss 0.0777276, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:31.541056: step 5864, loss 0.0463224, acc 1, learning_rate 0.0001
2017-10-11T15:15:31.839836: step 5865, loss 0.0351289, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:32.139076: step 5866, loss 0.137362, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:32.456700: step 5867, loss 0.0426724, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:32.738176: step 5868, loss 0.04699, acc 1, learning_rate 0.0001
2017-10-11T15:15:33.047611: step 5869, loss 0.0591887, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:33.365864: step 5870, loss 0.0343504, acc 1, learning_rate 0.0001
2017-10-11T15:15:33.668312: step 5871, loss 0.0337858, acc 1, learning_rate 0.0001
2017-10-11T15:15:33.992688: step 5872, loss 0.116642, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:34.263633: step 5873, loss 0.108675, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:34.576385: step 5874, loss 0.0903815, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:34.864399: step 5875, loss 0.0656182, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:35.109641: step 5876, loss 0.139167, acc 0.9375, learning_rate 0.0001
2017-10-11T15:15:35.396532: step 5877, loss 0.0105827, acc 1, learning_rate 0.0001
2017-10-11T15:15:35.664803: step 5878, loss 0.0377704, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:35.913315: step 5879, loss 0.112668, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:36.182578: step 5880, loss 0.204555, acc 0.921569, learning_rate 0.0001

Evaluation:
2017-10-11T15:15:36.405634: step 5880, loss 0.219751, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5880

2017-10-11T15:15:38.669926: step 5881, loss 0.0299683, acc 1, learning_rate 0.0001
2017-10-11T15:15:38.986322: step 5882, loss 0.0601852, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:39.244851: step 5883, loss 0.0263827, acc 1, learning_rate 0.0001
2017-10-11T15:15:39.525610: step 5884, loss 0.0656005, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:39.749944: step 5885, loss 0.0337077, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:40.021370: step 5886, loss 0.0359227, acc 1, learning_rate 0.0001
2017-10-11T15:15:40.254690: step 5887, loss 0.0231639, acc 1, learning_rate 0.0001
2017-10-11T15:15:40.477525: step 5888, loss 0.0382243, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:40.701183: step 5889, loss 0.0292928, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:40.934260: step 5890, loss 0.0447125, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:41.167378: step 5891, loss 0.0930284, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:41.436568: step 5892, loss 0.0588187, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:41.717437: step 5893, loss 0.0272459, acc 1, learning_rate 0.0001
2017-10-11T15:15:42.037368: step 5894, loss 0.0317248, acc 1, learning_rate 0.0001
2017-10-11T15:15:42.399413: step 5895, loss 0.0353871, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:42.712414: step 5896, loss 0.0609763, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:43.012963: step 5897, loss 0.0808472, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:43.333163: step 5898, loss 0.0324826, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:43.605583: step 5899, loss 0.0275394, acc 1, learning_rate 0.0001
2017-10-11T15:15:43.893385: step 5900, loss 0.0438715, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:44.173352: step 5901, loss 0.0420431, acc 1, learning_rate 0.0001
2017-10-11T15:15:44.480698: step 5902, loss 0.0711506, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:44.783394: step 5903, loss 0.046746, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:45.095795: step 5904, loss 0.10631, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:45.445654: step 5905, loss 0.0848241, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:45.761545: step 5906, loss 0.0469975, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:46.086045: step 5907, loss 0.0254468, acc 1, learning_rate 0.0001
2017-10-11T15:15:46.368426: step 5908, loss 0.101637, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:46.664673: step 5909, loss 0.0567443, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:46.943238: step 5910, loss 0.103788, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:47.263122: step 5911, loss 0.046326, acc 1, learning_rate 0.0001
2017-10-11T15:15:47.594669: step 5912, loss 0.133481, acc 0.9375, learning_rate 0.0001
2017-10-11T15:15:47.891552: step 5913, loss 0.0212547, acc 1, learning_rate 0.0001
2017-10-11T15:15:48.184698: step 5914, loss 0.0487423, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:48.489351: step 5915, loss 0.0448759, acc 1, learning_rate 0.0001
2017-10-11T15:15:48.733906: step 5916, loss 0.0518593, acc 1, learning_rate 0.0001
2017-10-11T15:15:49.002168: step 5917, loss 0.0296039, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:49.263977: step 5918, loss 0.0511725, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:49.523033: step 5919, loss 0.0832143, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:49.786161: step 5920, loss 0.0480526, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:15:50.065544: step 5920, loss 0.223733, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5920

2017-10-11T15:15:53.200347: step 5921, loss 0.148044, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:53.504301: step 5922, loss 0.0237623, acc 1, learning_rate 0.0001
2017-10-11T15:15:53.764441: step 5923, loss 0.075526, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:54.007170: step 5924, loss 0.0235654, acc 1, learning_rate 0.0001
2017-10-11T15:15:54.246808: step 5925, loss 0.116668, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:54.472508: step 5926, loss 0.10878, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:54.706031: step 5927, loss 0.070668, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:54.944090: step 5928, loss 0.0779511, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:55.169503: step 5929, loss 0.0664864, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:55.442578: step 5930, loss 0.0557954, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:55.737851: step 5931, loss 0.0447631, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:56.019682: step 5932, loss 0.0908125, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:56.309394: step 5933, loss 0.0536605, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:56.614638: step 5934, loss 0.0179216, acc 1, learning_rate 0.0001
2017-10-11T15:15:56.958012: step 5935, loss 0.0740001, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:57.304984: step 5936, loss 0.0446357, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:57.623526: step 5937, loss 0.123192, acc 0.9375, learning_rate 0.0001
2017-10-11T15:15:57.964100: step 5938, loss 0.0404485, acc 1, learning_rate 0.0001
2017-10-11T15:15:58.268388: step 5939, loss 0.0467191, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:58.585340: step 5940, loss 0.0301106, acc 1, learning_rate 0.0001
2017-10-11T15:15:58.875942: step 5941, loss 0.102774, acc 0.953125, learning_rate 0.0001
2017-10-11T15:15:59.166746: step 5942, loss 0.07568, acc 0.984375, learning_rate 0.0001
2017-10-11T15:15:59.488522: step 5943, loss 0.0655093, acc 0.96875, learning_rate 0.0001
2017-10-11T15:15:59.817584: step 5944, loss 0.0164474, acc 1, learning_rate 0.0001
2017-10-11T15:16:00.113650: step 5945, loss 0.0683501, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:00.429635: step 5946, loss 0.0144493, acc 1, learning_rate 0.0001
2017-10-11T15:16:00.709561: step 5947, loss 0.0727065, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:01.008268: step 5948, loss 0.0907118, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:01.330753: step 5949, loss 0.0401399, acc 1, learning_rate 0.0001
2017-10-11T15:16:01.646244: step 5950, loss 0.115808, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:01.920371: step 5951, loss 0.0461968, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:02.210727: step 5952, loss 0.0187797, acc 1, learning_rate 0.0001
2017-10-11T15:16:02.497481: step 5953, loss 0.0648625, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:02.750789: step 5954, loss 0.0766292, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:03.016557: step 5955, loss 0.0842329, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:03.276572: step 5956, loss 0.0526047, acc 1, learning_rate 0.0001
2017-10-11T15:16:03.574445: step 5957, loss 0.0791005, acc 0.953125, learning_rate 0.0001
2017-10-11T15:16:03.898910: step 5958, loss 0.0796814, acc 0.953125, learning_rate 0.0001
2017-10-11T15:16:04.256157: step 5959, loss 0.0388486, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:04.589501: step 5960, loss 0.0319298, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:16:04.901274: step 5960, loss 0.216069, acc 0.923741

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-5960

2017-10-11T15:16:06.921940: step 5961, loss 0.0714483, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:07.235160: step 5962, loss 0.0609749, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:07.556178: step 5963, loss 0.0940468, acc 0.953125, learning_rate 0.0001
2017-10-11T15:16:07.832167: step 5964, loss 0.0821881, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:08.110120: step 5965, loss 0.0780302, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:08.386624: step 5966, loss 0.0201042, acc 1, learning_rate 0.0001
2017-10-11T15:16:08.690985: step 5967, loss 0.138629, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:08.985757: step 5968, loss 0.0600177, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:09.232497: step 5969, loss 0.0207674, acc 1, learning_rate 0.0001
2017-10-11T15:16:09.464011: step 5970, loss 0.0182798, acc 1, learning_rate 0.0001
2017-10-11T15:16:09.708362: step 5971, loss 0.00741012, acc 1, learning_rate 0.0001
2017-10-11T15:16:09.943372: step 5972, loss 0.0886394, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:10.189034: step 5973, loss 0.0250326, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:10.447997: step 5974, loss 0.041551, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:10.789338: step 5975, loss 0.0309465, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:11.085879: step 5976, loss 0.0629906, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:11.713832: step 5977, loss 0.0426552, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:11.997072: step 5978, loss 0.0306194, acc 1, learning_rate 0.0001
2017-10-11T15:16:12.339924: step 5979, loss 0.0577082, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:12.632405: step 5980, loss 0.0447957, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:12.946107: step 5981, loss 0.108404, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:13.262711: step 5982, loss 0.0166899, acc 1, learning_rate 0.0001
2017-10-11T15:16:13.564749: step 5983, loss 0.0213603, acc 1, learning_rate 0.0001
2017-10-11T15:16:13.874951: step 5984, loss 0.0593426, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:14.211437: step 5985, loss 0.0469484, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:14.553085: step 5986, loss 0.0803014, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:14.863427: step 5987, loss 0.103574, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:15.160938: step 5988, loss 0.0304526, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:15.442296: step 5989, loss 0.0829242, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:15.736199: step 5990, loss 0.0518164, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:16.050491: step 5991, loss 0.0429159, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:16.270366: step 5992, loss 0.037885, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:16.561282: step 5993, loss 0.0359465, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:16.828925: step 5994, loss 0.0474706, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:17.094976: step 5995, loss 0.146401, acc 0.953125, learning_rate 0.0001
2017-10-11T15:16:17.349219: step 5996, loss 0.0639051, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:17.649067: step 5997, loss 0.0696661, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:17.902267: step 5998, loss 0.011318, acc 1, learning_rate 0.0001
2017-10-11T15:16:18.177836: step 5999, loss 0.122665, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:18.528926: step 6000, loss 0.033718, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:16:18.842321: step 6000, loss 0.22003, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6000

2017-10-11T15:16:22.462524: step 6001, loss 0.0478329, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:22.766373: step 6002, loss 0.0898761, acc 0.953125, learning_rate 0.0001
2017-10-11T15:16:22.976146: step 6003, loss 0.0611158, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:23.197600: step 6004, loss 0.0762397, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:23.430456: step 6005, loss 0.0691466, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:23.646472: step 6006, loss 0.0346531, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:23.881351: step 6007, loss 0.0373659, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:24.133437: step 6008, loss 0.0550131, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:24.359551: step 6009, loss 0.0869563, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:24.660000: step 6010, loss 0.0868311, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:24.917517: step 6011, loss 0.0514789, acc 1, learning_rate 0.0001
2017-10-11T15:16:25.167270: step 6012, loss 0.104091, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:25.490040: step 6013, loss 0.0963845, acc 0.953125, learning_rate 0.0001
2017-10-11T15:16:25.776727: step 6014, loss 0.0308058, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:26.094716: step 6015, loss 0.040408, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:26.382864: step 6016, loss 0.0666228, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:26.681873: step 6017, loss 0.124919, acc 0.953125, learning_rate 0.0001
2017-10-11T15:16:27.023359: step 6018, loss 0.0583289, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:27.316817: step 6019, loss 0.0927628, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:27.616988: step 6020, loss 0.0506884, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:27.923423: step 6021, loss 0.0585291, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:28.262059: step 6022, loss 0.115269, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:28.599879: step 6023, loss 0.0430077, acc 1, learning_rate 0.0001
2017-10-11T15:16:28.895243: step 6024, loss 0.0303809, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:29.190013: step 6025, loss 0.0443233, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:29.498483: step 6026, loss 0.0575138, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:29.827290: step 6027, loss 0.0768666, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:30.104667: step 6028, loss 0.0797025, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:30.387878: step 6029, loss 0.015758, acc 1, learning_rate 0.0001
2017-10-11T15:16:30.684075: step 6030, loss 0.0655584, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:30.942729: step 6031, loss 0.0833266, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:31.230336: step 6032, loss 0.137834, acc 0.9375, learning_rate 0.0001
2017-10-11T15:16:31.524325: step 6033, loss 0.0458188, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:31.837428: step 6034, loss 0.0460904, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:32.128341: step 6035, loss 0.0277275, acc 1, learning_rate 0.0001
2017-10-11T15:16:32.441180: step 6036, loss 0.0586764, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:32.744435: step 6037, loss 0.039859, acc 1, learning_rate 0.0001
2017-10-11T15:16:33.038410: step 6038, loss 0.186808, acc 0.9375, learning_rate 0.0001
2017-10-11T15:16:33.353000: step 6039, loss 0.0785758, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:33.643625: step 6040, loss 0.0408119, acc 1, learning_rate 0.0001

Evaluation:
2017-10-11T15:16:33.936562: step 6040, loss 0.215157, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6040

2017-10-11T15:16:35.947495: step 6041, loss 0.0720128, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:36.257926: step 6042, loss 0.024334, acc 1, learning_rate 0.0001
2017-10-11T15:16:36.593297: step 6043, loss 0.0477794, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:36.917169: step 6044, loss 0.0488984, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:37.170740: step 6045, loss 0.0558949, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:37.433893: step 6046, loss 0.0167677, acc 1, learning_rate 0.0001
2017-10-11T15:16:37.657322: step 6047, loss 0.056978, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:37.887369: step 6048, loss 0.143092, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:38.123411: step 6049, loss 0.0892653, acc 0.953125, learning_rate 0.0001
2017-10-11T15:16:38.372222: step 6050, loss 0.101249, acc 0.953125, learning_rate 0.0001
2017-10-11T15:16:38.655504: step 6051, loss 0.0276912, acc 1, learning_rate 0.0001
2017-10-11T15:16:38.927070: step 6052, loss 0.0428708, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:39.283952: step 6053, loss 0.0655354, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:39.640758: step 6054, loss 0.0828251, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:39.962533: step 6055, loss 0.0387332, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:40.294105: step 6056, loss 0.0759283, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:40.583881: step 6057, loss 0.0158867, acc 1, learning_rate 0.0001
2017-10-11T15:16:40.886984: step 6058, loss 0.0717406, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:41.201518: step 6059, loss 0.0787174, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:41.515748: step 6060, loss 0.0897128, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:41.802079: step 6061, loss 0.0527844, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:42.093597: step 6062, loss 0.072935, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:42.381813: step 6063, loss 0.102179, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:42.679068: step 6064, loss 0.0648499, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:42.979606: step 6065, loss 0.049723, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:43.254607: step 6066, loss 0.0598636, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:43.557587: step 6067, loss 0.0834755, acc 0.953125, learning_rate 0.0001
2017-10-11T15:16:43.857293: step 6068, loss 0.0576106, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:44.149986: step 6069, loss 0.032739, acc 1, learning_rate 0.0001
2017-10-11T15:16:44.395210: step 6070, loss 0.1237, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:44.663303: step 6071, loss 0.0870549, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:44.941236: step 6072, loss 0.0977852, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:45.247605: step 6073, loss 0.148489, acc 0.953125, learning_rate 0.0001
2017-10-11T15:16:45.603476: step 6074, loss 0.0274639, acc 1, learning_rate 0.0001
2017-10-11T15:16:45.932330: step 6075, loss 0.0541319, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:46.208145: step 6076, loss 0.0394108, acc 1, learning_rate 0.0001
2017-10-11T15:16:46.520983: step 6077, loss 0.1078, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:46.845216: step 6078, loss 0.029261, acc 1, learning_rate 0.0001
2017-10-11T15:16:47.156783: step 6079, loss 0.0363688, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:47.468426: step 6080, loss 0.00856879, acc 1, learning_rate 0.0001

Evaluation:
2017-10-11T15:16:47.775248: step 6080, loss 0.218403, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6080

2017-10-11T15:16:50.293331: step 6081, loss 0.0435374, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:50.610381: step 6082, loss 0.0639749, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:50.920038: step 6083, loss 0.0325691, acc 1, learning_rate 0.0001
2017-10-11T15:16:51.185657: step 6084, loss 0.0400631, acc 1, learning_rate 0.0001
2017-10-11T15:16:51.440982: step 6085, loss 0.0510802, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:51.697186: step 6086, loss 0.0240958, acc 1, learning_rate 0.0001
2017-10-11T15:16:51.920285: step 6087, loss 0.0750313, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:52.174713: step 6088, loss 0.0512442, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:52.406879: step 6089, loss 0.0125648, acc 1, learning_rate 0.0001
2017-10-11T15:16:52.629692: step 6090, loss 0.0332167, acc 1, learning_rate 0.0001
2017-10-11T15:16:52.839516: step 6091, loss 0.02543, acc 1, learning_rate 0.0001
2017-10-11T15:16:53.078149: step 6092, loss 0.102849, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:53.427371: step 6093, loss 0.0951843, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:53.726607: step 6094, loss 0.0332276, acc 1, learning_rate 0.0001
2017-10-11T15:16:54.037489: step 6095, loss 0.0572004, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:54.323762: step 6096, loss 0.0541599, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:54.632127: step 6097, loss 0.0332078, acc 1, learning_rate 0.0001
2017-10-11T15:16:54.945103: step 6098, loss 0.0913461, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:55.235271: step 6099, loss 0.0948169, acc 0.9375, learning_rate 0.0001
2017-10-11T15:16:55.518077: step 6100, loss 0.0161906, acc 1, learning_rate 0.0001
2017-10-11T15:16:55.835156: step 6101, loss 0.0352469, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:56.152232: step 6102, loss 0.0659767, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:56.464404: step 6103, loss 0.0724722, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:56.756657: step 6104, loss 0.0352363, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:57.045949: step 6105, loss 0.0341937, acc 1, learning_rate 0.0001
2017-10-11T15:16:57.353899: step 6106, loss 0.0255574, acc 1, learning_rate 0.0001
2017-10-11T15:16:57.650692: step 6107, loss 0.056241, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:57.912845: step 6108, loss 0.0602792, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:58.148703: step 6109, loss 0.0507351, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:58.415166: step 6110, loss 0.0400664, acc 1, learning_rate 0.0001
2017-10-11T15:16:58.668920: step 6111, loss 0.0408126, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:58.924402: step 6112, loss 0.0817621, acc 0.96875, learning_rate 0.0001
2017-10-11T15:16:59.220810: step 6113, loss 0.056993, acc 0.953125, learning_rate 0.0001
2017-10-11T15:16:59.581428: step 6114, loss 0.0470018, acc 0.984375, learning_rate 0.0001
2017-10-11T15:16:59.876975: step 6115, loss 0.0960588, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:00.172575: step 6116, loss 0.0624902, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:00.495610: step 6117, loss 0.0666676, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:00.797873: step 6118, loss 0.0527786, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:01.106494: step 6119, loss 0.0674739, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:01.438690: step 6120, loss 0.0452329, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:17:01.730594: step 6120, loss 0.217181, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6120

2017-10-11T15:17:05.008235: step 6121, loss 0.0382101, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:05.252374: step 6122, loss 0.0394471, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:05.496438: step 6123, loss 0.0220378, acc 1, learning_rate 0.0001
2017-10-11T15:17:05.790049: step 6124, loss 0.0583685, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:06.013509: step 6125, loss 0.130742, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:06.219022: step 6126, loss 0.082655, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:06.434550: step 6127, loss 0.0402266, acc 1, learning_rate 0.0001
2017-10-11T15:17:06.752039: step 6128, loss 0.0349677, acc 1, learning_rate 0.0001
2017-10-11T15:17:07.060432: step 6129, loss 0.0734521, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:07.344923: step 6130, loss 0.0517353, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:07.645905: step 6131, loss 0.0641646, acc 1, learning_rate 0.0001
2017-10-11T15:17:07.999317: step 6132, loss 0.100783, acc 0.9375, learning_rate 0.0001
2017-10-11T15:17:08.361298: step 6133, loss 0.128723, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:08.735426: step 6134, loss 0.0635117, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:09.045295: step 6135, loss 0.0355526, acc 1, learning_rate 0.0001
2017-10-11T15:17:09.358238: step 6136, loss 0.0566328, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:09.696068: step 6137, loss 0.0965067, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:10.012458: step 6138, loss 0.019737, acc 1, learning_rate 0.0001
2017-10-11T15:17:10.356532: step 6139, loss 0.0522819, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:10.653582: step 6140, loss 0.0540718, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:10.977270: step 6141, loss 0.0647387, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:11.246649: step 6142, loss 0.174007, acc 0.9375, learning_rate 0.0001
2017-10-11T15:17:11.521650: step 6143, loss 0.039602, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:11.783972: step 6144, loss 0.0377736, acc 1, learning_rate 0.0001
2017-10-11T15:17:12.083437: step 6145, loss 0.108886, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:12.365600: step 6146, loss 0.0996026, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:12.676800: step 6147, loss 0.0767128, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:13.005709: step 6148, loss 0.0365256, acc 1, learning_rate 0.0001
2017-10-11T15:17:13.321797: step 6149, loss 0.0707084, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:13.613867: step 6150, loss 0.0206925, acc 1, learning_rate 0.0001
2017-10-11T15:17:13.936049: step 6151, loss 0.0259394, acc 1, learning_rate 0.0001
2017-10-11T15:17:14.263711: step 6152, loss 0.0433864, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:14.618304: step 6153, loss 0.036402, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:14.959380: step 6154, loss 0.0591525, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:15.306898: step 6155, loss 0.13118, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:15.634520: step 6156, loss 0.0753817, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:15.959782: step 6157, loss 0.0259248, acc 1, learning_rate 0.0001
2017-10-11T15:17:16.266344: step 6158, loss 0.0338923, acc 1, learning_rate 0.0001
2017-10-11T15:17:16.584265: step 6159, loss 0.0206716, acc 1, learning_rate 0.0001
2017-10-11T15:17:16.892977: step 6160, loss 0.0462935, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:17:17.217957: step 6160, loss 0.216326, acc 0.920863

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6160

2017-10-11T15:17:19.073822: step 6161, loss 0.0517237, acc 1, learning_rate 0.0001
2017-10-11T15:17:19.367954: step 6162, loss 0.047001, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:19.647254: step 6163, loss 0.0149393, acc 1, learning_rate 0.0001
2017-10-11T15:17:19.925614: step 6164, loss 0.0369498, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:20.202672: step 6165, loss 0.146228, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:20.416416: step 6166, loss 0.104018, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:20.629249: step 6167, loss 0.0597471, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:20.823931: step 6168, loss 0.0214604, acc 1, learning_rate 0.0001
2017-10-11T15:17:21.052939: step 6169, loss 0.0453295, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:21.281457: step 6170, loss 0.0785016, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:21.618904: step 6171, loss 0.133203, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:21.983727: step 6172, loss 0.0584439, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:22.316939: step 6173, loss 0.0324041, acc 1, learning_rate 0.0001
2017-10-11T15:17:22.615126: step 6174, loss 0.0696073, acc 0.980392, learning_rate 0.0001
2017-10-11T15:17:22.952757: step 6175, loss 0.0626322, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:23.329209: step 6176, loss 0.0375862, acc 1, learning_rate 0.0001
2017-10-11T15:17:23.628282: step 6177, loss 0.00811746, acc 1, learning_rate 0.0001
2017-10-11T15:17:23.920637: step 6178, loss 0.0549699, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:24.237018: step 6179, loss 0.0248675, acc 1, learning_rate 0.0001
2017-10-11T15:17:24.579752: step 6180, loss 0.115954, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:24.869240: step 6181, loss 0.0701705, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:25.105403: step 6182, loss 0.0475884, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:25.362721: step 6183, loss 0.0995494, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:25.636221: step 6184, loss 0.0715624, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:25.902780: step 6185, loss 0.0944287, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:26.162692: step 6186, loss 0.0214529, acc 1, learning_rate 0.0001
2017-10-11T15:17:26.442849: step 6187, loss 0.0615836, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:26.730521: step 6188, loss 0.0811466, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:27.053410: step 6189, loss 0.0497181, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:27.370123: step 6190, loss 0.052017, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:27.714025: step 6191, loss 0.0279457, acc 1, learning_rate 0.0001
2017-10-11T15:17:28.045741: step 6192, loss 0.0357765, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:28.326619: step 6193, loss 0.0453629, acc 1, learning_rate 0.0001
2017-10-11T15:17:28.620730: step 6194, loss 0.0398335, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:28.946336: step 6195, loss 0.0474812, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:29.281908: step 6196, loss 0.0808303, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:29.578726: step 6197, loss 0.0574939, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:29.891310: step 6198, loss 0.117682, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:30.193839: step 6199, loss 0.018597, acc 1, learning_rate 0.0001
2017-10-11T15:17:30.548227: step 6200, loss 0.0387172, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:17:30.811605: step 6200, loss 0.216212, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6200

2017-10-11T15:17:33.231482: step 6201, loss 0.148612, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:33.542981: step 6202, loss 0.0117443, acc 1, learning_rate 0.0001
2017-10-11T15:17:33.855159: step 6203, loss 0.0296794, acc 1, learning_rate 0.0001
2017-10-11T15:17:34.081834: step 6204, loss 0.0354452, acc 1, learning_rate 0.0001
2017-10-11T15:17:34.320115: step 6205, loss 0.0370061, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:34.554470: step 6206, loss 0.0561094, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:34.819620: step 6207, loss 0.133083, acc 0.9375, learning_rate 0.0001
2017-10-11T15:17:35.030145: step 6208, loss 0.0635667, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:35.265212: step 6209, loss 0.0584633, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:35.500447: step 6210, loss 0.0330371, acc 1, learning_rate 0.0001
2017-10-11T15:17:35.730468: step 6211, loss 0.0389318, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:35.931931: step 6212, loss 0.0260316, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:36.172739: step 6213, loss 0.0511204, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:36.500535: step 6214, loss 0.0585929, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:36.801342: step 6215, loss 0.0346463, acc 1, learning_rate 0.0001
2017-10-11T15:17:37.096767: step 6216, loss 0.0665287, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:37.408176: step 6217, loss 0.104426, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:37.707787: step 6218, loss 0.0253701, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:38.024879: step 6219, loss 0.0234522, acc 1, learning_rate 0.0001
2017-10-11T15:17:38.334503: step 6220, loss 0.0952252, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:38.662331: step 6221, loss 0.0605773, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:38.946474: step 6222, loss 0.0333857, acc 1, learning_rate 0.0001
2017-10-11T15:17:39.227693: step 6223, loss 0.0711742, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:39.495493: step 6224, loss 0.036748, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:39.774096: step 6225, loss 0.0314406, acc 1, learning_rate 0.0001
2017-10-11T15:17:40.069863: step 6226, loss 0.0234226, acc 1, learning_rate 0.0001
2017-10-11T15:17:40.376471: step 6227, loss 0.0606457, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:40.707748: step 6228, loss 0.0883158, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:41.022279: step 6229, loss 0.0632988, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:41.312549: step 6230, loss 0.0446766, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:41.615893: step 6231, loss 0.0571723, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:41.894857: step 6232, loss 0.0339361, acc 1, learning_rate 0.0001
2017-10-11T15:17:42.199674: step 6233, loss 0.0654388, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:42.519268: step 6234, loss 0.0671324, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:42.804308: step 6235, loss 0.0333947, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:43.113419: step 6236, loss 0.0426778, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:43.438392: step 6237, loss 0.0309666, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:43.730478: step 6238, loss 0.0532241, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:44.017173: step 6239, loss 0.0203127, acc 1, learning_rate 0.0001
2017-10-11T15:17:44.332792: step 6240, loss 0.0646752, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:17:44.619759: step 6240, loss 0.217024, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6240

2017-10-11T15:17:46.971867: step 6241, loss 0.00632663, acc 1, learning_rate 0.0001
2017-10-11T15:17:47.280336: step 6242, loss 0.0435736, acc 1, learning_rate 0.0001
2017-10-11T15:17:47.563740: step 6243, loss 0.0366353, acc 1, learning_rate 0.0001
2017-10-11T15:17:47.860313: step 6244, loss 0.0525075, acc 1, learning_rate 0.0001
2017-10-11T15:17:48.181245: step 6245, loss 0.0573139, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:48.488275: step 6246, loss 0.019272, acc 1, learning_rate 0.0001
2017-10-11T15:17:48.703829: step 6247, loss 0.0767333, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:48.950482: step 6248, loss 0.123692, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:49.183510: step 6249, loss 0.0339601, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:49.412665: step 6250, loss 0.0811, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:49.660373: step 6251, loss 0.0211446, acc 1, learning_rate 0.0001
2017-10-11T15:17:49.868427: step 6252, loss 0.0882581, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:50.138283: step 6253, loss 0.0303976, acc 1, learning_rate 0.0001
2017-10-11T15:17:50.396470: step 6254, loss 0.113631, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:50.677347: step 6255, loss 0.0721752, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:51.002997: step 6256, loss 0.0516334, acc 1, learning_rate 0.0001
2017-10-11T15:17:51.353292: step 6257, loss 0.0230843, acc 1, learning_rate 0.0001
2017-10-11T15:17:51.682714: step 6258, loss 0.042731, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:52.008576: step 6259, loss 0.0207047, acc 1, learning_rate 0.0001
2017-10-11T15:17:52.292458: step 6260, loss 0.0181751, acc 1, learning_rate 0.0001
2017-10-11T15:17:52.551904: step 6261, loss 0.0266579, acc 1, learning_rate 0.0001
2017-10-11T15:17:52.822532: step 6262, loss 0.0925251, acc 0.9375, learning_rate 0.0001
2017-10-11T15:17:53.096183: step 6263, loss 0.0583313, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:53.492686: step 6264, loss 0.0757958, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:53.789500: step 6265, loss 0.0309848, acc 1, learning_rate 0.0001
2017-10-11T15:17:54.066391: step 6266, loss 0.0889003, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:54.430638: step 6267, loss 0.0562055, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:54.740028: step 6268, loss 0.0500805, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:55.061736: step 6269, loss 0.0723638, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:55.364539: step 6270, loss 0.0623841, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:55.682454: step 6271, loss 0.0644488, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:55.949885: step 6272, loss 0.0455874, acc 0.980392, learning_rate 0.0001
2017-10-11T15:17:56.275595: step 6273, loss 0.0765159, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:56.592514: step 6274, loss 0.123199, acc 0.953125, learning_rate 0.0001
2017-10-11T15:17:56.908976: step 6275, loss 0.0703158, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:57.235161: step 6276, loss 0.0894656, acc 0.984375, learning_rate 0.0001
2017-10-11T15:17:57.533126: step 6277, loss 0.0357685, acc 1, learning_rate 0.0001
2017-10-11T15:17:57.838643: step 6278, loss 0.0798173, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:58.129672: step 6279, loss 0.0530856, acc 0.96875, learning_rate 0.0001
2017-10-11T15:17:58.435995: step 6280, loss 0.146445, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T15:17:58.724294: step 6280, loss 0.217646, acc 0.920863

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6280

2017-10-11T15:18:00.669920: step 6281, loss 0.0701902, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:00.956665: step 6282, loss 0.0286721, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:01.247842: step 6283, loss 0.0369839, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:01.532825: step 6284, loss 0.0350914, acc 1, learning_rate 0.0001
2017-10-11T15:18:01.870344: step 6285, loss 0.150082, acc 0.9375, learning_rate 0.0001
2017-10-11T15:18:02.150663: step 6286, loss 0.0540358, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:02.410217: step 6287, loss 0.0657071, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:02.682100: step 6288, loss 0.0986878, acc 0.953125, learning_rate 0.0001
2017-10-11T15:18:02.984919: step 6289, loss 0.153054, acc 0.9375, learning_rate 0.0001
2017-10-11T15:18:03.209940: step 6290, loss 0.0864827, acc 0.953125, learning_rate 0.0001
2017-10-11T15:18:03.452756: step 6291, loss 0.0272107, acc 1, learning_rate 0.0001
2017-10-11T15:18:03.689372: step 6292, loss 0.0665601, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:03.910936: step 6293, loss 0.0180519, acc 1, learning_rate 0.0001
2017-10-11T15:18:04.158394: step 6294, loss 0.157106, acc 0.953125, learning_rate 0.0001
2017-10-11T15:18:04.370580: step 6295, loss 0.0338616, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:04.598492: step 6296, loss 0.0249142, acc 1, learning_rate 0.0001
2017-10-11T15:18:04.853160: step 6297, loss 0.0988209, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:05.161782: step 6298, loss 0.0525656, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:05.516462: step 6299, loss 0.121468, acc 0.953125, learning_rate 0.0001
2017-10-11T15:18:05.864058: step 6300, loss 0.0457903, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:06.139234: step 6301, loss 0.0946706, acc 0.953125, learning_rate 0.0001
2017-10-11T15:18:06.433326: step 6302, loss 0.0328416, acc 1, learning_rate 0.0001
2017-10-11T15:18:06.733517: step 6303, loss 0.0171796, acc 1, learning_rate 0.0001
2017-10-11T15:18:07.029880: step 6304, loss 0.0178266, acc 1, learning_rate 0.0001
2017-10-11T15:18:07.299687: step 6305, loss 0.0424183, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:07.570232: step 6306, loss 0.087029, acc 0.953125, learning_rate 0.0001
2017-10-11T15:18:07.827202: step 6307, loss 0.030558, acc 1, learning_rate 0.0001
2017-10-11T15:18:08.097125: step 6308, loss 0.118801, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:08.386043: step 6309, loss 0.0916446, acc 0.9375, learning_rate 0.0001
2017-10-11T15:18:08.731611: step 6310, loss 0.0220798, acc 1, learning_rate 0.0001
2017-10-11T15:18:09.017460: step 6311, loss 0.104173, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:09.328181: step 6312, loss 0.0474807, acc 1, learning_rate 0.0001
2017-10-11T15:18:09.655841: step 6313, loss 0.00731481, acc 1, learning_rate 0.0001
2017-10-11T15:18:09.918062: step 6314, loss 0.0375393, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:10.220347: step 6315, loss 0.131879, acc 0.953125, learning_rate 0.0001
2017-10-11T15:18:10.496232: step 6316, loss 0.0606805, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:10.785575: step 6317, loss 0.0432759, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:11.074715: step 6318, loss 0.0532934, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:11.384007: step 6319, loss 0.0345377, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:11.680020: step 6320, loss 0.0259891, acc 1, learning_rate 0.0001

Evaluation:
2017-10-11T15:18:11.989037: step 6320, loss 0.215067, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6320

2017-10-11T15:18:15.287165: step 6321, loss 0.0451302, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:15.605104: step 6322, loss 0.0325208, acc 1, learning_rate 0.0001
2017-10-11T15:18:15.908695: step 6323, loss 0.0498572, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:16.236321: step 6324, loss 0.0533891, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:16.578225: step 6325, loss 0.0169372, acc 1, learning_rate 0.0001
2017-10-11T15:18:16.826411: step 6326, loss 0.0594526, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:17.067645: step 6327, loss 0.0970769, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:17.294217: step 6328, loss 0.0657356, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:17.516674: step 6329, loss 0.0370591, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:17.749893: step 6330, loss 0.0278224, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:17.985406: step 6331, loss 0.14188, acc 0.9375, learning_rate 0.0001
2017-10-11T15:18:18.195647: step 6332, loss 0.0142992, acc 1, learning_rate 0.0001
2017-10-11T15:18:18.398414: step 6333, loss 0.0600962, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:18.694916: step 6334, loss 0.109033, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:19.037424: step 6335, loss 0.0706119, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:19.348844: step 6336, loss 0.0261956, acc 1, learning_rate 0.0001
2017-10-11T15:18:19.631276: step 6337, loss 0.0557729, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:19.910308: step 6338, loss 0.0379564, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:20.193526: step 6339, loss 0.0300914, acc 1, learning_rate 0.0001
2017-10-11T15:18:20.484932: step 6340, loss 0.0473176, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:20.749938: step 6341, loss 0.0360378, acc 1, learning_rate 0.0001
2017-10-11T15:18:21.036738: step 6342, loss 0.0468599, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:21.371538: step 6343, loss 0.0557844, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:21.702218: step 6344, loss 0.0315338, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:22.029405: step 6345, loss 0.0272012, acc 1, learning_rate 0.0001
2017-10-11T15:18:22.357210: step 6346, loss 0.0334089, acc 1, learning_rate 0.0001
2017-10-11T15:18:22.698691: step 6347, loss 0.0336159, acc 1, learning_rate 0.0001
2017-10-11T15:18:23.012884: step 6348, loss 0.0561977, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:23.325683: step 6349, loss 0.069994, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:23.645395: step 6350, loss 0.0696304, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:23.978957: step 6351, loss 0.075853, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:24.312723: step 6352, loss 0.0734813, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:24.652591: step 6353, loss 0.106574, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:24.945056: step 6354, loss 0.0483767, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:25.235807: step 6355, loss 0.0645129, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:25.584679: step 6356, loss 0.163545, acc 0.9375, learning_rate 0.0001
2017-10-11T15:18:25.916488: step 6357, loss 0.0294716, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:26.217284: step 6358, loss 0.068433, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:26.568567: step 6359, loss 0.0497329, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:26.909423: step 6360, loss 0.091371, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:18:27.254164: step 6360, loss 0.218487, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6360

2017-10-11T15:18:29.575814: step 6361, loss 0.191709, acc 0.90625, learning_rate 0.0001
2017-10-11T15:18:29.894363: step 6362, loss 0.053242, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:30.248908: step 6363, loss 0.0543919, acc 1, learning_rate 0.0001
2017-10-11T15:18:30.555749: step 6364, loss 0.081403, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:30.867183: step 6365, loss 0.0665489, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:31.160139: step 6366, loss 0.017549, acc 1, learning_rate 0.0001
2017-10-11T15:18:31.434280: step 6367, loss 0.0555287, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:31.677977: step 6368, loss 0.0191927, acc 1, learning_rate 0.0001
2017-10-11T15:18:31.923794: step 6369, loss 0.0313752, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:32.133222: step 6370, loss 0.00739801, acc 1, learning_rate 0.0001
2017-10-11T15:18:32.354515: step 6371, loss 0.067663, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:32.584983: step 6372, loss 0.0445971, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:32.846496: step 6373, loss 0.0464785, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:33.086392: step 6374, loss 0.0478333, acc 1, learning_rate 0.0001
2017-10-11T15:18:33.360808: step 6375, loss 0.120288, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:33.578573: step 6376, loss 0.0625479, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:33.869492: step 6377, loss 0.0599539, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:34.157790: step 6378, loss 0.0139985, acc 1, learning_rate 0.0001
2017-10-11T15:18:34.441597: step 6379, loss 0.0459808, acc 1, learning_rate 0.0001
2017-10-11T15:18:34.698564: step 6380, loss 0.0492697, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:34.984693: step 6381, loss 0.0642141, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:35.264158: step 6382, loss 0.0199493, acc 1, learning_rate 0.0001
2017-10-11T15:18:35.569190: step 6383, loss 0.0527074, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:35.910423: step 6384, loss 0.0300796, acc 1, learning_rate 0.0001
2017-10-11T15:18:36.247944: step 6385, loss 0.113439, acc 0.9375, learning_rate 0.0001
2017-10-11T15:18:36.541549: step 6386, loss 0.0171879, acc 1, learning_rate 0.0001
2017-10-11T15:18:36.876822: step 6387, loss 0.0481282, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:37.186432: step 6388, loss 0.0591687, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:37.500350: step 6389, loss 0.0533436, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:37.830913: step 6390, loss 0.0310937, acc 1, learning_rate 0.0001
2017-10-11T15:18:38.133197: step 6391, loss 0.0211149, acc 1, learning_rate 0.0001
2017-10-11T15:18:38.451874: step 6392, loss 0.0968654, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:38.737005: step 6393, loss 0.0104935, acc 1, learning_rate 0.0001
2017-10-11T15:18:39.028349: step 6394, loss 0.0806384, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:39.324250: step 6395, loss 0.033102, acc 1, learning_rate 0.0001
2017-10-11T15:18:39.639601: step 6396, loss 0.128585, acc 0.953125, learning_rate 0.0001
2017-10-11T15:18:39.964220: step 6397, loss 0.0732828, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:40.281128: step 6398, loss 0.04898, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:40.570132: step 6399, loss 0.0340416, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:40.896542: step 6400, loss 0.087266, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:18:41.192114: step 6400, loss 0.219078, acc 0.920863

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6400

2017-10-11T15:18:44.078254: step 6401, loss 0.116588, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:44.392385: step 6402, loss 0.097612, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:44.736540: step 6403, loss 0.146133, acc 0.9375, learning_rate 0.0001
2017-10-11T15:18:45.050436: step 6404, loss 0.0757779, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:45.395754: step 6405, loss 0.0725228, acc 0.953125, learning_rate 0.0001
2017-10-11T15:18:45.657797: step 6406, loss 0.0375072, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:45.920512: step 6407, loss 0.141767, acc 0.953125, learning_rate 0.0001
2017-10-11T15:18:46.148881: step 6408, loss 0.0208567, acc 1, learning_rate 0.0001
2017-10-11T15:18:46.381282: step 6409, loss 0.0264893, acc 1, learning_rate 0.0001
2017-10-11T15:18:46.615614: step 6410, loss 0.149489, acc 0.921875, learning_rate 0.0001
2017-10-11T15:18:46.820135: step 6411, loss 0.0850275, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:47.023890: step 6412, loss 0.0191719, acc 1, learning_rate 0.0001
2017-10-11T15:18:47.286214: step 6413, loss 0.0209541, acc 1, learning_rate 0.0001
2017-10-11T15:18:47.515037: step 6414, loss 0.0141026, acc 1, learning_rate 0.0001
2017-10-11T15:18:47.761118: step 6415, loss 0.0848308, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:48.039799: step 6416, loss 0.0888188, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:48.258513: step 6417, loss 0.0447262, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:48.460473: step 6418, loss 0.0657166, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:48.684230: step 6419, loss 0.0706193, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:48.940314: step 6420, loss 0.0930331, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:49.172459: step 6421, loss 0.0438421, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:49.461834: step 6422, loss 0.0516834, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:49.744180: step 6423, loss 0.0802667, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:50.044376: step 6424, loss 0.0149249, acc 1, learning_rate 0.0001
2017-10-11T15:18:50.326454: step 6425, loss 0.104287, acc 0.953125, learning_rate 0.0001
2017-10-11T15:18:50.606221: step 6426, loss 0.124053, acc 0.953125, learning_rate 0.0001
2017-10-11T15:18:50.879361: step 6427, loss 0.0224533, acc 1, learning_rate 0.0001
2017-10-11T15:18:51.190427: step 6428, loss 0.0346018, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:51.517853: step 6429, loss 0.0304174, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:51.834341: step 6430, loss 0.0546633, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:52.161534: step 6431, loss 0.053658, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:52.425623: step 6432, loss 0.0209061, acc 1, learning_rate 0.0001
2017-10-11T15:18:52.703826: step 6433, loss 0.0384655, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:53.016569: step 6434, loss 0.0532675, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:53.342526: step 6435, loss 0.176965, acc 0.9375, learning_rate 0.0001
2017-10-11T15:18:53.677622: step 6436, loss 0.082783, acc 0.953125, learning_rate 0.0001
2017-10-11T15:18:54.012427: step 6437, loss 0.0470113, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:54.313812: step 6438, loss 0.0258746, acc 1, learning_rate 0.0001
2017-10-11T15:18:54.649461: step 6439, loss 0.0366152, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:54.936870: step 6440, loss 0.103003, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:18:55.222102: step 6440, loss 0.215998, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6440

2017-10-11T15:18:58.009062: step 6441, loss 0.0111087, acc 1, learning_rate 0.0001
2017-10-11T15:18:58.316177: step 6442, loss 0.0276736, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:58.677137: step 6443, loss 0.0548608, acc 0.984375, learning_rate 0.0001
2017-10-11T15:18:59.021935: step 6444, loss 0.0779319, acc 0.953125, learning_rate 0.0001
2017-10-11T15:18:59.301620: step 6445, loss 0.100289, acc 0.96875, learning_rate 0.0001
2017-10-11T15:18:59.555397: step 6446, loss 0.0200697, acc 1, learning_rate 0.0001
2017-10-11T15:18:59.840769: step 6447, loss 0.0146953, acc 1, learning_rate 0.0001
2017-10-11T15:19:00.120971: step 6448, loss 0.0284551, acc 1, learning_rate 0.0001
2017-10-11T15:19:00.360650: step 6449, loss 0.0896255, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:00.626577: step 6450, loss 0.0644464, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:00.909584: step 6451, loss 0.0818863, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:01.203213: step 6452, loss 0.0315044, acc 1, learning_rate 0.0001
2017-10-11T15:19:01.427507: step 6453, loss 0.0163838, acc 1, learning_rate 0.0001
2017-10-11T15:19:01.666756: step 6454, loss 0.040529, acc 1, learning_rate 0.0001
2017-10-11T15:19:01.905352: step 6455, loss 0.0702769, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:02.220159: step 6456, loss 0.0368181, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:02.494801: step 6457, loss 0.0856352, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:02.768101: step 6458, loss 0.063653, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:03.064057: step 6459, loss 0.0735296, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:03.372897: step 6460, loss 0.0823835, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:03.655264: step 6461, loss 0.133101, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:03.937027: step 6462, loss 0.084942, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:04.194270: step 6463, loss 0.0822407, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:04.552299: step 6464, loss 0.0466691, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:04.853103: step 6465, loss 0.0547278, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:05.133639: step 6466, loss 0.0200065, acc 1, learning_rate 0.0001
2017-10-11T15:19:05.435359: step 6467, loss 0.0691664, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:05.696042: step 6468, loss 0.0221521, acc 1, learning_rate 0.0001
2017-10-11T15:19:06.025794: step 6469, loss 0.0261913, acc 1, learning_rate 0.0001
2017-10-11T15:19:06.321238: step 6470, loss 0.144268, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:06.639204: step 6471, loss 0.0329079, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:06.965901: step 6472, loss 0.0630796, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:07.299784: step 6473, loss 0.0460784, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:07.592185: step 6474, loss 0.0312331, acc 1, learning_rate 0.0001
2017-10-11T15:19:07.898371: step 6475, loss 0.0991991, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:08.219433: step 6476, loss 0.035767, acc 1, learning_rate 0.0001
2017-10-11T15:19:08.531347: step 6477, loss 0.08714, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:08.833370: step 6478, loss 0.0939076, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:09.159322: step 6479, loss 0.13465, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:09.472895: step 6480, loss 0.122646, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T15:19:09.794911: step 6480, loss 0.221538, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6480

2017-10-11T15:19:12.058440: step 6481, loss 0.0988262, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:12.374712: step 6482, loss 0.0630426, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:12.702361: step 6483, loss 0.0862318, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:13.002525: step 6484, loss 0.0956795, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:13.320085: step 6485, loss 0.0625742, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:13.623226: step 6486, loss 0.0557983, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:13.962151: step 6487, loss 0.0209452, acc 1, learning_rate 0.0001
2017-10-11T15:19:14.298530: step 6488, loss 0.126583, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:14.575991: step 6489, loss 0.0356003, acc 1, learning_rate 0.0001
2017-10-11T15:19:14.834229: step 6490, loss 0.0560042, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:15.121785: step 6491, loss 0.0428458, acc 1, learning_rate 0.0001
2017-10-11T15:19:15.330571: step 6492, loss 0.0496699, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:15.565884: step 6493, loss 0.0740273, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:15.872241: step 6494, loss 0.0770067, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:16.169160: step 6495, loss 0.0546828, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:16.419683: step 6496, loss 0.0952229, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:16.667547: step 6497, loss 0.028555, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:16.927060: step 6498, loss 0.0212051, acc 1, learning_rate 0.0001
2017-10-11T15:19:17.165212: step 6499, loss 0.0675589, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:17.396972: step 6500, loss 0.0742204, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:17.658397: step 6501, loss 0.0147653, acc 1, learning_rate 0.0001
2017-10-11T15:19:17.948048: step 6502, loss 0.00719492, acc 1, learning_rate 0.0001
2017-10-11T15:19:18.234630: step 6503, loss 0.0294224, acc 1, learning_rate 0.0001
2017-10-11T15:19:18.501048: step 6504, loss 0.0205322, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:18.786516: step 6505, loss 0.0646109, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:19.040124: step 6506, loss 0.0381594, acc 1, learning_rate 0.0001
2017-10-11T15:19:19.323740: step 6507, loss 0.0484482, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:19.666386: step 6508, loss 0.0582284, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:19.986147: step 6509, loss 0.0437646, acc 1, learning_rate 0.0001
2017-10-11T15:19:20.352281: step 6510, loss 0.0973578, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:20.648210: step 6511, loss 0.0476233, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:20.956311: step 6512, loss 0.0622146, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:21.282812: step 6513, loss 0.0627557, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:21.569570: step 6514, loss 0.0506376, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:21.888581: step 6515, loss 0.0552785, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:22.165723: step 6516, loss 0.0404985, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:22.432435: step 6517, loss 0.0864258, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:22.706740: step 6518, loss 0.0949555, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:23.000690: step 6519, loss 0.0287289, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:23.295712: step 6520, loss 0.0868366, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:19:23.585434: step 6520, loss 0.218239, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6520

2017-10-11T15:19:26.042073: step 6521, loss 0.09928, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:26.330656: step 6522, loss 0.10043, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:26.593477: step 6523, loss 0.0143254, acc 1, learning_rate 0.0001
2017-10-11T15:19:26.901331: step 6524, loss 0.0553068, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:27.160727: step 6525, loss 0.0694089, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:27.446040: step 6526, loss 0.0814757, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:27.746945: step 6527, loss 0.0349243, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:28.045985: step 6528, loss 0.0271973, acc 1, learning_rate 0.0001
2017-10-11T15:19:28.332599: step 6529, loss 0.0270474, acc 1, learning_rate 0.0001
2017-10-11T15:19:28.611947: step 6530, loss 0.0231692, acc 1, learning_rate 0.0001
2017-10-11T15:19:28.877393: step 6531, loss 0.0624611, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:29.178936: step 6532, loss 0.112204, acc 0.9375, learning_rate 0.0001
2017-10-11T15:19:29.431608: step 6533, loss 0.0238395, acc 1, learning_rate 0.0001
2017-10-11T15:19:29.718959: step 6534, loss 0.0172857, acc 1, learning_rate 0.0001
2017-10-11T15:19:30.017491: step 6535, loss 0.0404952, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:30.311464: step 6536, loss 0.0365682, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:30.555049: step 6537, loss 0.0211848, acc 1, learning_rate 0.0001
2017-10-11T15:19:30.846442: step 6538, loss 0.0499966, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:31.120322: step 6539, loss 0.014965, acc 1, learning_rate 0.0001
2017-10-11T15:19:31.331630: step 6540, loss 0.0484495, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:31.568016: step 6541, loss 0.0366932, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:31.807203: step 6542, loss 0.0184777, acc 1, learning_rate 0.0001
2017-10-11T15:19:32.036952: step 6543, loss 0.0402089, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:32.297194: step 6544, loss 0.085854, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:32.538252: step 6545, loss 0.0370382, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:32.847272: step 6546, loss 0.0189866, acc 1, learning_rate 0.0001
2017-10-11T15:19:33.158858: step 6547, loss 0.0199461, acc 1, learning_rate 0.0001
2017-10-11T15:19:33.478761: step 6548, loss 0.0161773, acc 1, learning_rate 0.0001
2017-10-11T15:19:33.803780: step 6549, loss 0.0320796, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:34.076535: step 6550, loss 0.0417481, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:34.369850: step 6551, loss 0.0784401, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:34.639665: step 6552, loss 0.105654, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:34.919669: step 6553, loss 0.052524, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:35.201075: step 6554, loss 0.0817255, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:35.486392: step 6555, loss 0.045486, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:35.799722: step 6556, loss 0.0324584, acc 1, learning_rate 0.0001
2017-10-11T15:19:36.105392: step 6557, loss 0.0502937, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:36.403477: step 6558, loss 0.0314478, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:36.718976: step 6559, loss 0.0563027, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:37.041506: step 6560, loss 0.0416478, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:19:37.351488: step 6560, loss 0.218886, acc 0.920863

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6560

2017-10-11T15:19:39.873676: step 6561, loss 0.0669889, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:40.179251: step 6562, loss 0.086275, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:40.525876: step 6563, loss 0.0225298, acc 1, learning_rate 0.0001
2017-10-11T15:19:40.846008: step 6564, loss 0.0875247, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:41.151675: step 6565, loss 0.0266027, acc 1, learning_rate 0.0001
2017-10-11T15:19:41.428480: step 6566, loss 0.056801, acc 0.980392, learning_rate 0.0001
2017-10-11T15:19:41.704926: step 6567, loss 0.0641497, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:41.985698: step 6568, loss 0.117803, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:42.269325: step 6569, loss 0.0375863, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:42.565852: step 6570, loss 0.0813705, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:42.851901: step 6571, loss 0.0768394, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:43.110523: step 6572, loss 0.0565402, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:43.357184: step 6573, loss 0.109971, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:43.647318: step 6574, loss 0.0627624, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:43.943158: step 6575, loss 0.0403434, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:44.236641: step 6576, loss 0.0720332, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:44.496964: step 6577, loss 0.0861609, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:44.744910: step 6578, loss 0.0524495, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:44.998666: step 6579, loss 0.0340561, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:45.224813: step 6580, loss 0.0811461, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:45.477347: step 6581, loss 0.09733, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:45.792169: step 6582, loss 0.0428475, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:46.075943: step 6583, loss 0.062115, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:46.361157: step 6584, loss 0.0636048, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:46.670938: step 6585, loss 0.074478, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:46.996992: step 6586, loss 0.0506674, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:47.367250: step 6587, loss 0.0571646, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:47.686233: step 6588, loss 0.0452537, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:48.009891: step 6589, loss 0.0565202, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:48.351713: step 6590, loss 0.0282423, acc 1, learning_rate 0.0001
2017-10-11T15:19:48.674008: step 6591, loss 0.0655831, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:49.010767: step 6592, loss 0.0359388, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:49.352339: step 6593, loss 0.0298135, acc 1, learning_rate 0.0001
2017-10-11T15:19:49.610322: step 6594, loss 0.122207, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:49.911833: step 6595, loss 0.0369768, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:50.202152: step 6596, loss 0.115263, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:50.476053: step 6597, loss 0.0529271, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:50.781832: step 6598, loss 0.0783477, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:51.066940: step 6599, loss 0.0614276, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:51.361170: step 6600, loss 0.0287663, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:19:51.655914: step 6600, loss 0.222361, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6600

2017-10-11T15:19:53.321695: step 6601, loss 0.024265, acc 1, learning_rate 0.0001
2017-10-11T15:19:53.639521: step 6602, loss 0.0350966, acc 1, learning_rate 0.0001
2017-10-11T15:19:53.983725: step 6603, loss 0.0271582, acc 1, learning_rate 0.0001
2017-10-11T15:19:54.306398: step 6604, loss 0.0877803, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:54.624652: step 6605, loss 0.0434261, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:54.944297: step 6606, loss 0.0560145, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:55.241468: step 6607, loss 0.0336502, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:55.500224: step 6608, loss 0.0520817, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:55.793103: step 6609, loss 0.0221472, acc 1, learning_rate 0.0001
2017-10-11T15:19:56.080742: step 6610, loss 0.0719616, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:56.361995: step 6611, loss 0.0764656, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:56.646978: step 6612, loss 0.0399721, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:56.919787: step 6613, loss 0.0437969, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:57.213231: step 6614, loss 0.0411041, acc 1, learning_rate 0.0001
2017-10-11T15:19:57.502137: step 6615, loss 0.0676496, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:57.809517: step 6616, loss 0.0446504, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:58.055747: step 6617, loss 0.116139, acc 0.953125, learning_rate 0.0001
2017-10-11T15:19:58.319923: step 6618, loss 0.0335393, acc 1, learning_rate 0.0001
2017-10-11T15:19:58.587212: step 6619, loss 0.0992715, acc 0.96875, learning_rate 0.0001
2017-10-11T15:19:58.816675: step 6620, loss 0.0251839, acc 1, learning_rate 0.0001
2017-10-11T15:19:59.044976: step 6621, loss 0.0355008, acc 1, learning_rate 0.0001
2017-10-11T15:19:59.283009: step 6622, loss 0.0289078, acc 1, learning_rate 0.0001
2017-10-11T15:19:59.535652: step 6623, loss 0.0502216, acc 0.984375, learning_rate 0.0001
2017-10-11T15:19:59.790129: step 6624, loss 0.117892, acc 0.953125, learning_rate 0.0001
2017-10-11T15:20:00.036801: step 6625, loss 0.121792, acc 0.9375, learning_rate 0.0001
2017-10-11T15:20:00.247074: step 6626, loss 0.0811033, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:00.524495: step 6627, loss 0.0200247, acc 1, learning_rate 0.0001
2017-10-11T15:20:00.826951: step 6628, loss 0.0119273, acc 1, learning_rate 0.0001
2017-10-11T15:20:01.144467: step 6629, loss 0.0157895, acc 1, learning_rate 0.0001
2017-10-11T15:20:01.495495: step 6630, loss 0.0548022, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:01.806360: step 6631, loss 0.0244756, acc 1, learning_rate 0.0001
2017-10-11T15:20:02.195076: step 6632, loss 0.149596, acc 0.9375, learning_rate 0.0001
2017-10-11T15:20:02.577233: step 6633, loss 0.0427009, acc 1, learning_rate 0.0001
2017-10-11T15:20:02.886822: step 6634, loss 0.0655246, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:03.167805: step 6635, loss 0.0763616, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:03.496962: step 6636, loss 0.134165, acc 0.953125, learning_rate 0.0001
2017-10-11T15:20:03.793896: step 6637, loss 0.0285148, acc 1, learning_rate 0.0001
2017-10-11T15:20:04.095541: step 6638, loss 0.0509614, acc 1, learning_rate 0.0001
2017-10-11T15:20:04.490908: step 6639, loss 0.0170666, acc 1, learning_rate 0.0001
2017-10-11T15:20:04.856541: step 6640, loss 0.0195477, acc 1, learning_rate 0.0001

Evaluation:
2017-10-11T15:20:05.163271: step 6640, loss 0.217543, acc 0.920863

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6640

2017-10-11T15:20:08.459135: step 6641, loss 0.131318, acc 0.953125, learning_rate 0.0001
2017-10-11T15:20:08.708021: step 6642, loss 0.103881, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:08.978556: step 6643, loss 0.0876692, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:09.226906: step 6644, loss 0.0591219, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:09.463399: step 6645, loss 0.0877126, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:09.712626: step 6646, loss 0.0318836, acc 1, learning_rate 0.0001
2017-10-11T15:20:09.974920: step 6647, loss 0.138971, acc 0.9375, learning_rate 0.0001
2017-10-11T15:20:10.288625: step 6648, loss 0.0107128, acc 1, learning_rate 0.0001
2017-10-11T15:20:10.574326: step 6649, loss 0.085645, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:10.849968: step 6650, loss 0.0299844, acc 1, learning_rate 0.0001
2017-10-11T15:20:11.085805: step 6651, loss 0.0518414, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:11.423220: step 6652, loss 0.0307657, acc 1, learning_rate 0.0001
2017-10-11T15:20:11.747784: step 6653, loss 0.0393812, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:12.074308: step 6654, loss 0.0738871, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:12.366380: step 6655, loss 0.0282887, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:12.625141: step 6656, loss 0.0344008, acc 1, learning_rate 0.0001
2017-10-11T15:20:12.885445: step 6657, loss 0.0257985, acc 1, learning_rate 0.0001
2017-10-11T15:20:13.111030: step 6658, loss 0.0246457, acc 1, learning_rate 0.0001
2017-10-11T15:20:13.359507: step 6659, loss 0.0180062, acc 1, learning_rate 0.0001
2017-10-11T15:20:13.612735: step 6660, loss 0.00821662, acc 1, learning_rate 0.0001
2017-10-11T15:20:13.828116: step 6661, loss 0.0497105, acc 1, learning_rate 0.0001
2017-10-11T15:20:14.117542: step 6662, loss 0.0146137, acc 1, learning_rate 0.0001
2017-10-11T15:20:14.385702: step 6663, loss 0.0554367, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:14.686590: step 6664, loss 0.0582422, acc 0.980392, learning_rate 0.0001
2017-10-11T15:20:14.985157: step 6665, loss 0.043708, acc 1, learning_rate 0.0001
2017-10-11T15:20:15.278396: step 6666, loss 0.0814195, acc 0.953125, learning_rate 0.0001
2017-10-11T15:20:15.611765: step 6667, loss 0.0200486, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:15.899982: step 6668, loss 0.0867729, acc 0.9375, learning_rate 0.0001
2017-10-11T15:20:16.235262: step 6669, loss 0.043801, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:16.550165: step 6670, loss 0.0581027, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:16.872892: step 6671, loss 0.0587209, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:17.188272: step 6672, loss 0.0345643, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:17.494402: step 6673, loss 0.028147, acc 1, learning_rate 0.0001
2017-10-11T15:20:17.794353: step 6674, loss 0.0653895, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:18.079175: step 6675, loss 0.0237487, acc 1, learning_rate 0.0001
2017-10-11T15:20:18.364852: step 6676, loss 0.0193352, acc 1, learning_rate 0.0001
2017-10-11T15:20:18.679467: step 6677, loss 0.0556136, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:19.017224: step 6678, loss 0.0639729, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:19.321389: step 6679, loss 0.0331803, acc 1, learning_rate 0.0001
2017-10-11T15:20:19.642461: step 6680, loss 0.0442717, acc 1, learning_rate 0.0001

Evaluation:
2017-10-11T15:20:20.004353: step 6680, loss 0.220403, acc 0.920863

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6680

2017-10-11T15:20:22.179834: step 6681, loss 0.112866, acc 0.9375, learning_rate 0.0001
2017-10-11T15:20:22.480493: step 6682, loss 0.084224, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:22.816919: step 6683, loss 0.10397, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:23.112382: step 6684, loss 0.0893096, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:23.397402: step 6685, loss 0.0651425, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:23.662106: step 6686, loss 0.0575259, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:23.954004: step 6687, loss 0.149287, acc 0.953125, learning_rate 0.0001
2017-10-11T15:20:24.255041: step 6688, loss 0.0258414, acc 1, learning_rate 0.0001
2017-10-11T15:20:24.550428: step 6689, loss 0.0289072, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:24.828915: step 6690, loss 0.0284319, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:25.104001: step 6691, loss 0.0320046, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:25.392316: step 6692, loss 0.0787199, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:25.708552: step 6693, loss 0.0293916, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:26.017147: step 6694, loss 0.0536043, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:26.292777: step 6695, loss 0.0221549, acc 1, learning_rate 0.0001
2017-10-11T15:20:26.606343: step 6696, loss 0.0617723, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:26.846129: step 6697, loss 0.032209, acc 1, learning_rate 0.0001
2017-10-11T15:20:27.088590: step 6698, loss 0.0473171, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:27.313996: step 6699, loss 0.0543343, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:27.519352: step 6700, loss 0.0566259, acc 1, learning_rate 0.0001
2017-10-11T15:20:27.724914: step 6701, loss 0.0928909, acc 0.9375, learning_rate 0.0001
2017-10-11T15:20:27.955922: step 6702, loss 0.0673949, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:28.200638: step 6703, loss 0.0476231, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:28.474716: step 6704, loss 0.0676156, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:28.764987: step 6705, loss 0.06322, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:29.097372: step 6706, loss 0.0394054, acc 1, learning_rate 0.0001
2017-10-11T15:20:29.403288: step 6707, loss 0.0252447, acc 1, learning_rate 0.0001
2017-10-11T15:20:29.699931: step 6708, loss 0.0284532, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:29.999055: step 6709, loss 0.0630329, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:30.328406: step 6710, loss 0.0503643, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:30.616118: step 6711, loss 0.0437993, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:30.936356: step 6712, loss 0.0203454, acc 1, learning_rate 0.0001
2017-10-11T15:20:31.241359: step 6713, loss 0.0180439, acc 1, learning_rate 0.0001
2017-10-11T15:20:31.524646: step 6714, loss 0.0477779, acc 1, learning_rate 0.0001
2017-10-11T15:20:31.858900: step 6715, loss 0.0732768, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:32.171823: step 6716, loss 0.0813776, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:32.514786: step 6717, loss 0.0792525, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:32.811775: step 6718, loss 0.0357243, acc 1, learning_rate 0.0001
2017-10-11T15:20:33.081771: step 6719, loss 0.0319777, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:33.375009: step 6720, loss 0.0590914, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:20:33.707218: step 6720, loss 0.219379, acc 0.923741

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6720

2017-10-11T15:20:36.267690: step 6721, loss 0.0297616, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:36.568308: step 6722, loss 0.0214755, acc 1, learning_rate 0.0001
2017-10-11T15:20:36.891144: step 6723, loss 0.0766511, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:37.175488: step 6724, loss 0.124931, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:37.476240: step 6725, loss 0.0619083, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:37.739467: step 6726, loss 0.0368312, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:38.004557: step 6727, loss 0.0230526, acc 1, learning_rate 0.0001
2017-10-11T15:20:38.328478: step 6728, loss 0.0632046, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:38.605077: step 6729, loss 0.0336536, acc 1, learning_rate 0.0001
2017-10-11T15:20:38.891486: step 6730, loss 0.10254, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:39.140392: step 6731, loss 0.0756957, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:39.451270: step 6732, loss 0.0642934, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:39.736032: step 6733, loss 0.0543977, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:40.036373: step 6734, loss 0.0130413, acc 1, learning_rate 0.0001
2017-10-11T15:20:40.331334: step 6735, loss 0.0473677, acc 1, learning_rate 0.0001
2017-10-11T15:20:40.615421: step 6736, loss 0.0735332, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:40.836962: step 6737, loss 0.0872542, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:41.055923: step 6738, loss 0.0683222, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:41.253641: step 6739, loss 0.0437729, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:41.475611: step 6740, loss 0.0759077, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:41.684986: step 6741, loss 0.0170637, acc 1, learning_rate 0.0001
2017-10-11T15:20:41.922789: step 6742, loss 0.0784582, acc 0.953125, learning_rate 0.0001
2017-10-11T15:20:42.145199: step 6743, loss 0.06683, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:42.366183: step 6744, loss 0.0970161, acc 0.953125, learning_rate 0.0001
2017-10-11T15:20:42.609607: step 6745, loss 0.0417266, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:42.859431: step 6746, loss 0.100545, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:43.165542: step 6747, loss 0.110898, acc 0.953125, learning_rate 0.0001
2017-10-11T15:20:43.467812: step 6748, loss 0.0256247, acc 1, learning_rate 0.0001
2017-10-11T15:20:43.784646: step 6749, loss 0.0421351, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:44.100709: step 6750, loss 0.0931589, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:44.391470: step 6751, loss 0.0733448, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:44.675536: step 6752, loss 0.103263, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:44.953445: step 6753, loss 0.0153519, acc 1, learning_rate 0.0001
2017-10-11T15:20:45.269337: step 6754, loss 0.0521607, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:45.587300: step 6755, loss 0.0323002, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:45.914989: step 6756, loss 0.0535555, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:46.227910: step 6757, loss 0.031158, acc 1, learning_rate 0.0001
2017-10-11T15:20:46.549626: step 6758, loss 0.0535307, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:46.880799: step 6759, loss 0.0869797, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:47.190304: step 6760, loss 0.184941, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:20:47.491873: step 6760, loss 0.214973, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6760

2017-10-11T15:20:50.642555: step 6761, loss 0.0178293, acc 1, learning_rate 0.0001
2017-10-11T15:20:50.891224: step 6762, loss 0.0589517, acc 0.980392, learning_rate 0.0001
2017-10-11T15:20:51.180876: step 6763, loss 0.0418103, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:51.454166: step 6764, loss 0.0615422, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:51.736395: step 6765, loss 0.0926921, acc 0.953125, learning_rate 0.0001
2017-10-11T15:20:51.980808: step 6766, loss 0.0479164, acc 1, learning_rate 0.0001
2017-10-11T15:20:52.268593: step 6767, loss 0.0615049, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:52.564709: step 6768, loss 0.0489776, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:52.917264: step 6769, loss 0.0896646, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:53.294357: step 6770, loss 0.036153, acc 1, learning_rate 0.0001
2017-10-11T15:20:53.625635: step 6771, loss 0.0299062, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:53.987937: step 6772, loss 0.0241266, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:54.331501: step 6773, loss 0.0579491, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:54.667735: step 6774, loss 0.0285671, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:55.009629: step 6775, loss 0.0391281, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:55.318194: step 6776, loss 0.0234421, acc 1, learning_rate 0.0001
2017-10-11T15:20:55.538934: step 6777, loss 0.0999026, acc 0.953125, learning_rate 0.0001
2017-10-11T15:20:55.808825: step 6778, loss 0.0320568, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:56.036024: step 6779, loss 0.0353193, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:56.272050: step 6780, loss 0.10889, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:56.490380: step 6781, loss 0.0276681, acc 1, learning_rate 0.0001
2017-10-11T15:20:56.734909: step 6782, loss 0.0608843, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:56.934438: step 6783, loss 0.060854, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:57.209491: step 6784, loss 0.0487848, acc 0.96875, learning_rate 0.0001
2017-10-11T15:20:57.513032: step 6785, loss 0.020592, acc 1, learning_rate 0.0001
2017-10-11T15:20:57.859759: step 6786, loss 0.0470565, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:58.191940: step 6787, loss 0.0170199, acc 1, learning_rate 0.0001
2017-10-11T15:20:58.519925: step 6788, loss 0.0705719, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:58.850120: step 6789, loss 0.026601, acc 1, learning_rate 0.0001
2017-10-11T15:20:59.160635: step 6790, loss 0.0581749, acc 0.984375, learning_rate 0.0001
2017-10-11T15:20:59.468724: step 6791, loss 0.0976752, acc 0.953125, learning_rate 0.0001
2017-10-11T15:20:59.797930: step 6792, loss 0.0224847, acc 1, learning_rate 0.0001
2017-10-11T15:21:00.106661: step 6793, loss 0.12901, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:00.425080: step 6794, loss 0.0745251, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:00.754692: step 6795, loss 0.0409673, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:01.045323: step 6796, loss 0.026791, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:01.343215: step 6797, loss 0.065826, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:01.657521: step 6798, loss 0.0459353, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:01.981103: step 6799, loss 0.076897, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:02.267086: step 6800, loss 0.0203452, acc 1, learning_rate 0.0001

Evaluation:
2017-10-11T15:21:02.579061: step 6800, loss 0.220744, acc 0.923741

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6800

2017-10-11T15:21:04.418952: step 6801, loss 0.0173512, acc 1, learning_rate 0.0001
2017-10-11T15:21:04.773859: step 6802, loss 0.0987249, acc 0.9375, learning_rate 0.0001
2017-10-11T15:21:05.082519: step 6803, loss 0.0304204, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:05.379158: step 6804, loss 0.0209964, acc 1, learning_rate 0.0001
2017-10-11T15:21:05.630013: step 6805, loss 0.0272433, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:05.896333: step 6806, loss 0.0711447, acc 0.953125, learning_rate 0.0001
2017-10-11T15:21:06.171855: step 6807, loss 0.0502621, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:06.499540: step 6808, loss 0.0730328, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:06.827529: step 6809, loss 0.0210483, acc 1, learning_rate 0.0001
2017-10-11T15:21:07.132787: step 6810, loss 0.0639276, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:07.415397: step 6811, loss 0.0242532, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:07.713671: step 6812, loss 0.031275, acc 1, learning_rate 0.0001
2017-10-11T15:21:08.033932: step 6813, loss 0.0803667, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:08.338000: step 6814, loss 0.0423341, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:08.649436: step 6815, loss 0.0181156, acc 1, learning_rate 0.0001
2017-10-11T15:21:08.947051: step 6816, loss 0.0889052, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:09.230831: step 6817, loss 0.0322618, acc 1, learning_rate 0.0001
2017-10-11T15:21:09.494750: step 6818, loss 0.0264874, acc 1, learning_rate 0.0001
2017-10-11T15:21:09.739138: step 6819, loss 0.042133, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:09.951317: step 6820, loss 0.0194392, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:10.192905: step 6821, loss 0.0483873, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:10.398034: step 6822, loss 0.0370493, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:10.621830: step 6823, loss 0.0482936, acc 1, learning_rate 0.0001
2017-10-11T15:21:10.851040: step 6824, loss 0.0443914, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:11.074360: step 6825, loss 0.0481764, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:11.362380: step 6826, loss 0.0796515, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:11.631505: step 6827, loss 0.052725, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:11.931763: step 6828, loss 0.0537339, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:12.183337: step 6829, loss 0.026494, acc 1, learning_rate 0.0001
2017-10-11T15:21:12.436621: step 6830, loss 0.00802989, acc 1, learning_rate 0.0001
2017-10-11T15:21:12.806759: step 6831, loss 0.0237805, acc 1, learning_rate 0.0001
2017-10-11T15:21:13.143542: step 6832, loss 0.0818475, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:13.426567: step 6833, loss 0.0615266, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:13.762391: step 6834, loss 0.124398, acc 0.953125, learning_rate 0.0001
2017-10-11T15:21:14.075063: step 6835, loss 0.0241809, acc 1, learning_rate 0.0001
2017-10-11T15:21:14.377655: step 6836, loss 0.0352869, acc 1, learning_rate 0.0001
2017-10-11T15:21:14.704546: step 6837, loss 0.0927341, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:15.042478: step 6838, loss 0.078669, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:15.378479: step 6839, loss 0.0320932, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:15.726016: step 6840, loss 0.0259296, acc 1, learning_rate 0.0001

Evaluation:
2017-10-11T15:21:16.046787: step 6840, loss 0.217631, acc 0.920863

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6840

2017-10-11T15:21:18.324986: step 6841, loss 0.0769933, acc 0.953125, learning_rate 0.0001
2017-10-11T15:21:18.586914: step 6842, loss 0.0159227, acc 1, learning_rate 0.0001
2017-10-11T15:21:18.865316: step 6843, loss 0.0516929, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:19.155763: step 6844, loss 0.0930971, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:19.428943: step 6845, loss 0.0869763, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:19.678935: step 6846, loss 0.0928923, acc 0.953125, learning_rate 0.0001
2017-10-11T15:21:19.944154: step 6847, loss 0.00882804, acc 1, learning_rate 0.0001
2017-10-11T15:21:20.290531: step 6848, loss 0.0563872, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:20.622995: step 6849, loss 0.0766188, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:20.932799: step 6850, loss 0.0840877, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:21.235841: step 6851, loss 0.0328559, acc 1, learning_rate 0.0001
2017-10-11T15:21:21.524040: step 6852, loss 0.0672121, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:21.789977: step 6853, loss 0.0792802, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:22.103256: step 6854, loss 0.0494887, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:22.447610: step 6855, loss 0.0960142, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:22.736950: step 6856, loss 0.0376143, acc 1, learning_rate 0.0001
2017-10-11T15:21:23.047272: step 6857, loss 0.148812, acc 0.9375, learning_rate 0.0001
2017-10-11T15:21:23.366152: step 6858, loss 0.0313213, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:23.662230: step 6859, loss 0.0295308, acc 1, learning_rate 0.0001
2017-10-11T15:21:23.932323: step 6860, loss 0.0645073, acc 0.960784, learning_rate 0.0001
2017-10-11T15:21:24.218133: step 6861, loss 0.131168, acc 0.9375, learning_rate 0.0001
2017-10-11T15:21:24.462230: step 6862, loss 0.151899, acc 0.953125, learning_rate 0.0001
2017-10-11T15:21:24.743709: step 6863, loss 0.0259078, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:24.989486: step 6864, loss 0.0164638, acc 1, learning_rate 0.0001
2017-10-11T15:21:25.228299: step 6865, loss 0.0285194, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:25.461234: step 6866, loss 0.00707653, acc 1, learning_rate 0.0001
2017-10-11T15:21:25.668977: step 6867, loss 0.0501332, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:25.902472: step 6868, loss 0.0410174, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:26.203328: step 6869, loss 0.0369265, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:26.548786: step 6870, loss 0.0152808, acc 1, learning_rate 0.0001
2017-10-11T15:21:26.860230: step 6871, loss 0.0473384, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:27.190397: step 6872, loss 0.0427453, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:27.511178: step 6873, loss 0.0217656, acc 1, learning_rate 0.0001
2017-10-11T15:21:27.825625: step 6874, loss 0.0485022, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:28.171184: step 6875, loss 0.0626534, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:28.456830: step 6876, loss 0.0850634, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:28.769001: step 6877, loss 0.0242588, acc 1, learning_rate 0.0001
2017-10-11T15:21:29.087410: step 6878, loss 0.133259, acc 0.953125, learning_rate 0.0001
2017-10-11T15:21:29.410499: step 6879, loss 0.0586537, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:29.714396: step 6880, loss 0.0543494, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:21:29.989233: step 6880, loss 0.216691, acc 0.920863

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6880

2017-10-11T15:21:33.383571: step 6881, loss 0.0354551, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:33.665638: step 6882, loss 0.0794376, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:33.942169: step 6883, loss 0.0996798, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:34.251905: step 6884, loss 0.059768, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:34.564860: step 6885, loss 0.058434, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:34.894792: step 6886, loss 0.0817878, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:35.164705: step 6887, loss 0.0708061, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:35.445952: step 6888, loss 0.107489, acc 0.953125, learning_rate 0.0001
2017-10-11T15:21:35.758093: step 6889, loss 0.127896, acc 0.953125, learning_rate 0.0001
2017-10-11T15:21:36.046439: step 6890, loss 0.15019, acc 0.9375, learning_rate 0.0001
2017-10-11T15:21:36.351973: step 6891, loss 0.0387632, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:36.641762: step 6892, loss 0.0118283, acc 1, learning_rate 0.0001
2017-10-11T15:21:36.942370: step 6893, loss 0.0526186, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:37.249737: step 6894, loss 0.0380184, acc 1, learning_rate 0.0001
2017-10-11T15:21:37.555727: step 6895, loss 0.016179, acc 1, learning_rate 0.0001
2017-10-11T15:21:37.865369: step 6896, loss 0.019953, acc 1, learning_rate 0.0001
2017-10-11T15:21:38.132211: step 6897, loss 0.0573861, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:38.390222: step 6898, loss 0.016079, acc 1, learning_rate 0.0001
2017-10-11T15:21:38.614812: step 6899, loss 0.0256397, acc 1, learning_rate 0.0001
2017-10-11T15:21:38.827748: step 6900, loss 0.0289534, acc 1, learning_rate 0.0001
2017-10-11T15:21:39.034684: step 6901, loss 0.0804766, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:39.266617: step 6902, loss 0.061269, acc 0.953125, learning_rate 0.0001
2017-10-11T15:21:39.517026: step 6903, loss 0.12114, acc 0.953125, learning_rate 0.0001
2017-10-11T15:21:39.813074: step 6904, loss 0.0212287, acc 1, learning_rate 0.0001
2017-10-11T15:21:40.074751: step 6905, loss 0.0521775, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:40.374370: step 6906, loss 0.0540883, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:40.768256: step 6907, loss 0.0364691, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:41.097388: step 6908, loss 0.0537891, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:41.410398: step 6909, loss 0.0186953, acc 1, learning_rate 0.0001
2017-10-11T15:21:41.701219: step 6910, loss 0.0245804, acc 1, learning_rate 0.0001
2017-10-11T15:21:41.995337: step 6911, loss 0.0249488, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:42.323590: step 6912, loss 0.0342265, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:42.635625: step 6913, loss 0.067772, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:42.919277: step 6914, loss 0.0421074, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:43.239487: step 6915, loss 0.0886837, acc 0.9375, learning_rate 0.0001
2017-10-11T15:21:43.555582: step 6916, loss 0.0781518, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:43.848958: step 6917, loss 0.0278419, acc 1, learning_rate 0.0001
2017-10-11T15:21:44.132965: step 6918, loss 0.0264342, acc 1, learning_rate 0.0001
2017-10-11T15:21:44.451716: step 6919, loss 0.0215781, acc 1, learning_rate 0.0001
2017-10-11T15:21:44.770817: step 6920, loss 0.0188074, acc 1, learning_rate 0.0001

Evaluation:
2017-10-11T15:21:45.055246: step 6920, loss 0.218856, acc 0.920863

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6920

2017-10-11T15:21:46.577491: step 6921, loss 0.024588, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:46.885400: step 6922, loss 0.0525865, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:47.164630: step 6923, loss 0.0560255, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:47.470367: step 6924, loss 0.045145, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:47.774579: step 6925, loss 0.0653645, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:48.081489: step 6926, loss 0.120837, acc 0.953125, learning_rate 0.0001
2017-10-11T15:21:48.428156: step 6927, loss 0.0536954, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:48.721109: step 6928, loss 0.108553, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:49.023619: step 6929, loss 0.0164354, acc 1, learning_rate 0.0001
2017-10-11T15:21:49.313832: step 6930, loss 0.029196, acc 1, learning_rate 0.0001
2017-10-11T15:21:49.636525: step 6931, loss 0.0403606, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:49.950414: step 6932, loss 0.054791, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:50.262576: step 6933, loss 0.0265294, acc 1, learning_rate 0.0001
2017-10-11T15:21:50.535986: step 6934, loss 0.0328706, acc 1, learning_rate 0.0001
2017-10-11T15:21:50.839209: step 6935, loss 0.0629587, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:51.150061: step 6936, loss 0.0223945, acc 1, learning_rate 0.0001
2017-10-11T15:21:51.493522: step 6937, loss 0.04312, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:51.775607: step 6938, loss 0.0253313, acc 1, learning_rate 0.0001
2017-10-11T15:21:52.064039: step 6939, loss 0.0382936, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:52.354308: step 6940, loss 0.130073, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:52.604570: step 6941, loss 0.0479476, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:52.849779: step 6942, loss 0.0802647, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:53.086529: step 6943, loss 0.106267, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:53.329165: step 6944, loss 0.115219, acc 0.953125, learning_rate 0.0001
2017-10-11T15:21:53.537044: step 6945, loss 0.0694752, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:53.782078: step 6946, loss 0.0515662, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:53.999623: step 6947, loss 0.115667, acc 0.953125, learning_rate 0.0001
2017-10-11T15:21:54.283783: step 6948, loss 0.0214115, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:54.583775: step 6949, loss 0.0752908, acc 0.96875, learning_rate 0.0001
2017-10-11T15:21:54.864065: step 6950, loss 0.0213223, acc 1, learning_rate 0.0001
2017-10-11T15:21:55.141238: step 6951, loss 0.0737153, acc 0.953125, learning_rate 0.0001
2017-10-11T15:21:55.439515: step 6952, loss 0.0303019, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:55.765305: step 6953, loss 0.057536, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:56.046489: step 6954, loss 0.0441004, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:56.335872: step 6955, loss 0.074969, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:56.628063: step 6956, loss 0.0227074, acc 1, learning_rate 0.0001
2017-10-11T15:21:56.943558: step 6957, loss 0.0248391, acc 1, learning_rate 0.0001
2017-10-11T15:21:57.203420: step 6958, loss 0.0545806, acc 0.980392, learning_rate 0.0001
2017-10-11T15:21:57.498017: step 6959, loss 0.0475766, acc 0.984375, learning_rate 0.0001
2017-10-11T15:21:57.806411: step 6960, loss 0.0588718, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:21:58.057072: step 6960, loss 0.220893, acc 0.920863

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-6960

2017-10-11T15:22:00.887405: step 6961, loss 0.0264943, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:01.166276: step 6962, loss 0.0384753, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:01.453628: step 6963, loss 0.0168787, acc 1, learning_rate 0.0001
2017-10-11T15:22:01.772492: step 6964, loss 0.0569508, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:02.062434: step 6965, loss 0.0787711, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:02.353662: step 6966, loss 0.0381972, acc 1, learning_rate 0.0001
2017-10-11T15:22:02.661638: step 6967, loss 0.0417426, acc 1, learning_rate 0.0001
2017-10-11T15:22:02.925708: step 6968, loss 0.0479168, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:03.206129: step 6969, loss 0.059627, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:03.503850: step 6970, loss 0.0575478, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:03.774302: step 6971, loss 0.0807417, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:04.081561: step 6972, loss 0.0765859, acc 0.953125, learning_rate 0.0001
2017-10-11T15:22:04.393952: step 6973, loss 0.0686011, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:04.719586: step 6974, loss 0.0409962, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:05.049198: step 6975, loss 0.0549621, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:05.367683: step 6976, loss 0.038856, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:05.676999: step 6977, loss 0.0925375, acc 0.9375, learning_rate 0.0001
2017-10-11T15:22:05.948959: step 6978, loss 0.0249286, acc 1, learning_rate 0.0001
2017-10-11T15:22:06.204267: step 6979, loss 0.046826, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:06.431453: step 6980, loss 0.0804993, acc 0.953125, learning_rate 0.0001
2017-10-11T15:22:06.723711: step 6981, loss 0.0844361, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:07.019927: step 6982, loss 0.0510692, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:07.311271: step 6983, loss 0.0310047, acc 1, learning_rate 0.0001
2017-10-11T15:22:07.588567: step 6984, loss 0.085093, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:07.900343: step 6985, loss 0.0377037, acc 1, learning_rate 0.0001
2017-10-11T15:22:08.153721: step 6986, loss 0.0860715, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:08.419182: step 6987, loss 0.0522782, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:08.705627: step 6988, loss 0.082285, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:09.015374: step 6989, loss 0.0409967, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:09.316609: step 6990, loss 0.0618253, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:09.592710: step 6991, loss 0.0227538, acc 1, learning_rate 0.0001
2017-10-11T15:22:09.952074: step 6992, loss 0.0128088, acc 1, learning_rate 0.0001
2017-10-11T15:22:10.279682: step 6993, loss 0.0315999, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:10.637193: step 6994, loss 0.0166296, acc 1, learning_rate 0.0001
2017-10-11T15:22:10.922540: step 6995, loss 0.0220143, acc 1, learning_rate 0.0001
2017-10-11T15:22:11.253418: step 6996, loss 0.0473558, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:11.546129: step 6997, loss 0.0601602, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:11.873512: step 6998, loss 0.00996478, acc 1, learning_rate 0.0001
2017-10-11T15:22:12.168793: step 6999, loss 0.0426622, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:12.478489: step 7000, loss 0.0798528, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T15:22:12.775803: step 7000, loss 0.224726, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7000

2017-10-11T15:22:14.772882: step 7001, loss 0.0366121, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:15.058199: step 7002, loss 0.0209525, acc 1, learning_rate 0.0001
2017-10-11T15:22:15.304833: step 7003, loss 0.0885459, acc 0.953125, learning_rate 0.0001
2017-10-11T15:22:15.560924: step 7004, loss 0.0835845, acc 0.953125, learning_rate 0.0001
2017-10-11T15:22:15.848883: step 7005, loss 0.0226496, acc 1, learning_rate 0.0001
2017-10-11T15:22:16.123615: step 7006, loss 0.0507406, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:16.406972: step 7007, loss 0.0222666, acc 1, learning_rate 0.0001
2017-10-11T15:22:16.706539: step 7008, loss 0.0311548, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:17.006704: step 7009, loss 0.0823114, acc 0.953125, learning_rate 0.0001
2017-10-11T15:22:17.318880: step 7010, loss 0.0528318, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:17.596703: step 7011, loss 0.0339375, acc 1, learning_rate 0.0001
2017-10-11T15:22:17.889421: step 7012, loss 0.0602241, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:18.197462: step 7013, loss 0.0511467, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:18.511096: step 7014, loss 0.122013, acc 0.953125, learning_rate 0.0001
2017-10-11T15:22:18.812157: step 7015, loss 0.0291521, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:19.127336: step 7016, loss 0.0634122, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:19.369416: step 7017, loss 0.027476, acc 1, learning_rate 0.0001
2017-10-11T15:22:19.597612: step 7018, loss 0.087953, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:19.880344: step 7019, loss 0.0450833, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:20.192787: step 7020, loss 0.0416692, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:20.475013: step 7021, loss 0.0348432, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:20.744740: step 7022, loss 0.0292575, acc 1, learning_rate 0.0001
2017-10-11T15:22:20.989870: step 7023, loss 0.0151352, acc 1, learning_rate 0.0001
2017-10-11T15:22:21.249407: step 7024, loss 0.0579301, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:21.487997: step 7025, loss 0.0561145, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:21.753659: step 7026, loss 0.0280594, acc 1, learning_rate 0.0001
2017-10-11T15:22:22.061133: step 7027, loss 0.116818, acc 0.953125, learning_rate 0.0001
2017-10-11T15:22:22.334190: step 7028, loss 0.0655201, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:22.606625: step 7029, loss 0.0952161, acc 0.953125, learning_rate 0.0001
2017-10-11T15:22:22.870271: step 7030, loss 0.0300336, acc 1, learning_rate 0.0001
2017-10-11T15:22:23.149209: step 7031, loss 0.0481975, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:23.440614: step 7032, loss 0.0231579, acc 1, learning_rate 0.0001
2017-10-11T15:22:23.772114: step 7033, loss 0.0546583, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:24.052810: step 7034, loss 0.0417821, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:24.350694: step 7035, loss 0.0599953, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:24.604574: step 7036, loss 0.0812223, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:24.926295: step 7037, loss 0.0217245, acc 1, learning_rate 0.0001
2017-10-11T15:22:25.246634: step 7038, loss 0.0332808, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:25.565126: step 7039, loss 0.0748499, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:25.890262: step 7040, loss 0.0607738, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T15:22:26.214991: step 7040, loss 0.216749, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7040

2017-10-11T15:22:28.988502: step 7041, loss 0.044624, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:29.298231: step 7042, loss 0.0427731, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:29.574198: step 7043, loss 0.0605968, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:29.883569: step 7044, loss 0.0521411, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:30.184777: step 7045, loss 0.0399166, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:30.484397: step 7046, loss 0.0683542, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:30.781978: step 7047, loss 0.0727656, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:31.078803: step 7048, loss 0.0144304, acc 1, learning_rate 0.0001
2017-10-11T15:22:31.397445: step 7049, loss 0.0743864, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:31.679860: step 7050, loss 0.0289015, acc 1, learning_rate 0.0001
2017-10-11T15:22:32.009224: step 7051, loss 0.0583285, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:32.318002: step 7052, loss 0.0261896, acc 1, learning_rate 0.0001
2017-10-11T15:22:32.630547: step 7053, loss 0.0285119, acc 1, learning_rate 0.0001
2017-10-11T15:22:32.926498: step 7054, loss 0.105568, acc 0.953125, learning_rate 0.0001
2017-10-11T15:22:33.219826: step 7055, loss 0.118352, acc 0.9375, learning_rate 0.0001
2017-10-11T15:22:33.476790: step 7056, loss 0.0462077, acc 0.980392, learning_rate 0.0001
2017-10-11T15:22:33.741071: step 7057, loss 0.0107497, acc 1, learning_rate 0.0001
2017-10-11T15:22:33.992055: step 7058, loss 0.0260451, acc 1, learning_rate 0.0001
2017-10-11T15:22:34.279643: step 7059, loss 0.0431759, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:34.512093: step 7060, loss 0.0346625, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:34.756479: step 7061, loss 0.049788, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:34.982712: step 7062, loss 0.0100774, acc 1, learning_rate 0.0001
2017-10-11T15:22:35.196211: step 7063, loss 0.0123248, acc 1, learning_rate 0.0001
2017-10-11T15:22:35.452749: step 7064, loss 0.0520576, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:35.652018: step 7065, loss 0.0446757, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:35.921966: step 7066, loss 0.0257233, acc 1, learning_rate 0.0001
2017-10-11T15:22:36.186550: step 7067, loss 0.101158, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:36.425958: step 7068, loss 0.0335948, acc 1, learning_rate 0.0001
2017-10-11T15:22:36.706143: step 7069, loss 0.0592317, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:36.990996: step 7070, loss 0.0817922, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:37.297312: step 7071, loss 0.0530334, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:37.581268: step 7072, loss 0.0250647, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:37.885996: step 7073, loss 0.00951934, acc 1, learning_rate 0.0001
2017-10-11T15:22:38.218234: step 7074, loss 0.0806158, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:38.564016: step 7075, loss 0.117253, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:38.872529: step 7076, loss 0.0297321, acc 1, learning_rate 0.0001
2017-10-11T15:22:39.149482: step 7077, loss 0.0681361, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:39.434230: step 7078, loss 0.0504844, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:39.710846: step 7079, loss 0.0319199, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:39.998534: step 7080, loss 0.0519525, acc 1, learning_rate 0.0001

Evaluation:
2017-10-11T15:22:40.285612: step 7080, loss 0.214586, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7080

2017-10-11T15:22:42.403515: step 7081, loss 0.0469157, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:42.629126: step 7082, loss 0.030777, acc 1, learning_rate 0.0001
2017-10-11T15:22:42.967977: step 7083, loss 0.0420188, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:43.287959: step 7084, loss 0.0958816, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:43.625909: step 7085, loss 0.0329137, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:43.932199: step 7086, loss 0.00881465, acc 1, learning_rate 0.0001
2017-10-11T15:22:44.261754: step 7087, loss 0.0256366, acc 1, learning_rate 0.0001
2017-10-11T15:22:44.570244: step 7088, loss 0.0603468, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:44.891221: step 7089, loss 0.0580115, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:45.185923: step 7090, loss 0.0643664, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:45.488851: step 7091, loss 0.0328189, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:45.827679: step 7092, loss 0.13476, acc 0.9375, learning_rate 0.0001
2017-10-11T15:22:46.176547: step 7093, loss 0.0411332, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:46.487151: step 7094, loss 0.0263704, acc 1, learning_rate 0.0001
2017-10-11T15:22:46.801084: step 7095, loss 0.0712697, acc 0.953125, learning_rate 0.0001
2017-10-11T15:22:47.134144: step 7096, loss 0.0460359, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:47.420961: step 7097, loss 0.0232975, acc 1, learning_rate 0.0001
2017-10-11T15:22:47.731400: step 7098, loss 0.0485969, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:48.049315: step 7099, loss 0.0253832, acc 1, learning_rate 0.0001
2017-10-11T15:22:48.308626: step 7100, loss 0.0856708, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:48.611385: step 7101, loss 0.0449245, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:48.909794: step 7102, loss 0.0336605, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:49.138036: step 7103, loss 0.0763263, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:49.370001: step 7104, loss 0.0366558, acc 1, learning_rate 0.0001
2017-10-11T15:22:49.605162: step 7105, loss 0.0387796, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:49.928490: step 7106, loss 0.148831, acc 0.9375, learning_rate 0.0001
2017-10-11T15:22:50.180949: step 7107, loss 0.0332255, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:50.473846: step 7108, loss 0.0233972, acc 1, learning_rate 0.0001
2017-10-11T15:22:50.784168: step 7109, loss 0.0746258, acc 0.953125, learning_rate 0.0001
2017-10-11T15:22:51.091122: step 7110, loss 0.0511716, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:51.375178: step 7111, loss 0.103938, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:51.696279: step 7112, loss 0.0541397, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:52.000932: step 7113, loss 0.0665627, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:52.313529: step 7114, loss 0.208808, acc 0.953125, learning_rate 0.0001
2017-10-11T15:22:52.628681: step 7115, loss 0.0806814, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:52.913158: step 7116, loss 0.0305072, acc 1, learning_rate 0.0001
2017-10-11T15:22:53.184730: step 7117, loss 0.0248935, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:53.486926: step 7118, loss 0.0170568, acc 1, learning_rate 0.0001
2017-10-11T15:22:53.763237: step 7119, loss 0.0675636, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:54.078641: step 7120, loss 0.0815687, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:22:54.355879: step 7120, loss 0.217248, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7120

2017-10-11T15:22:56.293721: step 7121, loss 0.0902927, acc 0.96875, learning_rate 0.0001
2017-10-11T15:22:56.630410: step 7122, loss 0.0468484, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:56.964506: step 7123, loss 0.0307128, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:57.330276: step 7124, loss 0.0580837, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:57.633430: step 7125, loss 0.0414314, acc 1, learning_rate 0.0001
2017-10-11T15:22:57.952245: step 7126, loss 0.0631047, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:58.260325: step 7127, loss 0.0478773, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:58.572296: step 7128, loss 0.0372782, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:58.867166: step 7129, loss 0.0246637, acc 1, learning_rate 0.0001
2017-10-11T15:22:59.177313: step 7130, loss 0.0601064, acc 0.984375, learning_rate 0.0001
2017-10-11T15:22:59.506702: step 7131, loss 0.0149876, acc 1, learning_rate 0.0001
2017-10-11T15:22:59.784831: step 7132, loss 0.0833634, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:00.103873: step 7133, loss 0.116223, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:00.425963: step 7134, loss 0.0797442, acc 0.953125, learning_rate 0.0001
2017-10-11T15:23:00.772062: step 7135, loss 0.0898079, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:01.097995: step 7136, loss 0.0606009, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:01.432523: step 7137, loss 0.0530172, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:01.716807: step 7138, loss 0.0351591, acc 1, learning_rate 0.0001
2017-10-11T15:23:02.049361: step 7139, loss 0.0522443, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:02.375249: step 7140, loss 0.0262199, acc 1, learning_rate 0.0001
2017-10-11T15:23:02.658206: step 7141, loss 0.0724429, acc 0.953125, learning_rate 0.0001
2017-10-11T15:23:02.911847: step 7142, loss 0.0821558, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:03.142225: step 7143, loss 0.0369221, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:03.381895: step 7144, loss 0.0535842, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:03.590944: step 7145, loss 0.0931044, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:03.882298: step 7146, loss 0.0834334, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:04.146806: step 7147, loss 0.0535725, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:04.389542: step 7148, loss 0.0497147, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:04.670021: step 7149, loss 0.086912, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:04.962725: step 7150, loss 0.0334631, acc 1, learning_rate 0.0001
2017-10-11T15:23:05.216333: step 7151, loss 0.0876823, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:05.539976: step 7152, loss 0.164324, acc 0.90625, learning_rate 0.0001
2017-10-11T15:23:05.845274: step 7153, loss 0.038605, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:06.109917: step 7154, loss 0.117688, acc 0.960784, learning_rate 0.0001
2017-10-11T15:23:06.423856: step 7155, loss 0.075277, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:06.742124: step 7156, loss 0.0406261, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:07.089519: step 7157, loss 0.0151017, acc 1, learning_rate 0.0001
2017-10-11T15:23:07.417515: step 7158, loss 0.0375747, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:07.740196: step 7159, loss 0.0927099, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:08.058694: step 7160, loss 0.0514755, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:23:08.367925: step 7160, loss 0.215223, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7160

2017-10-11T15:23:10.657344: step 7161, loss 0.0279026, acc 1, learning_rate 0.0001
2017-10-11T15:23:10.942156: step 7162, loss 0.0634166, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:11.259565: step 7163, loss 0.0516461, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:11.566151: step 7164, loss 0.0291992, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:11.841754: step 7165, loss 0.0627734, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:12.187073: step 7166, loss 0.0730111, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:12.525778: step 7167, loss 0.0431988, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:12.823023: step 7168, loss 0.0786437, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:13.139972: step 7169, loss 0.0741834, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:13.428417: step 7170, loss 0.0354605, acc 1, learning_rate 0.0001
2017-10-11T15:23:13.707649: step 7171, loss 0.131338, acc 0.953125, learning_rate 0.0001
2017-10-11T15:23:13.998519: step 7172, loss 0.0460265, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:14.300765: step 7173, loss 0.029647, acc 1, learning_rate 0.0001
2017-10-11T15:23:14.575750: step 7174, loss 0.0226588, acc 1, learning_rate 0.0001
2017-10-11T15:23:14.902692: step 7175, loss 0.0232584, acc 1, learning_rate 0.0001
2017-10-11T15:23:15.205603: step 7176, loss 0.0272362, acc 1, learning_rate 0.0001
2017-10-11T15:23:15.582129: step 7177, loss 0.0202723, acc 1, learning_rate 0.0001
2017-10-11T15:23:15.843356: step 7178, loss 0.0318471, acc 1, learning_rate 0.0001
2017-10-11T15:23:16.135611: step 7179, loss 0.0669565, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:16.406169: step 7180, loss 0.0259822, acc 1, learning_rate 0.0001
2017-10-11T15:23:16.677730: step 7181, loss 0.0669631, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:16.957014: step 7182, loss 0.0239056, acc 1, learning_rate 0.0001
2017-10-11T15:23:17.228133: step 7183, loss 0.0190355, acc 1, learning_rate 0.0001
2017-10-11T15:23:17.431820: step 7184, loss 0.0293872, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:17.669580: step 7185, loss 0.057357, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:17.905572: step 7186, loss 0.0235881, acc 1, learning_rate 0.0001
2017-10-11T15:23:18.148688: step 7187, loss 0.030617, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:18.436931: step 7188, loss 0.0446606, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:18.711708: step 7189, loss 0.0108222, acc 1, learning_rate 0.0001
2017-10-11T15:23:19.046753: step 7190, loss 0.0326005, acc 1, learning_rate 0.0001
2017-10-11T15:23:19.337211: step 7191, loss 0.0280029, acc 1, learning_rate 0.0001
2017-10-11T15:23:19.579583: step 7192, loss 0.027519, acc 1, learning_rate 0.0001
2017-10-11T15:23:19.897154: step 7193, loss 0.0850845, acc 0.953125, learning_rate 0.0001
2017-10-11T15:23:20.219848: step 7194, loss 0.00580654, acc 1, learning_rate 0.0001
2017-10-11T15:23:20.523298: step 7195, loss 0.0284498, acc 1, learning_rate 0.0001
2017-10-11T15:23:20.845795: step 7196, loss 0.02885, acc 1, learning_rate 0.0001
2017-10-11T15:23:21.149828: step 7197, loss 0.0329208, acc 1, learning_rate 0.0001
2017-10-11T15:23:21.470534: step 7198, loss 0.0845013, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:21.789044: step 7199, loss 0.0173524, acc 1, learning_rate 0.0001
2017-10-11T15:23:22.082313: step 7200, loss 0.0545043, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:23:22.377539: step 7200, loss 0.217104, acc 0.915108

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7200

2017-10-11T15:23:25.779098: step 7201, loss 0.063143, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:26.055767: step 7202, loss 0.0301276, acc 1, learning_rate 0.0001
2017-10-11T15:23:26.340911: step 7203, loss 0.0401056, acc 1, learning_rate 0.0001
2017-10-11T15:23:26.639387: step 7204, loss 0.0604678, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:26.968446: step 7205, loss 0.0145961, acc 1, learning_rate 0.0001
2017-10-11T15:23:27.308977: step 7206, loss 0.0553769, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:27.623995: step 7207, loss 0.0489346, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:27.922446: step 7208, loss 0.0608726, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:28.224718: step 7209, loss 0.0255264, acc 1, learning_rate 0.0001
2017-10-11T15:23:28.529172: step 7210, loss 0.0843912, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:28.818650: step 7211, loss 0.0636298, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:29.105660: step 7212, loss 0.0230133, acc 1, learning_rate 0.0001
2017-10-11T15:23:29.408894: step 7213, loss 0.081326, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:29.707631: step 7214, loss 0.0329816, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:30.051666: step 7215, loss 0.0594901, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:30.322882: step 7216, loss 0.0319359, acc 1, learning_rate 0.0001
2017-10-11T15:23:30.585495: step 7217, loss 0.0915395, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:30.836051: step 7218, loss 0.0542614, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:31.078199: step 7219, loss 0.0405632, acc 1, learning_rate 0.0001
2017-10-11T15:23:31.325704: step 7220, loss 0.0537757, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:31.551908: step 7221, loss 0.0244325, acc 1, learning_rate 0.0001
2017-10-11T15:23:31.857003: step 7222, loss 0.0993256, acc 0.953125, learning_rate 0.0001
2017-10-11T15:23:32.161233: step 7223, loss 0.0264801, acc 1, learning_rate 0.0001
2017-10-11T15:23:32.421872: step 7224, loss 0.0738369, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:32.681138: step 7225, loss 0.0294351, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:32.982329: step 7226, loss 0.0535612, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:33.265893: step 7227, loss 0.0797311, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:33.572292: step 7228, loss 0.0540684, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:33.874029: step 7229, loss 0.117017, acc 0.9375, learning_rate 0.0001
2017-10-11T15:23:34.150269: step 7230, loss 0.148691, acc 0.921875, learning_rate 0.0001
2017-10-11T15:23:34.475866: step 7231, loss 0.0826063, acc 0.953125, learning_rate 0.0001
2017-10-11T15:23:34.792919: step 7232, loss 0.0305173, acc 1, learning_rate 0.0001
2017-10-11T15:23:35.100114: step 7233, loss 0.0547283, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:35.414263: step 7234, loss 0.116144, acc 0.953125, learning_rate 0.0001
2017-10-11T15:23:35.714873: step 7235, loss 0.058831, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:36.006375: step 7236, loss 0.177334, acc 0.921875, learning_rate 0.0001
2017-10-11T15:23:36.320762: step 7237, loss 0.108679, acc 0.953125, learning_rate 0.0001
2017-10-11T15:23:36.618329: step 7238, loss 0.0371638, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:36.923576: step 7239, loss 0.0637843, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:37.248251: step 7240, loss 0.042526, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:23:37.540267: step 7240, loss 0.223453, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7240

2017-10-11T15:23:39.826172: step 7241, loss 0.0333633, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:40.133398: step 7242, loss 0.0510521, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:40.493596: step 7243, loss 0.0300306, acc 1, learning_rate 0.0001
2017-10-11T15:23:40.796770: step 7244, loss 0.0408334, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:41.096041: step 7245, loss 0.0521415, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:41.413232: step 7246, loss 0.0457958, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:41.688919: step 7247, loss 0.0177571, acc 1, learning_rate 0.0001
2017-10-11T15:23:42.009309: step 7248, loss 0.0901596, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:42.358312: step 7249, loss 0.022502, acc 1, learning_rate 0.0001
2017-10-11T15:23:42.666307: step 7250, loss 0.0518086, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:43.024061: step 7251, loss 0.023794, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:43.338846: step 7252, loss 0.0950055, acc 0.960784, learning_rate 0.0001
2017-10-11T15:23:43.664355: step 7253, loss 0.0588944, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:43.985814: step 7254, loss 0.0912187, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:44.270873: step 7255, loss 0.0475122, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:44.571407: step 7256, loss 0.0237324, acc 1, learning_rate 0.0001
2017-10-11T15:23:44.881318: step 7257, loss 0.101032, acc 0.953125, learning_rate 0.0001
2017-10-11T15:23:45.102446: step 7258, loss 0.0188636, acc 1, learning_rate 0.0001
2017-10-11T15:23:45.335606: step 7259, loss 0.0794671, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:45.549589: step 7260, loss 0.0884177, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:45.741029: step 7261, loss 0.0552583, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:45.998338: step 7262, loss 0.0351932, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:46.220358: step 7263, loss 0.0635545, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:46.422131: step 7264, loss 0.0183663, acc 1, learning_rate 0.0001
2017-10-11T15:23:46.721886: step 7265, loss 0.0588582, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:46.997025: step 7266, loss 0.0325301, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:47.332285: step 7267, loss 0.0209993, acc 1, learning_rate 0.0001
2017-10-11T15:23:47.697416: step 7268, loss 0.0498598, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:48.030605: step 7269, loss 0.0218893, acc 1, learning_rate 0.0001
2017-10-11T15:23:48.373429: step 7270, loss 0.0249798, acc 1, learning_rate 0.0001
2017-10-11T15:23:48.675441: step 7271, loss 0.0285775, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:48.981164: step 7272, loss 0.0500409, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:49.313754: step 7273, loss 0.0189023, acc 1, learning_rate 0.0001
2017-10-11T15:23:49.608164: step 7274, loss 0.0868781, acc 0.953125, learning_rate 0.0001
2017-10-11T15:23:49.881151: step 7275, loss 0.123756, acc 0.953125, learning_rate 0.0001
2017-10-11T15:23:50.186845: step 7276, loss 0.0634182, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:50.488576: step 7277, loss 0.0455369, acc 1, learning_rate 0.0001
2017-10-11T15:23:50.820046: step 7278, loss 0.0457673, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:51.119448: step 7279, loss 0.00757411, acc 1, learning_rate 0.0001
2017-10-11T15:23:51.410415: step 7280, loss 0.0457872, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:23:51.674482: step 7280, loss 0.213472, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7280

2017-10-11T15:23:53.887066: step 7281, loss 0.0634368, acc 1, learning_rate 0.0001
2017-10-11T15:23:54.175824: step 7282, loss 0.0618093, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:54.491991: step 7283, loss 0.0985808, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:54.819099: step 7284, loss 0.0274722, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:55.087618: step 7285, loss 0.0221078, acc 1, learning_rate 0.0001
2017-10-11T15:23:55.397859: step 7286, loss 0.168705, acc 0.921875, learning_rate 0.0001
2017-10-11T15:23:55.719812: step 7287, loss 0.0372369, acc 1, learning_rate 0.0001
2017-10-11T15:23:56.031390: step 7288, loss 0.0477962, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:56.288180: step 7289, loss 0.0297848, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:56.578643: step 7290, loss 0.0464512, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:56.845417: step 7291, loss 0.0441332, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:57.123656: step 7292, loss 0.0360438, acc 1, learning_rate 0.0001
2017-10-11T15:23:57.445108: step 7293, loss 0.0510881, acc 0.984375, learning_rate 0.0001
2017-10-11T15:23:57.730796: step 7294, loss 0.00630301, acc 1, learning_rate 0.0001
2017-10-11T15:23:58.027780: step 7295, loss 0.147864, acc 0.9375, learning_rate 0.0001
2017-10-11T15:23:58.323296: step 7296, loss 0.0265419, acc 1, learning_rate 0.0001
2017-10-11T15:23:58.591690: step 7297, loss 0.0793822, acc 0.96875, learning_rate 0.0001
2017-10-11T15:23:58.857145: step 7298, loss 0.0249567, acc 1, learning_rate 0.0001
2017-10-11T15:23:59.096316: step 7299, loss 0.0631556, acc 0.953125, learning_rate 0.0001
2017-10-11T15:23:59.359987: step 7300, loss 0.0182708, acc 1, learning_rate 0.0001
2017-10-11T15:23:59.571832: step 7301, loss 0.0863589, acc 0.953125, learning_rate 0.0001
2017-10-11T15:23:59.793835: step 7302, loss 0.0575837, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:00.081945: step 7303, loss 0.0119954, acc 1, learning_rate 0.0001
2017-10-11T15:24:00.361985: step 7304, loss 0.022678, acc 1, learning_rate 0.0001
2017-10-11T15:24:00.636949: step 7305, loss 0.0299661, acc 1, learning_rate 0.0001
2017-10-11T15:24:00.935588: step 7306, loss 0.11925, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:01.267357: step 7307, loss 0.0909341, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:01.583347: step 7308, loss 0.0521672, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:01.905084: step 7309, loss 0.0191355, acc 1, learning_rate 0.0001
2017-10-11T15:24:02.194305: step 7310, loss 0.0257817, acc 1, learning_rate 0.0001
2017-10-11T15:24:02.479722: step 7311, loss 0.0385417, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:02.811954: step 7312, loss 0.0759463, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:03.093988: step 7313, loss 0.0564854, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:03.401620: step 7314, loss 0.0485026, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:03.677147: step 7315, loss 0.102026, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:04.018746: step 7316, loss 0.0292226, acc 1, learning_rate 0.0001
2017-10-11T15:24:04.339850: step 7317, loss 0.0873187, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:04.660785: step 7318, loss 0.0432188, acc 1, learning_rate 0.0001
2017-10-11T15:24:04.987703: step 7319, loss 0.010477, acc 1, learning_rate 0.0001
2017-10-11T15:24:05.296665: step 7320, loss 0.0853872, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:24:05.596840: step 7320, loss 0.219688, acc 0.919424

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7320

2017-10-11T15:24:07.551005: step 7321, loss 0.0606874, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:07.831549: step 7322, loss 0.0222224, acc 1, learning_rate 0.0001
2017-10-11T15:24:08.070613: step 7323, loss 0.0470745, acc 1, learning_rate 0.0001
2017-10-11T15:24:08.318453: step 7324, loss 0.0338142, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:08.605786: step 7325, loss 0.0687164, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:08.900141: step 7326, loss 0.022481, acc 1, learning_rate 0.0001
2017-10-11T15:24:09.197172: step 7327, loss 0.121131, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:09.487401: step 7328, loss 0.0211298, acc 1, learning_rate 0.0001
2017-10-11T15:24:09.802365: step 7329, loss 0.0468506, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:10.106987: step 7330, loss 0.0213238, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:10.418061: step 7331, loss 0.0414495, acc 1, learning_rate 0.0001
2017-10-11T15:24:10.700119: step 7332, loss 0.108236, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:10.991219: step 7333, loss 0.0186619, acc 1, learning_rate 0.0001
2017-10-11T15:24:11.292819: step 7334, loss 0.101755, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:11.583031: step 7335, loss 0.0124817, acc 1, learning_rate 0.0001
2017-10-11T15:24:11.907579: step 7336, loss 0.0703166, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:12.238859: step 7337, loss 0.104333, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:12.534062: step 7338, loss 0.0775553, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:12.806117: step 7339, loss 0.0872294, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:13.037569: step 7340, loss 0.026453, acc 1, learning_rate 0.0001
2017-10-11T15:24:13.294510: step 7341, loss 0.0203217, acc 1, learning_rate 0.0001
2017-10-11T15:24:13.580449: step 7342, loss 0.205944, acc 0.921875, learning_rate 0.0001
2017-10-11T15:24:13.837833: step 7343, loss 0.0641102, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:14.122404: step 7344, loss 0.0239613, acc 1, learning_rate 0.0001
2017-10-11T15:24:14.381835: step 7345, loss 0.0475096, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:14.659240: step 7346, loss 0.0335808, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:15.005388: step 7347, loss 0.0881346, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:15.321014: step 7348, loss 0.0282606, acc 1, learning_rate 0.0001
2017-10-11T15:24:15.642189: step 7349, loss 0.0259155, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:15.926685: step 7350, loss 0.128019, acc 0.960784, learning_rate 0.0001
2017-10-11T15:24:16.252534: step 7351, loss 0.0330912, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:16.583070: step 7352, loss 0.0725076, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:16.911724: step 7353, loss 0.0316443, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:17.243588: step 7354, loss 0.114705, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:17.565171: step 7355, loss 0.063996, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:17.879524: step 7356, loss 0.0135366, acc 1, learning_rate 0.0001
2017-10-11T15:24:18.162681: step 7357, loss 0.0514037, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:18.486632: step 7358, loss 0.0343544, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:18.810132: step 7359, loss 0.0591916, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:19.113622: step 7360, loss 0.0326926, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:24:19.408917: step 7360, loss 0.218781, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7360

2017-10-11T15:24:21.637233: step 7361, loss 0.0767625, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:21.945364: step 7362, loss 0.012716, acc 1, learning_rate 0.0001
2017-10-11T15:24:22.244290: step 7363, loss 0.0243308, acc 1, learning_rate 0.0001
2017-10-11T15:24:22.561726: step 7364, loss 0.0819321, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:22.836614: step 7365, loss 0.0342629, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:23.162535: step 7366, loss 0.0706616, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:23.477514: step 7367, loss 0.0385634, acc 1, learning_rate 0.0001
2017-10-11T15:24:23.769003: step 7368, loss 0.0280956, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:24.055488: step 7369, loss 0.0434811, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:24.376776: step 7370, loss 0.0533016, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:24.667674: step 7371, loss 0.106011, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:24.980068: step 7372, loss 0.053785, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:25.306107: step 7373, loss 0.0246767, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:25.618991: step 7374, loss 0.0882273, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:25.948906: step 7375, loss 0.0186635, acc 1, learning_rate 0.0001
2017-10-11T15:24:26.219245: step 7376, loss 0.0314835, acc 1, learning_rate 0.0001
2017-10-11T15:24:26.539232: step 7377, loss 0.132861, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:26.833932: step 7378, loss 0.0404772, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:27.086516: step 7379, loss 0.0410558, acc 1, learning_rate 0.0001
2017-10-11T15:24:27.338684: step 7380, loss 0.0441655, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:27.571087: step 7381, loss 0.0437031, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:27.800483: step 7382, loss 0.0285141, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:28.037560: step 7383, loss 0.0622573, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:28.281743: step 7384, loss 0.0295879, acc 1, learning_rate 0.0001
2017-10-11T15:24:28.501738: step 7385, loss 0.0661368, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:28.819005: step 7386, loss 0.0628773, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:29.118053: step 7387, loss 0.0567591, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:29.380407: step 7388, loss 0.0552829, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:29.694902: step 7389, loss 0.0196759, acc 1, learning_rate 0.0001
2017-10-11T15:24:30.009233: step 7390, loss 0.0203778, acc 1, learning_rate 0.0001
2017-10-11T15:24:30.328177: step 7391, loss 0.043431, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:30.643209: step 7392, loss 0.132799, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:30.948171: step 7393, loss 0.0333579, acc 1, learning_rate 0.0001
2017-10-11T15:24:31.254153: step 7394, loss 0.0543785, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:31.572355: step 7395, loss 0.0383175, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:31.893810: step 7396, loss 0.110212, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:32.209244: step 7397, loss 0.0790733, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:32.592241: step 7398, loss 0.135316, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:32.908215: step 7399, loss 0.0728639, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:33.205932: step 7400, loss 0.0425134, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:24:33.482149: step 7400, loss 0.216351, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7400

2017-10-11T15:24:35.664154: step 7401, loss 0.0145411, acc 1, learning_rate 0.0001
2017-10-11T15:24:35.969738: step 7402, loss 0.0450522, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:36.269169: step 7403, loss 0.0902842, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:36.583984: step 7404, loss 0.0662818, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:36.905118: step 7405, loss 0.0664461, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:37.212018: step 7406, loss 0.0322515, acc 1, learning_rate 0.0001
2017-10-11T15:24:37.522791: step 7407, loss 0.046302, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:37.834339: step 7408, loss 0.020909, acc 1, learning_rate 0.0001
2017-10-11T15:24:38.146774: step 7409, loss 0.0308098, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:38.467110: step 7410, loss 0.0301327, acc 1, learning_rate 0.0001
2017-10-11T15:24:38.762724: step 7411, loss 0.0651572, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:39.059287: step 7412, loss 0.104103, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:39.365300: step 7413, loss 0.0327142, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:39.661704: step 7414, loss 0.141778, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:39.990007: step 7415, loss 0.0505513, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:40.321600: step 7416, loss 0.034096, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:40.651577: step 7417, loss 0.115522, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:40.911598: step 7418, loss 0.0226036, acc 1, learning_rate 0.0001
2017-10-11T15:24:41.198800: step 7419, loss 0.0907805, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:41.445616: step 7420, loss 0.0138446, acc 1, learning_rate 0.0001
2017-10-11T15:24:41.682262: step 7421, loss 0.0584207, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:41.926395: step 7422, loss 0.0248131, acc 1, learning_rate 0.0001
2017-10-11T15:24:42.170960: step 7423, loss 0.0843686, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:42.423362: step 7424, loss 0.0872521, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:42.659144: step 7425, loss 0.0342872, acc 1, learning_rate 0.0001
2017-10-11T15:24:42.929916: step 7426, loss 0.0952652, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:43.177395: step 7427, loss 0.0954361, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:43.494939: step 7428, loss 0.0415256, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:43.821117: step 7429, loss 0.0474436, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:44.136123: step 7430, loss 0.0214574, acc 1, learning_rate 0.0001
2017-10-11T15:24:44.429627: step 7431, loss 0.0396203, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:44.720173: step 7432, loss 0.0413299, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:45.036740: step 7433, loss 0.0420256, acc 1, learning_rate 0.0001
2017-10-11T15:24:45.343267: step 7434, loss 0.0247916, acc 1, learning_rate 0.0001
2017-10-11T15:24:45.663272: step 7435, loss 0.0115353, acc 1, learning_rate 0.0001
2017-10-11T15:24:45.962043: step 7436, loss 0.0999169, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:46.258220: step 7437, loss 0.0475669, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:46.598807: step 7438, loss 0.0585802, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:46.898948: step 7439, loss 0.0616362, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:47.204778: step 7440, loss 0.0617143, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:24:47.478397: step 7440, loss 0.216102, acc 0.922302

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7440

2017-10-11T15:24:49.695891: step 7441, loss 0.0334219, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:50.032490: step 7442, loss 0.0258363, acc 1, learning_rate 0.0001
2017-10-11T15:24:50.344510: step 7443, loss 0.0307859, acc 1, learning_rate 0.0001
2017-10-11T15:24:50.663835: step 7444, loss 0.0543827, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:50.981005: step 7445, loss 0.0539598, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:51.315943: step 7446, loss 0.0111239, acc 1, learning_rate 0.0001
2017-10-11T15:24:51.635981: step 7447, loss 0.0456833, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:51.911443: step 7448, loss 0.0540662, acc 0.980392, learning_rate 0.0001
2017-10-11T15:24:52.244871: step 7449, loss 0.0181103, acc 1, learning_rate 0.0001
2017-10-11T15:24:52.562656: step 7450, loss 0.0724859, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:52.853661: step 7451, loss 0.0518038, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:53.146157: step 7452, loss 0.0683047, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:53.440659: step 7453, loss 0.0148702, acc 1, learning_rate 0.0001
2017-10-11T15:24:53.756470: step 7454, loss 0.0344906, acc 1, learning_rate 0.0001
2017-10-11T15:24:54.104133: step 7455, loss 0.0831573, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:54.410077: step 7456, loss 0.0347344, acc 1, learning_rate 0.0001
2017-10-11T15:24:54.733356: step 7457, loss 0.0871946, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:55.020889: step 7458, loss 0.0336156, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:55.275935: step 7459, loss 0.0483883, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:55.497034: step 7460, loss 0.0629054, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:55.731736: step 7461, loss 0.062021, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:55.938789: step 7462, loss 0.068154, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:56.167782: step 7463, loss 0.107121, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:56.406969: step 7464, loss 0.0190724, acc 1, learning_rate 0.0001
2017-10-11T15:24:56.667443: step 7465, loss 0.0680111, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:56.909387: step 7466, loss 0.00983721, acc 1, learning_rate 0.0001
2017-10-11T15:24:57.207205: step 7467, loss 0.0992867, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:57.459167: step 7468, loss 0.0883472, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:57.742704: step 7469, loss 0.0169396, acc 1, learning_rate 0.0001
2017-10-11T15:24:58.011862: step 7470, loss 0.00686924, acc 1, learning_rate 0.0001
2017-10-11T15:24:58.309599: step 7471, loss 0.0129772, acc 1, learning_rate 0.0001
2017-10-11T15:24:58.612799: step 7472, loss 0.0323575, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:58.906006: step 7473, loss 0.0499272, acc 0.96875, learning_rate 0.0001
2017-10-11T15:24:59.184046: step 7474, loss 0.0925602, acc 0.953125, learning_rate 0.0001
2017-10-11T15:24:59.504708: step 7475, loss 0.051828, acc 0.984375, learning_rate 0.0001
2017-10-11T15:24:59.823097: step 7476, loss 0.0342898, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:00.131353: step 7477, loss 0.0262707, acc 1, learning_rate 0.0001
2017-10-11T15:25:00.436631: step 7478, loss 0.0638955, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:00.747383: step 7479, loss 0.0190225, acc 1, learning_rate 0.0001
2017-10-11T15:25:01.051684: step 7480, loss 0.0527133, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:25:01.343733: step 7480, loss 0.223176, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7480

2017-10-11T15:25:03.289303: step 7481, loss 0.0648136, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:03.592089: step 7482, loss 0.0403015, acc 1, learning_rate 0.0001
2017-10-11T15:25:03.875537: step 7483, loss 0.0249756, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:04.168982: step 7484, loss 0.036252, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:04.514200: step 7485, loss 0.0299711, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:04.856359: step 7486, loss 0.0216344, acc 1, learning_rate 0.0001
2017-10-11T15:25:05.189880: step 7487, loss 0.0821979, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:05.522233: step 7488, loss 0.0672074, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:05.826154: step 7489, loss 0.0037046, acc 1, learning_rate 0.0001
2017-10-11T15:25:06.118966: step 7490, loss 0.0319011, acc 1, learning_rate 0.0001
2017-10-11T15:25:06.433241: step 7491, loss 0.0617996, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:06.721904: step 7492, loss 0.0759834, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:06.994726: step 7493, loss 0.0417526, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:07.322458: step 7494, loss 0.0349072, acc 1, learning_rate 0.0001
2017-10-11T15:25:07.613618: step 7495, loss 0.0324661, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:07.967942: step 7496, loss 0.104417, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:08.313340: step 7497, loss 0.0196082, acc 1, learning_rate 0.0001
2017-10-11T15:25:08.639565: step 7498, loss 0.0306746, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:08.915939: step 7499, loss 0.0389929, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:09.198062: step 7500, loss 0.0879801, acc 0.953125, learning_rate 0.0001
2017-10-11T15:25:09.468836: step 7501, loss 0.103734, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:09.731401: step 7502, loss 0.0419595, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:09.995943: step 7503, loss 0.147544, acc 0.953125, learning_rate 0.0001
2017-10-11T15:25:10.243919: step 7504, loss 0.0640237, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:10.499096: step 7505, loss 0.0384976, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:10.736731: step 7506, loss 0.103716, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:10.968270: step 7507, loss 0.066763, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:11.170155: step 7508, loss 0.131138, acc 0.921875, learning_rate 0.0001
2017-10-11T15:25:11.424216: step 7509, loss 0.042729, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:11.727171: step 7510, loss 0.0992484, acc 0.953125, learning_rate 0.0001
2017-10-11T15:25:12.025525: step 7511, loss 0.0346833, acc 1, learning_rate 0.0001
2017-10-11T15:25:12.360868: step 7512, loss 0.0505979, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:12.659257: step 7513, loss 0.0265671, acc 1, learning_rate 0.0001
2017-10-11T15:25:13.279636: step 7514, loss 0.0351531, acc 1, learning_rate 0.0001
2017-10-11T15:25:13.588570: step 7515, loss 0.042129, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:13.910768: step 7516, loss 0.0678261, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:14.227688: step 7517, loss 0.105618, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:14.544556: step 7518, loss 0.0309472, acc 1, learning_rate 0.0001
2017-10-11T15:25:14.861805: step 7519, loss 0.0119152, acc 1, learning_rate 0.0001
2017-10-11T15:25:15.181960: step 7520, loss 0.038031, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:25:15.497911: step 7520, loss 0.221959, acc 0.922302

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7520

2017-10-11T15:25:18.714678: step 7521, loss 0.0121907, acc 1, learning_rate 0.0001
2017-10-11T15:25:19.013038: step 7522, loss 0.10348, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:19.279166: step 7523, loss 0.121739, acc 0.953125, learning_rate 0.0001
2017-10-11T15:25:19.585846: step 7524, loss 0.0814029, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:19.896138: step 7525, loss 0.0226512, acc 1, learning_rate 0.0001
2017-10-11T15:25:20.182169: step 7526, loss 0.0882953, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:20.466402: step 7527, loss 0.0249941, acc 1, learning_rate 0.0001
2017-10-11T15:25:20.797532: step 7528, loss 0.018296, acc 1, learning_rate 0.0001
2017-10-11T15:25:21.079444: step 7529, loss 0.104265, acc 0.9375, learning_rate 0.0001
2017-10-11T15:25:21.366163: step 7530, loss 0.0530353, acc 1, learning_rate 0.0001
2017-10-11T15:25:21.646660: step 7531, loss 0.0778561, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:21.929851: step 7532, loss 0.0383818, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:22.244134: step 7533, loss 0.0248617, acc 1, learning_rate 0.0001
2017-10-11T15:25:22.539652: step 7534, loss 0.0741053, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:22.831994: step 7535, loss 0.0613141, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:23.111826: step 7536, loss 0.0328074, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:23.364994: step 7537, loss 0.0414677, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:23.659148: step 7538, loss 0.0836634, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:23.889218: step 7539, loss 0.0203027, acc 1, learning_rate 0.0001
2017-10-11T15:25:24.126591: step 7540, loss 0.0626196, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:24.395640: step 7541, loss 0.118336, acc 0.953125, learning_rate 0.0001
2017-10-11T15:25:24.652685: step 7542, loss 0.0991662, acc 0.953125, learning_rate 0.0001
2017-10-11T15:25:24.916957: step 7543, loss 0.0199058, acc 1, learning_rate 0.0001
2017-10-11T15:25:25.203354: step 7544, loss 0.0574622, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:25.492202: step 7545, loss 0.0713262, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:25.730445: step 7546, loss 0.0433799, acc 0.980392, learning_rate 0.0001
2017-10-11T15:25:26.037273: step 7547, loss 0.069842, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:26.356071: step 7548, loss 0.042095, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:26.678395: step 7549, loss 0.0560089, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:26.989878: step 7550, loss 0.0383792, acc 1, learning_rate 0.0001
2017-10-11T15:25:27.291730: step 7551, loss 0.0860002, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:27.581339: step 7552, loss 0.0566315, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:27.888943: step 7553, loss 0.0230141, acc 1, learning_rate 0.0001
2017-10-11T15:25:28.193183: step 7554, loss 0.0439199, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:28.513661: step 7555, loss 0.0890846, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:28.816695: step 7556, loss 0.0644569, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:29.156959: step 7557, loss 0.114966, acc 0.953125, learning_rate 0.0001
2017-10-11T15:25:29.453136: step 7558, loss 0.0268343, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:29.730380: step 7559, loss 0.043067, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:30.023449: step 7560, loss 0.0607318, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:25:30.324319: step 7560, loss 0.216233, acc 0.922302

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7560

2017-10-11T15:25:31.812139: step 7561, loss 0.0608357, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:32.110541: step 7562, loss 0.0299356, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:32.391510: step 7563, loss 0.0829944, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:32.693068: step 7564, loss 0.0762352, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:32.994917: step 7565, loss 0.0506052, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:33.287211: step 7566, loss 0.0336319, acc 1, learning_rate 0.0001
2017-10-11T15:25:33.575611: step 7567, loss 0.0336802, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:33.869668: step 7568, loss 0.0175898, acc 1, learning_rate 0.0001
2017-10-11T15:25:34.174660: step 7569, loss 0.0255186, acc 1, learning_rate 0.0001
2017-10-11T15:25:34.518052: step 7570, loss 0.0506472, acc 1, learning_rate 0.0001
2017-10-11T15:25:34.843005: step 7571, loss 0.0185648, acc 1, learning_rate 0.0001
2017-10-11T15:25:35.146579: step 7572, loss 0.0329777, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:35.488866: step 7573, loss 0.0145868, acc 1, learning_rate 0.0001
2017-10-11T15:25:35.767389: step 7574, loss 0.0216146, acc 1, learning_rate 0.0001
2017-10-11T15:25:36.087957: step 7575, loss 0.043543, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:36.388861: step 7576, loss 0.138749, acc 0.953125, learning_rate 0.0001
2017-10-11T15:25:36.729074: step 7577, loss 0.0217671, acc 1, learning_rate 0.0001
2017-10-11T15:25:36.996828: step 7578, loss 0.0559327, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:37.265759: step 7579, loss 0.0437948, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:37.490508: step 7580, loss 0.0457274, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:37.740428: step 7581, loss 0.0496088, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:38.026299: step 7582, loss 0.0171422, acc 1, learning_rate 0.0001
2017-10-11T15:25:38.257324: step 7583, loss 0.0310425, acc 1, learning_rate 0.0001
2017-10-11T15:25:38.496499: step 7584, loss 0.0507637, acc 1, learning_rate 0.0001
2017-10-11T15:25:38.698695: step 7585, loss 0.0504721, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:38.931029: step 7586, loss 0.144209, acc 0.953125, learning_rate 0.0001
2017-10-11T15:25:39.154283: step 7587, loss 0.0146052, acc 1, learning_rate 0.0001
2017-10-11T15:25:39.378060: step 7588, loss 0.0494575, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:39.668075: step 7589, loss 0.0241852, acc 1, learning_rate 0.0001
2017-10-11T15:25:39.960909: step 7590, loss 0.0570146, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:40.221240: step 7591, loss 0.0151924, acc 1, learning_rate 0.0001
2017-10-11T15:25:40.518249: step 7592, loss 0.0247775, acc 1, learning_rate 0.0001
2017-10-11T15:25:40.816004: step 7593, loss 0.0430435, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:41.118428: step 7594, loss 0.0266394, acc 1, learning_rate 0.0001
2017-10-11T15:25:41.406975: step 7595, loss 0.0354861, acc 1, learning_rate 0.0001
2017-10-11T15:25:41.730592: step 7596, loss 0.0762599, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:42.029624: step 7597, loss 0.04186, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:42.339275: step 7598, loss 0.072304, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:42.654856: step 7599, loss 0.015719, acc 1, learning_rate 0.0001
2017-10-11T15:25:42.940171: step 7600, loss 0.0350315, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:25:43.274770: step 7600, loss 0.21746, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7600

2017-10-11T15:25:45.446804: step 7601, loss 0.0368151, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:45.707106: step 7602, loss 0.108005, acc 0.953125, learning_rate 0.0001
2017-10-11T15:25:46.005679: step 7603, loss 0.0515093, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:46.323726: step 7604, loss 0.118728, acc 0.9375, learning_rate 0.0001
2017-10-11T15:25:46.636908: step 7605, loss 0.0317903, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:46.935997: step 7606, loss 0.100953, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:47.245895: step 7607, loss 0.0419487, acc 1, learning_rate 0.0001
2017-10-11T15:25:47.569316: step 7608, loss 0.0457029, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:47.873955: step 7609, loss 0.0474465, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:48.185754: step 7610, loss 0.0610388, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:48.548078: step 7611, loss 0.0443347, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:48.854208: step 7612, loss 0.0136799, acc 1, learning_rate 0.0001
2017-10-11T15:25:49.153374: step 7613, loss 0.0187733, acc 1, learning_rate 0.0001
2017-10-11T15:25:49.502956: step 7614, loss 0.0400777, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:49.798428: step 7615, loss 0.0550959, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:50.127291: step 7616, loss 0.0563635, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:50.458558: step 7617, loss 0.00875401, acc 1, learning_rate 0.0001
2017-10-11T15:25:50.768318: step 7618, loss 0.110135, acc 0.953125, learning_rate 0.0001
2017-10-11T15:25:51.126373: step 7619, loss 0.0111879, acc 1, learning_rate 0.0001
2017-10-11T15:25:51.383804: step 7620, loss 0.0222182, acc 1, learning_rate 0.0001
2017-10-11T15:25:51.649204: step 7621, loss 0.0237439, acc 1, learning_rate 0.0001
2017-10-11T15:25:51.920039: step 7622, loss 0.0979579, acc 0.953125, learning_rate 0.0001
2017-10-11T15:25:52.192029: step 7623, loss 0.176488, acc 0.9375, learning_rate 0.0001
2017-10-11T15:25:52.461148: step 7624, loss 0.0623783, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:52.701315: step 7625, loss 0.023188, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:52.929447: step 7626, loss 0.0666131, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:53.212904: step 7627, loss 0.0444407, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:53.475204: step 7628, loss 0.0434661, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:53.746790: step 7629, loss 0.102257, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:54.049034: step 7630, loss 0.0162234, acc 1, learning_rate 0.0001
2017-10-11T15:25:54.380608: step 7631, loss 0.0378434, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:54.679938: step 7632, loss 0.0174054, acc 1, learning_rate 0.0001
2017-10-11T15:25:54.995567: step 7633, loss 0.100605, acc 0.953125, learning_rate 0.0001
2017-10-11T15:25:55.314738: step 7634, loss 0.0581285, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:55.642293: step 7635, loss 0.0640371, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:55.902579: step 7636, loss 0.0832317, acc 0.96875, learning_rate 0.0001
2017-10-11T15:25:56.211153: step 7637, loss 0.0185504, acc 1, learning_rate 0.0001
2017-10-11T15:25:56.512385: step 7638, loss 0.0177875, acc 1, learning_rate 0.0001
2017-10-11T15:25:56.837901: step 7639, loss 0.0524124, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:57.148019: step 7640, loss 0.0922669, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:25:57.446172: step 7640, loss 0.221252, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7640

2017-10-11T15:25:59.458821: step 7641, loss 0.0637366, acc 0.984375, learning_rate 0.0001
2017-10-11T15:25:59.751361: step 7642, loss 0.06477, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:00.067391: step 7643, loss 0.0487802, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:00.366259: step 7644, loss 0.177197, acc 0.941176, learning_rate 0.0001
2017-10-11T15:26:00.696784: step 7645, loss 0.101133, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:01.025526: step 7646, loss 0.0485753, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:01.333707: step 7647, loss 0.0740405, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:01.653488: step 7648, loss 0.00874268, acc 1, learning_rate 0.0001
2017-10-11T15:26:01.959933: step 7649, loss 0.050956, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:02.258454: step 7650, loss 0.0587845, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:02.559907: step 7651, loss 0.0858695, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:02.847492: step 7652, loss 0.0324668, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:03.161611: step 7653, loss 0.0285265, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:03.426537: step 7654, loss 0.0595297, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:03.703358: step 7655, loss 0.0399061, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:03.995577: step 7656, loss 0.023567, acc 1, learning_rate 0.0001
2017-10-11T15:26:04.331113: step 7657, loss 0.0946394, acc 0.953125, learning_rate 0.0001
2017-10-11T15:26:04.622284: step 7658, loss 0.0585477, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:04.934478: step 7659, loss 0.0321866, acc 1, learning_rate 0.0001
2017-10-11T15:26:05.227981: step 7660, loss 0.012781, acc 1, learning_rate 0.0001
2017-10-11T15:26:05.520442: step 7661, loss 0.0875913, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:05.799186: step 7662, loss 0.0302625, acc 1, learning_rate 0.0001
2017-10-11T15:26:06.077209: step 7663, loss 0.0210396, acc 1, learning_rate 0.0001
2017-10-11T15:26:06.319881: step 7664, loss 0.105275, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:06.549876: step 7665, loss 0.148168, acc 0.921875, learning_rate 0.0001
2017-10-11T15:26:06.767353: step 7666, loss 0.0187197, acc 1, learning_rate 0.0001
2017-10-11T15:26:07.042889: step 7667, loss 0.0424078, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:07.303894: step 7668, loss 0.0464485, acc 1, learning_rate 0.0001
2017-10-11T15:26:07.590808: step 7669, loss 0.0844848, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:07.865784: step 7670, loss 0.0195938, acc 1, learning_rate 0.0001
2017-10-11T15:26:08.176412: step 7671, loss 0.0185905, acc 1, learning_rate 0.0001
2017-10-11T15:26:08.519401: step 7672, loss 0.0643954, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:08.815342: step 7673, loss 0.037849, acc 1, learning_rate 0.0001
2017-10-11T15:26:09.107520: step 7674, loss 0.0604648, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:09.438206: step 7675, loss 0.0520738, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:09.746986: step 7676, loss 0.0674778, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:10.069258: step 7677, loss 0.0428309, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:10.374179: step 7678, loss 0.0630697, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:10.668270: step 7679, loss 0.115644, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:10.996296: step 7680, loss 0.0656658, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T15:26:11.300889: step 7680, loss 0.221792, acc 0.920863

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7680

2017-10-11T15:26:13.960050: step 7681, loss 0.0312969, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:14.270292: step 7682, loss 0.057515, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:14.589590: step 7683, loss 0.0658832, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:14.897785: step 7684, loss 0.0508194, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:15.201793: step 7685, loss 0.0116661, acc 1, learning_rate 0.0001
2017-10-11T15:26:15.518519: step 7686, loss 0.0295002, acc 1, learning_rate 0.0001
2017-10-11T15:26:15.827386: step 7687, loss 0.00919284, acc 1, learning_rate 0.0001
2017-10-11T15:26:16.170168: step 7688, loss 0.0402031, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:16.479383: step 7689, loss 0.0708261, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:16.769405: step 7690, loss 0.0294515, acc 1, learning_rate 0.0001
2017-10-11T15:26:17.091936: step 7691, loss 0.0960794, acc 0.9375, learning_rate 0.0001
2017-10-11T15:26:17.405904: step 7692, loss 0.0185106, acc 1, learning_rate 0.0001
2017-10-11T15:26:17.718698: step 7693, loss 0.15651, acc 0.953125, learning_rate 0.0001
2017-10-11T15:26:18.013214: step 7694, loss 0.0951401, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:18.290638: step 7695, loss 0.0329093, acc 1, learning_rate 0.0001
2017-10-11T15:26:18.612742: step 7696, loss 0.0938021, acc 0.9375, learning_rate 0.0001
2017-10-11T15:26:18.922275: step 7697, loss 0.0671003, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:19.180719: step 7698, loss 0.0451919, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:19.462532: step 7699, loss 0.111357, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:19.743207: step 7700, loss 0.140219, acc 0.9375, learning_rate 0.0001
2017-10-11T15:26:20.014748: step 7701, loss 0.0286175, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:20.302379: step 7702, loss 0.106023, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:20.525357: step 7703, loss 0.0370208, acc 1, learning_rate 0.0001
2017-10-11T15:26:20.779372: step 7704, loss 0.0361532, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:20.991271: step 7705, loss 0.184553, acc 0.953125, learning_rate 0.0001
2017-10-11T15:26:21.234318: step 7706, loss 0.0584341, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:21.432274: step 7707, loss 0.050478, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:21.682019: step 7708, loss 0.0992579, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:21.976199: step 7709, loss 0.0872929, acc 0.9375, learning_rate 0.0001
2017-10-11T15:26:22.269241: step 7710, loss 0.0237485, acc 1, learning_rate 0.0001
2017-10-11T15:26:22.529237: step 7711, loss 0.0127819, acc 1, learning_rate 0.0001
2017-10-11T15:26:22.839105: step 7712, loss 0.0398572, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:23.166449: step 7713, loss 0.0361482, acc 1, learning_rate 0.0001
2017-10-11T15:26:23.467374: step 7714, loss 0.0306689, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:23.730263: step 7715, loss 0.059163, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:24.039010: step 7716, loss 0.013456, acc 1, learning_rate 0.0001
2017-10-11T15:26:24.334801: step 7717, loss 0.0123, acc 1, learning_rate 0.0001
2017-10-11T15:26:24.634878: step 7718, loss 0.0217364, acc 1, learning_rate 0.0001
2017-10-11T15:26:24.982095: step 7719, loss 0.0237912, acc 1, learning_rate 0.0001
2017-10-11T15:26:25.303013: step 7720, loss 0.0322529, acc 1, learning_rate 0.0001

Evaluation:
2017-10-11T15:26:25.551332: step 7720, loss 0.21917, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7720

2017-10-11T15:26:27.791258: step 7721, loss 0.0469605, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:28.059605: step 7722, loss 0.0198599, acc 1, learning_rate 0.0001
2017-10-11T15:26:28.365921: step 7723, loss 0.00850515, acc 1, learning_rate 0.0001
2017-10-11T15:26:28.709570: step 7724, loss 0.0858863, acc 0.953125, learning_rate 0.0001
2017-10-11T15:26:29.010667: step 7725, loss 0.0163939, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:29.344348: step 7726, loss 0.0331825, acc 1, learning_rate 0.0001
2017-10-11T15:26:29.640384: step 7727, loss 0.0606079, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:29.953317: step 7728, loss 0.0273083, acc 1, learning_rate 0.0001
2017-10-11T15:26:30.226890: step 7729, loss 0.0165355, acc 1, learning_rate 0.0001
2017-10-11T15:26:30.504952: step 7730, loss 0.0215896, acc 1, learning_rate 0.0001
2017-10-11T15:26:30.790658: step 7731, loss 0.0329444, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:31.090040: step 7732, loss 0.038096, acc 1, learning_rate 0.0001
2017-10-11T15:26:31.405828: step 7733, loss 0.0660953, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:31.733272: step 7734, loss 0.0775499, acc 0.953125, learning_rate 0.0001
2017-10-11T15:26:32.045255: step 7735, loss 0.0526712, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:32.345977: step 7736, loss 0.0942513, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:32.646556: step 7737, loss 0.0119352, acc 1, learning_rate 0.0001
2017-10-11T15:26:32.942758: step 7738, loss 0.0217187, acc 1, learning_rate 0.0001
2017-10-11T15:26:33.240516: step 7739, loss 0.0206372, acc 1, learning_rate 0.0001
2017-10-11T15:26:33.536415: step 7740, loss 0.0333239, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:33.806437: step 7741, loss 0.0214475, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:34.055711: step 7742, loss 0.0524957, acc 0.980392, learning_rate 0.0001
2017-10-11T15:26:34.330163: step 7743, loss 0.0190246, acc 1, learning_rate 0.0001
2017-10-11T15:26:34.579725: step 7744, loss 0.0172639, acc 1, learning_rate 0.0001
2017-10-11T15:26:34.904179: step 7745, loss 0.0972847, acc 0.953125, learning_rate 0.0001
2017-10-11T15:26:35.233401: step 7746, loss 0.0913403, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:35.508571: step 7747, loss 0.0638917, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:35.784309: step 7748, loss 0.0256731, acc 1, learning_rate 0.0001
2017-10-11T15:26:36.064384: step 7749, loss 0.0239174, acc 1, learning_rate 0.0001
2017-10-11T15:26:36.345974: step 7750, loss 0.0116516, acc 1, learning_rate 0.0001
2017-10-11T15:26:36.647416: step 7751, loss 0.0111823, acc 1, learning_rate 0.0001
2017-10-11T15:26:36.930101: step 7752, loss 0.0331322, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:37.274490: step 7753, loss 0.0551602, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:37.596267: step 7754, loss 0.0308771, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:37.903558: step 7755, loss 0.0732224, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:38.161846: step 7756, loss 0.0336038, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:38.446018: step 7757, loss 0.0955858, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:38.756228: step 7758, loss 0.128588, acc 0.953125, learning_rate 0.0001
2017-10-11T15:26:39.071429: step 7759, loss 0.0401258, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:39.399694: step 7760, loss 0.0731497, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:26:39.715032: step 7760, loss 0.217386, acc 0.916547

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7760

2017-10-11T15:26:41.273697: step 7761, loss 0.0690414, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:41.531570: step 7762, loss 0.0170194, acc 1, learning_rate 0.0001
2017-10-11T15:26:41.795169: step 7763, loss 0.0519224, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:42.058680: step 7764, loss 0.0615292, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:42.424596: step 7765, loss 0.033698, acc 1, learning_rate 0.0001
2017-10-11T15:26:42.772611: step 7766, loss 0.0316267, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:43.145300: step 7767, loss 0.0324298, acc 1, learning_rate 0.0001
2017-10-11T15:26:43.469134: step 7768, loss 0.0375371, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:43.776671: step 7769, loss 0.0692325, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:44.105819: step 7770, loss 0.0406727, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:44.435816: step 7771, loss 0.0428603, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:44.760670: step 7772, loss 0.0110165, acc 1, learning_rate 0.0001
2017-10-11T15:26:45.097390: step 7773, loss 0.102631, acc 0.953125, learning_rate 0.0001
2017-10-11T15:26:45.369419: step 7774, loss 0.0445778, acc 1, learning_rate 0.0001
2017-10-11T15:26:45.690064: step 7775, loss 0.115355, acc 0.953125, learning_rate 0.0001
2017-10-11T15:26:46.040436: step 7776, loss 0.0581597, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:46.315096: step 7777, loss 0.0583648, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:46.611298: step 7778, loss 0.0548814, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:46.855753: step 7779, loss 0.0713607, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:47.129766: step 7780, loss 0.120069, acc 0.9375, learning_rate 0.0001
2017-10-11T15:26:47.409389: step 7781, loss 0.0378716, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:47.694376: step 7782, loss 0.0438847, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:47.972255: step 7783, loss 0.0220107, acc 1, learning_rate 0.0001
2017-10-11T15:26:48.220669: step 7784, loss 0.0753378, acc 0.953125, learning_rate 0.0001
2017-10-11T15:26:48.471402: step 7785, loss 0.00657657, acc 1, learning_rate 0.0001
2017-10-11T15:26:48.717919: step 7786, loss 0.0803644, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:48.994475: step 7787, loss 0.0311327, acc 1, learning_rate 0.0001
2017-10-11T15:26:49.267136: step 7788, loss 0.0674951, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:49.570661: step 7789, loss 0.00846746, acc 1, learning_rate 0.0001
2017-10-11T15:26:49.848679: step 7790, loss 0.013913, acc 1, learning_rate 0.0001
2017-10-11T15:26:50.120934: step 7791, loss 0.101355, acc 0.953125, learning_rate 0.0001
2017-10-11T15:26:50.395227: step 7792, loss 0.0315477, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:50.676618: step 7793, loss 0.0414218, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:50.922323: step 7794, loss 0.100888, acc 0.9375, learning_rate 0.0001
2017-10-11T15:26:51.254884: step 7795, loss 0.0151274, acc 1, learning_rate 0.0001
2017-10-11T15:26:51.587012: step 7796, loss 0.0728952, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:51.861319: step 7797, loss 0.0375908, acc 1, learning_rate 0.0001
2017-10-11T15:26:52.183232: step 7798, loss 0.0192813, acc 1, learning_rate 0.0001
2017-10-11T15:26:52.501395: step 7799, loss 0.0306779, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:52.823104: step 7800, loss 0.088231, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T15:26:53.108809: step 7800, loss 0.218431, acc 0.922302

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7800

2017-10-11T15:26:54.982618: step 7801, loss 0.0339114, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:55.250123: step 7802, loss 0.0143688, acc 1, learning_rate 0.0001
2017-10-11T15:26:55.516616: step 7803, loss 0.0318604, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:55.805678: step 7804, loss 0.019461, acc 1, learning_rate 0.0001
2017-10-11T15:26:56.134708: step 7805, loss 0.0145667, acc 1, learning_rate 0.0001
2017-10-11T15:26:56.446838: step 7806, loss 0.044417, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:56.719684: step 7807, loss 0.0307977, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:57.038981: step 7808, loss 0.0282667, acc 1, learning_rate 0.0001
2017-10-11T15:26:57.352177: step 7809, loss 0.0524245, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:57.644197: step 7810, loss 0.0113582, acc 1, learning_rate 0.0001
2017-10-11T15:26:57.917122: step 7811, loss 0.0888759, acc 0.96875, learning_rate 0.0001
2017-10-11T15:26:58.213263: step 7812, loss 0.0348976, acc 1, learning_rate 0.0001
2017-10-11T15:26:58.509192: step 7813, loss 0.0297242, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:58.817162: step 7814, loss 0.0148416, acc 1, learning_rate 0.0001
2017-10-11T15:26:59.130138: step 7815, loss 0.0370687, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:59.383792: step 7816, loss 0.0501394, acc 0.984375, learning_rate 0.0001
2017-10-11T15:26:59.689611: step 7817, loss 0.0271026, acc 0.984375, learning_rate 0.0001
2017-10-11T15:27:00.077535: step 7818, loss 0.0155645, acc 1, learning_rate 0.0001
2017-10-11T15:27:00.400698: step 7819, loss 0.0501839, acc 0.984375, learning_rate 0.0001
2017-10-11T15:27:00.681198: step 7820, loss 0.1275, acc 0.9375, learning_rate 0.0001
2017-10-11T15:27:00.956469: step 7821, loss 0.0547425, acc 0.96875, learning_rate 0.0001
2017-10-11T15:27:01.236479: step 7822, loss 0.0567216, acc 0.96875, learning_rate 0.0001
2017-10-11T15:27:01.502552: step 7823, loss 0.042842, acc 0.984375, learning_rate 0.0001
2017-10-11T15:27:01.800014: step 7824, loss 0.053184, acc 0.984375, learning_rate 0.0001
2017-10-11T15:27:02.047015: step 7825, loss 0.0240402, acc 0.984375, learning_rate 0.0001
2017-10-11T15:27:02.331896: step 7826, loss 0.0590818, acc 0.984375, learning_rate 0.0001
2017-10-11T15:27:02.625691: step 7827, loss 0.0545072, acc 0.984375, learning_rate 0.0001
2017-10-11T15:27:02.904509: step 7828, loss 0.0381291, acc 0.984375, learning_rate 0.0001
2017-10-11T15:27:03.182057: step 7829, loss 0.00753131, acc 1, learning_rate 0.0001
2017-10-11T15:27:03.474111: step 7830, loss 0.0140613, acc 1, learning_rate 0.0001
2017-10-11T15:27:03.793682: step 7831, loss 0.0539839, acc 0.96875, learning_rate 0.0001
2017-10-11T15:27:04.108589: step 7832, loss 0.0214177, acc 1, learning_rate 0.0001
2017-10-11T15:27:04.396284: step 7833, loss 0.0599211, acc 0.984375, learning_rate 0.0001
2017-10-11T15:27:04.686454: step 7834, loss 0.0800972, acc 0.984375, learning_rate 0.0001
2017-10-11T15:27:05.000159: step 7835, loss 0.0752487, acc 0.96875, learning_rate 0.0001
2017-10-11T15:27:05.335953: step 7836, loss 0.0944755, acc 0.96875, learning_rate 0.0001
2017-10-11T15:27:05.700772: step 7837, loss 0.0411574, acc 1, learning_rate 0.0001
2017-10-11T15:27:06.040082: step 7838, loss 0.0615162, acc 0.984375, learning_rate 0.0001
2017-10-11T15:27:06.379030: step 7839, loss 0.0483629, acc 0.96875, learning_rate 0.0001
2017-10-11T15:27:06.694238: step 7840, loss 0.0850895, acc 0.980392, learning_rate 0.0001

Evaluation:
2017-10-11T15:27:06.993605: step 7840, loss 0.219069, acc 0.917986

Saved model checkpoint to /home/xxliu10/bigdata/runs/1507751131/checkpoints/model-7840

