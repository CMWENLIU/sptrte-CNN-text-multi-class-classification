
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=128

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507664462

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-10T14:41:07.460152: step 1, loss 7.14724, acc 0.109375, learning_rate 0.005
2017-10-10T14:41:07.667495: step 2, loss 5.31286, acc 0.234375, learning_rate 0.00498
2017-10-10T14:41:07.832932: step 3, loss 4.27311, acc 0.265625, learning_rate 0.00496008
2017-10-10T14:41:07.996962: step 4, loss 4.18206, acc 0.359375, learning_rate 0.00494024
2017-10-10T14:41:08.200233: step 5, loss 5.12274, acc 0.265625, learning_rate 0.00492049
2017-10-10T14:41:08.354492: step 6, loss 4.08915, acc 0.40625, learning_rate 0.00490081
2017-10-10T14:41:08.497439: step 7, loss 3.86791, acc 0.25, learning_rate 0.00488121
2017-10-10T14:41:08.676866: step 8, loss 3.8007, acc 0.390625, learning_rate 0.0048617
2017-10-10T14:41:08.878202: step 9, loss 2.79432, acc 0.46875, learning_rate 0.00484226
2017-10-10T14:41:09.077118: step 10, loss 3.17991, acc 0.359375, learning_rate 0.00482291
2017-10-10T14:41:09.256887: step 11, loss 2.4889, acc 0.546875, learning_rate 0.00480363
2017-10-10T14:41:09.440957: step 12, loss 2.50289, acc 0.453125, learning_rate 0.00478443
2017-10-10T14:41:09.600982: step 13, loss 2.43933, acc 0.453125, learning_rate 0.00476531
2017-10-10T14:41:09.776669: step 14, loss 1.93532, acc 0.578125, learning_rate 0.00474627
2017-10-10T14:41:10.001814: step 15, loss 3.01653, acc 0.328125, learning_rate 0.0047273
2017-10-10T14:41:10.158011: step 16, loss 1.50834, acc 0.578125, learning_rate 0.00470841
2017-10-10T14:41:10.312835: step 17, loss 1.59989, acc 0.546875, learning_rate 0.0046896
2017-10-10T14:41:10.510744: step 18, loss 1.5402, acc 0.53125, learning_rate 0.00467087
2017-10-10T14:41:10.637804: step 19, loss 2.45741, acc 0.4375, learning_rate 0.00465221
2017-10-10T14:41:10.808235: step 20, loss 2.17135, acc 0.5, learning_rate 0.00463363
2017-10-10T14:41:10.974506: step 21, loss 1.67418, acc 0.59375, learning_rate 0.00461513
2017-10-10T14:41:11.113209: step 22, loss 1.8978, acc 0.609375, learning_rate 0.0045967
2017-10-10T14:41:11.296574: step 23, loss 1.29854, acc 0.65625, learning_rate 0.00457834
2017-10-10T14:41:11.494966: step 24, loss 1.93504, acc 0.546875, learning_rate 0.00456006
2017-10-10T14:41:11.625084: step 25, loss 2.02722, acc 0.546875, learning_rate 0.00454186
2017-10-10T14:41:11.819196: step 26, loss 1.35289, acc 0.6875, learning_rate 0.00452373
2017-10-10T14:41:11.977089: step 27, loss 1.47296, acc 0.625, learning_rate 0.00450567
2017-10-10T14:41:12.203929: step 28, loss 1.37216, acc 0.640625, learning_rate 0.00448769
2017-10-10T14:41:12.419600: step 29, loss 0.979098, acc 0.65625, learning_rate 0.00446978
2017-10-10T14:41:12.538005: step 30, loss 1.2785, acc 0.640625, learning_rate 0.00445194
2017-10-10T14:41:12.741525: step 31, loss 1.64728, acc 0.625, learning_rate 0.00443418
2017-10-10T14:41:12.890027: step 32, loss 1.73654, acc 0.609375, learning_rate 0.00441649
2017-10-10T14:41:13.037211: step 33, loss 1.3767, acc 0.671875, learning_rate 0.00439887
2017-10-10T14:41:13.197669: step 34, loss 1.44132, acc 0.625, learning_rate 0.00438132
2017-10-10T14:41:13.468685: step 35, loss 1.09072, acc 0.65625, learning_rate 0.00436385
2017-10-10T14:41:13.644793: step 36, loss 1.29328, acc 0.65625, learning_rate 0.00434644
2017-10-10T14:41:13.793742: step 37, loss 1.54344, acc 0.59375, learning_rate 0.00432911
2017-10-10T14:41:13.947934: step 38, loss 1.23376, acc 0.5625, learning_rate 0.00431185
2017-10-10T14:41:14.064931: step 39, loss 1.39232, acc 0.609375, learning_rate 0.00429465
2017-10-10T14:41:14.169626: step 40, loss 1.36811, acc 0.65625, learning_rate 0.00427753

Evaluation:
2017-10-10T14:41:14.400578: step 40, loss 0.360141, acc 0.871942

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-40

2017-10-10T14:41:15.143478: step 41, loss 1.25104, acc 0.65625, learning_rate 0.00426048
2017-10-10T14:41:15.305537: step 42, loss 0.998487, acc 0.75, learning_rate 0.0042435
2017-10-10T14:41:15.509036: step 43, loss 0.95918, acc 0.71875, learning_rate 0.00422659
2017-10-10T14:41:15.727777: step 44, loss 0.892323, acc 0.765625, learning_rate 0.00420974
2017-10-10T14:41:15.860870: step 45, loss 0.95289, acc 0.71875, learning_rate 0.00419297
2017-10-10T14:41:15.964095: step 46, loss 1.77519, acc 0.625, learning_rate 0.00417626
2017-10-10T14:41:16.111901: step 47, loss 0.782542, acc 0.75, learning_rate 0.00415962
2017-10-10T14:41:16.219994: step 48, loss 1.30651, acc 0.671875, learning_rate 0.00414305
2017-10-10T14:41:16.341072: step 49, loss 0.830224, acc 0.75, learning_rate 0.00412655
2017-10-10T14:41:16.491578: step 50, loss 0.80848, acc 0.671875, learning_rate 0.00411011
2017-10-10T14:41:16.592498: step 51, loss 1.27922, acc 0.671875, learning_rate 0.00409375
2017-10-10T14:41:16.749326: step 52, loss 0.914356, acc 0.75, learning_rate 0.00407744
2017-10-10T14:41:16.924396: step 53, loss 1.04753, acc 0.78125, learning_rate 0.00406121
2017-10-10T14:41:17.112939: step 54, loss 0.781428, acc 0.734375, learning_rate 0.00404504
2017-10-10T14:41:17.258080: step 55, loss 0.759947, acc 0.765625, learning_rate 0.00402894
2017-10-10T14:41:17.427347: step 56, loss 0.916227, acc 0.75, learning_rate 0.0040129
2017-10-10T14:41:17.598572: step 57, loss 1.27638, acc 0.6875, learning_rate 0.00399693
2017-10-10T14:41:17.758436: step 58, loss 0.722499, acc 0.75, learning_rate 0.00398102
2017-10-10T14:41:17.885383: step 59, loss 0.721618, acc 0.8125, learning_rate 0.00396518
2017-10-10T14:41:18.062169: step 60, loss 0.964523, acc 0.625, learning_rate 0.00394941
2017-10-10T14:41:18.193153: step 61, loss 1.09381, acc 0.703125, learning_rate 0.00393369
2017-10-10T14:41:18.351431: step 62, loss 0.672767, acc 0.78125, learning_rate 0.00391804
2017-10-10T14:41:18.570055: step 63, loss 0.879908, acc 0.75, learning_rate 0.00390246
2017-10-10T14:41:18.753118: step 64, loss 1.08567, acc 0.6875, learning_rate 0.00388694
2017-10-10T14:41:18.917127: step 65, loss 0.598522, acc 0.796875, learning_rate 0.00387148
2017-10-10T14:41:19.111825: step 66, loss 0.896505, acc 0.734375, learning_rate 0.00385609
2017-10-10T14:41:19.279778: step 67, loss 0.47415, acc 0.8125, learning_rate 0.00384076
2017-10-10T14:41:19.455309: step 68, loss 0.454419, acc 0.84375, learning_rate 0.00382549
2017-10-10T14:41:19.663971: step 69, loss 0.866631, acc 0.734375, learning_rate 0.00381028
2017-10-10T14:41:19.801089: step 70, loss 0.690893, acc 0.796875, learning_rate 0.00379514
2017-10-10T14:41:19.969385: step 71, loss 0.731759, acc 0.765625, learning_rate 0.00378005
2017-10-10T14:41:20.165115: step 72, loss 0.791905, acc 0.765625, learning_rate 0.00376503
2017-10-10T14:41:20.266051: step 73, loss 0.762272, acc 0.796875, learning_rate 0.00375007
2017-10-10T14:41:20.456816: step 74, loss 0.768874, acc 0.765625, learning_rate 0.00373517
2017-10-10T14:41:20.601107: step 75, loss 0.532475, acc 0.8125, learning_rate 0.00372034
2017-10-10T14:41:20.763447: step 76, loss 0.618893, acc 0.78125, learning_rate 0.00370556
2017-10-10T14:41:20.940551: step 77, loss 0.595651, acc 0.828125, learning_rate 0.00369084
2017-10-10T14:41:21.072967: step 78, loss 0.380184, acc 0.828125, learning_rate 0.00367619
2017-10-10T14:41:21.248813: step 79, loss 0.811652, acc 0.78125, learning_rate 0.00366159
2017-10-10T14:41:21.415470: step 80, loss 0.737202, acc 0.765625, learning_rate 0.00364705

Evaluation:
2017-10-10T14:41:21.683384: step 80, loss 0.328058, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-80

2017-10-10T14:41:22.578872: step 81, loss 0.877412, acc 0.71875, learning_rate 0.00363257
2017-10-10T14:41:22.724862: step 82, loss 0.532123, acc 0.84375, learning_rate 0.00361815
2017-10-10T14:41:22.898665: step 83, loss 0.711735, acc 0.796875, learning_rate 0.00360379
2017-10-10T14:41:23.113053: step 84, loss 0.594494, acc 0.8125, learning_rate 0.00358949
2017-10-10T14:41:23.274166: step 85, loss 0.909121, acc 0.765625, learning_rate 0.00357525
2017-10-10T14:41:23.436981: step 86, loss 0.413998, acc 0.875, learning_rate 0.00356106
2017-10-10T14:41:23.574498: step 87, loss 0.968487, acc 0.71875, learning_rate 0.00354694
2017-10-10T14:41:23.768817: step 88, loss 0.739927, acc 0.765625, learning_rate 0.00353287
2017-10-10T14:41:23.953032: step 89, loss 0.652977, acc 0.765625, learning_rate 0.00351885
2017-10-10T14:41:24.127416: step 90, loss 0.59515, acc 0.8125, learning_rate 0.0035049
2017-10-10T14:41:24.260845: step 91, loss 0.807146, acc 0.796875, learning_rate 0.003491
2017-10-10T14:41:24.441664: step 92, loss 0.597781, acc 0.828125, learning_rate 0.00347716
2017-10-10T14:41:24.617264: step 93, loss 0.715536, acc 0.765625, learning_rate 0.00346338
2017-10-10T14:41:24.813033: step 94, loss 0.503951, acc 0.734375, learning_rate 0.00344965
2017-10-10T14:41:25.062944: step 95, loss 0.618826, acc 0.8125, learning_rate 0.00343597
2017-10-10T14:41:25.197008: step 96, loss 0.447888, acc 0.828125, learning_rate 0.00342236
2017-10-10T14:41:25.324837: step 97, loss 0.983946, acc 0.765625, learning_rate 0.0034088
2017-10-10T14:41:25.454511: step 98, loss 0.783283, acc 0.784314, learning_rate 0.00339529
2017-10-10T14:41:25.580814: step 99, loss 0.662965, acc 0.796875, learning_rate 0.00338184
2017-10-10T14:41:25.678784: step 100, loss 0.691817, acc 0.796875, learning_rate 0.00336844
2017-10-10T14:41:25.825098: step 101, loss 0.602518, acc 0.796875, learning_rate 0.0033551
2017-10-10T14:41:25.965031: step 102, loss 0.501794, acc 0.796875, learning_rate 0.00334182
2017-10-10T14:41:26.176956: step 103, loss 0.352587, acc 0.859375, learning_rate 0.00332858
2017-10-10T14:41:26.377172: step 104, loss 0.364876, acc 0.875, learning_rate 0.00331541
2017-10-10T14:41:26.565149: step 105, loss 0.653677, acc 0.765625, learning_rate 0.00330228
2017-10-10T14:41:26.676931: step 106, loss 0.574961, acc 0.828125, learning_rate 0.00328921
2017-10-10T14:41:26.798261: step 107, loss 0.864132, acc 0.78125, learning_rate 0.00327619
2017-10-10T14:41:26.953677: step 108, loss 0.327564, acc 0.875, learning_rate 0.00326323
2017-10-10T14:41:27.052978: step 109, loss 0.325432, acc 0.875, learning_rate 0.00325032
2017-10-10T14:41:27.170964: step 110, loss 0.757704, acc 0.796875, learning_rate 0.00323746
2017-10-10T14:41:27.322058: step 111, loss 0.614997, acc 0.828125, learning_rate 0.00322465
2017-10-10T14:41:27.449167: step 112, loss 0.305506, acc 0.859375, learning_rate 0.0032119
2017-10-10T14:41:27.611930: step 113, loss 0.494999, acc 0.8125, learning_rate 0.0031992
2017-10-10T14:41:27.782700: step 114, loss 0.550736, acc 0.78125, learning_rate 0.00318655
2017-10-10T14:41:27.924876: step 115, loss 0.415958, acc 0.828125, learning_rate 0.00317395
2017-10-10T14:41:28.136583: step 116, loss 0.758895, acc 0.75, learning_rate 0.0031614
2017-10-10T14:41:28.258171: step 117, loss 0.552382, acc 0.78125, learning_rate 0.0031489
2017-10-10T14:41:28.436997: step 118, loss 0.393857, acc 0.875, learning_rate 0.00313646
2017-10-10T14:41:28.637779: step 119, loss 0.622779, acc 0.765625, learning_rate 0.00312407
2017-10-10T14:41:28.816656: step 120, loss 0.590505, acc 0.84375, learning_rate 0.00311172

Evaluation:
2017-10-10T14:41:29.083537: step 120, loss 0.313818, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-120

2017-10-10T14:41:29.956841: step 121, loss 0.158374, acc 0.9375, learning_rate 0.00309943
2017-10-10T14:41:30.156803: step 122, loss 0.473137, acc 0.84375, learning_rate 0.00308719
2017-10-10T14:41:30.287238: step 123, loss 0.501984, acc 0.796875, learning_rate 0.00307499
2017-10-10T14:41:30.455917: step 124, loss 0.724358, acc 0.78125, learning_rate 0.00306285
2017-10-10T14:41:30.652862: step 125, loss 0.507957, acc 0.8125, learning_rate 0.00305076
2017-10-10T14:41:30.787546: step 126, loss 0.417516, acc 0.84375, learning_rate 0.00303871
2017-10-10T14:41:30.981593: step 127, loss 0.495981, acc 0.828125, learning_rate 0.00302672
2017-10-10T14:41:31.160576: step 128, loss 0.501514, acc 0.734375, learning_rate 0.00301477
2017-10-10T14:41:31.297329: step 129, loss 0.711089, acc 0.765625, learning_rate 0.00300287
2017-10-10T14:41:31.467804: step 130, loss 0.583628, acc 0.828125, learning_rate 0.00299102
2017-10-10T14:41:31.611756: step 131, loss 0.727823, acc 0.765625, learning_rate 0.00297922
2017-10-10T14:41:31.789735: step 132, loss 0.403717, acc 0.859375, learning_rate 0.00296747
2017-10-10T14:41:31.938012: step 133, loss 0.416088, acc 0.84375, learning_rate 0.00295577
2017-10-10T14:41:32.122363: step 134, loss 0.355129, acc 0.875, learning_rate 0.00294411
2017-10-10T14:41:32.338782: step 135, loss 0.375282, acc 0.859375, learning_rate 0.0029325
2017-10-10T14:41:32.484468: step 136, loss 0.436575, acc 0.84375, learning_rate 0.00292094
2017-10-10T14:41:32.644089: step 137, loss 0.495035, acc 0.796875, learning_rate 0.00290943
2017-10-10T14:41:32.844400: step 138, loss 0.666948, acc 0.765625, learning_rate 0.00289796
2017-10-10T14:41:32.980724: step 139, loss 0.527727, acc 0.765625, learning_rate 0.00288654
2017-10-10T14:41:33.162056: step 140, loss 0.247265, acc 0.9375, learning_rate 0.00287516
2017-10-10T14:41:33.344875: step 141, loss 0.331546, acc 0.84375, learning_rate 0.00286384
2017-10-10T14:41:33.464221: step 142, loss 0.498583, acc 0.796875, learning_rate 0.00285256
2017-10-10T14:41:33.671855: step 143, loss 0.69553, acc 0.734375, learning_rate 0.00284132
2017-10-10T14:41:33.847833: step 144, loss 0.363874, acc 0.859375, learning_rate 0.00283013
2017-10-10T14:41:33.984860: step 145, loss 0.300785, acc 0.890625, learning_rate 0.00281899
2017-10-10T14:41:34.178630: step 146, loss 0.683212, acc 0.78125, learning_rate 0.00280789
2017-10-10T14:41:34.376936: step 147, loss 0.665467, acc 0.796875, learning_rate 0.00279684
2017-10-10T14:41:34.549070: step 148, loss 0.358978, acc 0.84375, learning_rate 0.00278583
2017-10-10T14:41:34.704356: step 149, loss 0.36768, acc 0.890625, learning_rate 0.00277486
2017-10-10T14:41:34.892938: step 150, loss 0.779195, acc 0.734375, learning_rate 0.00276395
2017-10-10T14:41:35.020926: step 151, loss 0.503043, acc 0.859375, learning_rate 0.00275307
2017-10-10T14:41:35.208827: step 152, loss 0.484509, acc 0.84375, learning_rate 0.00274224
2017-10-10T14:41:35.392552: step 153, loss 0.310923, acc 0.890625, learning_rate 0.00273146
2017-10-10T14:41:35.532160: step 154, loss 0.348885, acc 0.890625, learning_rate 0.00272072
2017-10-10T14:41:35.690075: step 155, loss 0.327386, acc 0.953125, learning_rate 0.00271002
2017-10-10T14:41:35.889087: step 156, loss 0.528874, acc 0.875, learning_rate 0.00269937
2017-10-10T14:41:36.048920: step 157, loss 0.250976, acc 0.921875, learning_rate 0.00268876
2017-10-10T14:41:36.225564: step 158, loss 0.347494, acc 0.890625, learning_rate 0.00267819
2017-10-10T14:41:36.432716: step 159, loss 0.592325, acc 0.84375, learning_rate 0.00266767
2017-10-10T14:41:36.628872: step 160, loss 0.395114, acc 0.890625, learning_rate 0.00265719

Evaluation:
2017-10-10T14:41:36.934880: step 160, loss 0.30755, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-160

2017-10-10T14:41:37.612502: step 161, loss 0.185746, acc 0.921875, learning_rate 0.00264675
2017-10-10T14:41:37.749466: step 162, loss 0.715853, acc 0.8125, learning_rate 0.00263635
2017-10-10T14:41:37.857139: step 163, loss 0.567527, acc 0.765625, learning_rate 0.002626
2017-10-10T14:41:37.974861: step 164, loss 0.392077, acc 0.859375, learning_rate 0.00261569
2017-10-10T14:41:38.106520: step 165, loss 0.397781, acc 0.890625, learning_rate 0.00260542
2017-10-10T14:41:38.271093: step 166, loss 0.420126, acc 0.8125, learning_rate 0.0025952
2017-10-10T14:41:38.444852: step 167, loss 0.204825, acc 0.921875, learning_rate 0.00258501
2017-10-10T14:41:38.576841: step 168, loss 0.384641, acc 0.84375, learning_rate 0.00257487
2017-10-10T14:41:38.788959: step 169, loss 0.277585, acc 0.9375, learning_rate 0.00256477
2017-10-10T14:41:38.913034: step 170, loss 0.43422, acc 0.859375, learning_rate 0.0025547
2017-10-10T14:41:39.068334: step 171, loss 0.433391, acc 0.84375, learning_rate 0.00254469
2017-10-10T14:41:39.268524: step 172, loss 0.539101, acc 0.796875, learning_rate 0.00253471
2017-10-10T14:41:39.372251: step 173, loss 0.534316, acc 0.828125, learning_rate 0.00252477
2017-10-10T14:41:39.572258: step 174, loss 0.407804, acc 0.84375, learning_rate 0.00251487
2017-10-10T14:41:39.692592: step 175, loss 0.307249, acc 0.890625, learning_rate 0.00250501
2017-10-10T14:41:39.863189: step 176, loss 0.310421, acc 0.875, learning_rate 0.0024952
2017-10-10T14:41:40.085102: step 177, loss 0.315159, acc 0.921875, learning_rate 0.00248542
2017-10-10T14:41:40.266999: step 178, loss 0.441011, acc 0.84375, learning_rate 0.00247568
2017-10-10T14:41:40.416511: step 179, loss 0.320791, acc 0.8125, learning_rate 0.00246599
2017-10-10T14:41:40.594279: step 180, loss 0.412585, acc 0.84375, learning_rate 0.00245633
2017-10-10T14:41:40.744251: step 181, loss 0.551369, acc 0.8125, learning_rate 0.00244671
2017-10-10T14:41:40.922497: step 182, loss 0.80873, acc 0.765625, learning_rate 0.00243713
2017-10-10T14:41:41.107342: step 183, loss 0.546708, acc 0.859375, learning_rate 0.00242759
2017-10-10T14:41:41.300847: step 184, loss 0.548163, acc 0.8125, learning_rate 0.00241809
2017-10-10T14:41:41.442181: step 185, loss 0.484996, acc 0.859375, learning_rate 0.00240863
2017-10-10T14:41:41.604812: step 186, loss 0.280188, acc 0.859375, learning_rate 0.00239921
2017-10-10T14:41:41.777123: step 187, loss 0.371658, acc 0.875, learning_rate 0.00238982
2017-10-10T14:41:41.911407: step 188, loss 0.301139, acc 0.90625, learning_rate 0.00238048
2017-10-10T14:41:42.080657: step 189, loss 0.285177, acc 0.890625, learning_rate 0.00237117
2017-10-10T14:41:42.241308: step 190, loss 0.333337, acc 0.875, learning_rate 0.0023619
2017-10-10T14:41:42.446718: step 191, loss 0.478385, acc 0.875, learning_rate 0.00235267
2017-10-10T14:41:42.608882: step 192, loss 0.517242, acc 0.875, learning_rate 0.00234347
2017-10-10T14:41:42.756628: step 193, loss 0.2281, acc 0.921875, learning_rate 0.00233431
2017-10-10T14:41:42.942919: step 194, loss 0.398093, acc 0.875, learning_rate 0.00232519
2017-10-10T14:41:43.097519: step 195, loss 0.372255, acc 0.875, learning_rate 0.00231611
2017-10-10T14:41:43.267637: step 196, loss 0.40974, acc 0.843137, learning_rate 0.00230707
2017-10-10T14:41:43.464838: step 197, loss 0.3066, acc 0.859375, learning_rate 0.00229806
2017-10-10T14:41:43.572312: step 198, loss 0.331702, acc 0.84375, learning_rate 0.00228908
2017-10-10T14:41:43.773296: step 199, loss 0.184511, acc 0.953125, learning_rate 0.00228015
2017-10-10T14:41:43.894213: step 200, loss 0.451165, acc 0.8125, learning_rate 0.00227125

Evaluation:
2017-10-10T14:41:44.137656: step 200, loss 0.293654, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-200

2017-10-10T14:41:44.839068: step 201, loss 0.382608, acc 0.828125, learning_rate 0.00226239
2017-10-10T14:41:45.048831: step 202, loss 0.423477, acc 0.890625, learning_rate 0.00225356
2017-10-10T14:41:45.201010: step 203, loss 0.334353, acc 0.859375, learning_rate 0.00224477
2017-10-10T14:41:45.365557: step 204, loss 0.368952, acc 0.84375, learning_rate 0.00223602
2017-10-10T14:41:45.554794: step 205, loss 0.215901, acc 0.90625, learning_rate 0.0022273
2017-10-10T14:41:45.703955: step 206, loss 0.342883, acc 0.9375, learning_rate 0.00221862
2017-10-10T14:41:45.850105: step 207, loss 0.333129, acc 0.890625, learning_rate 0.00220997
2017-10-10T14:41:46.070588: step 208, loss 0.294616, acc 0.90625, learning_rate 0.00220136
2017-10-10T14:41:46.252988: step 209, loss 0.326609, acc 0.890625, learning_rate 0.00219278
2017-10-10T14:41:46.368945: step 210, loss 0.342545, acc 0.859375, learning_rate 0.00218424
2017-10-10T14:41:46.584544: step 211, loss 0.413883, acc 0.890625, learning_rate 0.00217573
2017-10-10T14:41:46.745133: step 212, loss 0.207588, acc 0.90625, learning_rate 0.00216726
2017-10-10T14:41:46.888614: step 213, loss 0.252288, acc 0.921875, learning_rate 0.00215882
2017-10-10T14:41:47.066475: step 214, loss 0.469124, acc 0.84375, learning_rate 0.00215041
2017-10-10T14:41:47.239929: step 215, loss 0.415123, acc 0.828125, learning_rate 0.00214204
2017-10-10T14:41:47.396932: step 216, loss 0.409332, acc 0.84375, learning_rate 0.00213371
2017-10-10T14:41:47.531667: step 217, loss 0.308098, acc 0.875, learning_rate 0.00212541
2017-10-10T14:41:47.700756: step 218, loss 0.457699, acc 0.921875, learning_rate 0.00211714
2017-10-10T14:41:47.817205: step 219, loss 0.525927, acc 0.859375, learning_rate 0.00210891
2017-10-10T14:41:48.006558: step 220, loss 0.274345, acc 0.875, learning_rate 0.00210071
2017-10-10T14:41:48.201042: step 221, loss 0.346757, acc 0.859375, learning_rate 0.00209254
2017-10-10T14:41:48.403545: step 222, loss 0.246307, acc 0.9375, learning_rate 0.00208441
2017-10-10T14:41:48.573778: step 223, loss 0.310401, acc 0.84375, learning_rate 0.00207631
2017-10-10T14:41:48.749101: step 224, loss 0.425415, acc 0.796875, learning_rate 0.00206824
2017-10-10T14:41:48.835778: step 225, loss 0.383803, acc 0.90625, learning_rate 0.00206021
2017-10-10T14:41:48.921722: step 226, loss 0.258938, acc 0.921875, learning_rate 0.00205221
2017-10-10T14:41:49.007679: step 227, loss 0.34368, acc 0.859375, learning_rate 0.00204424
2017-10-10T14:41:49.093457: step 228, loss 0.587821, acc 0.78125, learning_rate 0.0020363
2017-10-10T14:41:49.186936: step 229, loss 0.335172, acc 0.90625, learning_rate 0.0020284
2017-10-10T14:41:49.276157: step 230, loss 0.39385, acc 0.828125, learning_rate 0.00202053
2017-10-10T14:41:49.486822: step 231, loss 0.341418, acc 0.84375, learning_rate 0.00201269
2017-10-10T14:41:49.620446: step 232, loss 0.344972, acc 0.875, learning_rate 0.00200488
2017-10-10T14:41:49.772491: step 233, loss 0.214418, acc 0.9375, learning_rate 0.00199711
2017-10-10T14:41:49.993061: step 234, loss 0.366595, acc 0.90625, learning_rate 0.00198936
2017-10-10T14:41:50.130225: step 235, loss 0.212333, acc 0.90625, learning_rate 0.00198165
2017-10-10T14:41:50.291672: step 236, loss 0.266406, acc 0.921875, learning_rate 0.00197397
2017-10-10T14:41:50.499478: step 237, loss 0.650642, acc 0.84375, learning_rate 0.00196632
2017-10-10T14:41:50.619866: step 238, loss 0.419407, acc 0.84375, learning_rate 0.0019587
2017-10-10T14:41:50.801825: step 239, loss 0.458236, acc 0.84375, learning_rate 0.00195112
2017-10-10T14:41:51.000919: step 240, loss 0.321105, acc 0.890625, learning_rate 0.00194356

Evaluation:
2017-10-10T14:41:51.268007: step 240, loss 0.276305, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-240

2017-10-10T14:41:53.510848: step 241, loss 0.202128, acc 0.96875, learning_rate 0.00193604
2017-10-10T14:41:53.683159: step 242, loss 0.372922, acc 0.859375, learning_rate 0.00192854
2017-10-10T14:41:53.873120: step 243, loss 0.429253, acc 0.828125, learning_rate 0.00192108
2017-10-10T14:41:54.064022: step 244, loss 0.379874, acc 0.859375, learning_rate 0.00191364
2017-10-10T14:41:54.241034: step 245, loss 0.561638, acc 0.796875, learning_rate 0.00190624
2017-10-10T14:41:54.376038: step 246, loss 0.316488, acc 0.890625, learning_rate 0.00189887
2017-10-10T14:41:54.574165: step 247, loss 0.461961, acc 0.84375, learning_rate 0.00189153
2017-10-10T14:41:54.760871: step 248, loss 0.342167, acc 0.890625, learning_rate 0.00188421
2017-10-10T14:41:54.909142: step 249, loss 0.419002, acc 0.828125, learning_rate 0.00187693
2017-10-10T14:41:55.075335: step 250, loss 0.325471, acc 0.90625, learning_rate 0.00186968
2017-10-10T14:41:55.285694: step 251, loss 0.264629, acc 0.890625, learning_rate 0.00186245
2017-10-10T14:41:55.413797: step 252, loss 0.141786, acc 0.953125, learning_rate 0.00185526
2017-10-10T14:41:55.587118: step 253, loss 0.405439, acc 0.875, learning_rate 0.0018481
2017-10-10T14:41:55.776905: step 254, loss 0.29295, acc 0.890625, learning_rate 0.00184096
2017-10-10T14:41:55.929088: step 255, loss 0.223663, acc 0.9375, learning_rate 0.00183385
2017-10-10T14:41:56.088846: step 256, loss 0.357898, acc 0.890625, learning_rate 0.00182678
2017-10-10T14:41:56.300506: step 257, loss 0.27648, acc 0.921875, learning_rate 0.00181973
2017-10-10T14:41:56.457031: step 258, loss 0.333682, acc 0.859375, learning_rate 0.00181271
2017-10-10T14:41:56.617261: step 259, loss 0.260493, acc 0.9375, learning_rate 0.00180572
2017-10-10T14:41:56.792846: step 260, loss 0.284082, acc 0.890625, learning_rate 0.00179876
2017-10-10T14:41:56.972036: step 261, loss 0.484443, acc 0.78125, learning_rate 0.00179182
2017-10-10T14:41:57.139034: step 262, loss 0.37149, acc 0.875, learning_rate 0.00178492
2017-10-10T14:41:57.319409: step 263, loss 0.454363, acc 0.875, learning_rate 0.00177804
2017-10-10T14:41:57.516860: step 264, loss 0.384277, acc 0.84375, learning_rate 0.00177119
2017-10-10T14:41:57.656927: step 265, loss 0.29379, acc 0.890625, learning_rate 0.00176437
2017-10-10T14:41:57.841362: step 266, loss 0.22474, acc 0.890625, learning_rate 0.00175758
2017-10-10T14:41:57.991895: step 267, loss 0.344322, acc 0.875, learning_rate 0.00175081
2017-10-10T14:41:58.176805: step 268, loss 0.494411, acc 0.859375, learning_rate 0.00174407
2017-10-10T14:41:58.360054: step 269, loss 0.501301, acc 0.78125, learning_rate 0.00173736
2017-10-10T14:41:58.507998: step 270, loss 0.240044, acc 0.90625, learning_rate 0.00173068
2017-10-10T14:41:58.683028: step 271, loss 0.271368, acc 0.921875, learning_rate 0.00172402
2017-10-10T14:41:58.876874: step 272, loss 0.333795, acc 0.875, learning_rate 0.00171739
2017-10-10T14:41:59.052832: step 273, loss 0.169973, acc 0.9375, learning_rate 0.00171079
2017-10-10T14:41:59.275244: step 274, loss 0.390221, acc 0.875, learning_rate 0.00170422
2017-10-10T14:41:59.379505: step 275, loss 0.349922, acc 0.890625, learning_rate 0.00169767
2017-10-10T14:41:59.552853: step 276, loss 0.199957, acc 0.9375, learning_rate 0.00169115
2017-10-10T14:41:59.754815: step 277, loss 0.365798, acc 0.828125, learning_rate 0.00168465
2017-10-10T14:41:59.838113: step 278, loss 0.463414, acc 0.828125, learning_rate 0.00167818
2017-10-10T14:41:59.927987: step 279, loss 0.360697, acc 0.84375, learning_rate 0.00167174
2017-10-10T14:42:00.010887: step 280, loss 0.215337, acc 0.921875, learning_rate 0.00166533

Evaluation:
2017-10-10T14:42:00.226553: step 280, loss 0.264969, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-280

2017-10-10T14:42:01.005179: step 281, loss 0.365553, acc 0.875, learning_rate 0.00165894
2017-10-10T14:42:01.206232: step 282, loss 0.542693, acc 0.796875, learning_rate 0.00165257
2017-10-10T14:42:01.304171: step 283, loss 0.207755, acc 0.9375, learning_rate 0.00164624
2017-10-10T14:42:01.489689: step 284, loss 0.324492, acc 0.90625, learning_rate 0.00163993
2017-10-10T14:42:01.620544: step 285, loss 0.335392, acc 0.890625, learning_rate 0.00163364
2017-10-10T14:42:01.802655: step 286, loss 0.176931, acc 0.953125, learning_rate 0.00162738
2017-10-10T14:42:01.970420: step 287, loss 0.258484, acc 0.90625, learning_rate 0.00162115
2017-10-10T14:42:02.126469: step 288, loss 0.324184, acc 0.921875, learning_rate 0.00161494
2017-10-10T14:42:02.288940: step 289, loss 0.384227, acc 0.84375, learning_rate 0.00160875
2017-10-10T14:42:02.448839: step 290, loss 0.414817, acc 0.828125, learning_rate 0.00160259
2017-10-10T14:42:02.668831: step 291, loss 0.350526, acc 0.90625, learning_rate 0.00159646
2017-10-10T14:42:02.845285: step 292, loss 0.331483, acc 0.875, learning_rate 0.00159035
2017-10-10T14:42:03.031492: step 293, loss 0.427419, acc 0.84375, learning_rate 0.00158427
2017-10-10T14:42:03.176666: step 294, loss 0.325157, acc 0.921569, learning_rate 0.00157821
2017-10-10T14:42:03.387485: step 295, loss 0.222266, acc 0.9375, learning_rate 0.00157218
2017-10-10T14:42:03.592523: step 296, loss 0.39094, acc 0.84375, learning_rate 0.00156617
2017-10-10T14:42:03.719568: step 297, loss 0.353673, acc 0.84375, learning_rate 0.00156018
2017-10-10T14:42:03.905676: step 298, loss 0.304874, acc 0.890625, learning_rate 0.00155422
2017-10-10T14:42:04.116832: step 299, loss 0.347696, acc 0.84375, learning_rate 0.00154829
2017-10-10T14:42:04.235083: step 300, loss 0.217953, acc 0.890625, learning_rate 0.00154238
2017-10-10T14:42:04.423309: step 301, loss 0.452979, acc 0.875, learning_rate 0.00153649
2017-10-10T14:42:04.600894: step 302, loss 0.32162, acc 0.859375, learning_rate 0.00153063
2017-10-10T14:42:04.747296: step 303, loss 0.230681, acc 0.90625, learning_rate 0.00152479
2017-10-10T14:42:04.936083: step 304, loss 0.330412, acc 0.90625, learning_rate 0.00151897
2017-10-10T14:42:05.121761: step 305, loss 0.281821, acc 0.921875, learning_rate 0.00151318
2017-10-10T14:42:05.289125: step 306, loss 0.321926, acc 0.828125, learning_rate 0.00150741
2017-10-10T14:42:05.445708: step 307, loss 0.375159, acc 0.875, learning_rate 0.00150167
2017-10-10T14:42:05.605358: step 308, loss 0.377641, acc 0.875, learning_rate 0.00149594
2017-10-10T14:42:05.796878: step 309, loss 0.328138, acc 0.890625, learning_rate 0.00149025
2017-10-10T14:42:05.984292: step 310, loss 0.147405, acc 0.96875, learning_rate 0.00148457
2017-10-10T14:42:06.128916: step 311, loss 0.246029, acc 0.890625, learning_rate 0.00147892
2017-10-10T14:42:06.329166: step 312, loss 0.157666, acc 0.953125, learning_rate 0.00147329
2017-10-10T14:42:06.466759: step 313, loss 0.244622, acc 0.875, learning_rate 0.00146769
2017-10-10T14:42:06.604583: step 314, loss 0.517636, acc 0.828125, learning_rate 0.0014621
2017-10-10T14:42:06.771279: step 315, loss 0.230326, acc 0.9375, learning_rate 0.00145654
2017-10-10T14:42:06.948498: step 316, loss 0.466948, acc 0.859375, learning_rate 0.00145101
2017-10-10T14:42:07.103852: step 317, loss 0.301222, acc 0.875, learning_rate 0.00144549
2017-10-10T14:42:07.266834: step 318, loss 0.432326, acc 0.859375, learning_rate 0.00144
2017-10-10T14:42:07.452126: step 319, loss 0.355572, acc 0.875, learning_rate 0.00143453
2017-10-10T14:42:07.628832: step 320, loss 0.451207, acc 0.84375, learning_rate 0.00142908

Evaluation:
2017-10-10T14:42:07.924846: step 320, loss 0.266147, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-320

2017-10-10T14:42:08.816708: step 321, loss 0.35368, acc 0.90625, learning_rate 0.00142366
2017-10-10T14:42:08.960417: step 322, loss 0.159303, acc 0.953125, learning_rate 0.00141826
2017-10-10T14:42:09.103875: step 323, loss 0.307679, acc 0.890625, learning_rate 0.00141288
2017-10-10T14:42:09.320394: step 324, loss 0.309106, acc 0.859375, learning_rate 0.00140752
2017-10-10T14:42:09.441782: step 325, loss 0.503759, acc 0.859375, learning_rate 0.00140218
2017-10-10T14:42:09.613476: step 326, loss 0.227349, acc 0.90625, learning_rate 0.00139686
2017-10-10T14:42:09.804863: step 327, loss 0.183142, acc 0.9375, learning_rate 0.00139157
2017-10-10T14:42:09.997042: step 328, loss 0.363711, acc 0.890625, learning_rate 0.0013863
2017-10-10T14:42:10.182311: step 329, loss 0.203264, acc 0.921875, learning_rate 0.00138105
2017-10-10T14:42:10.270927: step 330, loss 0.355051, acc 0.875, learning_rate 0.00137582
2017-10-10T14:42:10.416882: step 331, loss 0.313132, acc 0.90625, learning_rate 0.00137061
2017-10-10T14:42:10.532982: step 332, loss 0.2369, acc 0.90625, learning_rate 0.00136543
2017-10-10T14:42:10.648183: step 333, loss 0.251953, acc 0.921875, learning_rate 0.00136026
2017-10-10T14:42:10.794447: step 334, loss 0.376801, acc 0.84375, learning_rate 0.00135512
2017-10-10T14:42:10.988975: step 335, loss 0.142723, acc 0.984375, learning_rate 0.00134999
2017-10-10T14:42:11.200136: step 336, loss 0.394484, acc 0.890625, learning_rate 0.00134489
2017-10-10T14:42:11.343614: step 337, loss 0.300472, acc 0.90625, learning_rate 0.00133981
2017-10-10T14:42:11.452318: step 338, loss 0.418264, acc 0.84375, learning_rate 0.00133475
2017-10-10T14:42:11.565215: step 339, loss 0.404226, acc 0.890625, learning_rate 0.00132971
2017-10-10T14:42:11.704195: step 340, loss 0.419514, acc 0.859375, learning_rate 0.00132469
2017-10-10T14:42:11.852956: step 341, loss 0.247081, acc 0.859375, learning_rate 0.00131969
2017-10-10T14:42:11.956717: step 342, loss 0.110916, acc 0.984375, learning_rate 0.00131471
2017-10-10T14:42:12.152839: step 343, loss 0.133718, acc 0.953125, learning_rate 0.00130975
2017-10-10T14:42:12.360096: step 344, loss 0.207037, acc 0.953125, learning_rate 0.00130482
2017-10-10T14:42:12.517147: step 345, loss 0.314406, acc 0.859375, learning_rate 0.0012999
2017-10-10T14:42:12.666396: step 346, loss 0.252938, acc 0.9375, learning_rate 0.001295
2017-10-10T14:42:12.858782: step 347, loss 0.361871, acc 0.90625, learning_rate 0.00129012
2017-10-10T14:42:13.035438: step 348, loss 0.341826, acc 0.875, learning_rate 0.00128527
2017-10-10T14:42:13.154543: step 349, loss 0.278907, acc 0.921875, learning_rate 0.00128043
2017-10-10T14:42:13.341213: step 350, loss 0.192482, acc 0.96875, learning_rate 0.00127561
2017-10-10T14:42:13.462547: step 351, loss 0.251453, acc 0.921875, learning_rate 0.00127081
2017-10-10T14:42:13.603346: step 352, loss 0.194341, acc 0.953125, learning_rate 0.00126603
2017-10-10T14:42:13.781536: step 353, loss 0.419576, acc 0.828125, learning_rate 0.00126127
2017-10-10T14:42:13.966469: step 354, loss 0.171858, acc 0.953125, learning_rate 0.00125653
2017-10-10T14:42:14.091776: step 355, loss 0.181777, acc 0.953125, learning_rate 0.00125181
2017-10-10T14:42:14.287033: step 356, loss 0.252211, acc 0.890625, learning_rate 0.00124711
2017-10-10T14:42:14.447921: step 357, loss 0.233721, acc 0.921875, learning_rate 0.00124243
2017-10-10T14:42:14.569571: step 358, loss 0.347664, acc 0.890625, learning_rate 0.00123777
2017-10-10T14:42:14.774718: step 359, loss 0.233788, acc 0.90625, learning_rate 0.00123312
2017-10-10T14:42:14.947960: step 360, loss 0.224971, acc 0.96875, learning_rate 0.0012285

Evaluation:
2017-10-10T14:42:15.222179: step 360, loss 0.263354, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-360

2017-10-10T14:42:16.012929: step 361, loss 0.177095, acc 0.9375, learning_rate 0.00122389
2017-10-10T14:42:16.152047: step 362, loss 0.339127, acc 0.859375, learning_rate 0.0012193
2017-10-10T14:42:16.348004: step 363, loss 0.284503, acc 0.90625, learning_rate 0.00121473
2017-10-10T14:42:16.540224: step 364, loss 0.244533, acc 0.90625, learning_rate 0.00121018
2017-10-10T14:42:16.650367: step 365, loss 0.326373, acc 0.90625, learning_rate 0.00120565
2017-10-10T14:42:16.852860: step 366, loss 0.310124, acc 0.890625, learning_rate 0.00120114
2017-10-10T14:42:16.972507: step 367, loss 0.402714, acc 0.875, learning_rate 0.00119664
2017-10-10T14:42:17.154619: step 368, loss 0.155774, acc 0.953125, learning_rate 0.00119217
2017-10-10T14:42:17.307893: step 369, loss 0.268525, acc 0.90625, learning_rate 0.00118771
2017-10-10T14:42:17.453801: step 370, loss 0.238704, acc 0.890625, learning_rate 0.00118327
2017-10-10T14:42:17.657942: step 371, loss 0.153448, acc 0.953125, learning_rate 0.00117885
2017-10-10T14:42:17.803602: step 372, loss 0.32209, acc 0.90625, learning_rate 0.00117445
2017-10-10T14:42:17.967706: step 373, loss 0.198476, acc 0.9375, learning_rate 0.00117006
2017-10-10T14:42:18.169570: step 374, loss 0.394946, acc 0.875, learning_rate 0.00116569
2017-10-10T14:42:18.267664: step 375, loss 0.212589, acc 0.9375, learning_rate 0.00116134
2017-10-10T14:42:18.472813: step 376, loss 0.268979, acc 0.9375, learning_rate 0.00115701
2017-10-10T14:42:18.668391: step 377, loss 0.361538, acc 0.890625, learning_rate 0.0011527
2017-10-10T14:42:18.787107: step 378, loss 0.183506, acc 0.9375, learning_rate 0.0011484
2017-10-10T14:42:18.981177: step 379, loss 0.173073, acc 0.953125, learning_rate 0.00114412
2017-10-10T14:42:19.153522: step 380, loss 0.181549, acc 0.9375, learning_rate 0.00113986
2017-10-10T14:42:19.306489: step 381, loss 0.254629, acc 0.921875, learning_rate 0.00113561
2017-10-10T14:42:19.453301: step 382, loss 0.218983, acc 0.9375, learning_rate 0.00113139
2017-10-10T14:42:19.633801: step 383, loss 0.292436, acc 0.9375, learning_rate 0.00112718
2017-10-10T14:42:19.778711: step 384, loss 0.217035, acc 0.90625, learning_rate 0.00112298
2017-10-10T14:42:19.929798: step 385, loss 0.21917, acc 0.9375, learning_rate 0.00111881
2017-10-10T14:42:20.102743: step 386, loss 0.20776, acc 0.90625, learning_rate 0.00111465
2017-10-10T14:42:20.245255: step 387, loss 0.17159, acc 0.9375, learning_rate 0.00111051
2017-10-10T14:42:20.432736: step 388, loss 0.343221, acc 0.859375, learning_rate 0.00110638
2017-10-10T14:42:20.646401: step 389, loss 0.451225, acc 0.796875, learning_rate 0.00110228
2017-10-10T14:42:20.852711: step 390, loss 0.210199, acc 0.921875, learning_rate 0.00109818
2017-10-10T14:42:21.022294: step 391, loss 0.258551, acc 0.953125, learning_rate 0.00109411
2017-10-10T14:42:21.117074: step 392, loss 0.287622, acc 0.882353, learning_rate 0.00109005
2017-10-10T14:42:21.240806: step 393, loss 0.294816, acc 0.859375, learning_rate 0.00108601
2017-10-10T14:42:21.367978: step 394, loss 0.20035, acc 0.96875, learning_rate 0.00108199
2017-10-10T14:42:21.464142: step 395, loss 0.183058, acc 0.9375, learning_rate 0.00107798
2017-10-10T14:42:21.627244: step 396, loss 0.18056, acc 0.953125, learning_rate 0.00107399
2017-10-10T14:42:21.758972: step 397, loss 0.336614, acc 0.859375, learning_rate 0.00107001
2017-10-10T14:42:21.896176: step 398, loss 0.256803, acc 0.9375, learning_rate 0.00106605
2017-10-10T14:42:22.072885: step 399, loss 0.239308, acc 0.890625, learning_rate 0.00106211
2017-10-10T14:42:22.252850: step 400, loss 0.31252, acc 0.859375, learning_rate 0.00105818

Evaluation:
2017-10-10T14:42:22.543019: step 400, loss 0.251729, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-400

2017-10-10T14:42:23.500279: step 401, loss 0.216263, acc 0.90625, learning_rate 0.00105427
2017-10-10T14:42:23.612012: step 402, loss 0.359803, acc 0.875, learning_rate 0.00105037
2017-10-10T14:42:23.846272: step 403, loss 0.2732, acc 0.890625, learning_rate 0.0010465
2017-10-10T14:42:24.001082: step 404, loss 0.280662, acc 0.890625, learning_rate 0.00104263
2017-10-10T14:42:24.136060: step 405, loss 0.459653, acc 0.875, learning_rate 0.00103878
2017-10-10T14:42:24.327651: step 406, loss 0.187843, acc 0.953125, learning_rate 0.00103495
2017-10-10T14:42:24.469051: step 407, loss 0.463635, acc 0.859375, learning_rate 0.00103114
2017-10-10T14:42:24.642429: step 408, loss 0.159347, acc 0.953125, learning_rate 0.00102734
2017-10-10T14:42:24.839145: step 409, loss 0.273716, acc 0.890625, learning_rate 0.00102355
2017-10-10T14:42:25.001021: step 410, loss 0.246179, acc 0.90625, learning_rate 0.00101978
2017-10-10T14:42:25.142957: step 411, loss 0.295497, acc 0.90625, learning_rate 0.00101603
2017-10-10T14:42:25.356822: step 412, loss 0.275594, acc 0.9375, learning_rate 0.00101229
2017-10-10T14:42:25.543602: step 413, loss 0.267662, acc 0.921875, learning_rate 0.00100856
2017-10-10T14:42:25.651472: step 414, loss 0.161351, acc 0.9375, learning_rate 0.00100486
2017-10-10T14:42:25.852829: step 415, loss 0.205091, acc 0.9375, learning_rate 0.00100116
2017-10-10T14:42:26.024990: step 416, loss 0.246211, acc 0.921875, learning_rate 0.000997483
2017-10-10T14:42:26.165447: step 417, loss 0.192719, acc 0.921875, learning_rate 0.00099382
2017-10-10T14:42:26.374381: step 418, loss 0.222967, acc 0.9375, learning_rate 0.000990172
2017-10-10T14:42:26.585043: step 419, loss 0.129012, acc 0.96875, learning_rate 0.000986538
2017-10-10T14:42:26.690900: step 420, loss 0.258551, acc 0.890625, learning_rate 0.00098292
2017-10-10T14:42:26.886078: step 421, loss 0.144926, acc 0.96875, learning_rate 0.000979316
2017-10-10T14:42:27.068952: step 422, loss 0.242476, acc 0.90625, learning_rate 0.000975727
2017-10-10T14:42:27.200851: step 423, loss 0.2447, acc 0.90625, learning_rate 0.000972152
2017-10-10T14:42:27.388455: step 424, loss 0.202027, acc 0.921875, learning_rate 0.000968592
2017-10-10T14:42:27.510018: step 425, loss 0.271771, acc 0.90625, learning_rate 0.000965047
2017-10-10T14:42:27.684707: step 426, loss 0.288136, acc 0.921875, learning_rate 0.000961516
2017-10-10T14:42:27.904731: step 427, loss 0.118126, acc 0.953125, learning_rate 0.000958
2017-10-10T14:42:28.023299: step 428, loss 0.276827, acc 0.875, learning_rate 0.000954497
2017-10-10T14:42:28.204490: step 429, loss 0.334501, acc 0.828125, learning_rate 0.00095101
2017-10-10T14:42:28.405288: step 430, loss 0.266548, acc 0.875, learning_rate 0.000947536
2017-10-10T14:42:28.508075: step 431, loss 0.221718, acc 0.921875, learning_rate 0.000944076
2017-10-10T14:42:28.707683: step 432, loss 0.280706, acc 0.921875, learning_rate 0.000940631
2017-10-10T14:42:28.921756: step 433, loss 0.214449, acc 0.953125, learning_rate 0.0009372
2017-10-10T14:42:29.109560: step 434, loss 0.41435, acc 0.875, learning_rate 0.000933783
2017-10-10T14:42:29.243306: step 435, loss 0.288698, acc 0.890625, learning_rate 0.000930379
2017-10-10T14:42:29.424067: step 436, loss 0.188232, acc 0.953125, learning_rate 0.00092699
2017-10-10T14:42:29.604168: step 437, loss 0.157256, acc 0.96875, learning_rate 0.000923614
2017-10-10T14:42:29.776986: step 438, loss 0.158465, acc 0.9375, learning_rate 0.000920253
2017-10-10T14:42:29.960844: step 439, loss 0.190289, acc 0.9375, learning_rate 0.000916905
2017-10-10T14:42:30.156822: step 440, loss 0.328629, acc 0.890625, learning_rate 0.00091357

Evaluation:
2017-10-10T14:42:30.432984: step 440, loss 0.2472, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-440

2017-10-10T14:42:31.377478: step 441, loss 0.23053, acc 0.890625, learning_rate 0.000910249
2017-10-10T14:42:31.650381: step 442, loss 0.278393, acc 0.921875, learning_rate 0.000906942
2017-10-10T14:42:31.814252: step 443, loss 0.22649, acc 0.921875, learning_rate 0.000903648
2017-10-10T14:42:31.953612: step 444, loss 0.359893, acc 0.84375, learning_rate 0.000900368
2017-10-10T14:42:32.060931: step 445, loss 0.355809, acc 0.90625, learning_rate 0.000897101
2017-10-10T14:42:32.172867: step 446, loss 0.300427, acc 0.859375, learning_rate 0.000893848
2017-10-10T14:42:32.319370: step 447, loss 0.193781, acc 0.9375, learning_rate 0.000890607
2017-10-10T14:42:32.444162: step 448, loss 0.173905, acc 0.953125, learning_rate 0.00088738
2017-10-10T14:42:32.534429: step 449, loss 0.235473, acc 0.890625, learning_rate 0.000884166
2017-10-10T14:42:32.680416: step 450, loss 0.307843, acc 0.875, learning_rate 0.000880966
2017-10-10T14:42:32.820850: step 451, loss 0.101991, acc 0.96875, learning_rate 0.000877778
2017-10-10T14:42:32.935094: step 452, loss 0.233815, acc 0.921875, learning_rate 0.000874603
2017-10-10T14:42:33.072932: step 453, loss 0.385778, acc 0.875, learning_rate 0.000871441
2017-10-10T14:42:33.232835: step 454, loss 0.256689, acc 0.921875, learning_rate 0.000868293
2017-10-10T14:42:33.318276: step 455, loss 0.413753, acc 0.828125, learning_rate 0.000865157
2017-10-10T14:42:33.537003: step 456, loss 0.226197, acc 0.9375, learning_rate 0.000862033
2017-10-10T14:42:33.725354: step 457, loss 0.249317, acc 0.890625, learning_rate 0.000858923
2017-10-10T14:42:33.923639: step 458, loss 0.385837, acc 0.859375, learning_rate 0.000855825
2017-10-10T14:42:34.039303: step 459, loss 0.235549, acc 0.921875, learning_rate 0.00085274
2017-10-10T14:42:34.143628: step 460, loss 0.168044, acc 0.953125, learning_rate 0.000849668
2017-10-10T14:42:34.300712: step 461, loss 0.242824, acc 0.875, learning_rate 0.000846608
2017-10-10T14:42:34.427769: step 462, loss 0.191829, acc 0.953125, learning_rate 0.00084356
2017-10-10T14:42:34.520832: step 463, loss 0.412212, acc 0.859375, learning_rate 0.000840525
2017-10-10T14:42:34.654903: step 464, loss 0.18686, acc 0.921875, learning_rate 0.000837502
2017-10-10T14:42:34.800274: step 465, loss 0.261796, acc 0.890625, learning_rate 0.000834492
2017-10-10T14:42:34.949375: step 466, loss 0.261185, acc 0.921875, learning_rate 0.000831494
2017-10-10T14:42:35.162154: step 467, loss 0.212209, acc 0.9375, learning_rate 0.000828508
2017-10-10T14:42:35.297061: step 468, loss 0.194605, acc 0.9375, learning_rate 0.000825535
2017-10-10T14:42:35.480973: step 469, loss 0.327455, acc 0.875, learning_rate 0.000822573
2017-10-10T14:42:35.672968: step 470, loss 0.1882, acc 0.953125, learning_rate 0.000819624
2017-10-10T14:42:35.857487: step 471, loss 0.266482, acc 0.859375, learning_rate 0.000816687
2017-10-10T14:42:36.005046: step 472, loss 0.227683, acc 0.9375, learning_rate 0.000813761
2017-10-10T14:42:36.188994: step 473, loss 0.175332, acc 0.96875, learning_rate 0.000810848
2017-10-10T14:42:36.338251: step 474, loss 0.110468, acc 0.984375, learning_rate 0.000807946
2017-10-10T14:42:36.505785: step 475, loss 0.335664, acc 0.921875, learning_rate 0.000805057
2017-10-10T14:42:36.724852: step 476, loss 0.268637, acc 0.921875, learning_rate 0.000802179
2017-10-10T14:42:36.875179: step 477, loss 0.34745, acc 0.890625, learning_rate 0.000799313
2017-10-10T14:42:37.060592: step 478, loss 0.331325, acc 0.90625, learning_rate 0.000796458
2017-10-10T14:42:37.255287: step 479, loss 0.342017, acc 0.875, learning_rate 0.000793616
2017-10-10T14:42:37.361966: step 480, loss 0.201607, acc 0.953125, learning_rate 0.000790784

Evaluation:
2017-10-10T14:42:37.626272: step 480, loss 0.249159, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-480

2017-10-10T14:42:38.604478: step 481, loss 0.238656, acc 0.9375, learning_rate 0.000787965
2017-10-10T14:42:38.733134: step 482, loss 0.336671, acc 0.859375, learning_rate 0.000785157
2017-10-10T14:42:38.900328: step 483, loss 0.237267, acc 0.90625, learning_rate 0.00078236
2017-10-10T14:42:39.101831: step 484, loss 0.186039, acc 0.953125, learning_rate 0.000779575
2017-10-10T14:42:39.221273: step 485, loss 0.0850724, acc 0.984375, learning_rate 0.000776801
2017-10-10T14:42:39.409942: step 486, loss 0.324148, acc 0.890625, learning_rate 0.000774038
2017-10-10T14:42:39.555712: step 487, loss 0.178972, acc 0.9375, learning_rate 0.000771287
2017-10-10T14:42:39.702070: step 488, loss 0.247867, acc 0.875, learning_rate 0.000768547
2017-10-10T14:42:39.909349: step 489, loss 0.327005, acc 0.859375, learning_rate 0.000765818
2017-10-10T14:42:40.024851: step 490, loss 0.240798, acc 0.921569, learning_rate 0.000763101
2017-10-10T14:42:40.217182: step 491, loss 0.288919, acc 0.921875, learning_rate 0.000760394
2017-10-10T14:42:40.377779: step 492, loss 0.205103, acc 0.90625, learning_rate 0.000757698
2017-10-10T14:42:40.554177: step 493, loss 0.203237, acc 0.921875, learning_rate 0.000755014
2017-10-10T14:42:40.697048: step 494, loss 0.300123, acc 0.890625, learning_rate 0.00075234
2017-10-10T14:42:40.871911: step 495, loss 0.215443, acc 0.953125, learning_rate 0.000749677
2017-10-10T14:42:41.089989: step 496, loss 0.163129, acc 0.953125, learning_rate 0.000747026
2017-10-10T14:42:41.214324: step 497, loss 0.241058, acc 0.921875, learning_rate 0.000744385
2017-10-10T14:42:41.380529: step 498, loss 0.2344, acc 0.9375, learning_rate 0.000741754
2017-10-10T14:42:41.568836: step 499, loss 0.175397, acc 0.9375, learning_rate 0.000739135
2017-10-10T14:42:41.685748: step 500, loss 0.16151, acc 0.96875, learning_rate 0.000736526
2017-10-10T14:42:41.893585: step 501, loss 0.155362, acc 0.953125, learning_rate 0.000733928
2017-10-10T14:42:42.104829: step 502, loss 0.317674, acc 0.921875, learning_rate 0.00073134
2017-10-10T14:42:42.275091: step 503, loss 0.374262, acc 0.84375, learning_rate 0.000728763
2017-10-10T14:42:42.433576: step 504, loss 0.184012, acc 0.9375, learning_rate 0.000726197
2017-10-10T14:42:42.638806: step 505, loss 0.178923, acc 0.9375, learning_rate 0.000723641
2017-10-10T14:42:42.759015: step 506, loss 0.204166, acc 0.90625, learning_rate 0.000721095
2017-10-10T14:42:42.948318: step 507, loss 0.211256, acc 0.921875, learning_rate 0.00071856
2017-10-10T14:42:43.077062: step 508, loss 0.167806, acc 0.9375, learning_rate 0.000716036
2017-10-10T14:42:43.284328: step 509, loss 0.260038, acc 0.890625, learning_rate 0.000713521
2017-10-10T14:42:43.494851: step 510, loss 0.278247, acc 0.9375, learning_rate 0.000711017
2017-10-10T14:42:43.612844: step 511, loss 0.305952, acc 0.859375, learning_rate 0.000708523
2017-10-10T14:42:43.774858: step 512, loss 0.291791, acc 0.890625, learning_rate 0.000706039
2017-10-10T14:42:43.939343: step 513, loss 0.3078, acc 0.890625, learning_rate 0.000703565
2017-10-10T14:42:44.060885: step 514, loss 0.0811559, acc 0.96875, learning_rate 0.000701102
2017-10-10T14:42:44.191850: step 515, loss 0.245587, acc 0.90625, learning_rate 0.000698648
2017-10-10T14:42:44.337067: step 516, loss 0.203779, acc 0.90625, learning_rate 0.000696204
2017-10-10T14:42:44.490830: step 517, loss 0.207166, acc 0.921875, learning_rate 0.000693771
2017-10-10T14:42:44.652564: step 518, loss 0.274121, acc 0.890625, learning_rate 0.000691347
2017-10-10T14:42:44.808856: step 519, loss 0.256787, acc 0.890625, learning_rate 0.000688934
2017-10-10T14:42:44.960843: step 520, loss 0.174404, acc 0.953125, learning_rate 0.00068653

Evaluation:
2017-10-10T14:42:45.239253: step 520, loss 0.242349, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-520

2017-10-10T14:42:45.984967: step 521, loss 0.243851, acc 0.921875, learning_rate 0.000684136
2017-10-10T14:42:46.124832: step 522, loss 0.266621, acc 0.875, learning_rate 0.000681751
2017-10-10T14:42:46.248693: step 523, loss 0.22629, acc 0.90625, learning_rate 0.000679377
2017-10-10T14:42:46.385504: step 524, loss 0.235532, acc 0.921875, learning_rate 0.000677012
2017-10-10T14:42:46.590446: step 525, loss 0.275968, acc 0.921875, learning_rate 0.000674657
2017-10-10T14:42:46.717307: step 526, loss 0.242442, acc 0.921875, learning_rate 0.000672311
2017-10-10T14:42:46.904402: step 527, loss 0.138127, acc 0.921875, learning_rate 0.000669975
2017-10-10T14:42:47.095760: step 528, loss 0.332356, acc 0.875, learning_rate 0.000667648
2017-10-10T14:42:47.217785: step 529, loss 0.296777, acc 0.921875, learning_rate 0.000665331
2017-10-10T14:42:47.416013: step 530, loss 0.181088, acc 0.953125, learning_rate 0.000663024
2017-10-10T14:42:47.593951: step 531, loss 0.263067, acc 0.90625, learning_rate 0.000660726
2017-10-10T14:42:47.766393: step 532, loss 0.298516, acc 0.90625, learning_rate 0.000658437
2017-10-10T14:42:47.933183: step 533, loss 0.221319, acc 0.9375, learning_rate 0.000656158
2017-10-10T14:42:48.105926: step 534, loss 0.33059, acc 0.890625, learning_rate 0.000653888
2017-10-10T14:42:48.261940: step 535, loss 0.212406, acc 0.953125, learning_rate 0.000651627
2017-10-10T14:42:48.439849: step 536, loss 0.175856, acc 0.921875, learning_rate 0.000649375
2017-10-10T14:42:48.620554: step 537, loss 0.288166, acc 0.890625, learning_rate 0.000647133
2017-10-10T14:42:48.808015: step 538, loss 0.146121, acc 0.953125, learning_rate 0.000644899
2017-10-10T14:42:48.993184: step 539, loss 0.275228, acc 0.890625, learning_rate 0.000642675
2017-10-10T14:42:49.112782: step 540, loss 0.193009, acc 0.921875, learning_rate 0.00064046
2017-10-10T14:42:49.300961: step 541, loss 0.251465, acc 0.921875, learning_rate 0.000638254
2017-10-10T14:42:49.472945: step 542, loss 0.206383, acc 0.9375, learning_rate 0.000636057
2017-10-10T14:42:49.614264: step 543, loss 0.0710211, acc 1, learning_rate 0.000633869
2017-10-10T14:42:49.786931: step 544, loss 0.298327, acc 0.90625, learning_rate 0.00063169
2017-10-10T14:42:49.972236: step 545, loss 0.188531, acc 0.9375, learning_rate 0.00062952
2017-10-10T14:42:50.140308: step 546, loss 0.332349, acc 0.90625, learning_rate 0.000627358
2017-10-10T14:42:50.328687: step 547, loss 0.216825, acc 0.90625, learning_rate 0.000625206
2017-10-10T14:42:50.464348: step 548, loss 0.208995, acc 0.890625, learning_rate 0.000623062
2017-10-10T14:42:50.639052: step 549, loss 0.253078, acc 0.921875, learning_rate 0.000620927
2017-10-10T14:42:50.785026: step 550, loss 0.137619, acc 0.953125, learning_rate 0.000618801
2017-10-10T14:42:50.955241: step 551, loss 0.178711, acc 0.921875, learning_rate 0.000616683
2017-10-10T14:42:51.160461: step 552, loss 0.179155, acc 0.921875, learning_rate 0.000614574
2017-10-10T14:42:51.322638: step 553, loss 0.295664, acc 0.921875, learning_rate 0.000612474
2017-10-10T14:42:51.455847: step 554, loss 0.163129, acc 0.921875, learning_rate 0.000610382
2017-10-10T14:42:51.620416: step 555, loss 0.142562, acc 0.9375, learning_rate 0.000608299
2017-10-10T14:42:51.773039: step 556, loss 0.21284, acc 0.953125, learning_rate 0.000606224
2017-10-10T14:42:51.932586: step 557, loss 0.2009, acc 0.9375, learning_rate 0.000604158
2017-10-10T14:42:52.117876: step 558, loss 0.257049, acc 0.890625, learning_rate 0.0006021
2017-10-10T14:42:52.290905: step 559, loss 0.278696, acc 0.921875, learning_rate 0.00060005
2017-10-10T14:42:52.481645: step 560, loss 0.162234, acc 0.953125, learning_rate 0.000598009

Evaluation:
2017-10-10T14:42:52.753933: step 560, loss 0.242599, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-560

2017-10-10T14:42:53.745877: step 561, loss 0.313237, acc 0.890625, learning_rate 0.000595977
2017-10-10T14:42:53.884851: step 562, loss 0.355418, acc 0.875, learning_rate 0.000593952
2017-10-10T14:42:54.096609: step 563, loss 0.178078, acc 0.953125, learning_rate 0.000591936
2017-10-10T14:42:54.297713: step 564, loss 0.288999, acc 0.90625, learning_rate 0.000589928
2017-10-10T14:42:54.425943: step 565, loss 0.237056, acc 0.921875, learning_rate 0.000587928
2017-10-10T14:42:54.578802: step 566, loss 0.348161, acc 0.859375, learning_rate 0.000585937
2017-10-10T14:42:54.681119: step 567, loss 0.240085, acc 0.9375, learning_rate 0.000583953
2017-10-10T14:42:54.819195: step 568, loss 0.193558, acc 0.953125, learning_rate 0.000581978
2017-10-10T14:42:54.974248: step 569, loss 0.190685, acc 0.9375, learning_rate 0.00058001
2017-10-10T14:42:55.065583: step 570, loss 0.225901, acc 0.921875, learning_rate 0.000578051
2017-10-10T14:42:55.203096: step 571, loss 0.0905969, acc 0.984375, learning_rate 0.0005761
2017-10-10T14:42:55.389972: step 572, loss 0.203036, acc 0.9375, learning_rate 0.000574157
2017-10-10T14:42:55.519650: step 573, loss 0.283535, acc 0.90625, learning_rate 0.000572221
2017-10-10T14:42:55.701175: step 574, loss 0.16696, acc 0.953125, learning_rate 0.000570294
2017-10-10T14:42:55.909954: step 575, loss 0.19392, acc 0.921875, learning_rate 0.000568374
2017-10-10T14:42:56.051489: step 576, loss 0.263798, acc 0.9375, learning_rate 0.000566462
2017-10-10T14:42:56.210942: step 577, loss 0.334696, acc 0.859375, learning_rate 0.000564558
2017-10-10T14:42:56.426020: step 578, loss 0.168682, acc 0.953125, learning_rate 0.000562662
2017-10-10T14:42:56.580925: step 579, loss 0.193948, acc 0.921875, learning_rate 0.000560774
2017-10-10T14:42:56.720862: step 580, loss 0.298298, acc 0.90625, learning_rate 0.000558893
2017-10-10T14:42:56.936845: step 581, loss 0.341276, acc 0.859375, learning_rate 0.00055702
2017-10-10T14:42:57.156882: step 582, loss 0.182102, acc 0.9375, learning_rate 0.000555154
2017-10-10T14:42:57.343280: step 583, loss 0.236741, acc 0.90625, learning_rate 0.000553296
2017-10-10T14:42:57.451138: step 584, loss 0.253269, acc 0.921875, learning_rate 0.000551446
2017-10-10T14:42:57.596822: step 585, loss 0.185266, acc 0.953125, learning_rate 0.000549604
2017-10-10T14:42:57.712960: step 586, loss 0.260274, acc 0.921875, learning_rate 0.000547768
2017-10-10T14:42:57.814852: step 587, loss 0.121607, acc 0.96875, learning_rate 0.000545941
2017-10-10T14:42:57.950983: step 588, loss 0.463658, acc 0.882353, learning_rate 0.00054412
2017-10-10T14:42:58.065706: step 589, loss 0.163514, acc 0.921875, learning_rate 0.000542308
2017-10-10T14:42:58.161865: step 590, loss 0.143624, acc 0.9375, learning_rate 0.000540502
2017-10-10T14:42:58.308954: step 591, loss 0.16262, acc 0.96875, learning_rate 0.000538704
2017-10-10T14:42:58.473158: step 592, loss 0.27665, acc 0.921875, learning_rate 0.000536914
2017-10-10T14:42:58.675008: step 593, loss 0.28435, acc 0.921875, learning_rate 0.00053513
2017-10-10T14:42:58.835382: step 594, loss 0.321346, acc 0.921875, learning_rate 0.000533354
2017-10-10T14:42:59.051376: step 595, loss 0.228255, acc 0.96875, learning_rate 0.000531585
2017-10-10T14:42:59.268816: step 596, loss 0.247831, acc 0.90625, learning_rate 0.000529824
2017-10-10T14:42:59.429167: step 597, loss 0.268407, acc 0.921875, learning_rate 0.000528069
2017-10-10T14:42:59.601142: step 598, loss 0.139362, acc 0.96875, learning_rate 0.000526322
2017-10-10T14:42:59.764029: step 599, loss 0.318269, acc 0.90625, learning_rate 0.000524582
2017-10-10T14:42:59.920315: step 600, loss 0.229423, acc 0.921875, learning_rate 0.000522849

Evaluation:
2017-10-10T14:43:00.227061: step 600, loss 0.238144, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-600

2017-10-10T14:43:01.140820: step 601, loss 0.159327, acc 0.953125, learning_rate 0.000521123
2017-10-10T14:43:01.362678: step 602, loss 0.201183, acc 0.9375, learning_rate 0.000519404
2017-10-10T14:43:01.507655: step 603, loss 0.239109, acc 0.9375, learning_rate 0.000517692
2017-10-10T14:43:01.688995: step 604, loss 0.252429, acc 0.9375, learning_rate 0.000515987
2017-10-10T14:43:01.890153: step 605, loss 0.23469, acc 0.96875, learning_rate 0.000514289
2017-10-10T14:43:02.072748: step 606, loss 0.226636, acc 0.90625, learning_rate 0.000512598
2017-10-10T14:43:02.263534: step 607, loss 0.211303, acc 0.9375, learning_rate 0.000510914
2017-10-10T14:43:02.446882: step 608, loss 0.226668, acc 0.921875, learning_rate 0.000509237
2017-10-10T14:43:02.648856: step 609, loss 0.224922, acc 0.921875, learning_rate 0.000507566
2017-10-10T14:43:02.848865: step 610, loss 0.244024, acc 0.921875, learning_rate 0.000505903
2017-10-10T14:43:03.075687: step 611, loss 0.191039, acc 0.9375, learning_rate 0.000504246
2017-10-10T14:43:03.264863: step 612, loss 0.37146, acc 0.890625, learning_rate 0.000502596
2017-10-10T14:43:03.454369: step 613, loss 0.277305, acc 0.890625, learning_rate 0.000500953
2017-10-10T14:43:03.649030: step 614, loss 0.317872, acc 0.90625, learning_rate 0.000499316
2017-10-10T14:43:03.808815: step 615, loss 0.150442, acc 0.9375, learning_rate 0.000497686
2017-10-10T14:43:03.996844: step 616, loss 0.241101, acc 0.890625, learning_rate 0.000496063
2017-10-10T14:43:04.211785: step 617, loss 0.232379, acc 0.90625, learning_rate 0.000494446
2017-10-10T14:43:04.409342: step 618, loss 0.275447, acc 0.890625, learning_rate 0.000492836
2017-10-10T14:43:04.599991: step 619, loss 0.215692, acc 0.9375, learning_rate 0.000491233
2017-10-10T14:43:04.733641: step 620, loss 0.235956, acc 0.90625, learning_rate 0.000489636
2017-10-10T14:43:04.904443: step 621, loss 0.179391, acc 0.953125, learning_rate 0.000488045
2017-10-10T14:43:05.145267: step 622, loss 0.32036, acc 0.890625, learning_rate 0.000486461
2017-10-10T14:43:05.348885: step 623, loss 0.212719, acc 0.9375, learning_rate 0.000484884
2017-10-10T14:43:05.588814: step 624, loss 0.180835, acc 0.953125, learning_rate 0.000483313
2017-10-10T14:43:05.804803: step 625, loss 0.123046, acc 0.984375, learning_rate 0.000481748
2017-10-10T14:43:05.915098: step 626, loss 0.165682, acc 0.9375, learning_rate 0.00048019
2017-10-10T14:43:06.066121: step 627, loss 0.240363, acc 0.921875, learning_rate 0.000478638
2017-10-10T14:43:06.189238: step 628, loss 0.242396, acc 0.90625, learning_rate 0.000477093
2017-10-10T14:43:06.327802: step 629, loss 0.131019, acc 0.96875, learning_rate 0.000475554
2017-10-10T14:43:06.476581: step 630, loss 0.137385, acc 0.96875, learning_rate 0.000474021
2017-10-10T14:43:06.652849: step 631, loss 0.21129, acc 0.90625, learning_rate 0.000472494
2017-10-10T14:43:06.860220: step 632, loss 0.222952, acc 0.921875, learning_rate 0.000470974
2017-10-10T14:43:07.031643: step 633, loss 0.231739, acc 0.9375, learning_rate 0.000469459
2017-10-10T14:43:07.255884: step 634, loss 0.208183, acc 0.9375, learning_rate 0.000467951
2017-10-10T14:43:07.437915: step 635, loss 0.230202, acc 0.90625, learning_rate 0.000466449
2017-10-10T14:43:07.569380: step 636, loss 0.151038, acc 0.953125, learning_rate 0.000464954
2017-10-10T14:43:07.772910: step 637, loss 0.32834, acc 0.859375, learning_rate 0.000463464
2017-10-10T14:43:07.924775: step 638, loss 0.165348, acc 0.953125, learning_rate 0.00046198
2017-10-10T14:43:08.118435: step 639, loss 0.205795, acc 0.890625, learning_rate 0.000460503
2017-10-10T14:43:08.239620: step 640, loss 0.220047, acc 0.9375, learning_rate 0.000459031

Evaluation:
2017-10-10T14:43:08.527337: step 640, loss 0.237547, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-640

2017-10-10T14:43:09.397914: step 641, loss 0.157995, acc 0.9375, learning_rate 0.000457566
2017-10-10T14:43:09.613151: step 642, loss 0.148267, acc 0.96875, learning_rate 0.000456106
2017-10-10T14:43:09.753361: step 643, loss 0.284421, acc 0.921875, learning_rate 0.000454653
2017-10-10T14:43:09.932623: step 644, loss 0.156738, acc 0.921875, learning_rate 0.000453205
2017-10-10T14:43:10.095027: step 645, loss 0.320299, acc 0.875, learning_rate 0.000451764
2017-10-10T14:43:10.324924: step 646, loss 0.321135, acc 0.875, learning_rate 0.000450328
2017-10-10T14:43:10.541220: step 647, loss 0.13987, acc 0.953125, learning_rate 0.000448898
2017-10-10T14:43:10.720830: step 648, loss 0.262335, acc 0.9375, learning_rate 0.000447474
2017-10-10T14:43:10.839857: step 649, loss 0.20637, acc 0.9375, learning_rate 0.000446055
2017-10-10T14:43:10.994646: step 650, loss 0.216492, acc 0.921875, learning_rate 0.000444643
2017-10-10T14:43:11.148808: step 651, loss 0.38937, acc 0.875, learning_rate 0.000443236
2017-10-10T14:43:11.263053: step 652, loss 0.189169, acc 0.90625, learning_rate 0.000441835
2017-10-10T14:43:11.400297: step 653, loss 0.257477, acc 0.90625, learning_rate 0.00044044
2017-10-10T14:43:11.589150: step 654, loss 0.246175, acc 0.921875, learning_rate 0.00043905
2017-10-10T14:43:11.772840: step 655, loss 0.267275, acc 0.890625, learning_rate 0.000437666
2017-10-10T14:43:11.954100: step 656, loss 0.196936, acc 0.921875, learning_rate 0.000436288
2017-10-10T14:43:12.119899: step 657, loss 0.248983, acc 0.921875, learning_rate 0.000434915
2017-10-10T14:43:12.338155: step 658, loss 0.232754, acc 0.90625, learning_rate 0.000433548
2017-10-10T14:43:12.473808: step 659, loss 0.144426, acc 0.953125, learning_rate 0.000432187
2017-10-10T14:43:12.661042: step 660, loss 0.267335, acc 0.921875, learning_rate 0.000430831
2017-10-10T14:43:12.861333: step 661, loss 0.161846, acc 0.953125, learning_rate 0.000429481
2017-10-10T14:43:13.013115: step 662, loss 0.213113, acc 0.890625, learning_rate 0.000428136
2017-10-10T14:43:13.219454: step 663, loss 0.166891, acc 0.921875, learning_rate 0.000426796
2017-10-10T14:43:13.399670: step 664, loss 0.280434, acc 0.90625, learning_rate 0.000425463
2017-10-10T14:43:13.588890: step 665, loss 0.121441, acc 0.953125, learning_rate 0.000424134
2017-10-10T14:43:13.746471: step 666, loss 0.254189, acc 0.90625, learning_rate 0.000422811
2017-10-10T14:43:13.944544: step 667, loss 0.21768, acc 0.921875, learning_rate 0.000421493
2017-10-10T14:43:14.123636: step 668, loss 0.193407, acc 0.921875, learning_rate 0.000420181
2017-10-10T14:43:14.327845: step 669, loss 0.291273, acc 0.921875, learning_rate 0.000418874
2017-10-10T14:43:14.520786: step 670, loss 0.209524, acc 0.96875, learning_rate 0.000417573
2017-10-10T14:43:14.716930: step 671, loss 0.266896, acc 0.90625, learning_rate 0.000416276
2017-10-10T14:43:14.868874: step 672, loss 0.117925, acc 0.96875, learning_rate 0.000414985
2017-10-10T14:43:15.095800: step 673, loss 0.215206, acc 0.9375, learning_rate 0.0004137
2017-10-10T14:43:15.304764: step 674, loss 0.204622, acc 0.953125, learning_rate 0.000412419
2017-10-10T14:43:15.502503: step 675, loss 0.168269, acc 0.9375, learning_rate 0.000411144
2017-10-10T14:43:15.724871: step 676, loss 0.186405, acc 0.953125, learning_rate 0.000409874
2017-10-10T14:43:15.938365: step 677, loss 0.195287, acc 0.90625, learning_rate 0.000408609
2017-10-10T14:43:16.119310: step 678, loss 0.254772, acc 0.890625, learning_rate 0.00040735
2017-10-10T14:43:16.292886: step 679, loss 0.262807, acc 0.921875, learning_rate 0.000406095
2017-10-10T14:43:16.453081: step 680, loss 0.240928, acc 0.9375, learning_rate 0.000404846

Evaluation:
2017-10-10T14:43:16.745430: step 680, loss 0.236136, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-680

2017-10-10T14:43:17.691601: step 681, loss 0.197606, acc 0.890625, learning_rate 0.000403601
2017-10-10T14:43:17.832877: step 682, loss 0.203305, acc 0.9375, learning_rate 0.000402362
2017-10-10T14:43:18.001489: step 683, loss 0.222968, acc 0.953125, learning_rate 0.000401128
2017-10-10T14:43:18.189367: step 684, loss 0.182223, acc 0.953125, learning_rate 0.000399899
2017-10-10T14:43:18.320873: step 685, loss 0.357006, acc 0.890625, learning_rate 0.000398675
2017-10-10T14:43:18.474998: step 686, loss 0.350207, acc 0.823529, learning_rate 0.000397456
2017-10-10T14:43:18.668281: step 687, loss 0.230105, acc 0.953125, learning_rate 0.000396241
2017-10-10T14:43:18.849246: step 688, loss 0.191767, acc 0.921875, learning_rate 0.000395032
2017-10-10T14:43:19.049318: step 689, loss 0.170427, acc 0.9375, learning_rate 0.000393828
2017-10-10T14:43:19.262797: step 690, loss 0.107413, acc 0.96875, learning_rate 0.000392629
2017-10-10T14:43:19.503021: step 691, loss 0.291774, acc 0.90625, learning_rate 0.000391434
2017-10-10T14:43:19.649107: step 692, loss 0.252773, acc 0.921875, learning_rate 0.000390245
2017-10-10T14:43:19.865086: step 693, loss 0.167162, acc 0.9375, learning_rate 0.00038906
2017-10-10T14:43:20.032997: step 694, loss 0.156115, acc 0.9375, learning_rate 0.00038788
2017-10-10T14:43:20.248678: step 695, loss 0.161766, acc 0.953125, learning_rate 0.000386705
2017-10-10T14:43:20.463553: step 696, loss 0.308354, acc 0.875, learning_rate 0.000385535
2017-10-10T14:43:20.654747: step 697, loss 0.212342, acc 0.921875, learning_rate 0.000384369
2017-10-10T14:43:20.833016: step 698, loss 0.15769, acc 0.96875, learning_rate 0.000383209
2017-10-10T14:43:21.025666: step 699, loss 0.239083, acc 0.90625, learning_rate 0.000382053
2017-10-10T14:43:21.217159: step 700, loss 0.359964, acc 0.890625, learning_rate 0.000380901
2017-10-10T14:43:21.464847: step 701, loss 0.201576, acc 0.9375, learning_rate 0.000379755
2017-10-10T14:43:21.644296: step 702, loss 0.158006, acc 0.9375, learning_rate 0.000378613
2017-10-10T14:43:21.853654: step 703, loss 0.184328, acc 0.9375, learning_rate 0.000377476
2017-10-10T14:43:21.994163: step 704, loss 0.136192, acc 0.9375, learning_rate 0.000376343
2017-10-10T14:43:22.168590: step 705, loss 0.277428, acc 0.90625, learning_rate 0.000375215
2017-10-10T14:43:22.320439: step 706, loss 0.179312, acc 0.953125, learning_rate 0.000374092
2017-10-10T14:43:22.520869: step 707, loss 0.25614, acc 0.921875, learning_rate 0.000372973
2017-10-10T14:43:22.740813: step 708, loss 0.244154, acc 0.9375, learning_rate 0.000371859
2017-10-10T14:43:22.894333: step 709, loss 0.143511, acc 0.953125, learning_rate 0.000370749
2017-10-10T14:43:23.074245: step 710, loss 0.153944, acc 0.921875, learning_rate 0.000369644
2017-10-10T14:43:23.282891: step 711, loss 0.252967, acc 0.890625, learning_rate 0.000368543
2017-10-10T14:43:23.462283: step 712, loss 0.231583, acc 0.890625, learning_rate 0.000367447
2017-10-10T14:43:23.625874: step 713, loss 0.196904, acc 0.921875, learning_rate 0.000366356
2017-10-10T14:43:23.836864: step 714, loss 0.262141, acc 0.890625, learning_rate 0.000365268
2017-10-10T14:43:24.009656: step 715, loss 0.204521, acc 0.953125, learning_rate 0.000364186
2017-10-10T14:43:24.232815: step 716, loss 0.321923, acc 0.890625, learning_rate 0.000363107
2017-10-10T14:43:24.440652: step 717, loss 0.413964, acc 0.921875, learning_rate 0.000362033
2017-10-10T14:43:24.695342: step 718, loss 0.225633, acc 0.921875, learning_rate 0.000360964
2017-10-10T14:43:24.924850: step 719, loss 0.325012, acc 0.828125, learning_rate 0.000359899
2017-10-10T14:43:25.160807: step 720, loss 0.246219, acc 0.9375, learning_rate 0.000358838

Evaluation:
2017-10-10T14:43:25.431189: step 720, loss 0.233371, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-720

2017-10-10T14:43:26.300850: step 721, loss 0.39615, acc 0.828125, learning_rate 0.000357781
2017-10-10T14:43:26.452532: step 722, loss 0.222794, acc 0.921875, learning_rate 0.000356729
2017-10-10T14:43:26.678282: step 723, loss 0.166449, acc 0.9375, learning_rate 0.000355681
2017-10-10T14:43:26.868867: step 724, loss 0.194815, acc 0.953125, learning_rate 0.000354637
2017-10-10T14:43:27.093000: step 725, loss 0.178222, acc 0.9375, learning_rate 0.000353598
2017-10-10T14:43:27.259985: step 726, loss 0.302634, acc 0.921875, learning_rate 0.000352563
2017-10-10T14:43:27.510745: step 727, loss 0.170181, acc 0.9375, learning_rate 0.000351532
2017-10-10T14:43:27.704484: step 728, loss 0.283852, acc 0.921875, learning_rate 0.000350505
2017-10-10T14:43:27.900917: step 729, loss 0.132768, acc 0.96875, learning_rate 0.000349483
2017-10-10T14:43:28.096817: step 730, loss 0.182497, acc 0.9375, learning_rate 0.000348465
2017-10-10T14:43:28.299581: step 731, loss 0.214061, acc 0.9375, learning_rate 0.00034745
2017-10-10T14:43:28.490668: step 732, loss 0.0660183, acc 1, learning_rate 0.00034644
2017-10-10T14:43:28.691393: step 733, loss 0.151913, acc 0.96875, learning_rate 0.000345434
2017-10-10T14:43:28.924847: step 734, loss 0.231426, acc 0.921875, learning_rate 0.000344433
2017-10-10T14:43:29.135090: step 735, loss 0.129217, acc 0.953125, learning_rate 0.000343435
2017-10-10T14:43:29.342560: step 736, loss 0.283926, acc 0.90625, learning_rate 0.000342441
2017-10-10T14:43:29.558274: step 737, loss 0.17215, acc 0.953125, learning_rate 0.000341452
2017-10-10T14:43:29.768603: step 738, loss 0.161884, acc 0.9375, learning_rate 0.000340466
2017-10-10T14:43:29.972271: step 739, loss 0.202391, acc 0.9375, learning_rate 0.000339485
2017-10-10T14:43:30.133336: step 740, loss 0.163371, acc 0.9375, learning_rate 0.000338507
2017-10-10T14:43:30.382942: step 741, loss 0.199787, acc 0.921875, learning_rate 0.000337534
2017-10-10T14:43:30.597497: step 742, loss 0.144834, acc 0.953125, learning_rate 0.000336564
2017-10-10T14:43:30.854781: step 743, loss 0.269193, acc 0.875, learning_rate 0.000335598
2017-10-10T14:43:31.050832: step 744, loss 0.178426, acc 0.90625, learning_rate 0.000334637
2017-10-10T14:43:31.245792: step 745, loss 0.173185, acc 0.96875, learning_rate 0.000333679
2017-10-10T14:43:31.452820: step 746, loss 0.103726, acc 1, learning_rate 0.000332725
2017-10-10T14:43:31.593481: step 747, loss 0.264271, acc 0.90625, learning_rate 0.000331775
2017-10-10T14:43:31.804199: step 748, loss 0.209164, acc 0.890625, learning_rate 0.000330829
2017-10-10T14:43:31.978716: step 749, loss 0.188241, acc 0.921875, learning_rate 0.000329887
2017-10-10T14:43:32.253024: step 750, loss 0.0941028, acc 0.984375, learning_rate 0.000328949
2017-10-10T14:43:32.453714: step 751, loss 0.177922, acc 0.96875, learning_rate 0.000328014
2017-10-10T14:43:32.686771: step 752, loss 0.142133, acc 0.953125, learning_rate 0.000327083
2017-10-10T14:43:32.928844: step 753, loss 0.208491, acc 0.921875, learning_rate 0.000326157
2017-10-10T14:43:33.120934: step 754, loss 0.326838, acc 0.9375, learning_rate 0.000325233
2017-10-10T14:43:33.307345: step 755, loss 0.204518, acc 0.921875, learning_rate 0.000324314
2017-10-10T14:43:33.556865: step 756, loss 0.163498, acc 0.9375, learning_rate 0.000323399
2017-10-10T14:43:33.807616: step 757, loss 0.171743, acc 0.9375, learning_rate 0.000322487
2017-10-10T14:43:34.015136: step 758, loss 0.20232, acc 0.9375, learning_rate 0.000321579
2017-10-10T14:43:34.218456: step 759, loss 0.295132, acc 0.859375, learning_rate 0.000320674
2017-10-10T14:43:34.455763: step 760, loss 0.143342, acc 0.953125, learning_rate 0.000319773

Evaluation:
2017-10-10T14:43:34.792776: step 760, loss 0.236858, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-760

2017-10-10T14:43:35.845953: step 761, loss 0.197941, acc 0.90625, learning_rate 0.000318876
2017-10-10T14:43:36.036542: step 762, loss 0.162879, acc 0.953125, learning_rate 0.000317983
2017-10-10T14:43:36.270119: step 763, loss 0.167917, acc 0.921875, learning_rate 0.000317093
2017-10-10T14:43:36.456763: step 764, loss 0.0684765, acc 1, learning_rate 0.000316207
2017-10-10T14:43:36.704532: step 765, loss 0.150995, acc 0.984375, learning_rate 0.000315325
2017-10-10T14:43:36.921551: step 766, loss 0.268381, acc 0.875, learning_rate 0.000314446
2017-10-10T14:43:37.167879: step 767, loss 0.126497, acc 0.953125, learning_rate 0.00031357
2017-10-10T14:43:37.382353: step 768, loss 0.257859, acc 0.875, learning_rate 0.000312699
2017-10-10T14:43:37.595764: step 769, loss 0.360532, acc 0.828125, learning_rate 0.00031183
2017-10-10T14:43:37.818000: step 770, loss 0.24251, acc 0.890625, learning_rate 0.000310966
2017-10-10T14:43:38.029007: step 771, loss 0.338338, acc 0.875, learning_rate 0.000310105
2017-10-10T14:43:38.248354: step 772, loss 0.285953, acc 0.90625, learning_rate 0.000309247
2017-10-10T14:43:38.484393: step 773, loss 0.328152, acc 0.875, learning_rate 0.000308393
2017-10-10T14:43:38.717717: step 774, loss 0.267895, acc 0.859375, learning_rate 0.000307542
2017-10-10T14:43:38.985012: step 775, loss 0.266129, acc 0.90625, learning_rate 0.000306695
2017-10-10T14:43:39.157934: step 776, loss 0.161395, acc 0.9375, learning_rate 0.000305852
2017-10-10T14:43:39.400940: step 777, loss 0.180063, acc 0.96875, learning_rate 0.000305011
2017-10-10T14:43:39.651045: step 778, loss 0.181151, acc 0.9375, learning_rate 0.000304174
2017-10-10T14:43:39.844709: step 779, loss 0.235293, acc 0.90625, learning_rate 0.000303341
2017-10-10T14:43:40.080371: step 780, loss 0.187204, acc 0.953125, learning_rate 0.000302511
2017-10-10T14:43:40.304847: step 781, loss 0.185946, acc 0.953125, learning_rate 0.000301684
2017-10-10T14:43:40.545332: step 782, loss 0.257299, acc 0.90625, learning_rate 0.000300861
2017-10-10T14:43:40.776929: step 783, loss 0.16345, acc 0.953125, learning_rate 0.000300041
2017-10-10T14:43:40.980977: step 784, loss 0.141089, acc 0.941176, learning_rate 0.000299225
2017-10-10T14:43:41.186124: step 785, loss 0.140405, acc 0.953125, learning_rate 0.000298412
2017-10-10T14:43:41.392179: step 786, loss 0.182574, acc 0.9375, learning_rate 0.000297602
2017-10-10T14:43:41.535421: step 787, loss 0.100581, acc 0.984375, learning_rate 0.000296795
2017-10-10T14:43:41.661156: step 788, loss 0.311352, acc 0.90625, learning_rate 0.000295992
2017-10-10T14:43:41.836441: step 789, loss 0.151032, acc 0.9375, learning_rate 0.000295192
2017-10-10T14:43:42.013148: step 790, loss 0.237078, acc 0.9375, learning_rate 0.000294395
2017-10-10T14:43:42.164827: step 791, loss 0.209663, acc 0.921875, learning_rate 0.000293602
2017-10-10T14:43:42.368344: step 792, loss 0.162869, acc 0.953125, learning_rate 0.000292812
2017-10-10T14:43:42.606592: step 793, loss 0.279735, acc 0.875, learning_rate 0.000292025
2017-10-10T14:43:42.836916: step 794, loss 0.203528, acc 0.921875, learning_rate 0.000291241
2017-10-10T14:43:43.119106: step 795, loss 0.182749, acc 0.953125, learning_rate 0.00029046
2017-10-10T14:43:43.371123: step 796, loss 0.205832, acc 0.9375, learning_rate 0.000289683
2017-10-10T14:43:43.584181: step 797, loss 0.192047, acc 0.9375, learning_rate 0.000288908
2017-10-10T14:43:43.806582: step 798, loss 0.217953, acc 0.90625, learning_rate 0.000288137
2017-10-10T14:43:44.064976: step 799, loss 0.259196, acc 0.921875, learning_rate 0.000287369
2017-10-10T14:43:44.264249: step 800, loss 0.118894, acc 0.96875, learning_rate 0.000286605

Evaluation:
2017-10-10T14:43:44.673453: step 800, loss 0.234893, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-800

2017-10-10T14:43:45.731670: step 801, loss 0.25384, acc 0.90625, learning_rate 0.000285843
2017-10-10T14:43:45.922737: step 802, loss 0.138348, acc 0.984375, learning_rate 0.000285084
2017-10-10T14:43:46.094180: step 803, loss 0.227361, acc 0.9375, learning_rate 0.000284329
2017-10-10T14:43:46.291680: step 804, loss 0.181385, acc 0.921875, learning_rate 0.000283577
2017-10-10T14:43:46.459027: step 805, loss 0.276458, acc 0.90625, learning_rate 0.000282827
2017-10-10T14:43:46.664916: step 806, loss 0.170161, acc 0.953125, learning_rate 0.000282081
2017-10-10T14:43:46.837558: step 807, loss 0.112681, acc 0.953125, learning_rate 0.000281338
2017-10-10T14:43:47.084845: step 808, loss 0.185962, acc 0.953125, learning_rate 0.000280598
2017-10-10T14:43:47.298547: step 809, loss 0.234223, acc 0.9375, learning_rate 0.00027986
2017-10-10T14:43:47.518951: step 810, loss 0.148263, acc 0.9375, learning_rate 0.000279126
2017-10-10T14:43:47.720138: step 811, loss 0.179877, acc 0.953125, learning_rate 0.000278395
2017-10-10T14:43:47.967095: step 812, loss 0.190989, acc 0.9375, learning_rate 0.000277667
2017-10-10T14:43:48.238205: step 813, loss 0.243525, acc 0.921875, learning_rate 0.000276942
2017-10-10T14:43:48.448969: step 814, loss 0.246878, acc 0.90625, learning_rate 0.00027622
2017-10-10T14:43:48.680840: step 815, loss 0.125746, acc 0.984375, learning_rate 0.0002755
2017-10-10T14:43:48.997243: step 816, loss 0.199507, acc 0.921875, learning_rate 0.000274784
2017-10-10T14:43:49.216184: step 817, loss 0.103972, acc 0.984375, learning_rate 0.000274071
2017-10-10T14:43:49.459806: step 818, loss 0.155715, acc 0.953125, learning_rate 0.00027336
2017-10-10T14:43:49.766611: step 819, loss 0.242771, acc 0.921875, learning_rate 0.000272652
2017-10-10T14:43:49.973726: step 820, loss 0.227835, acc 0.90625, learning_rate 0.000271948
2017-10-10T14:43:50.195717: step 821, loss 0.249172, acc 0.921875, learning_rate 0.000271246
2017-10-10T14:43:50.405065: step 822, loss 0.246382, acc 0.953125, learning_rate 0.000270547
2017-10-10T14:43:50.691736: step 823, loss 0.266391, acc 0.890625, learning_rate 0.000269851
2017-10-10T14:43:50.984826: step 824, loss 0.175882, acc 0.9375, learning_rate 0.000269157
2017-10-10T14:43:51.245755: step 825, loss 0.134604, acc 0.984375, learning_rate 0.000268467
2017-10-10T14:43:51.452941: step 826, loss 0.241357, acc 0.859375, learning_rate 0.000267779
2017-10-10T14:43:51.756907: step 827, loss 0.270733, acc 0.90625, learning_rate 0.000267094
2017-10-10T14:43:52.043563: step 828, loss 0.193634, acc 0.921875, learning_rate 0.000266412
2017-10-10T14:43:52.294786: step 829, loss 0.112637, acc 0.953125, learning_rate 0.000265733
2017-10-10T14:43:52.561937: step 830, loss 0.286813, acc 0.875, learning_rate 0.000265057
2017-10-10T14:43:52.824674: step 831, loss 0.268092, acc 0.921875, learning_rate 0.000264383
2017-10-10T14:43:53.117343: step 832, loss 0.214176, acc 0.890625, learning_rate 0.000263712
2017-10-10T14:43:53.339224: step 833, loss 0.132704, acc 0.953125, learning_rate 0.000263044
2017-10-10T14:43:53.548473: step 834, loss 0.187688, acc 0.9375, learning_rate 0.000262378
2017-10-10T14:43:53.797297: step 835, loss 0.146578, acc 0.90625, learning_rate 0.000261715
2017-10-10T14:43:53.988856: step 836, loss 0.199675, acc 0.921875, learning_rate 0.000261055
2017-10-10T14:43:54.253946: step 837, loss 0.234495, acc 0.90625, learning_rate 0.000260398
2017-10-10T14:43:54.467245: step 838, loss 0.302226, acc 0.890625, learning_rate 0.000259743
2017-10-10T14:43:54.790441: step 839, loss 0.175963, acc 0.953125, learning_rate 0.000259091
2017-10-10T14:43:55.044679: step 840, loss 0.221981, acc 0.953125, learning_rate 0.000258442

Evaluation:
2017-10-10T14:43:55.459915: step 840, loss 0.234454, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-840

2017-10-10T14:43:56.378379: step 841, loss 0.186809, acc 0.90625, learning_rate 0.000257795
2017-10-10T14:43:56.630641: step 842, loss 0.260943, acc 0.9375, learning_rate 0.000257151
2017-10-10T14:43:56.849215: step 843, loss 0.213727, acc 0.921875, learning_rate 0.00025651
2017-10-10T14:43:57.091806: step 844, loss 0.0950141, acc 0.984375, learning_rate 0.000255871
2017-10-10T14:43:57.327590: step 845, loss 0.170485, acc 0.9375, learning_rate 0.000255235
2017-10-10T14:43:57.566474: step 846, loss 0.244061, acc 0.921875, learning_rate 0.000254601
2017-10-10T14:43:57.811992: step 847, loss 0.165647, acc 0.953125, learning_rate 0.00025397
2017-10-10T14:43:58.060307: step 848, loss 0.299824, acc 0.921875, learning_rate 0.000253341
2017-10-10T14:43:58.316484: step 849, loss 0.131852, acc 0.953125, learning_rate 0.000252716
2017-10-10T14:43:58.617109: step 850, loss 0.128334, acc 0.953125, learning_rate 0.000252092
2017-10-10T14:43:58.831979: step 851, loss 0.1602, acc 0.953125, learning_rate 0.000251471
2017-10-10T14:43:59.153069: step 852, loss 0.247596, acc 0.90625, learning_rate 0.000250853
2017-10-10T14:43:59.391699: step 853, loss 0.100145, acc 0.984375, learning_rate 0.000250237
2017-10-10T14:43:59.571481: step 854, loss 0.15234, acc 0.9375, learning_rate 0.000249624
2017-10-10T14:43:59.776967: step 855, loss 0.210405, acc 0.90625, learning_rate 0.000249013
2017-10-10T14:43:59.971162: step 856, loss 0.151966, acc 0.953125, learning_rate 0.000248405
2017-10-10T14:44:00.181830: step 857, loss 0.293481, acc 0.859375, learning_rate 0.000247799
2017-10-10T14:44:00.499561: step 858, loss 0.147728, acc 0.9375, learning_rate 0.000247196
2017-10-10T14:44:01.430018: step 859, loss 0.105264, acc 0.96875, learning_rate 0.000246595
2017-10-10T14:44:01.710555: step 860, loss 0.117675, acc 0.96875, learning_rate 0.000245997
2017-10-10T14:44:01.975233: step 861, loss 0.235144, acc 0.921875, learning_rate 0.000245401
2017-10-10T14:44:02.261124: step 862, loss 0.299414, acc 0.890625, learning_rate 0.000244808
2017-10-10T14:44:02.484888: step 863, loss 0.238901, acc 0.9375, learning_rate 0.000244216
2017-10-10T14:44:02.734107: step 864, loss 0.233781, acc 0.890625, learning_rate 0.000243628
2017-10-10T14:44:03.007630: step 865, loss 0.138609, acc 0.953125, learning_rate 0.000243042
2017-10-10T14:44:03.183277: step 866, loss 0.251614, acc 0.9375, learning_rate 0.000242458
2017-10-10T14:44:03.380705: step 867, loss 0.128981, acc 0.953125, learning_rate 0.000241876
2017-10-10T14:44:03.600850: step 868, loss 0.144199, acc 0.9375, learning_rate 0.000241297
2017-10-10T14:44:03.872812: step 869, loss 0.0818933, acc 0.984375, learning_rate 0.00024072
2017-10-10T14:44:04.084441: step 870, loss 0.240696, acc 0.9375, learning_rate 0.000240146
2017-10-10T14:44:04.349819: step 871, loss 0.139837, acc 0.984375, learning_rate 0.000239574
2017-10-10T14:44:04.549307: step 872, loss 0.139181, acc 0.96875, learning_rate 0.000239004
2017-10-10T14:44:04.823205: step 873, loss 0.179064, acc 0.96875, learning_rate 0.000238437
2017-10-10T14:44:05.204820: step 874, loss 0.159377, acc 0.9375, learning_rate 0.000237872
2017-10-10T14:44:05.482533: step 875, loss 0.259159, acc 0.890625, learning_rate 0.000237309
2017-10-10T14:44:05.678244: step 876, loss 0.154567, acc 0.953125, learning_rate 0.000236749
2017-10-10T14:44:05.928904: step 877, loss 0.140981, acc 0.96875, learning_rate 0.00023619
2017-10-10T14:44:06.168149: step 878, loss 0.260693, acc 0.921875, learning_rate 0.000235635
2017-10-10T14:44:06.409514: step 879, loss 0.251618, acc 0.90625, learning_rate 0.000235081
2017-10-10T14:44:06.704004: step 880, loss 0.170666, acc 0.9375, learning_rate 0.00023453

Evaluation:
2017-10-10T14:44:07.116363: step 880, loss 0.231089, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-880

2017-10-10T14:44:08.160866: step 881, loss 0.264744, acc 0.921875, learning_rate 0.00023398
2017-10-10T14:44:08.412878: step 882, loss 0.114, acc 0.980392, learning_rate 0.000233434
2017-10-10T14:44:08.612953: step 883, loss 0.115722, acc 0.984375, learning_rate 0.000232889
2017-10-10T14:44:08.869513: step 884, loss 0.311758, acc 0.859375, learning_rate 0.000232346
2017-10-10T14:44:09.106683: step 885, loss 0.125313, acc 0.984375, learning_rate 0.000231806
2017-10-10T14:44:09.316164: step 886, loss 0.091671, acc 0.96875, learning_rate 0.000231268
2017-10-10T14:44:09.582045: step 887, loss 0.181599, acc 0.9375, learning_rate 0.000230732
2017-10-10T14:44:09.768910: step 888, loss 0.183855, acc 0.9375, learning_rate 0.000230199
2017-10-10T14:44:10.088825: step 889, loss 0.464807, acc 0.875, learning_rate 0.000229667
2017-10-10T14:44:10.360946: step 890, loss 0.135644, acc 0.9375, learning_rate 0.000229138
2017-10-10T14:44:10.560901: step 891, loss 0.199752, acc 0.9375, learning_rate 0.000228611
2017-10-10T14:44:10.815001: step 892, loss 0.389962, acc 0.875, learning_rate 0.000228086
2017-10-10T14:44:11.092924: step 893, loss 0.164135, acc 0.921875, learning_rate 0.000227563
2017-10-10T14:44:11.312917: step 894, loss 0.107309, acc 1, learning_rate 0.000227043
2017-10-10T14:44:11.513322: step 895, loss 0.206917, acc 0.921875, learning_rate 0.000226524
2017-10-10T14:44:11.735672: step 896, loss 0.0980077, acc 0.984375, learning_rate 0.000226008
2017-10-10T14:44:11.978725: step 897, loss 0.201007, acc 0.9375, learning_rate 0.000225493
2017-10-10T14:44:12.248035: step 898, loss 0.121099, acc 0.96875, learning_rate 0.000224981
2017-10-10T14:44:12.524822: step 899, loss 0.214954, acc 0.9375, learning_rate 0.000224471
2017-10-10T14:44:12.771031: step 900, loss 0.362942, acc 0.859375, learning_rate 0.000223963
2017-10-10T14:44:13.020367: step 901, loss 0.124638, acc 0.953125, learning_rate 0.000223457
2017-10-10T14:44:13.229090: step 902, loss 0.209585, acc 0.953125, learning_rate 0.000222953
2017-10-10T14:44:13.450458: step 903, loss 0.142379, acc 0.953125, learning_rate 0.000222451
2017-10-10T14:44:13.678421: step 904, loss 0.219812, acc 0.9375, learning_rate 0.000221951
2017-10-10T14:44:13.948177: step 905, loss 0.267164, acc 0.921875, learning_rate 0.000221453
2017-10-10T14:44:14.241814: step 906, loss 0.140942, acc 0.96875, learning_rate 0.000220958
2017-10-10T14:44:14.488730: step 907, loss 0.238213, acc 0.90625, learning_rate 0.000220464
2017-10-10T14:44:14.758560: step 908, loss 0.228835, acc 0.90625, learning_rate 0.000219972
2017-10-10T14:44:14.953929: step 909, loss 0.0971881, acc 0.96875, learning_rate 0.000219483
2017-10-10T14:44:15.195340: step 910, loss 0.3095, acc 0.875, learning_rate 0.000218995
2017-10-10T14:44:15.496339: step 911, loss 0.221137, acc 0.9375, learning_rate 0.000218509
2017-10-10T14:44:15.699458: step 912, loss 0.216827, acc 0.9375, learning_rate 0.000218025
2017-10-10T14:44:15.972863: step 913, loss 0.264752, acc 0.890625, learning_rate 0.000217544
2017-10-10T14:44:16.169597: step 914, loss 0.166595, acc 0.9375, learning_rate 0.000217064
2017-10-10T14:44:16.425720: step 915, loss 0.300319, acc 0.890625, learning_rate 0.000216586
2017-10-10T14:44:16.656860: step 916, loss 0.136575, acc 0.953125, learning_rate 0.00021611
2017-10-10T14:44:16.905361: step 917, loss 0.177892, acc 0.953125, learning_rate 0.000215636
2017-10-10T14:44:17.169310: step 918, loss 0.126656, acc 0.984375, learning_rate 0.000215164
2017-10-10T14:44:17.494712: step 919, loss 0.164598, acc 0.9375, learning_rate 0.000214694
2017-10-10T14:44:17.709345: step 920, loss 0.334618, acc 0.875, learning_rate 0.000214226

Evaluation:
2017-10-10T14:44:17.993263: step 920, loss 0.230243, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-920

2017-10-10T14:44:19.003463: step 921, loss 0.163628, acc 0.9375, learning_rate 0.00021376
2017-10-10T14:44:19.208854: step 922, loss 0.189587, acc 0.9375, learning_rate 0.000213295
2017-10-10T14:44:19.427047: step 923, loss 0.184109, acc 0.90625, learning_rate 0.000212833
2017-10-10T14:44:19.626308: step 924, loss 0.15885, acc 0.953125, learning_rate 0.000212372
2017-10-10T14:44:19.820924: step 925, loss 0.130697, acc 0.96875, learning_rate 0.000211914
2017-10-10T14:44:20.032835: step 926, loss 0.176053, acc 0.953125, learning_rate 0.000211457
2017-10-10T14:44:20.255350: step 927, loss 0.178801, acc 0.9375, learning_rate 0.000211002
2017-10-10T14:44:20.502670: step 928, loss 0.203074, acc 0.9375, learning_rate 0.000210549
2017-10-10T14:44:20.802110: step 929, loss 0.0768476, acc 0.984375, learning_rate 0.000210098
2017-10-10T14:44:21.077100: step 930, loss 0.233126, acc 0.921875, learning_rate 0.000209648
2017-10-10T14:44:21.317001: step 931, loss 0.301857, acc 0.90625, learning_rate 0.000209201
2017-10-10T14:44:21.601680: step 932, loss 0.133065, acc 0.984375, learning_rate 0.000208755
2017-10-10T14:44:21.837859: step 933, loss 0.164471, acc 0.96875, learning_rate 0.000208311
2017-10-10T14:44:22.122656: step 934, loss 0.177244, acc 0.953125, learning_rate 0.000207869
2017-10-10T14:44:22.364900: step 935, loss 0.195599, acc 0.953125, learning_rate 0.000207429
2017-10-10T14:44:22.663543: step 936, loss 0.205049, acc 0.90625, learning_rate 0.00020699
2017-10-10T14:44:22.930067: step 937, loss 0.0726582, acc 0.96875, learning_rate 0.000206554
2017-10-10T14:44:23.121059: step 938, loss 0.101941, acc 0.984375, learning_rate 0.000206119
2017-10-10T14:44:23.386955: step 939, loss 0.38529, acc 0.859375, learning_rate 0.000205685
2017-10-10T14:44:23.679393: step 940, loss 0.178699, acc 0.921875, learning_rate 0.000205254
2017-10-10T14:44:23.904368: step 941, loss 0.246578, acc 0.90625, learning_rate 0.000204824
2017-10-10T14:44:24.159037: step 942, loss 0.301645, acc 0.859375, learning_rate 0.000204397
2017-10-10T14:44:24.423687: step 943, loss 0.192371, acc 0.90625, learning_rate 0.00020397
2017-10-10T14:44:24.622514: step 944, loss 0.203275, acc 0.9375, learning_rate 0.000203546
2017-10-10T14:44:24.890247: step 945, loss 0.172133, acc 0.953125, learning_rate 0.000203123
2017-10-10T14:44:25.220839: step 946, loss 0.308708, acc 0.921875, learning_rate 0.000202702
2017-10-10T14:44:25.412104: step 947, loss 0.228288, acc 0.90625, learning_rate 0.000202283
2017-10-10T14:44:25.668855: step 948, loss 0.156666, acc 0.953125, learning_rate 0.000201866
2017-10-10T14:44:25.887793: step 949, loss 0.174001, acc 0.953125, learning_rate 0.00020145
2017-10-10T14:44:26.182052: step 950, loss 0.216064, acc 0.921875, learning_rate 0.000201036
2017-10-10T14:44:26.395099: step 951, loss 0.171574, acc 0.953125, learning_rate 0.000200623
2017-10-10T14:44:26.688868: step 952, loss 0.196995, acc 0.921875, learning_rate 0.000200213
2017-10-10T14:44:26.992923: step 953, loss 0.293197, acc 0.875, learning_rate 0.000199804
2017-10-10T14:44:27.204887: step 954, loss 0.247781, acc 0.890625, learning_rate 0.000199396
2017-10-10T14:44:27.484149: step 955, loss 0.254741, acc 0.890625, learning_rate 0.000198991
2017-10-10T14:44:27.728821: step 956, loss 0.155341, acc 0.921875, learning_rate 0.000198587
2017-10-10T14:44:27.961034: step 957, loss 0.25059, acc 0.90625, learning_rate 0.000198184
2017-10-10T14:44:28.252162: step 958, loss 0.13719, acc 0.96875, learning_rate 0.000197783
2017-10-10T14:44:28.463520: step 959, loss 0.19089, acc 0.90625, learning_rate 0.000197384
2017-10-10T14:44:28.727985: step 960, loss 0.336937, acc 0.890625, learning_rate 0.000196987

Evaluation:
2017-10-10T14:44:29.135320: step 960, loss 0.231599, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-960

2017-10-10T14:44:30.028526: step 961, loss 0.307271, acc 0.921875, learning_rate 0.000196591
2017-10-10T14:44:30.228874: step 962, loss 0.195404, acc 0.953125, learning_rate 0.000196197
2017-10-10T14:44:30.451413: step 963, loss 0.252689, acc 0.953125, learning_rate 0.000195804
2017-10-10T14:44:30.681363: step 964, loss 0.179827, acc 0.953125, learning_rate 0.000195413
2017-10-10T14:44:30.893731: step 965, loss 0.183397, acc 0.953125, learning_rate 0.000195023
2017-10-10T14:44:31.152953: step 966, loss 0.17956, acc 0.9375, learning_rate 0.000194636
2017-10-10T14:44:31.398600: step 967, loss 0.210857, acc 0.9375, learning_rate 0.000194249
2017-10-10T14:44:31.632035: step 968, loss 0.270507, acc 0.90625, learning_rate 0.000193865
2017-10-10T14:44:31.952895: step 969, loss 0.186623, acc 0.953125, learning_rate 0.000193482
2017-10-10T14:44:32.226498: step 970, loss 0.243453, acc 0.921875, learning_rate 0.0001931
2017-10-10T14:44:32.409663: step 971, loss 0.171961, acc 0.921875, learning_rate 0.00019272
2017-10-10T14:44:32.667135: step 972, loss 0.211398, acc 0.921875, learning_rate 0.000192341
2017-10-10T14:44:32.933161: step 973, loss 0.138801, acc 0.953125, learning_rate 0.000191965
2017-10-10T14:44:33.163097: step 974, loss 0.24388, acc 0.9375, learning_rate 0.000191589
2017-10-10T14:44:33.389364: step 975, loss 0.164046, acc 0.953125, learning_rate 0.000191215
2017-10-10T14:44:33.608294: step 976, loss 0.164043, acc 0.953125, learning_rate 0.000190843
2017-10-10T14:44:33.893209: step 977, loss 0.135928, acc 0.96875, learning_rate 0.000190472
2017-10-10T14:44:34.156705: step 978, loss 0.213044, acc 0.9375, learning_rate 0.000190103
2017-10-10T14:44:34.410388: step 979, loss 0.117457, acc 1, learning_rate 0.000189735
2017-10-10T14:44:34.633217: step 980, loss 0.153042, acc 0.921569, learning_rate 0.000189369
2017-10-10T14:44:34.913907: step 981, loss 0.186535, acc 0.921875, learning_rate 0.000189004
2017-10-10T14:44:35.197023: step 982, loss 0.240749, acc 0.90625, learning_rate 0.000188641
2017-10-10T14:44:35.482211: step 983, loss 0.143797, acc 0.9375, learning_rate 0.000188279
2017-10-10T14:44:35.781533: step 984, loss 0.16748, acc 0.921875, learning_rate 0.000187919
2017-10-10T14:44:36.274215: step 985, loss 0.196227, acc 0.921875, learning_rate 0.00018756
2017-10-10T14:44:36.448852: step 986, loss 0.173975, acc 0.953125, learning_rate 0.000187202
2017-10-10T14:44:36.633664: step 987, loss 0.141148, acc 0.921875, learning_rate 0.000186846
2017-10-10T14:44:36.823872: step 988, loss 0.132039, acc 0.96875, learning_rate 0.000186492
2017-10-10T14:44:37.037054: step 989, loss 0.156754, acc 0.953125, learning_rate 0.000186139
2017-10-10T14:44:37.313762: step 990, loss 0.193957, acc 0.921875, learning_rate 0.000185787
2017-10-10T14:44:37.505158: step 991, loss 0.272051, acc 0.890625, learning_rate 0.000185437
2017-10-10T14:44:37.773029: step 992, loss 0.139073, acc 0.953125, learning_rate 0.000185088
2017-10-10T14:44:37.960852: step 993, loss 0.258459, acc 0.890625, learning_rate 0.000184741
2017-10-10T14:44:38.223140: step 994, loss 0.185312, acc 0.921875, learning_rate 0.000184395
2017-10-10T14:44:38.536821: step 995, loss 0.22944, acc 0.953125, learning_rate 0.000184051
2017-10-10T14:44:38.753798: step 996, loss 0.245231, acc 0.890625, learning_rate 0.000183708
2017-10-10T14:44:39.055862: step 997, loss 0.218068, acc 0.90625, learning_rate 0.000183366
2017-10-10T14:44:39.334266: step 998, loss 0.135294, acc 0.984375, learning_rate 0.000183026
2017-10-10T14:44:39.589336: step 999, loss 0.252542, acc 0.921875, learning_rate 0.000182687
2017-10-10T14:44:39.804595: step 1000, loss 0.129694, acc 0.953125, learning_rate 0.000182349

Evaluation:
2017-10-10T14:44:40.201102: step 1000, loss 0.230631, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1000

2017-10-10T14:44:41.279572: step 1001, loss 0.130989, acc 0.953125, learning_rate 0.000182013
2017-10-10T14:44:41.553180: step 1002, loss 0.208298, acc 0.90625, learning_rate 0.000181678
2017-10-10T14:44:41.823072: step 1003, loss 0.142167, acc 0.953125, learning_rate 0.000181345
2017-10-10T14:44:42.054236: step 1004, loss 0.300743, acc 0.921875, learning_rate 0.000181013
2017-10-10T14:44:42.272895: step 1005, loss 0.125704, acc 0.953125, learning_rate 0.000180682
2017-10-10T14:44:42.508829: step 1006, loss 0.289165, acc 0.890625, learning_rate 0.000180353
2017-10-10T14:44:42.709100: step 1007, loss 0.195034, acc 0.953125, learning_rate 0.000180025
2017-10-10T14:44:43.031997: step 1008, loss 0.175769, acc 0.953125, learning_rate 0.000179698
2017-10-10T14:44:43.261057: step 1009, loss 0.177792, acc 0.953125, learning_rate 0.000179373
2017-10-10T14:44:43.468581: step 1010, loss 0.243069, acc 0.9375, learning_rate 0.000179049
2017-10-10T14:44:43.736446: step 1011, loss 0.212664, acc 0.921875, learning_rate 0.000178726
2017-10-10T14:44:43.974967: step 1012, loss 0.102762, acc 0.96875, learning_rate 0.000178405
2017-10-10T14:44:44.200177: step 1013, loss 0.199238, acc 0.96875, learning_rate 0.000178085
2017-10-10T14:44:44.388113: step 1014, loss 0.167926, acc 0.921875, learning_rate 0.000177766
2017-10-10T14:44:44.632945: step 1015, loss 0.193249, acc 0.9375, learning_rate 0.000177449
2017-10-10T14:44:44.931900: step 1016, loss 0.111121, acc 0.96875, learning_rate 0.000177133
2017-10-10T14:44:45.207651: step 1017, loss 0.148966, acc 0.9375, learning_rate 0.000176818
2017-10-10T14:44:45.442233: step 1018, loss 0.259356, acc 0.890625, learning_rate 0.000176504
2017-10-10T14:44:45.704634: step 1019, loss 0.185638, acc 0.921875, learning_rate 0.000176192
2017-10-10T14:44:45.898174: step 1020, loss 0.0862287, acc 0.96875, learning_rate 0.000175881
2017-10-10T14:44:46.169286: step 1021, loss 0.199597, acc 0.921875, learning_rate 0.000175571
2017-10-10T14:44:46.373036: step 1022, loss 0.201148, acc 0.9375, learning_rate 0.000175263
2017-10-10T14:44:46.630923: step 1023, loss 0.12633, acc 0.96875, learning_rate 0.000174956
2017-10-10T14:44:46.922629: step 1024, loss 0.193549, acc 0.9375, learning_rate 0.00017465
2017-10-10T14:44:47.130782: step 1025, loss 0.238529, acc 0.9375, learning_rate 0.000174345
2017-10-10T14:44:47.411762: step 1026, loss 0.142188, acc 0.96875, learning_rate 0.000174042
2017-10-10T14:44:47.621624: step 1027, loss 0.212881, acc 0.9375, learning_rate 0.000173739
2017-10-10T14:44:47.833480: step 1028, loss 0.136745, acc 0.953125, learning_rate 0.000173438
2017-10-10T14:44:48.100418: step 1029, loss 0.107337, acc 0.96875, learning_rate 0.000173139
2017-10-10T14:44:48.349070: step 1030, loss 0.254146, acc 0.890625, learning_rate 0.00017284
2017-10-10T14:44:48.646592: step 1031, loss 0.159322, acc 0.9375, learning_rate 0.000172543
2017-10-10T14:44:48.874999: step 1032, loss 0.218773, acc 0.9375, learning_rate 0.000172247
2017-10-10T14:44:49.176878: step 1033, loss 0.199695, acc 0.9375, learning_rate 0.000171952
2017-10-10T14:44:49.388014: step 1034, loss 0.146773, acc 0.953125, learning_rate 0.000171658
2017-10-10T14:44:49.616399: step 1035, loss 0.226663, acc 0.921875, learning_rate 0.000171366
2017-10-10T14:44:49.888899: step 1036, loss 0.318084, acc 0.921875, learning_rate 0.000171074
2017-10-10T14:44:50.129183: step 1037, loss 0.20543, acc 0.953125, learning_rate 0.000170784
2017-10-10T14:44:50.410472: step 1038, loss 0.280341, acc 0.890625, learning_rate 0.000170495
2017-10-10T14:44:50.712343: step 1039, loss 0.28021, acc 0.890625, learning_rate 0.000170208
2017-10-10T14:44:50.960966: step 1040, loss 0.150099, acc 0.953125, learning_rate 0.000169921

Evaluation:
2017-10-10T14:44:51.464837: step 1040, loss 0.230341, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1040

2017-10-10T14:44:52.562740: step 1041, loss 0.119879, acc 0.953125, learning_rate 0.000169636
2017-10-10T14:44:52.848877: step 1042, loss 0.125868, acc 0.96875, learning_rate 0.000169351
2017-10-10T14:44:53.178110: step 1043, loss 0.207779, acc 0.953125, learning_rate 0.000169068
2017-10-10T14:44:53.388909: step 1044, loss 0.230846, acc 0.875, learning_rate 0.000168786
2017-10-10T14:44:53.642570: step 1045, loss 0.260046, acc 0.875, learning_rate 0.000168506
2017-10-10T14:44:53.855735: step 1046, loss 0.236404, acc 0.90625, learning_rate 0.000168226
2017-10-10T14:44:54.096037: step 1047, loss 0.302607, acc 0.90625, learning_rate 0.000167947
2017-10-10T14:44:54.273776: step 1048, loss 0.182569, acc 0.9375, learning_rate 0.00016767
2017-10-10T14:44:54.426737: step 1049, loss 0.248585, acc 0.921875, learning_rate 0.000167394
2017-10-10T14:44:54.580756: step 1050, loss 0.215943, acc 0.90625, learning_rate 0.000167119
2017-10-10T14:44:54.810735: step 1051, loss 0.116901, acc 0.953125, learning_rate 0.000166845
2017-10-10T14:44:55.029494: step 1052, loss 0.194814, acc 0.921875, learning_rate 0.000166572
2017-10-10T14:44:55.223732: step 1053, loss 0.108025, acc 0.96875, learning_rate 0.0001663
2017-10-10T14:44:55.524868: step 1054, loss 0.211518, acc 0.921875, learning_rate 0.00016603
2017-10-10T14:44:55.785046: step 1055, loss 0.197403, acc 0.9375, learning_rate 0.00016576
2017-10-10T14:44:56.003774: step 1056, loss 0.15113, acc 0.953125, learning_rate 0.000165492
2017-10-10T14:44:56.244956: step 1057, loss 0.191337, acc 0.9375, learning_rate 0.000165224
2017-10-10T14:44:56.445012: step 1058, loss 0.299892, acc 0.875, learning_rate 0.000164958
2017-10-10T14:44:56.696952: step 1059, loss 0.271354, acc 0.953125, learning_rate 0.000164693
2017-10-10T14:44:56.897039: step 1060, loss 0.252052, acc 0.9375, learning_rate 0.000164429
2017-10-10T14:44:57.163542: step 1061, loss 0.208427, acc 0.921875, learning_rate 0.000164166
2017-10-10T14:44:57.393102: step 1062, loss 0.128127, acc 0.953125, learning_rate 0.000163904
2017-10-10T14:44:57.580420: step 1063, loss 0.152862, acc 0.9375, learning_rate 0.000163643
2017-10-10T14:44:57.866358: step 1064, loss 0.081476, acc 0.984375, learning_rate 0.000163383
2017-10-10T14:44:58.109891: step 1065, loss 0.11441, acc 0.96875, learning_rate 0.000163125
2017-10-10T14:44:58.343067: step 1066, loss 0.226474, acc 0.921875, learning_rate 0.000162867
2017-10-10T14:44:58.602087: step 1067, loss 0.12234, acc 0.953125, learning_rate 0.00016261
2017-10-10T14:44:58.795814: step 1068, loss 0.131612, acc 0.96875, learning_rate 0.000162355
2017-10-10T14:44:59.013512: step 1069, loss 0.0901704, acc 0.96875, learning_rate 0.0001621
2017-10-10T14:44:59.300532: step 1070, loss 0.232962, acc 0.875, learning_rate 0.000161847
2017-10-10T14:44:59.560861: step 1071, loss 0.186237, acc 0.9375, learning_rate 0.000161594
2017-10-10T14:44:59.764879: step 1072, loss 0.192145, acc 0.9375, learning_rate 0.000161343
2017-10-10T14:45:00.093384: step 1073, loss 0.18848, acc 0.9375, learning_rate 0.000161093
2017-10-10T14:45:00.338579: step 1074, loss 0.217584, acc 0.921875, learning_rate 0.000160843
2017-10-10T14:45:00.544124: step 1075, loss 0.301776, acc 0.875, learning_rate 0.000160595
2017-10-10T14:45:00.797806: step 1076, loss 0.162541, acc 0.953125, learning_rate 0.000160348
2017-10-10T14:45:00.979988: step 1077, loss 0.15471, acc 0.953125, learning_rate 0.000160101
2017-10-10T14:45:01.264764: step 1078, loss 0.144844, acc 0.980392, learning_rate 0.000159856
2017-10-10T14:45:01.525710: step 1079, loss 0.227317, acc 0.921875, learning_rate 0.000159612
2017-10-10T14:45:01.789474: step 1080, loss 0.208156, acc 0.921875, learning_rate 0.000159368

Evaluation:
2017-10-10T14:45:02.140989: step 1080, loss 0.228732, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1080

2017-10-10T14:45:03.238035: step 1081, loss 0.186468, acc 0.9375, learning_rate 0.000159126
2017-10-10T14:45:03.472699: step 1082, loss 0.236044, acc 0.875, learning_rate 0.000158885
2017-10-10T14:45:03.744912: step 1083, loss 0.122351, acc 0.984375, learning_rate 0.000158644
2017-10-10T14:45:04.024808: step 1084, loss 0.173979, acc 0.9375, learning_rate 0.000158405
2017-10-10T14:45:04.287764: step 1085, loss 0.131947, acc 0.953125, learning_rate 0.000158167
2017-10-10T14:45:04.545030: step 1086, loss 0.157884, acc 0.953125, learning_rate 0.000157929
2017-10-10T14:45:04.775884: step 1087, loss 0.189628, acc 0.9375, learning_rate 0.000157693
2017-10-10T14:45:04.952325: step 1088, loss 0.301463, acc 0.890625, learning_rate 0.000157457
2017-10-10T14:45:05.232924: step 1089, loss 0.187334, acc 0.9375, learning_rate 0.000157223
2017-10-10T14:45:05.513815: step 1090, loss 0.196141, acc 0.9375, learning_rate 0.000156989
2017-10-10T14:45:05.703173: step 1091, loss 0.257442, acc 0.921875, learning_rate 0.000156757
2017-10-10T14:45:05.952266: step 1092, loss 0.189159, acc 0.921875, learning_rate 0.000156525
2017-10-10T14:45:06.216160: step 1093, loss 0.134039, acc 0.953125, learning_rate 0.000156294
2017-10-10T14:45:06.517622: step 1094, loss 0.28289, acc 0.890625, learning_rate 0.000156064
2017-10-10T14:45:06.721194: step 1095, loss 0.262167, acc 0.890625, learning_rate 0.000155836
2017-10-10T14:45:07.004809: step 1096, loss 0.33312, acc 0.890625, learning_rate 0.000155608
2017-10-10T14:45:07.254743: step 1097, loss 0.0865681, acc 1, learning_rate 0.000155381
2017-10-10T14:45:07.509309: step 1098, loss 0.190172, acc 0.921875, learning_rate 0.000155155
2017-10-10T14:45:07.712012: step 1099, loss 0.158548, acc 0.90625, learning_rate 0.000154929
2017-10-10T14:45:07.924324: step 1100, loss 0.395577, acc 0.84375, learning_rate 0.000154705
2017-10-10T14:45:08.139568: step 1101, loss 0.22253, acc 0.9375, learning_rate 0.000154482
2017-10-10T14:45:08.326995: step 1102, loss 0.0844658, acc 0.984375, learning_rate 0.00015426
2017-10-10T14:45:08.624878: step 1103, loss 0.361321, acc 0.90625, learning_rate 0.000154038
2017-10-10T14:45:08.942706: step 1104, loss 0.15458, acc 0.9375, learning_rate 0.000153818
2017-10-10T14:45:09.205070: step 1105, loss 0.232517, acc 0.90625, learning_rate 0.000153598
2017-10-10T14:45:09.456951: step 1106, loss 0.31925, acc 0.890625, learning_rate 0.000153379
2017-10-10T14:45:09.660543: step 1107, loss 0.30867, acc 0.90625, learning_rate 0.000153161
2017-10-10T14:45:09.853997: step 1108, loss 0.298502, acc 0.90625, learning_rate 0.000152944
2017-10-10T14:45:10.093629: step 1109, loss 0.0613841, acc 1, learning_rate 0.000152728
2017-10-10T14:45:10.320520: step 1110, loss 0.142558, acc 0.96875, learning_rate 0.000152513
2017-10-10T14:45:10.619669: step 1111, loss 0.143883, acc 0.953125, learning_rate 0.000152299
2017-10-10T14:45:10.842871: step 1112, loss 0.212261, acc 0.921875, learning_rate 0.000152085
2017-10-10T14:45:11.047010: step 1113, loss 0.111714, acc 0.96875, learning_rate 0.000151872
2017-10-10T14:45:11.312040: step 1114, loss 0.202792, acc 0.921875, learning_rate 0.000151661
2017-10-10T14:45:11.557630: step 1115, loss 0.154375, acc 0.9375, learning_rate 0.00015145
2017-10-10T14:45:11.861976: step 1116, loss 0.166161, acc 0.953125, learning_rate 0.00015124
2017-10-10T14:45:12.165102: step 1117, loss 0.214934, acc 0.921875, learning_rate 0.000151031
2017-10-10T14:45:12.368859: step 1118, loss 0.149436, acc 0.953125, learning_rate 0.000150822
2017-10-10T14:45:12.568929: step 1119, loss 0.242419, acc 0.90625, learning_rate 0.000150615
2017-10-10T14:45:12.777059: step 1120, loss 0.216502, acc 0.90625, learning_rate 0.000150408

Evaluation:
2017-10-10T14:45:13.162129: step 1120, loss 0.227634, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1120

2017-10-10T14:45:14.033784: step 1121, loss 0.151179, acc 0.96875, learning_rate 0.000150203
2017-10-10T14:45:14.266746: step 1122, loss 0.127994, acc 0.96875, learning_rate 0.000149998
2017-10-10T14:45:14.496838: step 1123, loss 0.162193, acc 0.953125, learning_rate 0.000149794
2017-10-10T14:45:14.747065: step 1124, loss 0.205803, acc 0.953125, learning_rate 0.00014959
2017-10-10T14:45:14.981979: step 1125, loss 0.116967, acc 0.953125, learning_rate 0.000149388
2017-10-10T14:45:15.248554: step 1126, loss 0.182988, acc 0.9375, learning_rate 0.000149186
2017-10-10T14:45:15.529364: step 1127, loss 0.163601, acc 0.9375, learning_rate 0.000148986
2017-10-10T14:45:15.808922: step 1128, loss 0.254547, acc 0.890625, learning_rate 0.000148786
2017-10-10T14:45:16.003148: step 1129, loss 0.242861, acc 0.90625, learning_rate 0.000148587
2017-10-10T14:45:16.248908: step 1130, loss 0.176429, acc 0.953125, learning_rate 0.000148388
2017-10-10T14:45:16.521156: step 1131, loss 0.278063, acc 0.921875, learning_rate 0.000148191
2017-10-10T14:45:16.774682: step 1132, loss 0.167403, acc 0.9375, learning_rate 0.000147994
2017-10-10T14:45:17.043177: step 1133, loss 0.144809, acc 0.953125, learning_rate 0.000147798
2017-10-10T14:45:17.291908: step 1134, loss 0.252053, acc 0.890625, learning_rate 0.000147603
2017-10-10T14:45:17.545584: step 1135, loss 0.122598, acc 0.96875, learning_rate 0.000147409
2017-10-10T14:45:17.798669: step 1136, loss 0.132297, acc 0.984375, learning_rate 0.000147215
2017-10-10T14:45:18.039733: step 1137, loss 0.172809, acc 0.9375, learning_rate 0.000147022
2017-10-10T14:45:18.298217: step 1138, loss 0.145266, acc 0.953125, learning_rate 0.000146831
2017-10-10T14:45:18.481328: step 1139, loss 0.23216, acc 0.9375, learning_rate 0.000146639
2017-10-10T14:45:18.619813: step 1140, loss 0.213995, acc 0.921875, learning_rate 0.000146449
2017-10-10T14:45:18.864062: step 1141, loss 0.191766, acc 0.9375, learning_rate 0.000146259
2017-10-10T14:45:19.089923: step 1142, loss 0.132764, acc 0.96875, learning_rate 0.000146071
2017-10-10T14:45:19.321292: step 1143, loss 0.103697, acc 0.96875, learning_rate 0.000145883
2017-10-10T14:45:19.631956: step 1144, loss 0.144406, acc 0.953125, learning_rate 0.000145695
2017-10-10T14:45:19.824857: step 1145, loss 0.497419, acc 0.859375, learning_rate 0.000145509
2017-10-10T14:45:20.102947: step 1146, loss 0.244659, acc 0.890625, learning_rate 0.000145323
2017-10-10T14:45:20.354095: step 1147, loss 0.168806, acc 0.96875, learning_rate 0.000145138
2017-10-10T14:45:20.592695: step 1148, loss 0.131832, acc 0.984375, learning_rate 0.000144954
2017-10-10T14:45:20.892359: step 1149, loss 0.158232, acc 0.96875, learning_rate 0.00014477
2017-10-10T14:45:21.121449: step 1150, loss 0.147757, acc 0.984375, learning_rate 0.000144588
2017-10-10T14:45:21.344868: step 1151, loss 0.132364, acc 0.96875, learning_rate 0.000144406
2017-10-10T14:45:21.660932: step 1152, loss 0.185443, acc 0.9375, learning_rate 0.000144224
2017-10-10T14:45:21.892773: step 1153, loss 0.213255, acc 0.953125, learning_rate 0.000144044
2017-10-10T14:45:22.111112: step 1154, loss 0.214961, acc 0.96875, learning_rate 0.000143864
2017-10-10T14:45:22.364835: step 1155, loss 0.204763, acc 0.953125, learning_rate 0.000143685
2017-10-10T14:45:22.557079: step 1156, loss 0.197753, acc 0.90625, learning_rate 0.000143507
2017-10-10T14:45:22.854301: step 1157, loss 0.126741, acc 0.96875, learning_rate 0.000143329
2017-10-10T14:45:23.112762: step 1158, loss 0.152034, acc 0.9375, learning_rate 0.000143152
2017-10-10T14:45:23.335182: step 1159, loss 0.0691601, acc 0.984375, learning_rate 0.000142976
2017-10-10T14:45:23.599832: step 1160, loss 0.110015, acc 0.984375, learning_rate 0.000142801

Evaluation:
2017-10-10T14:45:24.077469: step 1160, loss 0.226205, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1160

2017-10-10T14:45:25.256884: step 1161, loss 0.147199, acc 0.953125, learning_rate 0.000142626
2017-10-10T14:45:25.461001: step 1162, loss 0.122571, acc 0.953125, learning_rate 0.000142452
2017-10-10T14:45:25.770131: step 1163, loss 0.17386, acc 0.9375, learning_rate 0.000142279
2017-10-10T14:45:26.060855: step 1164, loss 0.145985, acc 0.953125, learning_rate 0.000142106
2017-10-10T14:45:26.380685: step 1165, loss 0.174786, acc 0.953125, learning_rate 0.000141934
2017-10-10T14:45:26.580931: step 1166, loss 0.106245, acc 0.96875, learning_rate 0.000141763
2017-10-10T14:45:26.820830: step 1167, loss 0.156643, acc 0.9375, learning_rate 0.000141593
2017-10-10T14:45:27.059390: step 1168, loss 0.194447, acc 0.921875, learning_rate 0.000141423
2017-10-10T14:45:27.258688: step 1169, loss 0.110859, acc 0.96875, learning_rate 0.000141254
2017-10-10T14:45:27.500306: step 1170, loss 0.136359, acc 0.9375, learning_rate 0.000141085
2017-10-10T14:45:27.720822: step 1171, loss 0.152722, acc 0.9375, learning_rate 0.000140918
2017-10-10T14:45:28.034993: step 1172, loss 0.176403, acc 0.90625, learning_rate 0.000140751
2017-10-10T14:45:28.228049: step 1173, loss 0.0827097, acc 0.984375, learning_rate 0.000140584
2017-10-10T14:45:28.462258: step 1174, loss 0.303942, acc 0.90625, learning_rate 0.000140419
2017-10-10T14:45:28.671736: step 1175, loss 0.159097, acc 0.9375, learning_rate 0.000140254
2017-10-10T14:45:28.886974: step 1176, loss 0.174929, acc 0.921569, learning_rate 0.000140089
2017-10-10T14:45:29.131783: step 1177, loss 0.120305, acc 0.96875, learning_rate 0.000139926
2017-10-10T14:45:29.369891: step 1178, loss 0.19492, acc 0.9375, learning_rate 0.000139763
2017-10-10T14:45:29.604950: step 1179, loss 0.104792, acc 0.96875, learning_rate 0.0001396
2017-10-10T14:45:29.818611: step 1180, loss 0.230226, acc 0.96875, learning_rate 0.000139439
2017-10-10T14:45:30.085623: step 1181, loss 0.10374, acc 0.96875, learning_rate 0.000139278
2017-10-10T14:45:30.307578: step 1182, loss 0.192226, acc 0.9375, learning_rate 0.000139118
2017-10-10T14:45:30.588831: step 1183, loss 0.197304, acc 0.9375, learning_rate 0.000138958
2017-10-10T14:45:30.858247: step 1184, loss 0.0937995, acc 1, learning_rate 0.000138799
2017-10-10T14:45:31.028859: step 1185, loss 0.212069, acc 0.921875, learning_rate 0.00013864
2017-10-10T14:45:31.210158: step 1186, loss 0.177647, acc 0.921875, learning_rate 0.000138483
2017-10-10T14:45:31.395539: step 1187, loss 0.299132, acc 0.90625, learning_rate 0.000138326
2017-10-10T14:45:31.637233: step 1188, loss 0.299113, acc 0.890625, learning_rate 0.000138169
2017-10-10T14:45:31.900937: step 1189, loss 0.184167, acc 0.953125, learning_rate 0.000138013
2017-10-10T14:45:32.085672: step 1190, loss 0.182009, acc 0.953125, learning_rate 0.000137858
2017-10-10T14:45:32.333378: step 1191, loss 0.141432, acc 0.9375, learning_rate 0.000137704
2017-10-10T14:45:32.555966: step 1192, loss 0.270406, acc 0.9375, learning_rate 0.00013755
2017-10-10T14:45:32.820313: step 1193, loss 0.215932, acc 0.90625, learning_rate 0.000137397
2017-10-10T14:45:33.025129: step 1194, loss 0.207715, acc 0.90625, learning_rate 0.000137244
2017-10-10T14:45:33.322352: step 1195, loss 0.190842, acc 0.9375, learning_rate 0.000137092
2017-10-10T14:45:33.559398: step 1196, loss 0.239125, acc 0.90625, learning_rate 0.000136941
2017-10-10T14:45:33.868837: step 1197, loss 0.185622, acc 0.9375, learning_rate 0.00013679
2017-10-10T14:45:34.080971: step 1198, loss 0.327101, acc 0.90625, learning_rate 0.00013664
2017-10-10T14:45:34.318886: step 1199, loss 0.215746, acc 0.921875, learning_rate 0.00013649
2017-10-10T14:45:34.589399: step 1200, loss 0.209297, acc 0.953125, learning_rate 0.000136341

Evaluation:
2017-10-10T14:45:35.005853: step 1200, loss 0.226306, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1200

2017-10-10T14:45:36.160833: step 1201, loss 0.13665, acc 0.96875, learning_rate 0.000136193
2017-10-10T14:45:36.356713: step 1202, loss 0.248337, acc 0.90625, learning_rate 0.000136045
2017-10-10T14:45:36.574330: step 1203, loss 0.136014, acc 0.953125, learning_rate 0.000135898
2017-10-10T14:45:36.825592: step 1204, loss 0.129766, acc 0.953125, learning_rate 0.000135751
2017-10-10T14:45:37.144317: step 1205, loss 0.287515, acc 0.90625, learning_rate 0.000135605
2017-10-10T14:45:37.365986: step 1206, loss 0.320747, acc 0.9375, learning_rate 0.00013546
2017-10-10T14:45:37.607664: step 1207, loss 0.121974, acc 0.953125, learning_rate 0.000135315
2017-10-10T14:45:37.933001: step 1208, loss 0.180169, acc 0.9375, learning_rate 0.000135171
2017-10-10T14:45:38.188979: step 1209, loss 0.24322, acc 0.921875, learning_rate 0.000135028
2017-10-10T14:45:38.413911: step 1210, loss 0.123012, acc 0.96875, learning_rate 0.000134885
2017-10-10T14:45:38.711363: step 1211, loss 0.16167, acc 0.953125, learning_rate 0.000134742
2017-10-10T14:45:38.973554: step 1212, loss 0.180248, acc 0.9375, learning_rate 0.0001346
2017-10-10T14:45:39.221838: step 1213, loss 0.227547, acc 0.921875, learning_rate 0.000134459
2017-10-10T14:45:39.425126: step 1214, loss 0.130885, acc 0.984375, learning_rate 0.000134319
2017-10-10T14:45:39.692816: step 1215, loss 0.182571, acc 0.9375, learning_rate 0.000134178
2017-10-10T14:45:39.952468: step 1216, loss 0.0896113, acc 1, learning_rate 0.000134039
2017-10-10T14:45:40.185288: step 1217, loss 0.292902, acc 0.890625, learning_rate 0.0001339
2017-10-10T14:45:40.382245: step 1218, loss 0.231269, acc 0.90625, learning_rate 0.000133762
2017-10-10T14:45:40.592054: step 1219, loss 0.249132, acc 0.921875, learning_rate 0.000133624
2017-10-10T14:45:40.894938: step 1220, loss 0.130624, acc 0.953125, learning_rate 0.000133487
2017-10-10T14:45:41.095419: step 1221, loss 0.107238, acc 0.984375, learning_rate 0.00013335
2017-10-10T14:45:41.348398: step 1222, loss 0.147676, acc 0.96875, learning_rate 0.000133214
2017-10-10T14:45:41.588922: step 1223, loss 0.178094, acc 0.9375, learning_rate 0.000133078
2017-10-10T14:45:41.785313: step 1224, loss 0.141341, acc 0.96875, learning_rate 0.000132943
2017-10-10T14:45:42.009558: step 1225, loss 0.251105, acc 0.921875, learning_rate 0.000132809
2017-10-10T14:45:42.277046: step 1226, loss 0.096015, acc 0.984375, learning_rate 0.000132675
2017-10-10T14:45:42.544278: step 1227, loss 0.184211, acc 0.90625, learning_rate 0.000132541
2017-10-10T14:45:42.804157: step 1228, loss 0.0451233, acc 1, learning_rate 0.000132409
2017-10-10T14:45:43.018935: step 1229, loss 0.116016, acc 0.96875, learning_rate 0.000132276
2017-10-10T14:45:43.170193: step 1230, loss 0.210619, acc 0.921875, learning_rate 0.000132145
2017-10-10T14:45:43.376853: step 1231, loss 0.167531, acc 0.9375, learning_rate 0.000132013
2017-10-10T14:45:43.560943: step 1232, loss 0.165605, acc 0.953125, learning_rate 0.000131883
2017-10-10T14:45:43.756805: step 1233, loss 0.215033, acc 0.90625, learning_rate 0.000131753
2017-10-10T14:45:43.984470: step 1234, loss 0.075301, acc 0.984375, learning_rate 0.000131623
2017-10-10T14:45:44.163948: step 1235, loss 0.152827, acc 0.9375, learning_rate 0.000131494
2017-10-10T14:45:44.471063: step 1236, loss 0.215266, acc 0.890625, learning_rate 0.000131365
2017-10-10T14:45:44.816243: step 1237, loss 0.091325, acc 0.984375, learning_rate 0.000131237
2017-10-10T14:45:45.100395: step 1238, loss 0.215587, acc 0.921875, learning_rate 0.00013111
2017-10-10T14:45:45.291526: step 1239, loss 0.210458, acc 0.921875, learning_rate 0.000130983
2017-10-10T14:45:45.578707: step 1240, loss 0.135521, acc 0.953125, learning_rate 0.000130856

Evaluation:
2017-10-10T14:45:45.904854: step 1240, loss 0.226524, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1240

2017-10-10T14:45:46.782636: step 1241, loss 0.186306, acc 0.921875, learning_rate 0.00013073
2017-10-10T14:45:47.063032: step 1242, loss 0.214278, acc 0.9375, learning_rate 0.000130605
2017-10-10T14:45:47.326404: step 1243, loss 0.172036, acc 0.953125, learning_rate 0.00013048
2017-10-10T14:45:47.489833: step 1244, loss 0.124208, acc 0.9375, learning_rate 0.000130356
2017-10-10T14:45:47.714733: step 1245, loss 0.320382, acc 0.859375, learning_rate 0.000130232
2017-10-10T14:45:47.923894: step 1246, loss 0.139828, acc 0.9375, learning_rate 0.000130108
2017-10-10T14:45:48.169777: step 1247, loss 0.17036, acc 0.921875, learning_rate 0.000129985
2017-10-10T14:45:48.428879: step 1248, loss 0.254002, acc 0.90625, learning_rate 0.000129863
2017-10-10T14:45:48.684931: step 1249, loss 0.175432, acc 0.953125, learning_rate 0.000129741
2017-10-10T14:45:48.973500: step 1250, loss 0.131961, acc 0.953125, learning_rate 0.00012962
2017-10-10T14:45:49.187083: step 1251, loss 0.28018, acc 0.890625, learning_rate 0.000129499
2017-10-10T14:45:49.411646: step 1252, loss 0.180632, acc 0.921875, learning_rate 0.000129378
2017-10-10T14:45:49.632601: step 1253, loss 0.101581, acc 0.96875, learning_rate 0.000129259
2017-10-10T14:45:49.821026: step 1254, loss 0.159569, acc 0.953125, learning_rate 0.000129139
2017-10-10T14:45:50.096959: step 1255, loss 0.119657, acc 0.953125, learning_rate 0.00012902
2017-10-10T14:45:50.366157: step 1256, loss 0.178447, acc 0.9375, learning_rate 0.000128902
2017-10-10T14:45:50.591238: step 1257, loss 0.212753, acc 0.9375, learning_rate 0.000128784
2017-10-10T14:45:50.820559: step 1258, loss 0.140143, acc 0.953125, learning_rate 0.000128666
2017-10-10T14:45:51.083482: step 1259, loss 0.211915, acc 0.9375, learning_rate 0.000128549
2017-10-10T14:45:51.316222: step 1260, loss 0.360548, acc 0.921875, learning_rate 0.000128433
2017-10-10T14:45:51.542708: step 1261, loss 0.171697, acc 0.9375, learning_rate 0.000128317
2017-10-10T14:45:51.841439: step 1262, loss 0.133611, acc 0.96875, learning_rate 0.000128201
2017-10-10T14:45:52.063976: step 1263, loss 0.201969, acc 0.9375, learning_rate 0.000128086
2017-10-10T14:45:52.340602: step 1264, loss 0.171401, acc 0.921875, learning_rate 0.000127971
2017-10-10T14:45:52.593050: step 1265, loss 0.0838692, acc 0.984375, learning_rate 0.000127857
2017-10-10T14:45:52.806404: step 1266, loss 0.370964, acc 0.875, learning_rate 0.000127743
2017-10-10T14:45:53.140933: step 1267, loss 0.114099, acc 0.984375, learning_rate 0.00012763
2017-10-10T14:45:53.375849: step 1268, loss 0.221718, acc 0.921875, learning_rate 0.000127517
2017-10-10T14:45:53.603983: step 1269, loss 0.197882, acc 0.921875, learning_rate 0.000127405
2017-10-10T14:45:53.879461: step 1270, loss 0.334943, acc 0.890625, learning_rate 0.000127293
2017-10-10T14:45:54.164841: step 1271, loss 0.264098, acc 0.875, learning_rate 0.000127182
2017-10-10T14:45:54.399288: step 1272, loss 0.198969, acc 0.953125, learning_rate 0.000127071
2017-10-10T14:45:54.652429: step 1273, loss 0.235616, acc 0.890625, learning_rate 0.00012696
2017-10-10T14:45:54.908838: step 1274, loss 0.226779, acc 0.921569, learning_rate 0.00012685
2017-10-10T14:45:55.137164: step 1275, loss 0.101346, acc 0.96875, learning_rate 0.000126741
2017-10-10T14:45:55.428455: step 1276, loss 0.132927, acc 0.96875, learning_rate 0.000126632
2017-10-10T14:45:55.597279: step 1277, loss 0.212546, acc 0.90625, learning_rate 0.000126523
2017-10-10T14:45:55.835332: step 1278, loss 0.152275, acc 0.9375, learning_rate 0.000126415
2017-10-10T14:45:56.120810: step 1279, loss 0.260714, acc 0.875, learning_rate 0.000126307
2017-10-10T14:45:56.347763: step 1280, loss 0.106153, acc 0.953125, learning_rate 0.000126199

Evaluation:
2017-10-10T14:45:56.732971: step 1280, loss 0.227262, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1280

2017-10-10T14:45:57.800320: step 1281, loss 0.20895, acc 0.921875, learning_rate 0.000126093
2017-10-10T14:45:58.084778: step 1282, loss 0.280722, acc 0.875, learning_rate 0.000125986
2017-10-10T14:45:58.316983: step 1283, loss 0.165843, acc 0.9375, learning_rate 0.00012588
2017-10-10T14:45:58.544826: step 1284, loss 0.123028, acc 0.96875, learning_rate 0.000125774
2017-10-10T14:45:58.791127: step 1285, loss 0.126995, acc 0.984375, learning_rate 0.000125669
2017-10-10T14:45:59.027899: step 1286, loss 0.239011, acc 0.921875, learning_rate 0.000125564
2017-10-10T14:45:59.276384: step 1287, loss 0.0776322, acc 0.984375, learning_rate 0.00012546
2017-10-10T14:45:59.553040: step 1288, loss 0.264789, acc 0.90625, learning_rate 0.000125356
2017-10-10T14:45:59.796827: step 1289, loss 0.260044, acc 0.90625, learning_rate 0.000125253
2017-10-10T14:46:00.113108: step 1290, loss 0.166182, acc 0.9375, learning_rate 0.00012515
2017-10-10T14:46:00.340411: step 1291, loss 0.279056, acc 0.890625, learning_rate 0.000125047
2017-10-10T14:46:00.537796: step 1292, loss 0.306645, acc 0.90625, learning_rate 0.000124945
2017-10-10T14:46:00.704874: step 1293, loss 0.386211, acc 0.859375, learning_rate 0.000124843
2017-10-10T14:46:00.930841: step 1294, loss 0.153465, acc 0.96875, learning_rate 0.000124741
2017-10-10T14:46:01.186369: step 1295, loss 0.156239, acc 0.953125, learning_rate 0.00012464
2017-10-10T14:46:01.407543: step 1296, loss 0.194118, acc 0.953125, learning_rate 0.00012454
2017-10-10T14:46:01.686028: step 1297, loss 0.163203, acc 0.9375, learning_rate 0.00012444
2017-10-10T14:46:01.864810: step 1298, loss 0.241933, acc 0.921875, learning_rate 0.00012434
2017-10-10T14:46:02.131965: step 1299, loss 0.113594, acc 0.96875, learning_rate 0.000124241
2017-10-10T14:46:02.331621: step 1300, loss 0.104081, acc 0.96875, learning_rate 0.000124142
2017-10-10T14:46:02.580111: step 1301, loss 0.100126, acc 0.953125, learning_rate 0.000124043
2017-10-10T14:46:02.829201: step 1302, loss 0.188747, acc 0.9375, learning_rate 0.000123945
2017-10-10T14:46:03.060977: step 1303, loss 0.190759, acc 0.9375, learning_rate 0.000123847
2017-10-10T14:46:03.351221: step 1304, loss 0.182787, acc 0.9375, learning_rate 0.00012375
2017-10-10T14:46:03.584799: step 1305, loss 0.247773, acc 0.90625, learning_rate 0.000123653
2017-10-10T14:46:03.836887: step 1306, loss 0.145463, acc 0.953125, learning_rate 0.000123556
2017-10-10T14:46:04.146685: step 1307, loss 0.134849, acc 0.96875, learning_rate 0.00012346
2017-10-10T14:46:04.423910: step 1308, loss 0.128712, acc 0.96875, learning_rate 0.000123364
2017-10-10T14:46:04.615849: step 1309, loss 0.104586, acc 0.953125, learning_rate 0.000123269
2017-10-10T14:46:04.849071: step 1310, loss 0.139488, acc 0.96875, learning_rate 0.000123174
2017-10-10T14:46:05.088876: step 1311, loss 0.257225, acc 0.9375, learning_rate 0.00012308
2017-10-10T14:46:05.341159: step 1312, loss 0.193796, acc 0.953125, learning_rate 0.000122985
2017-10-10T14:46:05.588883: step 1313, loss 0.15341, acc 0.953125, learning_rate 0.000122892
2017-10-10T14:46:05.833502: step 1314, loss 0.253374, acc 0.921875, learning_rate 0.000122798
2017-10-10T14:46:06.073070: step 1315, loss 0.11119, acc 0.984375, learning_rate 0.000122705
2017-10-10T14:46:06.395410: step 1316, loss 0.188631, acc 0.9375, learning_rate 0.000122612
2017-10-10T14:46:06.605028: step 1317, loss 0.107163, acc 0.96875, learning_rate 0.00012252
2017-10-10T14:46:06.895395: step 1318, loss 0.26409, acc 0.875, learning_rate 0.000122428
2017-10-10T14:46:07.172829: step 1319, loss 0.292331, acc 0.859375, learning_rate 0.000122337
2017-10-10T14:46:07.407298: step 1320, loss 0.085649, acc 1, learning_rate 0.000122245

Evaluation:
2017-10-10T14:46:07.674896: step 1320, loss 0.226994, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1320

2017-10-10T14:46:08.708850: step 1321, loss 0.220052, acc 0.9375, learning_rate 0.000122155
2017-10-10T14:46:08.944952: step 1322, loss 0.348854, acc 0.859375, learning_rate 0.000122064
2017-10-10T14:46:09.180996: step 1323, loss 0.153503, acc 0.96875, learning_rate 0.000121974
2017-10-10T14:46:09.463579: step 1324, loss 0.352931, acc 0.859375, learning_rate 0.000121884
2017-10-10T14:46:09.688324: step 1325, loss 0.14602, acc 0.953125, learning_rate 0.000121795
2017-10-10T14:46:09.905715: step 1326, loss 0.25372, acc 0.921875, learning_rate 0.000121706
2017-10-10T14:46:10.157242: step 1327, loss 0.082776, acc 1, learning_rate 0.000121618
2017-10-10T14:46:10.417043: step 1328, loss 0.243934, acc 0.890625, learning_rate 0.000121529
2017-10-10T14:46:10.612396: step 1329, loss 0.125056, acc 0.96875, learning_rate 0.000121441
2017-10-10T14:46:10.894183: step 1330, loss 0.146439, acc 0.953125, learning_rate 0.000121354
2017-10-10T14:46:11.156457: step 1331, loss 0.162213, acc 0.9375, learning_rate 0.000121267
2017-10-10T14:46:11.371495: step 1332, loss 0.131884, acc 0.96875, learning_rate 0.00012118
2017-10-10T14:46:11.629153: step 1333, loss 0.225776, acc 0.953125, learning_rate 0.000121093
2017-10-10T14:46:11.829281: step 1334, loss 0.198955, acc 0.9375, learning_rate 0.000121007
2017-10-10T14:46:12.117566: step 1335, loss 0.277674, acc 0.90625, learning_rate 0.000120922
2017-10-10T14:46:12.362266: step 1336, loss 0.152582, acc 0.953125, learning_rate 0.000120836
2017-10-10T14:46:12.547654: step 1337, loss 0.0909392, acc 0.984375, learning_rate 0.000120751
2017-10-10T14:46:12.813210: step 1338, loss 0.248602, acc 0.890625, learning_rate 0.000120666
2017-10-10T14:46:13.066201: step 1339, loss 0.105968, acc 0.953125, learning_rate 0.000120582
2017-10-10T14:46:13.364867: step 1340, loss 0.252605, acc 0.9375, learning_rate 0.000120498
2017-10-10T14:46:13.604904: step 1341, loss 0.180655, acc 0.953125, learning_rate 0.000120414
2017-10-10T14:46:13.817960: step 1342, loss 0.151504, acc 0.9375, learning_rate 0.000120331
2017-10-10T14:46:14.036740: step 1343, loss 0.108554, acc 0.96875, learning_rate 0.000120248
2017-10-10T14:46:14.236386: step 1344, loss 0.120589, acc 0.9375, learning_rate 0.000120165
2017-10-10T14:46:14.400662: step 1345, loss 0.190612, acc 0.921875, learning_rate 0.000120083
2017-10-10T14:46:14.661064: step 1346, loss 0.175525, acc 0.953125, learning_rate 0.000120001
2017-10-10T14:46:14.877047: step 1347, loss 0.185899, acc 0.9375, learning_rate 0.00011992
2017-10-10T14:46:15.136836: step 1348, loss 0.291753, acc 0.90625, learning_rate 0.000119838
2017-10-10T14:46:15.412062: step 1349, loss 0.209436, acc 0.9375, learning_rate 0.000119757
2017-10-10T14:46:15.622555: step 1350, loss 0.211968, acc 0.90625, learning_rate 0.000119677
2017-10-10T14:46:15.888836: step 1351, loss 0.206911, acc 0.9375, learning_rate 0.000119596
2017-10-10T14:46:16.156957: step 1352, loss 0.227653, acc 0.921875, learning_rate 0.000119516
2017-10-10T14:46:16.384878: step 1353, loss 0.163189, acc 0.96875, learning_rate 0.000119437
2017-10-10T14:46:16.663597: step 1354, loss 0.220897, acc 0.921875, learning_rate 0.000119357
2017-10-10T14:46:16.962844: step 1355, loss 0.141199, acc 0.953125, learning_rate 0.000119278
2017-10-10T14:46:17.241015: step 1356, loss 0.10873, acc 0.96875, learning_rate 0.0001192
2017-10-10T14:46:17.413986: step 1357, loss 0.130563, acc 0.921875, learning_rate 0.000119121
2017-10-10T14:46:17.643750: step 1358, loss 0.096576, acc 0.984375, learning_rate 0.000119043
2017-10-10T14:46:17.859671: step 1359, loss 0.257352, acc 0.921875, learning_rate 0.000118965
2017-10-10T14:46:18.064815: step 1360, loss 0.119557, acc 0.984375, learning_rate 0.000118888

Evaluation:
2017-10-10T14:46:18.491848: step 1360, loss 0.226959, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1360

2017-10-10T14:46:19.587729: step 1361, loss 0.248513, acc 0.90625, learning_rate 0.000118811
2017-10-10T14:46:19.817125: step 1362, loss 0.156004, acc 0.984375, learning_rate 0.000118734
2017-10-10T14:46:20.066410: step 1363, loss 0.229654, acc 0.890625, learning_rate 0.000118658
2017-10-10T14:46:20.288131: step 1364, loss 0.186068, acc 0.9375, learning_rate 0.000118582
2017-10-10T14:46:20.492535: step 1365, loss 0.168348, acc 0.953125, learning_rate 0.000118506
2017-10-10T14:46:20.782644: step 1366, loss 0.14733, acc 0.953125, learning_rate 0.00011843
2017-10-10T14:46:21.069166: step 1367, loss 0.183501, acc 0.953125, learning_rate 0.000118355
2017-10-10T14:46:21.265131: step 1368, loss 0.158989, acc 0.9375, learning_rate 0.00011828
2017-10-10T14:46:21.551212: step 1369, loss 0.240996, acc 0.921875, learning_rate 0.000118205
2017-10-10T14:46:21.792751: step 1370, loss 0.162155, acc 0.953125, learning_rate 0.000118131
2017-10-10T14:46:22.043380: step 1371, loss 0.106924, acc 0.984375, learning_rate 0.000118057
2017-10-10T14:46:22.267073: step 1372, loss 0.276168, acc 0.941176, learning_rate 0.000117983
2017-10-10T14:46:22.461012: step 1373, loss 0.202931, acc 0.921875, learning_rate 0.00011791
2017-10-10T14:46:22.747584: step 1374, loss 0.11321, acc 0.984375, learning_rate 0.000117837
2017-10-10T14:46:23.004978: step 1375, loss 0.131014, acc 0.96875, learning_rate 0.000117764
2017-10-10T14:46:23.256564: step 1376, loss 0.219418, acc 0.921875, learning_rate 0.000117692
2017-10-10T14:46:23.563785: step 1377, loss 0.141097, acc 0.9375, learning_rate 0.000117619
2017-10-10T14:46:23.820283: step 1378, loss 0.163134, acc 0.9375, learning_rate 0.000117547
2017-10-10T14:46:24.079172: step 1379, loss 0.104848, acc 0.953125, learning_rate 0.000117476
2017-10-10T14:46:24.349811: step 1380, loss 0.163239, acc 0.9375, learning_rate 0.000117404
2017-10-10T14:46:24.656911: step 1381, loss 0.198536, acc 0.9375, learning_rate 0.000117333
2017-10-10T14:46:24.924957: step 1382, loss 0.234962, acc 0.90625, learning_rate 0.000117263
2017-10-10T14:46:25.144766: step 1383, loss 0.12929, acc 0.96875, learning_rate 0.000117192
2017-10-10T14:46:25.373973: step 1384, loss 0.176905, acc 0.9375, learning_rate 0.000117122
2017-10-10T14:46:25.609498: step 1385, loss 0.122636, acc 0.96875, learning_rate 0.000117052
2017-10-10T14:46:25.802860: step 1386, loss 0.197423, acc 0.953125, learning_rate 0.000116983
2017-10-10T14:46:26.015974: step 1387, loss 0.156027, acc 0.953125, learning_rate 0.000116913
2017-10-10T14:46:26.279507: step 1388, loss 0.139446, acc 0.9375, learning_rate 0.000116844
2017-10-10T14:46:26.586449: step 1389, loss 0.154779, acc 0.921875, learning_rate 0.000116775
2017-10-10T14:46:26.832892: step 1390, loss 0.305632, acc 0.875, learning_rate 0.000116707
2017-10-10T14:46:27.104988: step 1391, loss 0.172073, acc 0.953125, learning_rate 0.000116639
2017-10-10T14:46:27.291112: step 1392, loss 0.216239, acc 0.90625, learning_rate 0.000116571
2017-10-10T14:46:27.521940: step 1393, loss 0.163721, acc 0.9375, learning_rate 0.000116503
2017-10-10T14:46:27.759894: step 1394, loss 0.252912, acc 0.875, learning_rate 0.000116436
2017-10-10T14:46:27.996862: step 1395, loss 0.198793, acc 0.921875, learning_rate 0.000116369
2017-10-10T14:46:28.253055: step 1396, loss 0.130605, acc 0.953125, learning_rate 0.000116302
2017-10-10T14:46:28.468391: step 1397, loss 0.149951, acc 0.953125, learning_rate 0.000116235
2017-10-10T14:46:28.765571: step 1398, loss 0.199256, acc 0.90625, learning_rate 0.000116169
2017-10-10T14:46:28.977709: step 1399, loss 0.178107, acc 0.953125, learning_rate 0.000116103
2017-10-10T14:46:29.224923: step 1400, loss 0.177459, acc 0.9375, learning_rate 0.000116037

Evaluation:
2017-10-10T14:46:29.611071: step 1400, loss 0.225439, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1400

2017-10-10T14:46:30.495179: step 1401, loss 0.168319, acc 0.96875, learning_rate 0.000115972
2017-10-10T14:46:30.748839: step 1402, loss 0.139069, acc 0.96875, learning_rate 0.000115907
2017-10-10T14:46:31.038983: step 1403, loss 0.101434, acc 0.984375, learning_rate 0.000115842
2017-10-10T14:46:31.329063: step 1404, loss 0.143645, acc 0.9375, learning_rate 0.000115777
2017-10-10T14:46:31.592947: step 1405, loss 0.223152, acc 0.921875, learning_rate 0.000115713
2017-10-10T14:46:31.851094: step 1406, loss 0.133187, acc 0.953125, learning_rate 0.000115649
2017-10-10T14:46:32.046490: step 1407, loss 0.151265, acc 0.96875, learning_rate 0.000115585
2017-10-10T14:46:32.248917: step 1408, loss 0.164752, acc 0.921875, learning_rate 0.000115521
2017-10-10T14:46:32.440807: step 1409, loss 0.142246, acc 0.953125, learning_rate 0.000115458
2017-10-10T14:46:32.644877: step 1410, loss 0.156934, acc 0.9375, learning_rate 0.000115395
2017-10-10T14:46:32.976955: step 1411, loss 0.256619, acc 0.921875, learning_rate 0.000115332
2017-10-10T14:46:33.199855: step 1412, loss 0.139811, acc 0.96875, learning_rate 0.000115269
2017-10-10T14:46:33.452810: step 1413, loss 0.13474, acc 0.96875, learning_rate 0.000115207
2017-10-10T14:46:33.728885: step 1414, loss 0.132663, acc 0.96875, learning_rate 0.000115145
2017-10-10T14:46:34.021269: step 1415, loss 0.194672, acc 0.9375, learning_rate 0.000115083
2017-10-10T14:46:34.258349: step 1416, loss 0.220815, acc 0.9375, learning_rate 0.000115022
2017-10-10T14:46:34.468837: step 1417, loss 0.203079, acc 0.921875, learning_rate 0.00011496
2017-10-10T14:46:34.702614: step 1418, loss 0.129696, acc 0.9375, learning_rate 0.000114899
2017-10-10T14:46:34.953277: step 1419, loss 0.296855, acc 0.890625, learning_rate 0.000114838
2017-10-10T14:46:35.161739: step 1420, loss 0.131167, acc 0.953125, learning_rate 0.000114778
2017-10-10T14:46:35.389681: step 1421, loss 0.22541, acc 0.953125, learning_rate 0.000114717
2017-10-10T14:46:35.652881: step 1422, loss 0.172468, acc 0.953125, learning_rate 0.000114657
2017-10-10T14:46:35.883859: step 1423, loss 0.0444437, acc 1, learning_rate 0.000114598
2017-10-10T14:46:36.120620: step 1424, loss 0.143091, acc 0.921875, learning_rate 0.000114538
2017-10-10T14:46:36.328103: step 1425, loss 0.278759, acc 0.890625, learning_rate 0.000114479
2017-10-10T14:46:36.584813: step 1426, loss 0.157319, acc 0.9375, learning_rate 0.00011442
2017-10-10T14:46:36.798980: step 1427, loss 0.165226, acc 0.953125, learning_rate 0.000114361
2017-10-10T14:46:37.075849: step 1428, loss 0.0968441, acc 0.96875, learning_rate 0.000114302
2017-10-10T14:46:37.364172: step 1429, loss 0.297151, acc 0.921875, learning_rate 0.000114244
2017-10-10T14:46:37.612264: step 1430, loss 0.181126, acc 0.9375, learning_rate 0.000114186
2017-10-10T14:46:37.848989: step 1431, loss 0.3202, acc 0.890625, learning_rate 0.000114128
2017-10-10T14:46:38.123203: step 1432, loss 0.12094, acc 0.953125, learning_rate 0.00011407
2017-10-10T14:46:38.284781: step 1433, loss 0.178212, acc 0.953125, learning_rate 0.000114013
2017-10-10T14:46:38.545418: step 1434, loss 0.306766, acc 0.90625, learning_rate 0.000113955
2017-10-10T14:46:38.801004: step 1435, loss 0.227657, acc 0.890625, learning_rate 0.000113898
2017-10-10T14:46:39.033085: step 1436, loss 0.0815139, acc 0.96875, learning_rate 0.000113842
2017-10-10T14:46:39.300908: step 1437, loss 0.176742, acc 0.9375, learning_rate 0.000113785
2017-10-10T14:46:39.612916: step 1438, loss 0.231513, acc 0.9375, learning_rate 0.000113729
2017-10-10T14:46:39.912144: step 1439, loss 0.297089, acc 0.90625, learning_rate 0.000113673
2017-10-10T14:46:40.150600: step 1440, loss 0.219443, acc 0.921875, learning_rate 0.000113617

Evaluation:
2017-10-10T14:46:40.506222: step 1440, loss 0.224457, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1440

2017-10-10T14:46:41.652857: step 1441, loss 0.219616, acc 0.9375, learning_rate 0.000113561
2017-10-10T14:46:41.914383: step 1442, loss 0.144943, acc 0.96875, learning_rate 0.000113506
2017-10-10T14:46:42.150958: step 1443, loss 0.195809, acc 0.96875, learning_rate 0.000113451
2017-10-10T14:46:42.445106: step 1444, loss 0.126304, acc 0.953125, learning_rate 0.000113396
2017-10-10T14:46:42.713085: step 1445, loss 0.180399, acc 0.953125, learning_rate 0.000113341
2017-10-10T14:46:43.040846: step 1446, loss 0.230171, acc 0.90625, learning_rate 0.000113287
2017-10-10T14:46:43.317960: step 1447, loss 0.217081, acc 0.953125, learning_rate 0.000113233
2017-10-10T14:46:43.524141: step 1448, loss 0.274939, acc 0.9375, learning_rate 0.000113179
2017-10-10T14:46:43.780002: step 1449, loss 0.11028, acc 0.96875, learning_rate 0.000113125
2017-10-10T14:46:43.989471: step 1450, loss 0.182617, acc 0.921875, learning_rate 0.000113071
2017-10-10T14:46:44.216828: step 1451, loss 0.126334, acc 0.96875, learning_rate 0.000113018
2017-10-10T14:46:44.435722: step 1452, loss 0.195536, acc 0.9375, learning_rate 0.000112965
2017-10-10T14:46:44.613828: step 1453, loss 0.206942, acc 0.921875, learning_rate 0.000112912
2017-10-10T14:46:44.901190: step 1454, loss 0.297481, acc 0.90625, learning_rate 0.000112859
2017-10-10T14:46:45.147561: step 1455, loss 0.219821, acc 0.921875, learning_rate 0.000112807
2017-10-10T14:46:45.385105: step 1456, loss 0.209502, acc 0.90625, learning_rate 0.000112754
2017-10-10T14:46:45.671146: step 1457, loss 0.138962, acc 0.953125, learning_rate 0.000112702
2017-10-10T14:46:45.940429: step 1458, loss 0.202698, acc 0.9375, learning_rate 0.000112651
2017-10-10T14:46:46.151012: step 1459, loss 0.245166, acc 0.921875, learning_rate 0.000112599
2017-10-10T14:46:46.448542: step 1460, loss 0.221711, acc 0.90625, learning_rate 0.000112547
2017-10-10T14:46:46.637049: step 1461, loss 0.313095, acc 0.890625, learning_rate 0.000112496
2017-10-10T14:46:46.890571: step 1462, loss 0.166123, acc 0.921875, learning_rate 0.000112445
2017-10-10T14:46:47.183906: step 1463, loss 0.182277, acc 0.96875, learning_rate 0.000112394
2017-10-10T14:46:47.374286: step 1464, loss 0.196827, acc 0.921875, learning_rate 0.000112344
2017-10-10T14:46:47.671522: step 1465, loss 0.211393, acc 0.921875, learning_rate 0.000112293
2017-10-10T14:46:47.915507: step 1466, loss 0.27651, acc 0.90625, learning_rate 0.000112243
2017-10-10T14:46:48.153100: step 1467, loss 0.233924, acc 0.90625, learning_rate 0.000112193
2017-10-10T14:46:48.444939: step 1468, loss 0.062756, acc 0.984375, learning_rate 0.000112144
2017-10-10T14:46:48.693076: step 1469, loss 0.159653, acc 0.953125, learning_rate 0.000112094
2017-10-10T14:46:48.896675: step 1470, loss 0.119816, acc 0.960784, learning_rate 0.000112045
2017-10-10T14:46:49.119109: step 1471, loss 0.15282, acc 0.9375, learning_rate 0.000111995
2017-10-10T14:46:49.425025: step 1472, loss 0.322648, acc 0.90625, learning_rate 0.000111946
2017-10-10T14:46:49.643733: step 1473, loss 0.140177, acc 0.96875, learning_rate 0.000111898
2017-10-10T14:46:49.892834: step 1474, loss 0.212216, acc 0.921875, learning_rate 0.000111849
2017-10-10T14:46:50.144864: step 1475, loss 0.147095, acc 0.96875, learning_rate 0.000111801
2017-10-10T14:46:50.404629: step 1476, loss 0.183371, acc 0.953125, learning_rate 0.000111753
2017-10-10T14:46:50.655661: step 1477, loss 0.2605, acc 0.921875, learning_rate 0.000111705
2017-10-10T14:46:50.843841: step 1478, loss 0.181187, acc 0.953125, learning_rate 0.000111657
2017-10-10T14:46:51.080861: step 1479, loss 0.133698, acc 0.953125, learning_rate 0.000111609
2017-10-10T14:46:51.356818: step 1480, loss 0.177463, acc 0.9375, learning_rate 0.000111562

Evaluation:
2017-10-10T14:46:51.680230: step 1480, loss 0.222637, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1480

2017-10-10T14:46:52.756885: step 1481, loss 0.188969, acc 0.9375, learning_rate 0.000111515
2017-10-10T14:46:53.041355: step 1482, loss 0.212739, acc 0.921875, learning_rate 0.000111468
2017-10-10T14:46:53.286924: step 1483, loss 0.230527, acc 0.9375, learning_rate 0.000111421
2017-10-10T14:46:53.532902: step 1484, loss 0.0777574, acc 0.984375, learning_rate 0.000111374
2017-10-10T14:46:53.752825: step 1485, loss 0.252313, acc 0.90625, learning_rate 0.000111328
2017-10-10T14:46:54.012068: step 1486, loss 0.143151, acc 0.953125, learning_rate 0.000111282
2017-10-10T14:46:54.230734: step 1487, loss 0.227946, acc 0.953125, learning_rate 0.000111236
2017-10-10T14:46:54.453026: step 1488, loss 0.122123, acc 0.96875, learning_rate 0.00011119
2017-10-10T14:46:54.728861: step 1489, loss 0.227424, acc 0.875, learning_rate 0.000111144
2017-10-10T14:46:54.964496: step 1490, loss 0.207151, acc 0.9375, learning_rate 0.000111099
2017-10-10T14:46:55.296862: step 1491, loss 0.191116, acc 0.9375, learning_rate 0.000111053
2017-10-10T14:46:55.480862: step 1492, loss 0.0936105, acc 0.984375, learning_rate 0.000111008
2017-10-10T14:46:55.780496: step 1493, loss 0.18522, acc 0.9375, learning_rate 0.000110963
2017-10-10T14:46:56.020774: step 1494, loss 0.300894, acc 0.90625, learning_rate 0.000110918
2017-10-10T14:46:56.278309: step 1495, loss 0.123973, acc 0.953125, learning_rate 0.000110874
2017-10-10T14:46:56.507958: step 1496, loss 0.217928, acc 0.921875, learning_rate 0.00011083
2017-10-10T14:46:56.706883: step 1497, loss 0.188733, acc 0.96875, learning_rate 0.000110785
2017-10-10T14:46:56.908997: step 1498, loss 0.154377, acc 0.9375, learning_rate 0.000110741
2017-10-10T14:46:57.176969: step 1499, loss 0.0906042, acc 0.984375, learning_rate 0.000110697
2017-10-10T14:46:57.412221: step 1500, loss 0.261144, acc 0.9375, learning_rate 0.000110654
2017-10-10T14:46:57.674621: step 1501, loss 0.166196, acc 0.9375, learning_rate 0.00011061
2017-10-10T14:46:57.920864: step 1502, loss 0.086054, acc 0.96875, learning_rate 0.000110567
2017-10-10T14:46:58.176856: step 1503, loss 0.148165, acc 0.9375, learning_rate 0.000110524
2017-10-10T14:46:58.447188: step 1504, loss 0.127781, acc 0.953125, learning_rate 0.000110481
2017-10-10T14:46:58.702782: step 1505, loss 0.117712, acc 0.953125, learning_rate 0.000110438
2017-10-10T14:46:58.929444: step 1506, loss 0.180837, acc 0.921875, learning_rate 0.000110396
2017-10-10T14:46:59.180957: step 1507, loss 0.110271, acc 0.984375, learning_rate 0.000110353
2017-10-10T14:46:59.459104: step 1508, loss 0.0924101, acc 0.984375, learning_rate 0.000110311
2017-10-10T14:46:59.684290: step 1509, loss 0.214767, acc 0.921875, learning_rate 0.000110269
2017-10-10T14:46:59.930006: step 1510, loss 0.120592, acc 0.953125, learning_rate 0.000110227
2017-10-10T14:47:00.194361: step 1511, loss 0.130699, acc 0.9375, learning_rate 0.000110185
2017-10-10T14:47:00.412842: step 1512, loss 0.153906, acc 0.96875, learning_rate 0.000110144
2017-10-10T14:47:00.684661: step 1513, loss 0.113962, acc 0.96875, learning_rate 0.000110102
2017-10-10T14:47:00.989939: step 1514, loss 0.220841, acc 0.921875, learning_rate 0.000110061
2017-10-10T14:47:01.261245: step 1515, loss 0.223759, acc 0.9375, learning_rate 0.00011002
2017-10-10T14:47:01.516958: step 1516, loss 0.231753, acc 0.9375, learning_rate 0.000109979
2017-10-10T14:47:01.697033: step 1517, loss 0.173877, acc 0.921875, learning_rate 0.000109938
2017-10-10T14:47:01.900245: step 1518, loss 0.172424, acc 0.90625, learning_rate 0.000109898
2017-10-10T14:47:02.136857: step 1519, loss 0.167105, acc 0.9375, learning_rate 0.000109857
2017-10-10T14:47:02.303715: step 1520, loss 0.224796, acc 0.890625, learning_rate 0.000109817

Evaluation:
2017-10-10T14:47:02.756696: step 1520, loss 0.222988, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1520

2017-10-10T14:47:03.684914: step 1521, loss 0.2277, acc 0.890625, learning_rate 0.000109777
2017-10-10T14:47:03.908874: step 1522, loss 0.131808, acc 0.96875, learning_rate 0.000109737
2017-10-10T14:47:04.150448: step 1523, loss 0.0716986, acc 0.984375, learning_rate 0.000109697
2017-10-10T14:47:04.408478: step 1524, loss 0.25652, acc 0.90625, learning_rate 0.000109658
2017-10-10T14:47:04.667644: step 1525, loss 0.15062, acc 0.9375, learning_rate 0.000109618
2017-10-10T14:47:04.926454: step 1526, loss 0.0872549, acc 0.984375, learning_rate 0.000109579
2017-10-10T14:47:05.226853: step 1527, loss 0.262449, acc 0.890625, learning_rate 0.00010954
2017-10-10T14:47:05.455344: step 1528, loss 0.198144, acc 0.921875, learning_rate 0.000109501
2017-10-10T14:47:05.784286: step 1529, loss 0.159204, acc 0.921875, learning_rate 0.000109462
2017-10-10T14:47:06.053194: step 1530, loss 0.298984, acc 0.875, learning_rate 0.000109424
2017-10-10T14:47:06.301725: step 1531, loss 0.107259, acc 0.984375, learning_rate 0.000109385
2017-10-10T14:47:06.562343: step 1532, loss 0.135666, acc 0.984375, learning_rate 0.000109347
2017-10-10T14:47:06.704973: step 1533, loss 0.254723, acc 0.90625, learning_rate 0.000109309
2017-10-10T14:47:07.004861: step 1534, loss 0.261993, acc 0.953125, learning_rate 0.000109271
2017-10-10T14:47:07.320288: step 1535, loss 0.143237, acc 0.9375, learning_rate 0.000109233
2017-10-10T14:47:07.579236: step 1536, loss 0.179059, acc 0.9375, learning_rate 0.000109195
2017-10-10T14:47:07.784945: step 1537, loss 0.256925, acc 0.875, learning_rate 0.000109158
2017-10-10T14:47:07.955479: step 1538, loss 0.157333, acc 0.9375, learning_rate 0.00010912
2017-10-10T14:47:08.184899: step 1539, loss 0.270176, acc 0.921875, learning_rate 0.000109083
2017-10-10T14:47:08.413015: step 1540, loss 0.0968986, acc 0.984375, learning_rate 0.000109046
2017-10-10T14:47:08.627002: step 1541, loss 0.276984, acc 0.859375, learning_rate 0.000109009
2017-10-10T14:47:08.867727: step 1542, loss 0.190329, acc 0.953125, learning_rate 0.000108972
2017-10-10T14:47:09.136203: step 1543, loss 0.19463, acc 0.953125, learning_rate 0.000108936
2017-10-10T14:47:09.452846: step 1544, loss 0.130274, acc 0.953125, learning_rate 0.000108899
2017-10-10T14:47:09.653049: step 1545, loss 0.0607148, acc 0.984375, learning_rate 0.000108863
2017-10-10T14:47:09.926275: step 1546, loss 0.157751, acc 0.9375, learning_rate 0.000108827
2017-10-10T14:47:10.104413: step 1547, loss 0.196282, acc 0.9375, learning_rate 0.000108791
2017-10-10T14:47:10.390111: step 1548, loss 0.231858, acc 0.921875, learning_rate 0.000108755
2017-10-10T14:47:10.624408: step 1549, loss 0.142143, acc 0.953125, learning_rate 0.000108719
2017-10-10T14:47:10.915634: step 1550, loss 0.16202, acc 0.9375, learning_rate 0.000108683
2017-10-10T14:47:11.159105: step 1551, loss 0.251005, acc 0.921875, learning_rate 0.000108648
2017-10-10T14:47:11.391497: step 1552, loss 0.19392, acc 0.921875, learning_rate 0.000108613
2017-10-10T14:47:11.660660: step 1553, loss 0.0915878, acc 0.96875, learning_rate 0.000108577
2017-10-10T14:47:11.868833: step 1554, loss 0.321003, acc 0.890625, learning_rate 0.000108542
2017-10-10T14:47:12.133293: step 1555, loss 0.178102, acc 0.953125, learning_rate 0.000108508
2017-10-10T14:47:12.367505: step 1556, loss 0.129646, acc 0.953125, learning_rate 0.000108473
2017-10-10T14:47:12.577208: step 1557, loss 0.147527, acc 0.953125, learning_rate 0.000108438
2017-10-10T14:47:12.848472: step 1558, loss 0.152795, acc 0.921875, learning_rate 0.000108404
2017-10-10T14:47:13.090478: step 1559, loss 0.176726, acc 0.9375, learning_rate 0.00010837
2017-10-10T14:47:13.331279: step 1560, loss 0.0894284, acc 0.96875, learning_rate 0.000108335

Evaluation:
2017-10-10T14:47:13.716091: step 1560, loss 0.222599, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1560

2017-10-10T14:47:14.874319: step 1561, loss 0.0953644, acc 0.953125, learning_rate 0.000108301
2017-10-10T14:47:15.072390: step 1562, loss 0.124982, acc 0.96875, learning_rate 0.000108267
2017-10-10T14:47:15.325211: step 1563, loss 0.209086, acc 0.921875, learning_rate 0.000108234
2017-10-10T14:47:15.592777: step 1564, loss 0.324701, acc 0.84375, learning_rate 0.0001082
2017-10-10T14:47:15.882849: step 1565, loss 0.106509, acc 0.984375, learning_rate 0.000108167
2017-10-10T14:47:16.095020: step 1566, loss 0.183284, acc 0.90625, learning_rate 0.000108133
2017-10-10T14:47:16.349212: step 1567, loss 0.274602, acc 0.890625, learning_rate 0.0001081
2017-10-10T14:47:16.561034: step 1568, loss 0.161439, acc 0.960784, learning_rate 0.000108067
2017-10-10T14:47:16.790073: step 1569, loss 0.147561, acc 0.953125, learning_rate 0.000108034
2017-10-10T14:47:17.031950: step 1570, loss 0.236133, acc 0.90625, learning_rate 0.000108001
2017-10-10T14:47:17.259235: step 1571, loss 0.251604, acc 0.90625, learning_rate 0.000107969
2017-10-10T14:47:17.521169: step 1572, loss 0.192611, acc 0.9375, learning_rate 0.000107936
2017-10-10T14:47:17.792800: step 1573, loss 0.124591, acc 1, learning_rate 0.000107904
2017-10-10T14:47:18.084802: step 1574, loss 0.125408, acc 0.953125, learning_rate 0.000107871
2017-10-10T14:47:18.301163: step 1575, loss 0.200428, acc 0.921875, learning_rate 0.000107839
2017-10-10T14:47:18.544870: step 1576, loss 0.293522, acc 0.921875, learning_rate 0.000107807
2017-10-10T14:47:18.799390: step 1577, loss 0.180768, acc 0.90625, learning_rate 0.000107775
2017-10-10T14:47:19.069706: step 1578, loss 0.187524, acc 0.921875, learning_rate 0.000107744
2017-10-10T14:47:19.348893: step 1579, loss 0.176185, acc 0.9375, learning_rate 0.000107712
2017-10-10T14:47:19.583119: step 1580, loss 0.142535, acc 0.953125, learning_rate 0.000107681
2017-10-10T14:47:19.833152: step 1581, loss 0.101141, acc 0.96875, learning_rate 0.000107649
2017-10-10T14:47:20.080609: step 1582, loss 0.106973, acc 0.953125, learning_rate 0.000107618
2017-10-10T14:47:20.217087: step 1583, loss 0.24005, acc 0.890625, learning_rate 0.000107587
2017-10-10T14:47:20.396310: step 1584, loss 0.0951363, acc 0.96875, learning_rate 0.000107556
2017-10-10T14:47:20.644487: step 1585, loss 0.236512, acc 0.90625, learning_rate 0.000107525
2017-10-10T14:47:20.794581: step 1586, loss 0.160088, acc 0.953125, learning_rate 0.000107494
2017-10-10T14:47:20.989247: step 1587, loss 0.170746, acc 0.9375, learning_rate 0.000107464
2017-10-10T14:47:21.184958: step 1588, loss 0.178004, acc 0.921875, learning_rate 0.000107433
2017-10-10T14:47:21.425012: step 1589, loss 0.232586, acc 0.875, learning_rate 0.000107403
2017-10-10T14:47:21.736075: step 1590, loss 0.149677, acc 0.953125, learning_rate 0.000107373
2017-10-10T14:47:21.970432: step 1591, loss 0.217146, acc 0.921875, learning_rate 0.000107343
2017-10-10T14:47:22.255201: step 1592, loss 0.30918, acc 0.890625, learning_rate 0.000107313
2017-10-10T14:47:22.455211: step 1593, loss 0.128633, acc 0.953125, learning_rate 0.000107283
2017-10-10T14:47:22.694923: step 1594, loss 0.113327, acc 0.96875, learning_rate 0.000107253
2017-10-10T14:47:22.927097: step 1595, loss 0.165788, acc 0.953125, learning_rate 0.000107224
2017-10-10T14:47:23.170051: step 1596, loss 0.253555, acc 0.90625, learning_rate 0.000107194
2017-10-10T14:47:23.439870: step 1597, loss 0.143075, acc 0.9375, learning_rate 0.000107165
2017-10-10T14:47:23.755696: step 1598, loss 0.157048, acc 0.953125, learning_rate 0.000107136
2017-10-10T14:47:23.975106: step 1599, loss 0.182832, acc 0.953125, learning_rate 0.000107106
2017-10-10T14:47:24.143701: step 1600, loss 0.239642, acc 0.90625, learning_rate 0.000107077

Evaluation:
2017-10-10T14:47:24.468450: step 1600, loss 0.222415, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1600

2017-10-10T14:47:25.600855: step 1601, loss 0.221482, acc 0.9375, learning_rate 0.000107048
2017-10-10T14:47:25.897466: step 1602, loss 0.133986, acc 0.953125, learning_rate 0.00010702
2017-10-10T14:47:26.138859: step 1603, loss 0.197709, acc 0.9375, learning_rate 0.000106991
2017-10-10T14:47:26.409831: step 1604, loss 0.269287, acc 0.890625, learning_rate 0.000106963
2017-10-10T14:47:26.674416: step 1605, loss 0.11214, acc 0.984375, learning_rate 0.000106934
2017-10-10T14:47:26.883560: step 1606, loss 0.132138, acc 0.96875, learning_rate 0.000106906
2017-10-10T14:47:27.167931: step 1607, loss 0.284563, acc 0.90625, learning_rate 0.000106878
2017-10-10T14:47:27.407854: step 1608, loss 0.15823, acc 0.953125, learning_rate 0.00010685
2017-10-10T14:47:27.642654: step 1609, loss 0.187498, acc 0.953125, learning_rate 0.000106822
2017-10-10T14:47:27.940702: step 1610, loss 0.182748, acc 0.953125, learning_rate 0.000106794
2017-10-10T14:47:28.104960: step 1611, loss 0.106931, acc 0.96875, learning_rate 0.000106766
2017-10-10T14:47:28.344965: step 1612, loss 0.193889, acc 0.921875, learning_rate 0.000106738
2017-10-10T14:47:28.617855: step 1613, loss 0.257111, acc 0.90625, learning_rate 0.000106711
2017-10-10T14:47:28.827158: step 1614, loss 0.0954109, acc 0.984375, learning_rate 0.000106684
2017-10-10T14:47:29.120957: step 1615, loss 0.156478, acc 0.96875, learning_rate 0.000106656
2017-10-10T14:47:29.340975: step 1616, loss 0.148188, acc 0.953125, learning_rate 0.000106629
2017-10-10T14:47:29.587075: step 1617, loss 0.0927892, acc 0.984375, learning_rate 0.000106602
2017-10-10T14:47:29.881997: step 1618, loss 0.19908, acc 0.9375, learning_rate 0.000106575
2017-10-10T14:47:30.136982: step 1619, loss 0.109623, acc 0.984375, learning_rate 0.000106548
2017-10-10T14:47:30.393502: step 1620, loss 0.126574, acc 0.9375, learning_rate 0.000106521
2017-10-10T14:47:30.696922: step 1621, loss 0.159435, acc 0.953125, learning_rate 0.000106495
2017-10-10T14:47:30.891765: step 1622, loss 0.298643, acc 0.90625, learning_rate 0.000106468
2017-10-10T14:47:31.120061: step 1623, loss 0.0951202, acc 0.96875, learning_rate 0.000106442
2017-10-10T14:47:31.373967: step 1624, loss 0.187308, acc 0.9375, learning_rate 0.000106416
2017-10-10T14:47:31.671104: step 1625, loss 0.246444, acc 0.90625, learning_rate 0.000106389
2017-10-10T14:47:31.924949: step 1626, loss 0.13638, acc 0.96875, learning_rate 0.000106363
2017-10-10T14:47:32.184036: step 1627, loss 0.179235, acc 0.953125, learning_rate 0.000106337
2017-10-10T14:47:32.424309: step 1628, loss 0.19503, acc 0.96875, learning_rate 0.000106312
2017-10-10T14:47:32.648878: step 1629, loss 0.187997, acc 0.9375, learning_rate 0.000106286
2017-10-10T14:47:32.903612: step 1630, loss 0.114192, acc 0.984375, learning_rate 0.00010626
2017-10-10T14:47:33.181516: step 1631, loss 0.256711, acc 0.90625, learning_rate 0.000106235
2017-10-10T14:47:33.430451: step 1632, loss 0.186837, acc 0.9375, learning_rate 0.000106209
2017-10-10T14:47:33.646045: step 1633, loss 0.251238, acc 0.9375, learning_rate 0.000106184
2017-10-10T14:47:33.804930: step 1634, loss 0.124366, acc 0.953125, learning_rate 0.000106159
2017-10-10T14:47:34.018493: step 1635, loss 0.123041, acc 0.953125, learning_rate 0.000106133
2017-10-10T14:47:34.244108: step 1636, loss 0.115117, acc 0.96875, learning_rate 0.000106108
2017-10-10T14:47:34.498617: step 1637, loss 0.24369, acc 0.921875, learning_rate 0.000106083
2017-10-10T14:47:34.737803: step 1638, loss 0.162459, acc 0.9375, learning_rate 0.000106059
2017-10-10T14:47:35.023329: step 1639, loss 0.206988, acc 0.9375, learning_rate 0.000106034
2017-10-10T14:47:35.321129: step 1640, loss 0.169946, acc 0.953125, learning_rate 0.000106009

Evaluation:
2017-10-10T14:47:35.675523: step 1640, loss 0.222485, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1640

2017-10-10T14:47:37.010095: step 1641, loss 0.184592, acc 0.953125, learning_rate 0.000105985
2017-10-10T14:47:37.288443: step 1642, loss 0.229966, acc 0.953125, learning_rate 0.00010596
2017-10-10T14:47:37.528899: step 1643, loss 0.232855, acc 0.890625, learning_rate 0.000105936
2017-10-10T14:47:37.799446: step 1644, loss 0.133868, acc 0.96875, learning_rate 0.000105912
2017-10-10T14:47:38.050050: step 1645, loss 0.151771, acc 0.96875, learning_rate 0.000105888
2017-10-10T14:47:38.359962: step 1646, loss 0.258943, acc 0.859375, learning_rate 0.000105864
2017-10-10T14:47:38.556473: step 1647, loss 0.0748293, acc 0.984375, learning_rate 0.00010584
2017-10-10T14:47:38.765278: step 1648, loss 0.204962, acc 0.90625, learning_rate 0.000105816
2017-10-10T14:47:38.962262: step 1649, loss 0.238439, acc 0.921875, learning_rate 0.000105792
2017-10-10T14:47:39.147358: step 1650, loss 0.158086, acc 0.9375, learning_rate 0.000105768
2017-10-10T14:47:39.371894: step 1651, loss 0.0746853, acc 0.984375, learning_rate 0.000105745
2017-10-10T14:47:39.612038: step 1652, loss 0.125388, acc 0.984375, learning_rate 0.000105721
2017-10-10T14:47:39.940673: step 1653, loss 0.122311, acc 0.953125, learning_rate 0.000105698
2017-10-10T14:47:40.211049: step 1654, loss 0.148818, acc 0.96875, learning_rate 0.000105675
2017-10-10T14:47:40.422076: step 1655, loss 0.139206, acc 0.96875, learning_rate 0.000105652
2017-10-10T14:47:40.613063: step 1656, loss 0.19234, acc 0.9375, learning_rate 0.000105629
2017-10-10T14:47:40.814739: step 1657, loss 0.143289, acc 0.9375, learning_rate 0.000105606
2017-10-10T14:47:41.044947: step 1658, loss 0.205012, acc 0.921875, learning_rate 0.000105583
2017-10-10T14:47:41.260763: step 1659, loss 0.150949, acc 0.953125, learning_rate 0.00010556
2017-10-10T14:47:41.552950: step 1660, loss 0.240003, acc 0.921875, learning_rate 0.000105537
2017-10-10T14:47:41.858123: step 1661, loss 0.114249, acc 0.984375, learning_rate 0.000105515
2017-10-10T14:47:42.112917: step 1662, loss 0.0747213, acc 0.984375, learning_rate 0.000105492
2017-10-10T14:47:42.337376: step 1663, loss 0.086465, acc 0.984375, learning_rate 0.00010547
2017-10-10T14:47:42.573997: step 1664, loss 0.216696, acc 0.921875, learning_rate 0.000105447
2017-10-10T14:47:42.820174: step 1665, loss 0.103954, acc 0.984375, learning_rate 0.000105425
2017-10-10T14:47:43.003273: step 1666, loss 0.144928, acc 0.960784, learning_rate 0.000105403
2017-10-10T14:47:43.294037: step 1667, loss 0.133409, acc 0.9375, learning_rate 0.000105381
2017-10-10T14:47:43.540935: step 1668, loss 0.223158, acc 0.921875, learning_rate 0.000105359
2017-10-10T14:47:43.788949: step 1669, loss 0.165055, acc 0.921875, learning_rate 0.000105337
2017-10-10T14:47:44.088828: step 1670, loss 0.0994959, acc 0.96875, learning_rate 0.000105315
2017-10-10T14:47:44.327007: step 1671, loss 0.16057, acc 0.96875, learning_rate 0.000105294
2017-10-10T14:47:44.545102: step 1672, loss 0.160472, acc 0.953125, learning_rate 0.000105272
2017-10-10T14:47:44.868881: step 1673, loss 0.171461, acc 0.921875, learning_rate 0.000105251
2017-10-10T14:47:45.136948: step 1674, loss 0.124094, acc 0.953125, learning_rate 0.000105229
2017-10-10T14:47:45.345440: step 1675, loss 0.167928, acc 0.921875, learning_rate 0.000105208
2017-10-10T14:47:45.572068: step 1676, loss 0.202673, acc 0.9375, learning_rate 0.000105186
2017-10-10T14:47:45.791511: step 1677, loss 0.169198, acc 0.953125, learning_rate 0.000105165
2017-10-10T14:47:46.072592: step 1678, loss 0.131854, acc 0.953125, learning_rate 0.000105144
2017-10-10T14:47:46.305008: step 1679, loss 0.156547, acc 0.96875, learning_rate 0.000105123
2017-10-10T14:47:46.580591: step 1680, loss 0.139868, acc 0.984375, learning_rate 0.000105102

Evaluation:
2017-10-10T14:47:46.963161: step 1680, loss 0.224056, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1680

2017-10-10T14:47:47.886709: step 1681, loss 0.242193, acc 0.90625, learning_rate 0.000105081
2017-10-10T14:47:48.113974: step 1682, loss 0.10211, acc 0.96875, learning_rate 0.000105061
2017-10-10T14:47:48.354525: step 1683, loss 0.135343, acc 0.953125, learning_rate 0.00010504
2017-10-10T14:47:48.547019: step 1684, loss 0.179484, acc 0.953125, learning_rate 0.00010502
2017-10-10T14:47:48.829681: step 1685, loss 0.141357, acc 0.953125, learning_rate 0.000104999
2017-10-10T14:47:49.090421: step 1686, loss 0.11475, acc 0.953125, learning_rate 0.000104979
2017-10-10T14:47:49.296832: step 1687, loss 0.221317, acc 0.921875, learning_rate 0.000104958
2017-10-10T14:47:49.521053: step 1688, loss 0.136358, acc 0.953125, learning_rate 0.000104938
2017-10-10T14:47:49.800858: step 1689, loss 0.286381, acc 0.921875, learning_rate 0.000104918
2017-10-10T14:47:50.036261: step 1690, loss 0.138692, acc 0.953125, learning_rate 0.000104898
2017-10-10T14:47:50.269968: step 1691, loss 0.0907695, acc 0.96875, learning_rate 0.000104878
2017-10-10T14:47:50.497037: step 1692, loss 0.235143, acc 0.890625, learning_rate 0.000104858
2017-10-10T14:47:50.721358: step 1693, loss 0.263113, acc 0.921875, learning_rate 0.000104838
2017-10-10T14:47:51.001013: step 1694, loss 0.141668, acc 0.9375, learning_rate 0.000104818
2017-10-10T14:47:51.279153: step 1695, loss 0.134554, acc 0.9375, learning_rate 0.000104799
2017-10-10T14:47:51.511341: step 1696, loss 0.15472, acc 0.953125, learning_rate 0.000104779
2017-10-10T14:47:51.737880: step 1697, loss 0.239076, acc 0.921875, learning_rate 0.00010476
2017-10-10T14:47:51.905616: step 1698, loss 0.157871, acc 0.953125, learning_rate 0.00010474
2017-10-10T14:47:52.183390: step 1699, loss 0.0545925, acc 0.984375, learning_rate 0.000104721
2017-10-10T14:47:52.430289: step 1700, loss 0.0694885, acc 1, learning_rate 0.000104702
2017-10-10T14:47:52.650860: step 1701, loss 0.206982, acc 0.9375, learning_rate 0.000104682
2017-10-10T14:47:52.951078: step 1702, loss 0.142672, acc 0.9375, learning_rate 0.000104663
2017-10-10T14:47:53.193323: step 1703, loss 0.0873282, acc 0.984375, learning_rate 0.000104644
2017-10-10T14:47:53.459570: step 1704, loss 0.11742, acc 0.96875, learning_rate 0.000104625
2017-10-10T14:47:53.678011: step 1705, loss 0.235974, acc 0.921875, learning_rate 0.000104606
2017-10-10T14:47:53.916923: step 1706, loss 0.0926384, acc 0.984375, learning_rate 0.000104588
2017-10-10T14:47:54.131795: step 1707, loss 0.228833, acc 0.90625, learning_rate 0.000104569
2017-10-10T14:47:54.400994: step 1708, loss 0.23548, acc 0.921875, learning_rate 0.00010455
2017-10-10T14:47:54.660217: step 1709, loss 0.193876, acc 0.9375, learning_rate 0.000104532
2017-10-10T14:47:54.900999: step 1710, loss 0.226034, acc 0.9375, learning_rate 0.000104513
2017-10-10T14:47:55.194363: step 1711, loss 0.181641, acc 0.9375, learning_rate 0.000104495
2017-10-10T14:47:55.411104: step 1712, loss 0.242478, acc 0.9375, learning_rate 0.000104476
2017-10-10T14:47:55.650613: step 1713, loss 0.127107, acc 0.953125, learning_rate 0.000104458
2017-10-10T14:47:56.024886: step 1714, loss 0.0925241, acc 0.984375, learning_rate 0.00010444
2017-10-10T14:47:56.339695: step 1715, loss 0.152389, acc 0.953125, learning_rate 0.000104422
2017-10-10T14:47:56.616448: step 1716, loss 0.0696738, acc 0.984375, learning_rate 0.000104404
2017-10-10T14:47:56.884451: step 1717, loss 0.095682, acc 0.96875, learning_rate 0.000104386
2017-10-10T14:47:57.160826: step 1718, loss 0.28218, acc 0.890625, learning_rate 0.000104368
2017-10-10T14:47:57.391356: step 1719, loss 0.215482, acc 0.921875, learning_rate 0.00010435
2017-10-10T14:47:57.626378: step 1720, loss 0.178821, acc 0.953125, learning_rate 0.000104332

Evaluation:
2017-10-10T14:47:57.913057: step 1720, loss 0.224198, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1720

2017-10-10T14:47:58.875588: step 1721, loss 0.355807, acc 0.859375, learning_rate 0.000104315
2017-10-10T14:47:59.116339: step 1722, loss 0.173425, acc 0.953125, learning_rate 0.000104297
2017-10-10T14:47:59.349848: step 1723, loss 0.254171, acc 0.9375, learning_rate 0.000104279
2017-10-10T14:47:59.624866: step 1724, loss 0.155098, acc 0.9375, learning_rate 0.000104262
2017-10-10T14:47:59.874090: step 1725, loss 0.2846, acc 0.90625, learning_rate 0.000104245
2017-10-10T14:48:00.110824: step 1726, loss 0.0850804, acc 0.984375, learning_rate 0.000104227
2017-10-10T14:48:00.335914: step 1727, loss 0.154744, acc 0.9375, learning_rate 0.00010421
2017-10-10T14:48:00.563902: step 1728, loss 0.156098, acc 0.921875, learning_rate 0.000104193
2017-10-10T14:48:00.881659: step 1729, loss 0.16778, acc 0.9375, learning_rate 0.000104176
2017-10-10T14:48:01.070101: step 1730, loss 0.171103, acc 0.9375, learning_rate 0.000104159
2017-10-10T14:48:01.352849: step 1731, loss 0.109772, acc 0.984375, learning_rate 0.000104142
2017-10-10T14:48:01.600993: step 1732, loss 0.218429, acc 0.90625, learning_rate 0.000104125
2017-10-10T14:48:01.830880: step 1733, loss 0.0687271, acc 0.96875, learning_rate 0.000104108
2017-10-10T14:48:02.066672: step 1734, loss 0.250049, acc 0.921875, learning_rate 0.000104091
2017-10-10T14:48:02.344822: step 1735, loss 0.364708, acc 0.890625, learning_rate 0.000104074
2017-10-10T14:48:02.608848: step 1736, loss 0.178647, acc 0.9375, learning_rate 0.000104058
2017-10-10T14:48:02.871754: step 1737, loss 0.109328, acc 0.96875, learning_rate 0.000104041
2017-10-10T14:48:03.155733: step 1738, loss 0.223829, acc 0.953125, learning_rate 0.000104025
2017-10-10T14:48:03.464916: step 1739, loss 0.24294, acc 0.9375, learning_rate 0.000104008
2017-10-10T14:48:03.656972: step 1740, loss 0.193152, acc 0.890625, learning_rate 0.000103992
2017-10-10T14:48:03.900173: step 1741, loss 0.0784438, acc 0.984375, learning_rate 0.000103976
2017-10-10T14:48:04.145004: step 1742, loss 0.182095, acc 0.9375, learning_rate 0.000103959
2017-10-10T14:48:04.371849: step 1743, loss 0.159544, acc 0.953125, learning_rate 0.000103943
2017-10-10T14:48:04.657175: step 1744, loss 0.203712, acc 0.9375, learning_rate 0.000103927
2017-10-10T14:48:04.885468: step 1745, loss 0.204158, acc 0.90625, learning_rate 0.000103911
2017-10-10T14:48:05.144417: step 1746, loss 0.262647, acc 0.921875, learning_rate 0.000103895
2017-10-10T14:48:05.424979: step 1747, loss 0.159419, acc 0.96875, learning_rate 0.000103879
2017-10-10T14:48:05.696942: step 1748, loss 0.150324, acc 0.96875, learning_rate 0.000103863
2017-10-10T14:48:05.964854: step 1749, loss 0.10103, acc 0.96875, learning_rate 0.000103848
2017-10-10T14:48:06.252904: step 1750, loss 0.173728, acc 0.96875, learning_rate 0.000103832
2017-10-10T14:48:06.481093: step 1751, loss 0.22119, acc 0.921875, learning_rate 0.000103816
2017-10-10T14:48:06.737012: step 1752, loss 0.142169, acc 0.953125, learning_rate 0.000103801
2017-10-10T14:48:06.960955: step 1753, loss 0.206395, acc 0.953125, learning_rate 0.000103785
2017-10-10T14:48:07.201307: step 1754, loss 0.152096, acc 0.9375, learning_rate 0.00010377
2017-10-10T14:48:07.471331: step 1755, loss 0.144606, acc 0.953125, learning_rate 0.000103754
2017-10-10T14:48:07.725353: step 1756, loss 0.110919, acc 0.984375, learning_rate 0.000103739
2017-10-10T14:48:07.995302: step 1757, loss 0.228233, acc 0.890625, learning_rate 0.000103724
2017-10-10T14:48:08.207316: step 1758, loss 0.148031, acc 0.953125, learning_rate 0.000103709
2017-10-10T14:48:08.453931: step 1759, loss 0.172981, acc 0.9375, learning_rate 0.000103694
2017-10-10T14:48:08.691635: step 1760, loss 0.114256, acc 0.984375, learning_rate 0.000103678

Evaluation:
2017-10-10T14:48:09.095819: step 1760, loss 0.220647, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1760

2017-10-10T14:48:10.068438: step 1761, loss 0.110166, acc 0.984375, learning_rate 0.000103663
2017-10-10T14:48:10.360752: step 1762, loss 0.0726219, acc 1, learning_rate 0.000103648
2017-10-10T14:48:10.596259: step 1763, loss 0.248482, acc 0.921875, learning_rate 0.000103634
2017-10-10T14:48:10.800314: step 1764, loss 0.15674, acc 0.941176, learning_rate 0.000103619
2017-10-10T14:48:11.091063: step 1765, loss 0.1335, acc 0.9375, learning_rate 0.000103604
2017-10-10T14:48:11.354940: step 1766, loss 0.218623, acc 0.9375, learning_rate 0.000103589
2017-10-10T14:48:11.569855: step 1767, loss 0.194914, acc 0.953125, learning_rate 0.000103575
2017-10-10T14:48:11.864836: step 1768, loss 0.0770599, acc 1, learning_rate 0.00010356
2017-10-10T14:48:12.065902: step 1769, loss 0.10297, acc 0.984375, learning_rate 0.000103545
2017-10-10T14:48:12.279636: step 1770, loss 0.151872, acc 0.921875, learning_rate 0.000103531
2017-10-10T14:48:12.510986: step 1771, loss 0.261348, acc 0.875, learning_rate 0.000103517
2017-10-10T14:48:12.773381: step 1772, loss 0.201307, acc 0.921875, learning_rate 0.000103502
2017-10-10T14:48:13.035637: step 1773, loss 0.0958333, acc 1, learning_rate 0.000103488
2017-10-10T14:48:13.364886: step 1774, loss 0.144779, acc 0.9375, learning_rate 0.000103474
2017-10-10T14:48:13.561016: step 1775, loss 0.1207, acc 0.96875, learning_rate 0.00010346
2017-10-10T14:48:13.825176: step 1776, loss 0.206158, acc 0.9375, learning_rate 0.000103445
2017-10-10T14:48:14.024998: step 1777, loss 0.079643, acc 0.96875, learning_rate 0.000103431
2017-10-10T14:48:14.310015: step 1778, loss 0.356129, acc 0.890625, learning_rate 0.000103417
2017-10-10T14:48:14.529895: step 1779, loss 0.149404, acc 0.953125, learning_rate 0.000103403
2017-10-10T14:48:14.691039: step 1780, loss 0.189689, acc 0.9375, learning_rate 0.00010339
2017-10-10T14:48:14.960875: step 1781, loss 0.161624, acc 0.953125, learning_rate 0.000103376
2017-10-10T14:48:15.196924: step 1782, loss 0.128476, acc 0.96875, learning_rate 0.000103362
2017-10-10T14:48:15.393056: step 1783, loss 0.203163, acc 0.953125, learning_rate 0.000103348
2017-10-10T14:48:15.652410: step 1784, loss 0.226483, acc 0.90625, learning_rate 0.000103335
2017-10-10T14:48:15.907203: step 1785, loss 0.147222, acc 0.9375, learning_rate 0.000103321
2017-10-10T14:48:16.128834: step 1786, loss 0.270628, acc 0.921875, learning_rate 0.000103307
2017-10-10T14:48:16.358480: step 1787, loss 0.118552, acc 0.96875, learning_rate 0.000103294
2017-10-10T14:48:16.689197: step 1788, loss 0.167173, acc 0.9375, learning_rate 0.00010328
2017-10-10T14:48:16.946128: step 1789, loss 0.13538, acc 0.953125, learning_rate 0.000103267
2017-10-10T14:48:17.240286: step 1790, loss 0.0794948, acc 0.984375, learning_rate 0.000103254
2017-10-10T14:48:17.485164: step 1791, loss 0.214152, acc 0.9375, learning_rate 0.00010324
2017-10-10T14:48:17.773040: step 1792, loss 0.223806, acc 0.921875, learning_rate 0.000103227
2017-10-10T14:48:18.016970: step 1793, loss 0.135298, acc 0.9375, learning_rate 0.000103214
2017-10-10T14:48:18.256965: step 1794, loss 0.242485, acc 0.921875, learning_rate 0.000103201
2017-10-10T14:48:18.556875: step 1795, loss 0.218319, acc 0.921875, learning_rate 0.000103188
2017-10-10T14:48:18.785031: step 1796, loss 0.149705, acc 0.9375, learning_rate 0.000103175
2017-10-10T14:48:19.032892: step 1797, loss 0.134094, acc 0.96875, learning_rate 0.000103162
2017-10-10T14:48:19.269004: step 1798, loss 0.182075, acc 0.921875, learning_rate 0.000103149
2017-10-10T14:48:19.547800: step 1799, loss 0.265415, acc 0.921875, learning_rate 0.000103136
2017-10-10T14:48:19.773082: step 1800, loss 0.154951, acc 0.96875, learning_rate 0.000103123

Evaluation:
2017-10-10T14:48:20.236372: step 1800, loss 0.222263, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1800

2017-10-10T14:48:21.374765: step 1801, loss 0.161844, acc 0.9375, learning_rate 0.000103111
2017-10-10T14:48:21.688252: step 1802, loss 0.291088, acc 0.953125, learning_rate 0.000103098
2017-10-10T14:48:21.976953: step 1803, loss 0.086814, acc 0.984375, learning_rate 0.000103085
2017-10-10T14:48:22.192173: step 1804, loss 0.121104, acc 0.9375, learning_rate 0.000103073
2017-10-10T14:48:22.416991: step 1805, loss 0.153461, acc 0.953125, learning_rate 0.00010306
2017-10-10T14:48:22.606914: step 1806, loss 0.0939775, acc 0.984375, learning_rate 0.000103048
2017-10-10T14:48:22.851965: step 1807, loss 0.269282, acc 0.921875, learning_rate 0.000103035
2017-10-10T14:48:23.104991: step 1808, loss 0.187, acc 0.953125, learning_rate 0.000103023
2017-10-10T14:48:23.296767: step 1809, loss 0.108515, acc 0.96875, learning_rate 0.00010301
2017-10-10T14:48:23.519368: step 1810, loss 0.196163, acc 0.953125, learning_rate 0.000102998
2017-10-10T14:48:23.781036: step 1811, loss 0.14945, acc 0.953125, learning_rate 0.000102986
2017-10-10T14:48:24.048732: step 1812, loss 0.205064, acc 0.921875, learning_rate 0.000102974
2017-10-10T14:48:24.273031: step 1813, loss 0.24314, acc 0.9375, learning_rate 0.000102962
2017-10-10T14:48:24.589264: step 1814, loss 0.0968944, acc 0.96875, learning_rate 0.000102949
2017-10-10T14:48:24.797067: step 1815, loss 0.1607, acc 0.9375, learning_rate 0.000102937
2017-10-10T14:48:25.016855: step 1816, loss 0.167802, acc 0.921875, learning_rate 0.000102925
2017-10-10T14:48:25.272917: step 1817, loss 0.197168, acc 0.953125, learning_rate 0.000102913
2017-10-10T14:48:25.509942: step 1818, loss 0.181852, acc 0.921875, learning_rate 0.000102902
2017-10-10T14:48:25.791704: step 1819, loss 0.422539, acc 0.84375, learning_rate 0.00010289
2017-10-10T14:48:26.071033: step 1820, loss 0.177065, acc 0.9375, learning_rate 0.000102878
2017-10-10T14:48:26.356738: step 1821, loss 0.153786, acc 0.921875, learning_rate 0.000102866
2017-10-10T14:48:26.580826: step 1822, loss 0.149791, acc 0.9375, learning_rate 0.000102855
2017-10-10T14:48:26.821346: step 1823, loss 0.145831, acc 0.953125, learning_rate 0.000102843
2017-10-10T14:48:27.030784: step 1824, loss 0.254212, acc 0.90625, learning_rate 0.000102831
2017-10-10T14:48:27.268421: step 1825, loss 0.166673, acc 0.96875, learning_rate 0.00010282
2017-10-10T14:48:27.539823: step 1826, loss 0.174555, acc 0.9375, learning_rate 0.000102808
2017-10-10T14:48:27.788998: step 1827, loss 0.149014, acc 0.9375, learning_rate 0.000102797
2017-10-10T14:48:28.043283: step 1828, loss 0.148474, acc 0.953125, learning_rate 0.000102785
2017-10-10T14:48:28.308845: step 1829, loss 0.12655, acc 0.96875, learning_rate 0.000102774
2017-10-10T14:48:28.530833: step 1830, loss 0.229002, acc 0.953125, learning_rate 0.000102763
2017-10-10T14:48:28.799747: step 1831, loss 0.090501, acc 0.96875, learning_rate 0.000102751
2017-10-10T14:48:29.012959: step 1832, loss 0.119934, acc 0.96875, learning_rate 0.00010274
2017-10-10T14:48:29.228961: step 1833, loss 0.0832602, acc 0.984375, learning_rate 0.000102729
2017-10-10T14:48:29.468582: step 1834, loss 0.089749, acc 0.984375, learning_rate 0.000102718
2017-10-10T14:48:29.714163: step 1835, loss 0.133399, acc 0.96875, learning_rate 0.000102707
2017-10-10T14:48:30.049062: step 1836, loss 0.260974, acc 0.890625, learning_rate 0.000102696
2017-10-10T14:48:30.353084: step 1837, loss 0.227975, acc 0.9375, learning_rate 0.000102685
2017-10-10T14:48:30.586871: step 1838, loss 0.175965, acc 0.953125, learning_rate 0.000102674
2017-10-10T14:48:30.844756: step 1839, loss 0.350811, acc 0.859375, learning_rate 0.000102663
2017-10-10T14:48:30.996879: step 1840, loss 0.17274, acc 0.953125, learning_rate 0.000102652

Evaluation:
2017-10-10T14:48:31.349047: step 1840, loss 0.220496, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1840

2017-10-10T14:48:32.216561: step 1841, loss 0.2035, acc 0.9375, learning_rate 0.000102641
2017-10-10T14:48:32.510981: step 1842, loss 0.211773, acc 0.9375, learning_rate 0.00010263
2017-10-10T14:48:32.857032: step 1843, loss 0.143218, acc 0.9375, learning_rate 0.00010262
2017-10-10T14:48:33.143448: step 1844, loss 0.19031, acc 0.921875, learning_rate 0.000102609
2017-10-10T14:48:33.376815: step 1845, loss 0.12952, acc 0.96875, learning_rate 0.000102598
2017-10-10T14:48:33.563372: step 1846, loss 0.224424, acc 0.9375, learning_rate 0.000102588
2017-10-10T14:48:33.795880: step 1847, loss 0.173903, acc 0.9375, learning_rate 0.000102577
2017-10-10T14:48:34.058814: step 1848, loss 0.100267, acc 0.984375, learning_rate 0.000102567
2017-10-10T14:48:34.256878: step 1849, loss 0.100984, acc 0.96875, learning_rate 0.000102556
2017-10-10T14:48:34.465450: step 1850, loss 0.10857, acc 0.953125, learning_rate 0.000102546
2017-10-10T14:48:34.680831: step 1851, loss 0.181556, acc 0.9375, learning_rate 0.000102535
2017-10-10T14:48:34.928999: step 1852, loss 0.184429, acc 0.96875, learning_rate 0.000102525
2017-10-10T14:48:35.144463: step 1853, loss 0.266033, acc 0.921875, learning_rate 0.000102515
2017-10-10T14:48:35.373898: step 1854, loss 0.128311, acc 0.984375, learning_rate 0.000102504
2017-10-10T14:48:35.605192: step 1855, loss 0.102926, acc 0.96875, learning_rate 0.000102494
2017-10-10T14:48:35.833135: step 1856, loss 0.12351, acc 0.984375, learning_rate 0.000102484
2017-10-10T14:48:36.024617: step 1857, loss 0.153482, acc 0.9375, learning_rate 0.000102474
2017-10-10T14:48:36.260846: step 1858, loss 0.173397, acc 0.9375, learning_rate 0.000102464
2017-10-10T14:48:36.512943: step 1859, loss 0.130536, acc 0.953125, learning_rate 0.000102454
2017-10-10T14:48:36.732908: step 1860, loss 0.200781, acc 0.9375, learning_rate 0.000102444
2017-10-10T14:48:36.953002: step 1861, loss 0.0644364, acc 1, learning_rate 0.000102434
2017-10-10T14:48:37.146850: step 1862, loss 0.248215, acc 0.921569, learning_rate 0.000102424
2017-10-10T14:48:37.374416: step 1863, loss 0.102649, acc 0.96875, learning_rate 0.000102414
2017-10-10T14:48:37.660279: step 1864, loss 0.223169, acc 0.9375, learning_rate 0.000102404
2017-10-10T14:48:37.898441: step 1865, loss 0.101022, acc 0.984375, learning_rate 0.000102394
2017-10-10T14:48:38.079743: step 1866, loss 0.166311, acc 0.953125, learning_rate 0.000102384
2017-10-10T14:48:38.349036: step 1867, loss 0.166185, acc 0.921875, learning_rate 0.000102375
2017-10-10T14:48:38.617234: step 1868, loss 0.144969, acc 0.96875, learning_rate 0.000102365
2017-10-10T14:48:38.879401: step 1869, loss 0.237662, acc 0.921875, learning_rate 0.000102355
2017-10-10T14:48:39.134188: step 1870, loss 0.07923, acc 0.984375, learning_rate 0.000102346
2017-10-10T14:48:39.373646: step 1871, loss 0.194813, acc 0.921875, learning_rate 0.000102336
2017-10-10T14:48:39.652810: step 1872, loss 0.176298, acc 0.9375, learning_rate 0.000102327
2017-10-10T14:48:39.935707: step 1873, loss 0.101353, acc 0.953125, learning_rate 0.000102317
2017-10-10T14:48:40.134912: step 1874, loss 0.28558, acc 0.875, learning_rate 0.000102308
2017-10-10T14:48:40.344989: step 1875, loss 0.122581, acc 0.953125, learning_rate 0.000102298
2017-10-10T14:48:40.600872: step 1876, loss 0.191832, acc 0.96875, learning_rate 0.000102289
2017-10-10T14:48:40.844966: step 1877, loss 0.144005, acc 0.953125, learning_rate 0.000102279
2017-10-10T14:48:41.054951: step 1878, loss 0.194556, acc 0.9375, learning_rate 0.00010227
2017-10-10T14:48:41.330660: step 1879, loss 0.144344, acc 0.96875, learning_rate 0.000102261
2017-10-10T14:48:41.576895: step 1880, loss 0.0950684, acc 0.953125, learning_rate 0.000102252

Evaluation:
2017-10-10T14:48:41.947474: step 1880, loss 0.221441, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1880

2017-10-10T14:48:43.064088: step 1881, loss 0.106788, acc 0.96875, learning_rate 0.000102242
2017-10-10T14:48:43.352871: step 1882, loss 0.275461, acc 0.828125, learning_rate 0.000102233
2017-10-10T14:48:43.641463: step 1883, loss 0.107732, acc 0.953125, learning_rate 0.000102224
2017-10-10T14:48:43.886785: step 1884, loss 0.140794, acc 0.953125, learning_rate 0.000102215
2017-10-10T14:48:44.129514: step 1885, loss 0.112362, acc 0.984375, learning_rate 0.000102206
2017-10-10T14:48:44.328860: step 1886, loss 0.0949826, acc 0.96875, learning_rate 0.000102197
2017-10-10T14:48:44.624830: step 1887, loss 0.160739, acc 0.953125, learning_rate 0.000102188
2017-10-10T14:48:44.891336: step 1888, loss 0.23013, acc 0.90625, learning_rate 0.000102179
2017-10-10T14:48:45.088245: step 1889, loss 0.166063, acc 0.96875, learning_rate 0.00010217
2017-10-10T14:48:45.411411: step 1890, loss 0.235499, acc 0.90625, learning_rate 0.000102161
2017-10-10T14:48:45.580702: step 1891, loss 0.158662, acc 0.9375, learning_rate 0.000102153
2017-10-10T14:48:45.840307: step 1892, loss 0.16553, acc 0.984375, learning_rate 0.000102144
2017-10-10T14:48:46.109680: step 1893, loss 0.139765, acc 0.96875, learning_rate 0.000102135
2017-10-10T14:48:46.366975: step 1894, loss 0.160075, acc 0.9375, learning_rate 0.000102126
2017-10-10T14:48:46.681029: step 1895, loss 0.0724148, acc 1, learning_rate 0.000102118
2017-10-10T14:48:46.896843: step 1896, loss 0.0910876, acc 0.984375, learning_rate 0.000102109
2017-10-10T14:48:47.115721: step 1897, loss 0.270776, acc 0.9375, learning_rate 0.0001021
2017-10-10T14:48:47.231253: step 1898, loss 0.228185, acc 0.90625, learning_rate 0.000102092
2017-10-10T14:48:47.422688: step 1899, loss 0.170568, acc 0.9375, learning_rate 0.000102083
2017-10-10T14:48:47.561302: step 1900, loss 0.128079, acc 0.96875, learning_rate 0.000102075
2017-10-10T14:48:47.751961: step 1901, loss 0.216014, acc 0.875, learning_rate 0.000102066
2017-10-10T14:48:47.955557: step 1902, loss 0.102777, acc 0.953125, learning_rate 0.000102058
2017-10-10T14:48:48.229153: step 1903, loss 0.123033, acc 0.96875, learning_rate 0.00010205
2017-10-10T14:48:48.440915: step 1904, loss 0.167094, acc 0.921875, learning_rate 0.000102041
2017-10-10T14:48:48.678414: step 1905, loss 0.206548, acc 0.90625, learning_rate 0.000102033
2017-10-10T14:48:48.932939: step 1906, loss 0.0945558, acc 0.953125, learning_rate 0.000102025
2017-10-10T14:48:49.166083: step 1907, loss 0.101214, acc 0.96875, learning_rate 0.000102016
2017-10-10T14:48:49.448865: step 1908, loss 0.131003, acc 0.953125, learning_rate 0.000102008
2017-10-10T14:48:49.750982: step 1909, loss 0.201735, acc 0.90625, learning_rate 0.000102
2017-10-10T14:48:50.000166: step 1910, loss 0.211427, acc 0.953125, learning_rate 0.000101992
2017-10-10T14:48:50.230874: step 1911, loss 0.176052, acc 0.9375, learning_rate 0.000101984
2017-10-10T14:48:50.473161: step 1912, loss 0.139371, acc 0.96875, learning_rate 0.000101975
2017-10-10T14:48:50.736951: step 1913, loss 0.190692, acc 0.90625, learning_rate 0.000101967
2017-10-10T14:48:51.036371: step 1914, loss 0.162129, acc 0.953125, learning_rate 0.000101959
2017-10-10T14:48:51.264881: step 1915, loss 0.187275, acc 0.96875, learning_rate 0.000101951
2017-10-10T14:48:51.512484: step 1916, loss 0.257282, acc 0.9375, learning_rate 0.000101943
2017-10-10T14:48:51.738551: step 1917, loss 0.150702, acc 0.921875, learning_rate 0.000101935
2017-10-10T14:48:51.968819: step 1918, loss 0.175999, acc 0.953125, learning_rate 0.000101928
2017-10-10T14:48:52.175744: step 1919, loss 0.173864, acc 0.9375, learning_rate 0.00010192
2017-10-10T14:48:52.452401: step 1920, loss 0.256296, acc 0.890625, learning_rate 0.000101912

Evaluation:
2017-10-10T14:48:52.963801: step 1920, loss 0.221555, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1920

2017-10-10T14:48:54.123525: step 1921, loss 0.0770673, acc 0.984375, learning_rate 0.000101904
2017-10-10T14:48:54.412610: step 1922, loss 0.211853, acc 0.921875, learning_rate 0.000101896
2017-10-10T14:48:54.669353: step 1923, loss 0.259713, acc 0.921875, learning_rate 0.000101889
2017-10-10T14:48:54.941100: step 1924, loss 0.239934, acc 0.9375, learning_rate 0.000101881
2017-10-10T14:48:55.181387: step 1925, loss 0.15817, acc 0.953125, learning_rate 0.000101873
2017-10-10T14:48:55.414400: step 1926, loss 0.176015, acc 0.9375, learning_rate 0.000101865
2017-10-10T14:48:55.700727: step 1927, loss 0.272222, acc 0.890625, learning_rate 0.000101858
2017-10-10T14:48:55.964935: step 1928, loss 0.130504, acc 0.953125, learning_rate 0.00010185
2017-10-10T14:48:56.221135: step 1929, loss 0.182386, acc 0.953125, learning_rate 0.000101843
2017-10-10T14:48:56.430235: step 1930, loss 0.0986472, acc 0.953125, learning_rate 0.000101835
2017-10-10T14:48:56.678166: step 1931, loss 0.0517935, acc 1, learning_rate 0.000101828
2017-10-10T14:48:56.879228: step 1932, loss 0.133983, acc 0.96875, learning_rate 0.00010182
2017-10-10T14:48:57.119152: step 1933, loss 0.168557, acc 0.921875, learning_rate 0.000101813
2017-10-10T14:48:57.349799: step 1934, loss 0.159694, acc 0.953125, learning_rate 0.000101805
2017-10-10T14:48:57.611089: step 1935, loss 0.295071, acc 0.90625, learning_rate 0.000101798
2017-10-10T14:48:57.846430: step 1936, loss 0.0805501, acc 0.984375, learning_rate 0.000101791
2017-10-10T14:48:58.119888: step 1937, loss 0.154833, acc 0.953125, learning_rate 0.000101783
2017-10-10T14:48:58.413003: step 1938, loss 0.11651, acc 0.953125, learning_rate 0.000101776
2017-10-10T14:48:58.717034: step 1939, loss 0.162017, acc 0.953125, learning_rate 0.000101769
2017-10-10T14:48:59.020025: step 1940, loss 0.20907, acc 0.953125, learning_rate 0.000101762
2017-10-10T14:48:59.219718: step 1941, loss 0.0839076, acc 0.984375, learning_rate 0.000101754
2017-10-10T14:48:59.440823: step 1942, loss 0.190936, acc 0.9375, learning_rate 0.000101747
2017-10-10T14:48:59.711581: step 1943, loss 0.103042, acc 0.96875, learning_rate 0.00010174
2017-10-10T14:48:59.934278: step 1944, loss 0.127816, acc 0.9375, learning_rate 0.000101733
2017-10-10T14:49:00.177259: step 1945, loss 0.129342, acc 0.953125, learning_rate 0.000101726
2017-10-10T14:49:00.467182: step 1946, loss 0.228625, acc 0.9375, learning_rate 0.000101719
2017-10-10T14:49:00.652239: step 1947, loss 0.139789, acc 0.953125, learning_rate 0.000101712
2017-10-10T14:49:00.897073: step 1948, loss 0.152933, acc 0.953125, learning_rate 0.000101705
2017-10-10T14:49:01.191426: step 1949, loss 0.165305, acc 0.9375, learning_rate 0.000101698
2017-10-10T14:49:01.466210: step 1950, loss 0.162078, acc 0.953125, learning_rate 0.000101691
2017-10-10T14:49:01.668486: step 1951, loss 0.17525, acc 0.953125, learning_rate 0.000101684
2017-10-10T14:49:01.935384: step 1952, loss 0.145257, acc 0.953125, learning_rate 0.000101677
2017-10-10T14:49:02.220953: step 1953, loss 0.188239, acc 0.953125, learning_rate 0.00010167
2017-10-10T14:49:02.479119: step 1954, loss 0.158282, acc 0.9375, learning_rate 0.000101664
2017-10-10T14:49:02.754209: step 1955, loss 0.143754, acc 0.953125, learning_rate 0.000101657
2017-10-10T14:49:03.006268: step 1956, loss 0.17064, acc 0.921875, learning_rate 0.00010165
2017-10-10T14:49:03.273632: step 1957, loss 0.157801, acc 0.90625, learning_rate 0.000101643
2017-10-10T14:49:03.534483: step 1958, loss 0.218012, acc 0.9375, learning_rate 0.000101637
2017-10-10T14:49:03.761499: step 1959, loss 0.183244, acc 0.953125, learning_rate 0.00010163
2017-10-10T14:49:03.950677: step 1960, loss 0.124757, acc 0.941176, learning_rate 0.000101623

Evaluation:
2017-10-10T14:49:04.260825: step 1960, loss 0.219336, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-1960

2017-10-10T14:49:05.446514: step 1961, loss 0.0669087, acc 0.96875, learning_rate 0.000101617
2017-10-10T14:49:05.651443: step 1962, loss 0.141619, acc 0.953125, learning_rate 0.00010161
2017-10-10T14:49:05.854879: step 1963, loss 0.182777, acc 0.921875, learning_rate 0.000101604
2017-10-10T14:49:06.143359: step 1964, loss 0.189768, acc 0.90625, learning_rate 0.000101597
2017-10-10T14:49:06.398150: step 1965, loss 0.211689, acc 0.90625, learning_rate 0.00010159
2017-10-10T14:49:06.620310: step 1966, loss 0.207094, acc 0.953125, learning_rate 0.000101584
2017-10-10T14:49:06.832658: step 1967, loss 0.149366, acc 0.9375, learning_rate 0.000101577
2017-10-10T14:49:07.043884: step 1968, loss 0.290555, acc 0.90625, learning_rate 0.000101571
2017-10-10T14:49:07.375449: step 1969, loss 0.308345, acc 0.859375, learning_rate 0.000101565
2017-10-10T14:49:07.653900: step 1970, loss 0.15392, acc 0.953125, learning_rate 0.000101558
2017-10-10T14:49:07.936026: step 1971, loss 0.136468, acc 0.9375, learning_rate 0.000101552
2017-10-10T14:49:08.164546: step 1972, loss 0.162958, acc 0.9375, learning_rate 0.000101546
2017-10-10T14:49:08.392399: step 1973, loss 0.228732, acc 0.890625, learning_rate 0.000101539
2017-10-10T14:49:08.636889: step 1974, loss 0.0875186, acc 0.96875, learning_rate 0.000101533
2017-10-10T14:49:08.936920: step 1975, loss 0.308232, acc 0.9375, learning_rate 0.000101527
2017-10-10T14:49:09.183187: step 1976, loss 0.181745, acc 0.9375, learning_rate 0.00010152
2017-10-10T14:49:09.472998: step 1977, loss 0.129208, acc 0.96875, learning_rate 0.000101514
2017-10-10T14:49:09.655603: step 1978, loss 0.0687167, acc 0.984375, learning_rate 0.000101508
2017-10-10T14:49:09.906729: step 1979, loss 0.176777, acc 0.9375, learning_rate 0.000101502
2017-10-10T14:49:10.132673: step 1980, loss 0.13032, acc 0.9375, learning_rate 0.000101496
2017-10-10T14:49:10.363197: step 1981, loss 0.146236, acc 0.953125, learning_rate 0.00010149
2017-10-10T14:49:10.651719: step 1982, loss 0.145977, acc 0.96875, learning_rate 0.000101484
2017-10-10T14:49:10.878724: step 1983, loss 0.221659, acc 0.890625, learning_rate 0.000101478
2017-10-10T14:49:11.177642: step 1984, loss 0.227448, acc 0.9375, learning_rate 0.000101472
2017-10-10T14:49:11.447347: step 1985, loss 0.171959, acc 0.9375, learning_rate 0.000101466
2017-10-10T14:49:11.621020: step 1986, loss 0.14964, acc 0.953125, learning_rate 0.00010146
2017-10-10T14:49:11.892873: step 1987, loss 0.0754297, acc 0.984375, learning_rate 0.000101454
2017-10-10T14:49:12.076910: step 1988, loss 0.257008, acc 0.9375, learning_rate 0.000101448
2017-10-10T14:49:12.373002: step 1989, loss 0.263447, acc 0.90625, learning_rate 0.000101442
2017-10-10T14:49:12.625267: step 1990, loss 0.16012, acc 0.953125, learning_rate 0.000101436
2017-10-10T14:49:12.864877: step 1991, loss 0.133457, acc 0.96875, learning_rate 0.00010143
2017-10-10T14:49:13.144876: step 1992, loss 0.131505, acc 0.96875, learning_rate 0.000101424
2017-10-10T14:49:13.375690: step 1993, loss 0.142272, acc 0.96875, learning_rate 0.000101418
2017-10-10T14:49:13.672872: step 1994, loss 0.182069, acc 0.9375, learning_rate 0.000101413
2017-10-10T14:49:13.948945: step 1995, loss 0.0674961, acc 0.984375, learning_rate 0.000101407
2017-10-10T14:49:14.284981: step 1996, loss 0.136834, acc 0.96875, learning_rate 0.000101401
2017-10-10T14:49:14.551022: step 1997, loss 0.121017, acc 0.984375, learning_rate 0.000101395
2017-10-10T14:49:14.856350: step 1998, loss 0.203045, acc 0.90625, learning_rate 0.00010139
2017-10-10T14:49:15.052024: step 1999, loss 0.0998893, acc 0.96875, learning_rate 0.000101384
2017-10-10T14:49:15.336828: step 2000, loss 0.200494, acc 0.9375, learning_rate 0.000101378

Evaluation:
2017-10-10T14:49:15.776106: step 2000, loss 0.218875, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2000

2017-10-10T14:49:16.733556: step 2001, loss 0.107957, acc 0.96875, learning_rate 0.000101373
2017-10-10T14:49:17.020580: step 2002, loss 0.177266, acc 0.953125, learning_rate 0.000101367
2017-10-10T14:49:17.259810: step 2003, loss 0.164858, acc 0.9375, learning_rate 0.000101362
2017-10-10T14:49:17.514677: step 2004, loss 0.26656, acc 0.875, learning_rate 0.000101356
2017-10-10T14:49:17.708031: step 2005, loss 0.219505, acc 0.9375, learning_rate 0.00010135
2017-10-10T14:49:18.004333: step 2006, loss 0.140415, acc 0.9375, learning_rate 0.000101345
2017-10-10T14:49:18.269326: step 2007, loss 0.105128, acc 0.96875, learning_rate 0.000101339
2017-10-10T14:49:18.541034: step 2008, loss 0.265637, acc 0.9375, learning_rate 0.000101334
2017-10-10T14:49:18.808028: step 2009, loss 0.18219, acc 0.953125, learning_rate 0.000101328
2017-10-10T14:49:19.034282: step 2010, loss 0.332384, acc 0.875, learning_rate 0.000101323
2017-10-10T14:49:19.246296: step 2011, loss 0.177681, acc 0.953125, learning_rate 0.000101318
2017-10-10T14:49:19.529203: step 2012, loss 0.240879, acc 0.90625, learning_rate 0.000101312
2017-10-10T14:49:19.938558: step 2013, loss 0.125339, acc 0.96875, learning_rate 0.000101307
2017-10-10T14:49:20.134712: step 2014, loss 0.156038, acc 0.9375, learning_rate 0.000101302
2017-10-10T14:49:20.278061: step 2015, loss 0.116986, acc 0.953125, learning_rate 0.000101296
2017-10-10T14:49:20.497214: step 2016, loss 0.155652, acc 0.921875, learning_rate 0.000101291
2017-10-10T14:49:20.732890: step 2017, loss 0.176357, acc 0.953125, learning_rate 0.000101286
2017-10-10T14:49:20.924808: step 2018, loss 0.161436, acc 0.9375, learning_rate 0.00010128
2017-10-10T14:49:21.164816: step 2019, loss 0.0992953, acc 0.96875, learning_rate 0.000101275
2017-10-10T14:49:21.441011: step 2020, loss 0.142978, acc 0.984375, learning_rate 0.00010127
2017-10-10T14:49:21.649795: step 2021, loss 0.308717, acc 0.890625, learning_rate 0.000101265
2017-10-10T14:49:21.924851: step 2022, loss 0.200513, acc 0.921875, learning_rate 0.00010126
2017-10-10T14:49:22.210927: step 2023, loss 0.104649, acc 0.96875, learning_rate 0.000101255
2017-10-10T14:49:22.453252: step 2024, loss 0.120107, acc 0.984375, learning_rate 0.000101249
2017-10-10T14:49:22.728918: step 2025, loss 0.177009, acc 0.9375, learning_rate 0.000101244
2017-10-10T14:49:23.008362: step 2026, loss 0.205817, acc 0.953125, learning_rate 0.000101239
2017-10-10T14:49:23.346926: step 2027, loss 0.200817, acc 0.9375, learning_rate 0.000101234
2017-10-10T14:49:23.592946: step 2028, loss 0.111601, acc 0.984375, learning_rate 0.000101229
2017-10-10T14:49:23.761415: step 2029, loss 0.16379, acc 0.953125, learning_rate 0.000101224
2017-10-10T14:49:24.020112: step 2030, loss 0.169606, acc 0.9375, learning_rate 0.000101219
2017-10-10T14:49:24.211362: step 2031, loss 0.230496, acc 0.90625, learning_rate 0.000101214
2017-10-10T14:49:24.500912: step 2032, loss 0.162098, acc 0.921875, learning_rate 0.000101209
2017-10-10T14:49:24.830630: step 2033, loss 0.133372, acc 0.984375, learning_rate 0.000101204
2017-10-10T14:49:25.021565: step 2034, loss 0.143456, acc 0.984375, learning_rate 0.000101199
2017-10-10T14:49:25.262784: step 2035, loss 0.130331, acc 0.953125, learning_rate 0.000101194
2017-10-10T14:49:25.498822: step 2036, loss 0.0861396, acc 0.96875, learning_rate 0.00010119
2017-10-10T14:49:25.740834: step 2037, loss 0.107704, acc 0.984375, learning_rate 0.000101185
2017-10-10T14:49:26.039406: step 2038, loss 0.301582, acc 0.890625, learning_rate 0.00010118
2017-10-10T14:49:26.281668: step 2039, loss 0.221357, acc 0.9375, learning_rate 0.000101175
2017-10-10T14:49:26.556222: step 2040, loss 0.189417, acc 0.953125, learning_rate 0.00010117

Evaluation:
2017-10-10T14:49:27.016900: step 2040, loss 0.219465, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2040

2017-10-10T14:49:28.009259: step 2041, loss 0.182538, acc 0.9375, learning_rate 0.000101166
2017-10-10T14:49:28.255120: step 2042, loss 0.188945, acc 0.9375, learning_rate 0.000101161
2017-10-10T14:49:28.454296: step 2043, loss 0.184578, acc 0.90625, learning_rate 0.000101156
2017-10-10T14:49:28.659406: step 2044, loss 0.173624, acc 0.9375, learning_rate 0.000101151
2017-10-10T14:49:28.912212: step 2045, loss 0.166463, acc 0.9375, learning_rate 0.000101147
2017-10-10T14:49:29.128269: step 2046, loss 0.142399, acc 0.953125, learning_rate 0.000101142
2017-10-10T14:49:29.418320: step 2047, loss 0.25241, acc 0.921875, learning_rate 0.000101137
2017-10-10T14:49:29.641007: step 2048, loss 0.132754, acc 0.96875, learning_rate 0.000101133
2017-10-10T14:49:29.868941: step 2049, loss 0.248811, acc 0.90625, learning_rate 0.000101128
2017-10-10T14:49:30.136816: step 2050, loss 0.238431, acc 0.921875, learning_rate 0.000101123
2017-10-10T14:49:30.356954: step 2051, loss 0.195547, acc 0.90625, learning_rate 0.000101119
2017-10-10T14:49:30.633921: step 2052, loss 0.0824129, acc 0.96875, learning_rate 0.000101114
2017-10-10T14:49:30.844798: step 2053, loss 0.141512, acc 0.96875, learning_rate 0.00010111
2017-10-10T14:49:31.071630: step 2054, loss 0.112672, acc 0.96875, learning_rate 0.000101105
2017-10-10T14:49:31.319678: step 2055, loss 0.270444, acc 0.921875, learning_rate 0.000101101
2017-10-10T14:49:31.537983: step 2056, loss 0.123944, acc 0.96875, learning_rate 0.000101096
2017-10-10T14:49:31.773518: step 2057, loss 0.199281, acc 0.9375, learning_rate 0.000101092
2017-10-10T14:49:31.993465: step 2058, loss 0.189154, acc 0.901961, learning_rate 0.000101087
2017-10-10T14:49:32.278763: step 2059, loss 0.095879, acc 0.984375, learning_rate 0.000101083
2017-10-10T14:49:32.552436: step 2060, loss 0.0753474, acc 0.953125, learning_rate 0.000101078
2017-10-10T14:49:32.932914: step 2061, loss 0.220035, acc 0.90625, learning_rate 0.000101074
2017-10-10T14:49:33.196605: step 2062, loss 0.297991, acc 0.90625, learning_rate 0.00010107
2017-10-10T14:49:33.454943: step 2063, loss 0.186633, acc 0.953125, learning_rate 0.000101065
2017-10-10T14:49:33.654587: step 2064, loss 0.114558, acc 0.953125, learning_rate 0.000101061
2017-10-10T14:49:33.896059: step 2065, loss 0.152788, acc 0.96875, learning_rate 0.000101057
2017-10-10T14:49:34.139048: step 2066, loss 0.133156, acc 0.9375, learning_rate 0.000101052
2017-10-10T14:49:34.380249: step 2067, loss 0.219525, acc 0.9375, learning_rate 0.000101048
2017-10-10T14:49:34.605411: step 2068, loss 0.0721797, acc 1, learning_rate 0.000101044
2017-10-10T14:49:34.853066: step 2069, loss 0.0963409, acc 0.96875, learning_rate 0.000101039
2017-10-10T14:49:35.088852: step 2070, loss 0.174469, acc 0.921875, learning_rate 0.000101035
2017-10-10T14:49:35.400836: step 2071, loss 0.358888, acc 0.921875, learning_rate 0.000101031
2017-10-10T14:49:35.648840: step 2072, loss 0.219545, acc 0.90625, learning_rate 0.000101027
2017-10-10T14:49:35.883202: step 2073, loss 0.122518, acc 0.953125, learning_rate 0.000101023
2017-10-10T14:49:36.115968: step 2074, loss 0.148601, acc 0.953125, learning_rate 0.000101018
2017-10-10T14:49:36.351842: step 2075, loss 0.149159, acc 0.953125, learning_rate 0.000101014
2017-10-10T14:49:36.623935: step 2076, loss 0.233224, acc 0.890625, learning_rate 0.00010101
2017-10-10T14:49:36.891871: step 2077, loss 0.278641, acc 0.890625, learning_rate 0.000101006
2017-10-10T14:49:37.108142: step 2078, loss 0.191133, acc 0.890625, learning_rate 0.000101002
2017-10-10T14:49:37.292873: step 2079, loss 0.204656, acc 0.9375, learning_rate 0.000100998
2017-10-10T14:49:37.497738: step 2080, loss 0.18216, acc 0.953125, learning_rate 0.000100994

Evaluation:
2017-10-10T14:49:37.897854: step 2080, loss 0.218458, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2080

2017-10-10T14:49:38.869099: step 2081, loss 0.1036, acc 0.96875, learning_rate 0.00010099
2017-10-10T14:49:39.137938: step 2082, loss 0.150915, acc 0.96875, learning_rate 0.000100986
2017-10-10T14:49:39.421065: step 2083, loss 0.233889, acc 0.875, learning_rate 0.000100982
2017-10-10T14:49:39.611064: step 2084, loss 0.124338, acc 0.953125, learning_rate 0.000100978
2017-10-10T14:49:39.852884: step 2085, loss 0.174706, acc 0.9375, learning_rate 0.000100974
2017-10-10T14:49:40.085498: step 2086, loss 0.139176, acc 0.953125, learning_rate 0.00010097
2017-10-10T14:49:40.360204: step 2087, loss 0.24581, acc 0.9375, learning_rate 0.000100966
2017-10-10T14:49:40.630584: step 2088, loss 0.161921, acc 0.953125, learning_rate 0.000100962
2017-10-10T14:49:40.891291: step 2089, loss 0.195396, acc 0.90625, learning_rate 0.000100958
2017-10-10T14:49:41.168824: step 2090, loss 0.113224, acc 0.96875, learning_rate 0.000100954
2017-10-10T14:49:41.429034: step 2091, loss 0.229557, acc 0.9375, learning_rate 0.00010095
2017-10-10T14:49:41.712936: step 2092, loss 0.138535, acc 0.96875, learning_rate 0.000100946
2017-10-10T14:49:41.988853: step 2093, loss 0.23651, acc 0.921875, learning_rate 0.000100942
2017-10-10T14:49:42.252931: step 2094, loss 0.15494, acc 0.96875, learning_rate 0.000100938
2017-10-10T14:49:42.516907: step 2095, loss 0.188577, acc 0.953125, learning_rate 0.000100935
2017-10-10T14:49:42.722281: step 2096, loss 0.108849, acc 0.984375, learning_rate 0.000100931
2017-10-10T14:49:42.994349: step 2097, loss 0.136126, acc 0.953125, learning_rate 0.000100927
2017-10-10T14:49:43.249575: step 2098, loss 0.150857, acc 0.96875, learning_rate 0.000100923
2017-10-10T14:49:43.488845: step 2099, loss 0.249021, acc 0.875, learning_rate 0.000100919
2017-10-10T14:49:43.790179: step 2100, loss 0.157023, acc 0.953125, learning_rate 0.000100916
2017-10-10T14:49:44.008281: step 2101, loss 0.0995068, acc 0.96875, learning_rate 0.000100912
2017-10-10T14:49:44.222021: step 2102, loss 0.0644356, acc 0.984375, learning_rate 0.000100908
2017-10-10T14:49:44.457287: step 2103, loss 0.092188, acc 0.96875, learning_rate 0.000100904
2017-10-10T14:49:44.724878: step 2104, loss 0.193275, acc 0.953125, learning_rate 0.000100901
2017-10-10T14:49:44.959236: step 2105, loss 0.217986, acc 0.9375, learning_rate 0.000100897
2017-10-10T14:49:45.200989: step 2106, loss 0.117004, acc 0.984375, learning_rate 0.000100893
2017-10-10T14:49:45.395071: step 2107, loss 0.0986785, acc 0.984375, learning_rate 0.00010089
2017-10-10T14:49:45.719520: step 2108, loss 0.120197, acc 0.96875, learning_rate 0.000100886
2017-10-10T14:49:45.950071: step 2109, loss 0.129671, acc 0.96875, learning_rate 0.000100883
2017-10-10T14:49:46.269383: step 2110, loss 0.147728, acc 0.921875, learning_rate 0.000100879
2017-10-10T14:49:46.443072: step 2111, loss 0.173649, acc 0.953125, learning_rate 0.000100875
2017-10-10T14:49:46.603629: step 2112, loss 0.341125, acc 0.90625, learning_rate 0.000100872
2017-10-10T14:49:46.796826: step 2113, loss 0.157882, acc 0.953125, learning_rate 0.000100868
2017-10-10T14:49:46.940974: step 2114, loss 0.216848, acc 0.921875, learning_rate 0.000100865
2017-10-10T14:49:47.193843: step 2115, loss 0.0883109, acc 1, learning_rate 0.000100861
2017-10-10T14:49:47.493895: step 2116, loss 0.0849846, acc 0.984375, learning_rate 0.000100858
2017-10-10T14:49:47.668825: step 2117, loss 0.121011, acc 0.96875, learning_rate 0.000100854
2017-10-10T14:49:47.941133: step 2118, loss 0.134955, acc 0.953125, learning_rate 0.000100851
2017-10-10T14:49:48.124990: step 2119, loss 0.229246, acc 0.90625, learning_rate 0.000100847
2017-10-10T14:49:48.350288: step 2120, loss 0.227619, acc 0.921875, learning_rate 0.000100844

Evaluation:
2017-10-10T14:49:48.735577: step 2120, loss 0.217545, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2120

2017-10-10T14:49:49.880899: step 2121, loss 0.110652, acc 0.984375, learning_rate 0.00010084
2017-10-10T14:49:50.077807: step 2122, loss 0.243191, acc 0.875, learning_rate 0.000100837
2017-10-10T14:49:50.333214: step 2123, loss 0.107951, acc 0.9375, learning_rate 0.000100833
2017-10-10T14:49:50.604781: step 2124, loss 0.082871, acc 0.96875, learning_rate 0.00010083
2017-10-10T14:49:50.856150: step 2125, loss 0.0506822, acc 1, learning_rate 0.000100827
2017-10-10T14:49:51.115209: step 2126, loss 0.185104, acc 0.921875, learning_rate 0.000100823
2017-10-10T14:49:51.416952: step 2127, loss 0.16845, acc 0.96875, learning_rate 0.00010082
2017-10-10T14:49:51.641264: step 2128, loss 0.153819, acc 0.921875, learning_rate 0.000100817
2017-10-10T14:49:51.816345: step 2129, loss 0.216365, acc 0.9375, learning_rate 0.000100813
2017-10-10T14:49:52.111558: step 2130, loss 0.266312, acc 0.90625, learning_rate 0.00010081
2017-10-10T14:49:52.446625: step 2131, loss 0.119631, acc 0.953125, learning_rate 0.000100807
2017-10-10T14:49:52.657491: step 2132, loss 0.13408, acc 0.953125, learning_rate 0.000100803
2017-10-10T14:49:52.888498: step 2133, loss 0.214433, acc 0.9375, learning_rate 0.0001008
2017-10-10T14:49:53.142615: step 2134, loss 0.148418, acc 0.9375, learning_rate 0.000100797
2017-10-10T14:49:53.368223: step 2135, loss 0.139654, acc 0.953125, learning_rate 0.000100793
2017-10-10T14:49:53.644915: step 2136, loss 0.113053, acc 0.984375, learning_rate 0.00010079
2017-10-10T14:49:53.896336: step 2137, loss 0.19638, acc 0.9375, learning_rate 0.000100787
2017-10-10T14:49:54.122258: step 2138, loss 0.133979, acc 0.9375, learning_rate 0.000100784
2017-10-10T14:49:54.325352: step 2139, loss 0.150526, acc 0.9375, learning_rate 0.000100781
2017-10-10T14:49:54.549054: step 2140, loss 0.228361, acc 0.90625, learning_rate 0.000100777
2017-10-10T14:49:54.785844: step 2141, loss 0.120411, acc 0.96875, learning_rate 0.000100774
2017-10-10T14:49:54.999391: step 2142, loss 0.110955, acc 0.953125, learning_rate 0.000100771
2017-10-10T14:49:55.255112: step 2143, loss 0.131638, acc 0.953125, learning_rate 0.000100768
2017-10-10T14:49:55.514900: step 2144, loss 0.118258, acc 0.96875, learning_rate 0.000100765
2017-10-10T14:49:55.769097: step 2145, loss 0.135302, acc 0.953125, learning_rate 0.000100762
2017-10-10T14:49:56.032921: step 2146, loss 0.177017, acc 0.921875, learning_rate 0.000100759
2017-10-10T14:49:56.261616: step 2147, loss 0.154994, acc 0.953125, learning_rate 0.000100755
2017-10-10T14:49:56.476047: step 2148, loss 0.144509, acc 0.96875, learning_rate 0.000100752
2017-10-10T14:49:56.727267: step 2149, loss 0.249572, acc 0.9375, learning_rate 0.000100749
2017-10-10T14:49:56.930118: step 2150, loss 0.155533, acc 0.9375, learning_rate 0.000100746
2017-10-10T14:49:57.184890: step 2151, loss 0.194742, acc 0.921875, learning_rate 0.000100743
2017-10-10T14:49:57.428735: step 2152, loss 0.281366, acc 0.921875, learning_rate 0.00010074
2017-10-10T14:49:57.672929: step 2153, loss 0.146754, acc 0.96875, learning_rate 0.000100737
2017-10-10T14:49:57.916922: step 2154, loss 0.210821, acc 0.921875, learning_rate 0.000100734
2017-10-10T14:49:58.136104: step 2155, loss 0.159665, acc 0.953125, learning_rate 0.000100731
2017-10-10T14:49:58.387434: step 2156, loss 0.171134, acc 0.941176, learning_rate 0.000100728
2017-10-10T14:49:58.602301: step 2157, loss 0.165212, acc 0.9375, learning_rate 0.000100725
2017-10-10T14:49:58.892893: step 2158, loss 0.118111, acc 0.96875, learning_rate 0.000100722
2017-10-10T14:49:59.144977: step 2159, loss 0.134251, acc 0.953125, learning_rate 0.000100719
2017-10-10T14:49:59.412879: step 2160, loss 0.139272, acc 0.9375, learning_rate 0.000100716

Evaluation:
2017-10-10T14:49:59.900084: step 2160, loss 0.215859, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2160

2017-10-10T14:50:00.780056: step 2161, loss 0.182899, acc 0.953125, learning_rate 0.000100713
2017-10-10T14:50:01.016256: step 2162, loss 0.169045, acc 0.9375, learning_rate 0.000100711
2017-10-10T14:50:01.240894: step 2163, loss 0.158043, acc 0.953125, learning_rate 0.000100708
2017-10-10T14:50:01.487592: step 2164, loss 0.213035, acc 0.9375, learning_rate 0.000100705
2017-10-10T14:50:01.734480: step 2165, loss 0.174882, acc 0.9375, learning_rate 0.000100702
2017-10-10T14:50:01.990838: step 2166, loss 0.294447, acc 0.921875, learning_rate 0.000100699
2017-10-10T14:50:02.237119: step 2167, loss 0.183141, acc 0.953125, learning_rate 0.000100696
2017-10-10T14:50:02.451952: step 2168, loss 0.174266, acc 0.9375, learning_rate 0.000100693
2017-10-10T14:50:02.761083: step 2169, loss 0.141302, acc 0.953125, learning_rate 0.00010069
2017-10-10T14:50:02.997622: step 2170, loss 0.166727, acc 0.953125, learning_rate 0.000100688
2017-10-10T14:50:03.232042: step 2171, loss 0.191583, acc 0.9375, learning_rate 0.000100685
2017-10-10T14:50:03.512809: step 2172, loss 0.187497, acc 0.921875, learning_rate 0.000100682
2017-10-10T14:50:03.877079: step 2173, loss 0.135194, acc 0.953125, learning_rate 0.000100679
2017-10-10T14:50:04.176917: step 2174, loss 0.272159, acc 0.890625, learning_rate 0.000100677
2017-10-10T14:50:04.418231: step 2175, loss 0.180736, acc 0.90625, learning_rate 0.000100674
2017-10-10T14:50:04.641371: step 2176, loss 0.0940499, acc 0.984375, learning_rate 0.000100671
2017-10-10T14:50:04.860962: step 2177, loss 0.0648691, acc 0.984375, learning_rate 0.000100668
2017-10-10T14:50:05.070630: step 2178, loss 0.171182, acc 0.9375, learning_rate 0.000100666
2017-10-10T14:50:05.318646: step 2179, loss 0.251364, acc 0.875, learning_rate 0.000100663
2017-10-10T14:50:05.622935: step 2180, loss 0.159364, acc 0.953125, learning_rate 0.00010066
2017-10-10T14:50:05.912865: step 2181, loss 0.291331, acc 0.9375, learning_rate 0.000100657
2017-10-10T14:50:06.131617: step 2182, loss 0.211871, acc 0.9375, learning_rate 0.000100655
2017-10-10T14:50:06.402178: step 2183, loss 0.17153, acc 0.9375, learning_rate 0.000100652
2017-10-10T14:50:06.625290: step 2184, loss 0.182189, acc 0.90625, learning_rate 0.000100649
2017-10-10T14:50:06.848597: step 2185, loss 0.0778838, acc 1, learning_rate 0.000100647
2017-10-10T14:50:07.127439: step 2186, loss 0.169443, acc 0.9375, learning_rate 0.000100644
2017-10-10T14:50:07.333039: step 2187, loss 0.1954, acc 0.953125, learning_rate 0.000100641
2017-10-10T14:50:07.562707: step 2188, loss 0.268486, acc 0.921875, learning_rate 0.000100639
2017-10-10T14:50:07.807784: step 2189, loss 0.0838899, acc 0.984375, learning_rate 0.000100636
2017-10-10T14:50:08.072970: step 2190, loss 0.110371, acc 0.96875, learning_rate 0.000100634
2017-10-10T14:50:08.348489: step 2191, loss 0.243647, acc 0.890625, learning_rate 0.000100631
2017-10-10T14:50:08.590514: step 2192, loss 0.185196, acc 0.921875, learning_rate 0.000100628
2017-10-10T14:50:08.848234: step 2193, loss 0.166051, acc 0.953125, learning_rate 0.000100626
2017-10-10T14:50:09.089104: step 2194, loss 0.0948892, acc 1, learning_rate 0.000100623
2017-10-10T14:50:09.362022: step 2195, loss 0.0936916, acc 0.96875, learning_rate 0.000100621
2017-10-10T14:50:09.626968: step 2196, loss 0.158186, acc 0.9375, learning_rate 0.000100618
2017-10-10T14:50:09.907323: step 2197, loss 0.112073, acc 1, learning_rate 0.000100616
2017-10-10T14:50:10.216834: step 2198, loss 0.0906973, acc 0.96875, learning_rate 0.000100613
2017-10-10T14:50:10.412788: step 2199, loss 0.080565, acc 0.984375, learning_rate 0.000100611
2017-10-10T14:50:10.629686: step 2200, loss 0.16415, acc 0.953125, learning_rate 0.000100608

Evaluation:
2017-10-10T14:50:10.975859: step 2200, loss 0.216131, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2200

2017-10-10T14:50:11.981045: step 2201, loss 0.157796, acc 0.953125, learning_rate 0.000100606
2017-10-10T14:50:12.194219: step 2202, loss 0.201778, acc 0.96875, learning_rate 0.000100603
2017-10-10T14:50:12.433042: step 2203, loss 0.123954, acc 0.96875, learning_rate 0.000100601
2017-10-10T14:50:12.635059: step 2204, loss 0.0784024, acc 1, learning_rate 0.000100598
2017-10-10T14:50:12.928831: step 2205, loss 0.125064, acc 0.953125, learning_rate 0.000100596
2017-10-10T14:50:13.156147: step 2206, loss 0.142614, acc 0.96875, learning_rate 0.000100594
2017-10-10T14:50:13.348295: step 2207, loss 0.157979, acc 0.953125, learning_rate 0.000100591
2017-10-10T14:50:13.543884: step 2208, loss 0.0888224, acc 0.984375, learning_rate 0.000100589
2017-10-10T14:50:13.762613: step 2209, loss 0.220254, acc 0.890625, learning_rate 0.000100586
2017-10-10T14:50:14.004967: step 2210, loss 0.120338, acc 0.953125, learning_rate 0.000100584
2017-10-10T14:50:14.239829: step 2211, loss 0.0753708, acc 0.984375, learning_rate 0.000100581
2017-10-10T14:50:14.500342: step 2212, loss 0.151667, acc 0.953125, learning_rate 0.000100579
2017-10-10T14:50:14.738331: step 2213, loss 0.205215, acc 0.953125, learning_rate 0.000100577
2017-10-10T14:50:14.995886: step 2214, loss 0.0905144, acc 0.96875, learning_rate 0.000100574
2017-10-10T14:50:15.209289: step 2215, loss 0.216843, acc 0.921875, learning_rate 0.000100572
2017-10-10T14:50:15.465542: step 2216, loss 0.0945159, acc 0.984375, learning_rate 0.00010057
2017-10-10T14:50:15.724078: step 2217, loss 0.0940444, acc 0.96875, learning_rate 0.000100567
2017-10-10T14:50:15.983141: step 2218, loss 0.34335, acc 0.890625, learning_rate 0.000100565
2017-10-10T14:50:16.213308: step 2219, loss 0.106146, acc 0.96875, learning_rate 0.000100563
2017-10-10T14:50:16.428422: step 2220, loss 0.106276, acc 0.984375, learning_rate 0.00010056
2017-10-10T14:50:16.664898: step 2221, loss 0.123643, acc 0.953125, learning_rate 0.000100558
2017-10-10T14:50:16.918401: step 2222, loss 0.130601, acc 0.953125, learning_rate 0.000100556
2017-10-10T14:50:17.159225: step 2223, loss 0.344549, acc 0.859375, learning_rate 0.000100554
2017-10-10T14:50:17.411534: step 2224, loss 0.144716, acc 0.953125, learning_rate 0.000100551
2017-10-10T14:50:17.673925: step 2225, loss 0.0506679, acc 1, learning_rate 0.000100549
2017-10-10T14:50:17.899016: step 2226, loss 0.181304, acc 0.9375, learning_rate 0.000100547
2017-10-10T14:50:18.192008: step 2227, loss 0.100828, acc 0.984375, learning_rate 0.000100545
2017-10-10T14:50:18.425147: step 2228, loss 0.162216, acc 0.96875, learning_rate 0.000100542
2017-10-10T14:50:18.664428: step 2229, loss 0.160944, acc 0.96875, learning_rate 0.00010054
2017-10-10T14:50:18.928904: step 2230, loss 0.142263, acc 0.9375, learning_rate 0.000100538
2017-10-10T14:50:19.177013: step 2231, loss 0.11484, acc 0.984375, learning_rate 0.000100536
2017-10-10T14:50:19.436835: step 2232, loss 0.0917312, acc 0.984375, learning_rate 0.000100534
2017-10-10T14:50:19.700170: step 2233, loss 0.222729, acc 0.90625, learning_rate 0.000100531
2017-10-10T14:50:19.933143: step 2234, loss 0.24209, acc 0.890625, learning_rate 0.000100529
2017-10-10T14:50:20.172782: step 2235, loss 0.18552, acc 0.921875, learning_rate 0.000100527
2017-10-10T14:50:20.420828: step 2236, loss 0.249795, acc 0.90625, learning_rate 0.000100525
2017-10-10T14:50:20.676897: step 2237, loss 0.141568, acc 0.96875, learning_rate 0.000100523
2017-10-10T14:50:20.882932: step 2238, loss 0.18217, acc 0.9375, learning_rate 0.000100521
2017-10-10T14:50:21.189247: step 2239, loss 0.112336, acc 0.96875, learning_rate 0.000100519
2017-10-10T14:50:21.458139: step 2240, loss 0.23981, acc 0.90625, learning_rate 0.000100516

Evaluation:
2017-10-10T14:50:21.756568: step 2240, loss 0.217173, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2240

2017-10-10T14:50:22.828811: step 2241, loss 0.156938, acc 0.9375, learning_rate 0.000100514
2017-10-10T14:50:23.016362: step 2242, loss 0.132218, acc 0.953125, learning_rate 0.000100512
2017-10-10T14:50:23.240868: step 2243, loss 0.161352, acc 0.9375, learning_rate 0.00010051
2017-10-10T14:50:23.493242: step 2244, loss 0.117232, acc 0.953125, learning_rate 0.000100508
2017-10-10T14:50:23.737095: step 2245, loss 0.132247, acc 0.953125, learning_rate 0.000100506
2017-10-10T14:50:23.965083: step 2246, loss 0.16428, acc 0.921875, learning_rate 0.000100504
2017-10-10T14:50:24.219229: step 2247, loss 0.106982, acc 0.96875, learning_rate 0.000100502
2017-10-10T14:50:24.492930: step 2248, loss 0.28282, acc 0.890625, learning_rate 0.0001005
2017-10-10T14:50:24.758184: step 2249, loss 0.131171, acc 0.953125, learning_rate 0.000100498
2017-10-10T14:50:25.035073: step 2250, loss 0.125701, acc 0.96875, learning_rate 0.000100496
2017-10-10T14:50:25.281357: step 2251, loss 0.18231, acc 0.90625, learning_rate 0.000100494
2017-10-10T14:50:25.450203: step 2252, loss 0.19325, acc 0.921875, learning_rate 0.000100492
2017-10-10T14:50:25.681014: step 2253, loss 0.126755, acc 0.953125, learning_rate 0.00010049
2017-10-10T14:50:25.862840: step 2254, loss 0.145523, acc 0.960784, learning_rate 0.000100488
2017-10-10T14:50:26.125179: step 2255, loss 0.191796, acc 0.9375, learning_rate 0.000100486
2017-10-10T14:50:26.374713: step 2256, loss 0.277626, acc 0.9375, learning_rate 0.000100484
2017-10-10T14:50:26.644894: step 2257, loss 0.159397, acc 0.9375, learning_rate 0.000100482
2017-10-10T14:50:26.849288: step 2258, loss 0.130128, acc 0.9375, learning_rate 0.00010048
2017-10-10T14:50:27.035239: step 2259, loss 0.139782, acc 0.9375, learning_rate 0.000100478
2017-10-10T14:50:27.253126: step 2260, loss 0.164597, acc 0.953125, learning_rate 0.000100476
2017-10-10T14:50:27.486220: step 2261, loss 0.0818621, acc 0.96875, learning_rate 0.000100474
2017-10-10T14:50:27.730546: step 2262, loss 0.247699, acc 0.890625, learning_rate 0.000100472
2017-10-10T14:50:28.040905: step 2263, loss 0.164695, acc 0.90625, learning_rate 0.00010047
2017-10-10T14:50:28.361126: step 2264, loss 0.0678771, acc 0.96875, learning_rate 0.000100468
2017-10-10T14:50:28.623856: step 2265, loss 0.230681, acc 0.90625, learning_rate 0.000100466
2017-10-10T14:50:28.814270: step 2266, loss 0.253499, acc 0.90625, learning_rate 0.000100464
2017-10-10T14:50:29.100724: step 2267, loss 0.150891, acc 0.984375, learning_rate 0.000100462
2017-10-10T14:50:29.420911: step 2268, loss 0.173422, acc 0.90625, learning_rate 0.000100461
2017-10-10T14:50:29.667293: step 2269, loss 0.183434, acc 0.921875, learning_rate 0.000100459
2017-10-10T14:50:29.944971: step 2270, loss 0.0965727, acc 0.984375, learning_rate 0.000100457
2017-10-10T14:50:30.173370: step 2271, loss 0.233746, acc 0.921875, learning_rate 0.000100455
2017-10-10T14:50:30.380926: step 2272, loss 0.164592, acc 0.953125, learning_rate 0.000100453
2017-10-10T14:50:30.598691: step 2273, loss 0.165588, acc 0.953125, learning_rate 0.000100451
2017-10-10T14:50:30.920862: step 2274, loss 0.120714, acc 0.953125, learning_rate 0.000100449
2017-10-10T14:50:31.132494: step 2275, loss 0.152743, acc 0.9375, learning_rate 0.000100448
2017-10-10T14:50:31.391021: step 2276, loss 0.130619, acc 0.9375, learning_rate 0.000100446
2017-10-10T14:50:31.665974: step 2277, loss 0.171643, acc 0.9375, learning_rate 0.000100444
2017-10-10T14:50:31.867537: step 2278, loss 0.200917, acc 0.9375, learning_rate 0.000100442
2017-10-10T14:50:32.124904: step 2279, loss 0.181081, acc 0.953125, learning_rate 0.00010044
2017-10-10T14:50:32.361303: step 2280, loss 0.252077, acc 0.953125, learning_rate 0.000100439

Evaluation:
2017-10-10T14:50:32.776708: step 2280, loss 0.21607, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2280

2017-10-10T14:50:33.828960: step 2281, loss 0.284103, acc 0.90625, learning_rate 0.000100437
2017-10-10T14:50:34.048526: step 2282, loss 0.153163, acc 0.96875, learning_rate 0.000100435
2017-10-10T14:50:34.300828: step 2283, loss 0.233004, acc 0.9375, learning_rate 0.000100433
2017-10-10T14:50:34.574247: step 2284, loss 0.0926147, acc 0.96875, learning_rate 0.000100431
2017-10-10T14:50:34.792865: step 2285, loss 0.190719, acc 0.921875, learning_rate 0.00010043
2017-10-10T14:50:35.067814: step 2286, loss 0.22203, acc 0.875, learning_rate 0.000100428
2017-10-10T14:50:35.303918: step 2287, loss 0.133616, acc 0.953125, learning_rate 0.000100426
2017-10-10T14:50:35.498243: step 2288, loss 0.179837, acc 0.953125, learning_rate 0.000100424
2017-10-10T14:50:35.816979: step 2289, loss 0.253803, acc 0.921875, learning_rate 0.000100423
2017-10-10T14:50:36.005245: step 2290, loss 0.13957, acc 0.9375, learning_rate 0.000100421
2017-10-10T14:50:36.281014: step 2291, loss 0.119423, acc 0.984375, learning_rate 0.000100419
2017-10-10T14:50:36.496696: step 2292, loss 0.0608383, acc 1, learning_rate 0.000100418
2017-10-10T14:50:36.741200: step 2293, loss 0.244926, acc 0.921875, learning_rate 0.000100416
2017-10-10T14:50:37.005020: step 2294, loss 0.163773, acc 0.953125, learning_rate 0.000100414
2017-10-10T14:50:37.244825: step 2295, loss 0.0766819, acc 0.96875, learning_rate 0.000100412
2017-10-10T14:50:37.546482: step 2296, loss 0.152086, acc 0.9375, learning_rate 0.000100411
2017-10-10T14:50:37.783654: step 2297, loss 0.102249, acc 0.984375, learning_rate 0.000100409
2017-10-10T14:50:38.031093: step 2298, loss 0.240407, acc 0.90625, learning_rate 0.000100407
2017-10-10T14:50:38.288754: step 2299, loss 0.198323, acc 0.953125, learning_rate 0.000100406
2017-10-10T14:50:38.455624: step 2300, loss 0.1313, acc 0.96875, learning_rate 0.000100404
2017-10-10T14:50:38.747624: step 2301, loss 0.199251, acc 0.921875, learning_rate 0.000100402
2017-10-10T14:50:38.991918: step 2302, loss 0.177756, acc 0.921875, learning_rate 0.000100401
2017-10-10T14:50:39.226721: step 2303, loss 0.0711174, acc 0.96875, learning_rate 0.000100399
2017-10-10T14:50:39.615377: step 2304, loss 0.114755, acc 0.9375, learning_rate 0.000100398
2017-10-10T14:50:39.837597: step 2305, loss 0.0903185, acc 1, learning_rate 0.000100396
2017-10-10T14:50:40.060986: step 2306, loss 0.152256, acc 0.9375, learning_rate 0.000100394
2017-10-10T14:50:40.360862: step 2307, loss 0.153246, acc 0.96875, learning_rate 0.000100393
2017-10-10T14:50:40.693646: step 2308, loss 0.189138, acc 0.96875, learning_rate 0.000100391
2017-10-10T14:50:40.996274: step 2309, loss 0.141751, acc 0.953125, learning_rate 0.000100389
2017-10-10T14:50:41.204911: step 2310, loss 0.123748, acc 0.96875, learning_rate 0.000100388
2017-10-10T14:50:41.424535: step 2311, loss 0.100623, acc 0.96875, learning_rate 0.000100386
2017-10-10T14:50:41.640996: step 2312, loss 0.0582624, acc 1, learning_rate 0.000100385
2017-10-10T14:50:41.803086: step 2313, loss 0.10185, acc 0.984375, learning_rate 0.000100383
2017-10-10T14:50:42.033477: step 2314, loss 0.107144, acc 0.984375, learning_rate 0.000100382
2017-10-10T14:50:42.291041: step 2315, loss 0.200433, acc 0.9375, learning_rate 0.00010038
2017-10-10T14:50:42.620908: step 2316, loss 0.231794, acc 0.890625, learning_rate 0.000100378
2017-10-10T14:50:42.810187: step 2317, loss 0.0613978, acc 0.984375, learning_rate 0.000100377
2017-10-10T14:50:43.087718: step 2318, loss 0.198619, acc 0.9375, learning_rate 0.000100375
2017-10-10T14:50:43.346250: step 2319, loss 0.167699, acc 0.9375, learning_rate 0.000100374
2017-10-10T14:50:43.608707: step 2320, loss 0.144639, acc 0.953125, learning_rate 0.000100372

Evaluation:
2017-10-10T14:50:43.924966: step 2320, loss 0.216113, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2320

2017-10-10T14:50:44.758980: step 2321, loss 0.136223, acc 0.953125, learning_rate 0.000100371
2017-10-10T14:50:45.035758: step 2322, loss 0.121474, acc 0.953125, learning_rate 0.000100369
2017-10-10T14:50:45.256248: step 2323, loss 0.14243, acc 0.9375, learning_rate 0.000100368
2017-10-10T14:50:45.576819: step 2324, loss 0.144903, acc 0.96875, learning_rate 0.000100366
2017-10-10T14:50:45.884317: step 2325, loss 0.146585, acc 0.96875, learning_rate 0.000100365
2017-10-10T14:50:46.054801: step 2326, loss 0.101016, acc 0.96875, learning_rate 0.000100363
2017-10-10T14:50:46.331814: step 2327, loss 0.127868, acc 0.96875, learning_rate 0.000100362
2017-10-10T14:50:46.547301: step 2328, loss 0.168153, acc 0.953125, learning_rate 0.00010036
2017-10-10T14:50:46.845475: step 2329, loss 0.106779, acc 0.96875, learning_rate 0.000100359
2017-10-10T14:50:47.088507: step 2330, loss 0.175575, acc 0.9375, learning_rate 0.000100357
2017-10-10T14:50:47.323218: step 2331, loss 0.122702, acc 0.96875, learning_rate 0.000100356
2017-10-10T14:50:47.580937: step 2332, loss 0.0900456, acc 0.984375, learning_rate 0.000100354
2017-10-10T14:50:47.799963: step 2333, loss 0.0917081, acc 0.96875, learning_rate 0.000100353
2017-10-10T14:50:48.052771: step 2334, loss 0.287631, acc 0.890625, learning_rate 0.000100352
2017-10-10T14:50:48.256859: step 2335, loss 0.107692, acc 0.984375, learning_rate 0.00010035
2017-10-10T14:50:48.547566: step 2336, loss 0.240877, acc 0.9375, learning_rate 0.000100349
2017-10-10T14:50:48.811539: step 2337, loss 0.213471, acc 0.921875, learning_rate 0.000100347
2017-10-10T14:50:49.056823: step 2338, loss 0.18001, acc 0.9375, learning_rate 0.000100346
2017-10-10T14:50:49.337143: step 2339, loss 0.222797, acc 0.921875, learning_rate 0.000100344
2017-10-10T14:50:49.532997: step 2340, loss 0.132136, acc 0.9375, learning_rate 0.000100343
2017-10-10T14:50:49.776848: step 2341, loss 0.0894382, acc 0.96875, learning_rate 0.000100342
2017-10-10T14:50:50.031760: step 2342, loss 0.0714793, acc 0.96875, learning_rate 0.00010034
2017-10-10T14:50:50.209064: step 2343, loss 0.140443, acc 0.921875, learning_rate 0.000100339
2017-10-10T14:50:50.472063: step 2344, loss 0.180733, acc 0.921875, learning_rate 0.000100338
2017-10-10T14:50:50.704650: step 2345, loss 0.16347, acc 0.9375, learning_rate 0.000100336
2017-10-10T14:50:50.912953: step 2346, loss 0.297914, acc 0.875, learning_rate 0.000100335
2017-10-10T14:50:51.120258: step 2347, loss 0.195278, acc 0.90625, learning_rate 0.000100333
2017-10-10T14:50:51.332947: step 2348, loss 0.114106, acc 0.96875, learning_rate 0.000100332
2017-10-10T14:50:51.620850: step 2349, loss 0.202654, acc 0.9375, learning_rate 0.000100331
2017-10-10T14:50:51.899817: step 2350, loss 0.135594, acc 0.984375, learning_rate 0.000100329
2017-10-10T14:50:52.122464: step 2351, loss 0.252399, acc 0.9375, learning_rate 0.000100328
2017-10-10T14:50:52.360983: step 2352, loss 0.0476865, acc 0.980392, learning_rate 0.000100327
2017-10-10T14:50:52.649179: step 2353, loss 0.133314, acc 0.953125, learning_rate 0.000100325
2017-10-10T14:50:52.945803: step 2354, loss 0.164001, acc 0.921875, learning_rate 0.000100324
2017-10-10T14:50:53.168446: step 2355, loss 0.12783, acc 0.953125, learning_rate 0.000100323
2017-10-10T14:50:53.336824: step 2356, loss 0.169302, acc 0.9375, learning_rate 0.000100321
2017-10-10T14:50:53.575084: step 2357, loss 0.158515, acc 0.953125, learning_rate 0.00010032
2017-10-10T14:50:53.849009: step 2358, loss 0.159811, acc 0.953125, learning_rate 0.000100319
2017-10-10T14:50:54.089071: step 2359, loss 0.237319, acc 0.9375, learning_rate 0.000100317
2017-10-10T14:50:54.369503: step 2360, loss 0.0675424, acc 1, learning_rate 0.000100316

Evaluation:
2017-10-10T14:50:54.727972: step 2360, loss 0.214846, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2360

2017-10-10T14:50:55.800230: step 2361, loss 0.0961273, acc 0.984375, learning_rate 0.000100315
2017-10-10T14:50:56.012954: step 2362, loss 0.189, acc 0.9375, learning_rate 0.000100314
2017-10-10T14:50:56.279346: step 2363, loss 0.287687, acc 0.921875, learning_rate 0.000100312
2017-10-10T14:50:56.577101: step 2364, loss 0.353283, acc 0.875, learning_rate 0.000100311
2017-10-10T14:50:56.760867: step 2365, loss 0.142822, acc 0.953125, learning_rate 0.00010031
2017-10-10T14:50:57.046479: step 2366, loss 0.155891, acc 0.953125, learning_rate 0.000100308
2017-10-10T14:50:57.279132: step 2367, loss 0.156905, acc 0.953125, learning_rate 0.000100307
2017-10-10T14:50:57.512962: step 2368, loss 0.113312, acc 0.96875, learning_rate 0.000100306
2017-10-10T14:50:57.772886: step 2369, loss 0.0932024, acc 0.984375, learning_rate 0.000100305
2017-10-10T14:50:57.992979: step 2370, loss 0.123897, acc 0.953125, learning_rate 0.000100303
2017-10-10T14:50:58.241005: step 2371, loss 0.220641, acc 0.9375, learning_rate 0.000100302
2017-10-10T14:50:58.481051: step 2372, loss 0.0647463, acc 1, learning_rate 0.000100301
2017-10-10T14:50:58.755398: step 2373, loss 0.225136, acc 0.921875, learning_rate 0.0001003
2017-10-10T14:50:59.049175: step 2374, loss 0.111593, acc 0.953125, learning_rate 0.000100299
2017-10-10T14:50:59.270832: step 2375, loss 0.183028, acc 0.921875, learning_rate 0.000100297
2017-10-10T14:50:59.481208: step 2376, loss 0.138934, acc 0.953125, learning_rate 0.000100296
2017-10-10T14:50:59.665552: step 2377, loss 0.217093, acc 0.921875, learning_rate 0.000100295
2017-10-10T14:50:59.943403: step 2378, loss 0.151479, acc 0.953125, learning_rate 0.000100294
2017-10-10T14:51:00.183084: step 2379, loss 0.114565, acc 0.96875, learning_rate 0.000100292
2017-10-10T14:51:00.434991: step 2380, loss 0.144116, acc 0.96875, learning_rate 0.000100291
2017-10-10T14:51:00.628950: step 2381, loss 0.157784, acc 0.953125, learning_rate 0.00010029
2017-10-10T14:51:00.829250: step 2382, loss 0.0846827, acc 0.96875, learning_rate 0.000100289
2017-10-10T14:51:01.032853: step 2383, loss 0.260349, acc 0.921875, learning_rate 0.000100288
2017-10-10T14:51:01.244987: step 2384, loss 0.187631, acc 0.9375, learning_rate 0.000100287
2017-10-10T14:51:01.560587: step 2385, loss 0.0874347, acc 1, learning_rate 0.000100285
2017-10-10T14:51:01.753132: step 2386, loss 0.0694285, acc 0.96875, learning_rate 0.000100284
2017-10-10T14:51:01.960097: step 2387, loss 0.247624, acc 0.890625, learning_rate 0.000100283
2017-10-10T14:51:02.165519: step 2388, loss 0.0687107, acc 0.984375, learning_rate 0.000100282
2017-10-10T14:51:02.365150: step 2389, loss 0.143781, acc 0.953125, learning_rate 0.000100281
2017-10-10T14:51:02.614862: step 2390, loss 0.196413, acc 0.9375, learning_rate 0.00010028
2017-10-10T14:51:02.862867: step 2391, loss 0.22538, acc 0.9375, learning_rate 0.000100278
2017-10-10T14:51:03.119166: step 2392, loss 0.206301, acc 0.90625, learning_rate 0.000100277
2017-10-10T14:51:03.342356: step 2393, loss 0.182811, acc 0.9375, learning_rate 0.000100276
2017-10-10T14:51:03.596896: step 2394, loss 0.10686, acc 0.96875, learning_rate 0.000100275
2017-10-10T14:51:03.812085: step 2395, loss 0.157377, acc 0.96875, learning_rate 0.000100274
2017-10-10T14:51:04.062841: step 2396, loss 0.183398, acc 0.890625, learning_rate 0.000100273
2017-10-10T14:51:04.289295: step 2397, loss 0.135786, acc 0.96875, learning_rate 0.000100272
2017-10-10T14:51:04.544980: step 2398, loss 0.15549, acc 0.921875, learning_rate 0.000100271
2017-10-10T14:51:04.863262: step 2399, loss 0.171047, acc 0.9375, learning_rate 0.00010027
2017-10-10T14:51:05.107036: step 2400, loss 0.174495, acc 0.921875, learning_rate 0.000100268

Evaluation:
2017-10-10T14:51:05.515088: step 2400, loss 0.214639, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2400

2017-10-10T14:51:06.559524: step 2401, loss 0.135428, acc 0.96875, learning_rate 0.000100267
2017-10-10T14:51:06.765482: step 2402, loss 0.103301, acc 0.984375, learning_rate 0.000100266
2017-10-10T14:51:06.923046: step 2403, loss 0.0876403, acc 0.984375, learning_rate 0.000100265
2017-10-10T14:51:07.169693: step 2404, loss 0.141562, acc 0.953125, learning_rate 0.000100264
2017-10-10T14:51:07.433887: step 2405, loss 0.117136, acc 0.96875, learning_rate 0.000100263
2017-10-10T14:51:07.660962: step 2406, loss 0.14305, acc 0.96875, learning_rate 0.000100262
2017-10-10T14:51:07.935329: step 2407, loss 0.119806, acc 0.9375, learning_rate 0.000100261
2017-10-10T14:51:08.184259: step 2408, loss 0.0889474, acc 0.984375, learning_rate 0.00010026
2017-10-10T14:51:08.398641: step 2409, loss 0.128741, acc 0.96875, learning_rate 0.000100259
2017-10-10T14:51:08.664820: step 2410, loss 0.157017, acc 0.96875, learning_rate 0.000100258
2017-10-10T14:51:08.945000: step 2411, loss 0.329405, acc 0.859375, learning_rate 0.000100257
2017-10-10T14:51:09.184873: step 2412, loss 0.110889, acc 0.96875, learning_rate 0.000100256
2017-10-10T14:51:09.397137: step 2413, loss 0.347751, acc 0.875, learning_rate 0.000100255
2017-10-10T14:51:09.642091: step 2414, loss 0.120842, acc 0.9375, learning_rate 0.000100253
2017-10-10T14:51:09.926524: step 2415, loss 0.107361, acc 0.96875, learning_rate 0.000100252
2017-10-10T14:51:10.189167: step 2416, loss 0.202983, acc 0.921875, learning_rate 0.000100251
2017-10-10T14:51:10.460397: step 2417, loss 0.0848745, acc 0.984375, learning_rate 0.00010025
2017-10-10T14:51:10.714502: step 2418, loss 0.103681, acc 0.96875, learning_rate 0.000100249
2017-10-10T14:51:11.008601: step 2419, loss 0.1724, acc 0.9375, learning_rate 0.000100248
2017-10-10T14:51:11.272806: step 2420, loss 0.231163, acc 0.953125, learning_rate 0.000100247
2017-10-10T14:51:11.536822: step 2421, loss 0.0788684, acc 0.96875, learning_rate 0.000100246
2017-10-10T14:51:11.769009: step 2422, loss 0.13301, acc 0.96875, learning_rate 0.000100245
2017-10-10T14:51:12.017105: step 2423, loss 0.175676, acc 0.9375, learning_rate 0.000100244
2017-10-10T14:51:12.331710: step 2424, loss 0.118547, acc 0.984375, learning_rate 0.000100243
2017-10-10T14:51:12.580845: step 2425, loss 0.312476, acc 0.875, learning_rate 0.000100242
2017-10-10T14:51:12.756973: step 2426, loss 0.0909285, acc 0.984375, learning_rate 0.000100241
2017-10-10T14:51:13.012532: step 2427, loss 0.190446, acc 0.9375, learning_rate 0.00010024
2017-10-10T14:51:13.316924: step 2428, loss 0.154493, acc 0.9375, learning_rate 0.000100239
2017-10-10T14:51:13.613974: step 2429, loss 0.194774, acc 0.90625, learning_rate 0.000100238
2017-10-10T14:51:13.872779: step 2430, loss 0.127112, acc 0.953125, learning_rate 0.000100237
2017-10-10T14:51:14.169043: step 2431, loss 0.149653, acc 0.9375, learning_rate 0.000100236
2017-10-10T14:51:14.424815: step 2432, loss 0.102024, acc 0.984375, learning_rate 0.000100235
2017-10-10T14:51:14.672958: step 2433, loss 0.0827702, acc 0.96875, learning_rate 0.000100235
2017-10-10T14:51:14.905199: step 2434, loss 0.137966, acc 0.953125, learning_rate 0.000100234
2017-10-10T14:51:15.178064: step 2435, loss 0.136025, acc 0.953125, learning_rate 0.000100233
2017-10-10T14:51:15.408885: step 2436, loss 0.107012, acc 0.984375, learning_rate 0.000100232
2017-10-10T14:51:15.648854: step 2437, loss 0.142358, acc 0.96875, learning_rate 0.000100231
2017-10-10T14:51:15.948847: step 2438, loss 0.164995, acc 0.96875, learning_rate 0.00010023
2017-10-10T14:51:16.156309: step 2439, loss 0.154871, acc 0.953125, learning_rate 0.000100229
2017-10-10T14:51:16.448296: step 2440, loss 0.251889, acc 0.90625, learning_rate 0.000100228

Evaluation:
2017-10-10T14:52:46.928811: step 2440, loss 0.215639, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2440

2017-10-10T14:56:53.111334: step 2441, loss 0.190878, acc 0.9375, learning_rate 0.000100227
2017-10-10T14:56:58.051990: step 2442, loss 0.156328, acc 0.984375, learning_rate 0.000100226
2017-10-10T14:57:00.622299: step 2443, loss 0.135438, acc 0.953125, learning_rate 0.000100225
2017-10-10T14:57:02.030501: step 2444, loss 0.203108, acc 0.953125, learning_rate 0.000100224
2017-10-10T14:57:03.121162: step 2445, loss 0.23974, acc 0.921875, learning_rate 0.000100223
2017-10-10T14:57:04.139095: step 2446, loss 0.113286, acc 0.96875, learning_rate 0.000100222
2017-10-10T14:57:05.629032: step 2447, loss 0.146791, acc 0.953125, learning_rate 0.000100221
2017-10-10T14:57:07.541585: step 2448, loss 0.139793, acc 0.96875, learning_rate 0.000100221
2017-10-10T14:57:09.619450: step 2449, loss 0.186376, acc 0.953125, learning_rate 0.00010022
2017-10-10T14:57:10.845992: step 2450, loss 0.252578, acc 0.960784, learning_rate 0.000100219
2017-10-10T14:57:13.069991: step 2451, loss 0.19364, acc 0.9375, learning_rate 0.000100218
2017-10-10T14:57:13.821661: step 2452, loss 0.135099, acc 0.96875, learning_rate 0.000100217
2017-10-10T14:57:17.054049: step 2453, loss 0.211484, acc 0.90625, learning_rate 0.000100216
2017-10-10T14:57:18.867648: step 2454, loss 0.120666, acc 0.96875, learning_rate 0.000100215
2017-10-10T14:57:23.460778: step 2455, loss 0.160481, acc 0.9375, learning_rate 0.000100214
2017-10-10T14:57:25.659363: step 2456, loss 0.0975362, acc 0.96875, learning_rate 0.000100213
2017-10-10T14:57:28.381791: step 2457, loss 0.237812, acc 0.90625, learning_rate 0.000100213
2017-10-10T14:57:29.918250: step 2458, loss 0.118678, acc 0.953125, learning_rate 0.000100212
2017-10-10T14:57:30.620655: step 2459, loss 0.109281, acc 0.96875, learning_rate 0.000100211
2017-10-10T14:57:31.637550: step 2460, loss 0.180561, acc 0.953125, learning_rate 0.00010021
2017-10-10T14:57:32.333044: step 2461, loss 0.137157, acc 0.96875, learning_rate 0.000100209
2017-10-10T14:57:33.787554: step 2462, loss 0.194805, acc 0.921875, learning_rate 0.000100208
2017-10-10T14:57:34.811281: step 2463, loss 0.188271, acc 0.9375, learning_rate 0.000100207
2017-10-10T14:57:35.998922: step 2464, loss 0.248196, acc 0.921875, learning_rate 0.000100207
2017-10-10T14:57:38.458059: step 2465, loss 0.265971, acc 0.921875, learning_rate 0.000100206
2017-10-10T14:57:39.656353: step 2466, loss 0.126367, acc 0.9375, learning_rate 0.000100205
2017-10-10T14:57:39.960497: step 2467, loss 0.102912, acc 0.96875, learning_rate 0.000100204
2017-10-10T14:57:40.724802: step 2468, loss 0.134442, acc 0.953125, learning_rate 0.000100203
2017-10-10T14:57:41.855215: step 2469, loss 0.149973, acc 0.953125, learning_rate 0.000100202
2017-10-10T14:57:42.270863: step 2470, loss 0.211171, acc 0.90625, learning_rate 0.000100202
2017-10-10T14:57:45.242745: step 2471, loss 0.135402, acc 0.953125, learning_rate 0.000100201
2017-10-10T14:57:46.056800: step 2472, loss 0.171144, acc 0.921875, learning_rate 0.0001002
2017-10-10T14:57:46.314225: step 2473, loss 0.224145, acc 0.90625, learning_rate 0.000100199
2017-10-10T14:57:46.649281: step 2474, loss 0.159467, acc 0.953125, learning_rate 0.000100198
2017-10-10T14:57:46.920842: step 2475, loss 0.177944, acc 0.9375, learning_rate 0.000100198
2017-10-10T14:57:47.574947: step 2476, loss 0.106744, acc 0.953125, learning_rate 0.000100197
2017-10-10T14:57:48.384516: step 2477, loss 0.187987, acc 0.9375, learning_rate 0.000100196
2017-10-10T14:57:49.258260: step 2478, loss 0.170377, acc 0.96875, learning_rate 0.000100195
2017-10-10T14:57:50.141977: step 2479, loss 0.13506, acc 0.96875, learning_rate 0.000100194
2017-10-10T14:57:50.786817: step 2480, loss 0.102477, acc 0.96875, learning_rate 0.000100194

Evaluation:
2017-10-10T14:57:54.085810: step 2480, loss 0.213755, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2480

2017-10-10T14:57:55.472344: step 2481, loss 0.212141, acc 0.90625, learning_rate 0.000100193
2017-10-10T14:57:55.848962: step 2482, loss 0.174262, acc 0.9375, learning_rate 0.000100192
2017-10-10T14:57:56.231538: step 2483, loss 0.178214, acc 0.921875, learning_rate 0.000100191
2017-10-10T14:57:56.593280: step 2484, loss 0.128151, acc 0.953125, learning_rate 0.00010019
2017-10-10T14:57:56.821783: step 2485, loss 0.211303, acc 0.9375, learning_rate 0.00010019
2017-10-10T14:57:57.230451: step 2486, loss 0.210547, acc 0.9375, learning_rate 0.000100189
2017-10-10T14:57:57.601018: step 2487, loss 0.155144, acc 0.9375, learning_rate 0.000100188
2017-10-10T14:57:58.190158: step 2488, loss 0.101606, acc 0.96875, learning_rate 0.000100187
2017-10-10T14:57:58.462513: step 2489, loss 0.188414, acc 0.90625, learning_rate 0.000100187
2017-10-10T14:57:58.812016: step 2490, loss 0.26374, acc 0.90625, learning_rate 0.000100186
2017-10-10T14:57:59.174486: step 2491, loss 0.157062, acc 0.9375, learning_rate 0.000100185
2017-10-10T14:57:59.506364: step 2492, loss 0.177047, acc 0.96875, learning_rate 0.000100184
2017-10-10T14:58:00.015997: step 2493, loss 0.0747231, acc 0.984375, learning_rate 0.000100183
2017-10-10T14:58:00.384779: step 2494, loss 0.190432, acc 0.9375, learning_rate 0.000100183
2017-10-10T14:58:00.784375: step 2495, loss 0.088692, acc 0.984375, learning_rate 0.000100182
2017-10-10T14:58:01.056962: step 2496, loss 0.154019, acc 0.9375, learning_rate 0.000100181
2017-10-10T14:58:01.415816: step 2497, loss 0.14881, acc 0.953125, learning_rate 0.000100181
2017-10-10T14:58:01.596649: step 2498, loss 0.265054, acc 0.921875, learning_rate 0.00010018
2017-10-10T14:58:01.756631: step 2499, loss 0.182722, acc 0.9375, learning_rate 0.000100179
2017-10-10T14:58:01.968966: step 2500, loss 0.161005, acc 0.953125, learning_rate 0.000100178
2017-10-10T14:58:02.158791: step 2501, loss 0.235307, acc 0.9375, learning_rate 0.000100178
2017-10-10T14:58:02.487691: step 2502, loss 0.132417, acc 0.96875, learning_rate 0.000100177
2017-10-10T14:58:02.809111: step 2503, loss 0.146747, acc 0.953125, learning_rate 0.000100176
2017-10-10T14:58:02.993880: step 2504, loss 0.192181, acc 0.9375, learning_rate 0.000100175
2017-10-10T14:58:03.112238: step 2505, loss 0.0784354, acc 0.984375, learning_rate 0.000100175
2017-10-10T14:58:03.420674: step 2506, loss 0.0801551, acc 0.984375, learning_rate 0.000100174
2017-10-10T14:58:03.582039: step 2507, loss 0.12192, acc 0.96875, learning_rate 0.000100173
2017-10-10T14:58:03.929908: step 2508, loss 0.139952, acc 0.953125, learning_rate 0.000100173
2017-10-10T14:58:04.138000: step 2509, loss 0.259294, acc 0.921875, learning_rate 0.000100172
2017-10-10T14:58:04.409699: step 2510, loss 0.107285, acc 0.96875, learning_rate 0.000100171
2017-10-10T14:58:04.535830: step 2511, loss 0.177694, acc 0.921875, learning_rate 0.00010017
2017-10-10T14:58:04.879714: step 2512, loss 0.0985735, acc 0.984375, learning_rate 0.00010017
2017-10-10T14:58:05.156167: step 2513, loss 0.147407, acc 0.96875, learning_rate 0.000100169
2017-10-10T14:58:05.347538: step 2514, loss 0.129705, acc 0.953125, learning_rate 0.000100168
2017-10-10T14:58:05.628187: step 2515, loss 0.204181, acc 0.90625, learning_rate 0.000100168
2017-10-10T14:58:05.812328: step 2516, loss 0.113641, acc 0.953125, learning_rate 0.000100167
2017-10-10T14:58:05.925719: step 2517, loss 0.201837, acc 0.90625, learning_rate 0.000100166
2017-10-10T14:58:06.294864: step 2518, loss 0.0738568, acc 0.96875, learning_rate 0.000100166
2017-10-10T14:58:06.413722: step 2519, loss 0.157224, acc 0.96875, learning_rate 0.000100165
2017-10-10T14:58:06.734866: step 2520, loss 0.159557, acc 0.96875, learning_rate 0.000100164

Evaluation:
2017-10-10T14:58:07.041839: step 2520, loss 0.214871, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2520

2017-10-10T14:58:07.895815: step 2521, loss 0.194793, acc 0.9375, learning_rate 0.000100164
2017-10-10T14:58:08.087736: step 2522, loss 0.206566, acc 0.9375, learning_rate 0.000100163
2017-10-10T14:58:08.271693: step 2523, loss 0.108257, acc 0.953125, learning_rate 0.000100162
2017-10-10T14:58:08.538181: step 2524, loss 0.148195, acc 0.953125, learning_rate 0.000100162
2017-10-10T14:58:08.792913: step 2525, loss 0.128898, acc 0.96875, learning_rate 0.000100161
2017-10-10T14:58:09.089423: step 2526, loss 0.141082, acc 0.953125, learning_rate 0.00010016
2017-10-10T14:58:09.235650: step 2527, loss 0.158741, acc 0.953125, learning_rate 0.00010016
2017-10-10T14:58:09.483804: step 2528, loss 0.0490793, acc 0.984375, learning_rate 0.000100159
2017-10-10T14:58:09.607585: step 2529, loss 0.205292, acc 0.9375, learning_rate 0.000100158
2017-10-10T14:58:09.715336: step 2530, loss 0.175777, acc 0.9375, learning_rate 0.000100158
2017-10-10T14:58:09.857222: step 2531, loss 0.167149, acc 0.9375, learning_rate 0.000100157
2017-10-10T14:58:10.036699: step 2532, loss 0.0969174, acc 0.96875, learning_rate 0.000100156
2017-10-10T14:58:10.242355: step 2533, loss 0.0940593, acc 0.984375, learning_rate 0.000100156
2017-10-10T14:58:10.599760: step 2534, loss 0.0961899, acc 0.96875, learning_rate 0.000100155
2017-10-10T14:58:10.767778: step 2535, loss 0.17731, acc 0.953125, learning_rate 0.000100155
2017-10-10T14:58:10.974726: step 2536, loss 0.223159, acc 0.9375, learning_rate 0.000100154
2017-10-10T14:58:11.130120: step 2537, loss 0.115971, acc 0.96875, learning_rate 0.000100153
2017-10-10T14:58:11.396040: step 2538, loss 0.181252, acc 0.90625, learning_rate 0.000100153
2017-10-10T14:58:11.516273: step 2539, loss 0.0778943, acc 0.984375, learning_rate 0.000100152
2017-10-10T14:58:11.664984: step 2540, loss 0.100849, acc 0.984375, learning_rate 0.000100151
2017-10-10T14:58:11.870712: step 2541, loss 0.278391, acc 0.890625, learning_rate 0.000100151
2017-10-10T14:58:12.110627: step 2542, loss 0.11842, acc 0.96875, learning_rate 0.00010015
2017-10-10T14:58:12.225723: step 2543, loss 0.270588, acc 0.90625, learning_rate 0.00010015
2017-10-10T14:58:12.338117: step 2544, loss 0.0666207, acc 1, learning_rate 0.000100149
2017-10-10T14:58:12.555101: step 2545, loss 0.285814, acc 0.890625, learning_rate 0.000100148
2017-10-10T14:58:12.754086: step 2546, loss 0.168459, acc 0.953125, learning_rate 0.000100148
2017-10-10T14:58:12.980838: step 2547, loss 0.150843, acc 0.953125, learning_rate 0.000100147
2017-10-10T14:58:13.148421: step 2548, loss 0.101297, acc 0.980392, learning_rate 0.000100147
2017-10-10T14:58:13.260108: step 2549, loss 0.165782, acc 0.9375, learning_rate 0.000100146
2017-10-10T14:58:13.348972: step 2550, loss 0.193068, acc 0.921875, learning_rate 0.000100145
2017-10-10T14:58:13.474649: step 2551, loss 0.150861, acc 0.9375, learning_rate 0.000100145
2017-10-10T14:58:13.602283: step 2552, loss 0.146435, acc 0.953125, learning_rate 0.000100144
2017-10-10T14:58:13.774884: step 2553, loss 0.113519, acc 0.953125, learning_rate 0.000100144
2017-10-10T14:58:13.936218: step 2554, loss 0.146851, acc 0.953125, learning_rate 0.000100143
2017-10-10T14:58:14.121006: step 2555, loss 0.126533, acc 0.96875, learning_rate 0.000100142
2017-10-10T14:58:14.331652: step 2556, loss 0.103684, acc 0.96875, learning_rate 0.000100142
2017-10-10T14:58:14.454412: step 2557, loss 0.135007, acc 0.9375, learning_rate 0.000100141
2017-10-10T14:58:14.615237: step 2558, loss 0.114779, acc 0.96875, learning_rate 0.000100141
2017-10-10T14:58:14.689568: step 2559, loss 0.162259, acc 0.953125, learning_rate 0.00010014
2017-10-10T14:58:14.759065: step 2560, loss 0.22032, acc 0.921875, learning_rate 0.00010014

Evaluation:
2017-10-10T14:58:15.003238: step 2560, loss 0.21629, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2560

2017-10-10T14:58:15.973010: step 2561, loss 0.19517, acc 0.9375, learning_rate 0.000100139
2017-10-10T14:58:16.227653: step 2562, loss 0.0889939, acc 0.984375, learning_rate 0.000100138
2017-10-10T14:58:16.350552: step 2563, loss 0.153091, acc 0.96875, learning_rate 0.000100138
2017-10-10T14:58:16.513026: step 2564, loss 0.153071, acc 0.953125, learning_rate 0.000100137
2017-10-10T14:58:16.783042: step 2565, loss 0.078105, acc 0.96875, learning_rate 0.000100137
2017-10-10T14:58:16.985224: step 2566, loss 0.281311, acc 0.875, learning_rate 0.000100136
2017-10-10T14:58:17.124967: step 2567, loss 0.132654, acc 0.96875, learning_rate 0.000100136
2017-10-10T14:58:17.462716: step 2568, loss 0.213732, acc 0.953125, learning_rate 0.000100135
2017-10-10T14:58:17.584370: step 2569, loss 0.195575, acc 0.921875, learning_rate 0.000100134
2017-10-10T14:58:17.808014: step 2570, loss 0.119296, acc 0.96875, learning_rate 0.000100134
2017-10-10T14:58:18.075081: step 2571, loss 0.0852661, acc 0.953125, learning_rate 0.000100133
2017-10-10T14:58:18.217233: step 2572, loss 0.0996393, acc 0.96875, learning_rate 0.000100133
2017-10-10T14:58:18.496806: step 2573, loss 0.238505, acc 0.90625, learning_rate 0.000100132
2017-10-10T14:58:18.598019: step 2574, loss 0.0839608, acc 0.984375, learning_rate 0.000100132
2017-10-10T14:58:18.858597: step 2575, loss 0.120383, acc 0.96875, learning_rate 0.000100131
2017-10-10T14:58:19.052744: step 2576, loss 0.176061, acc 0.9375, learning_rate 0.000100131
2017-10-10T14:58:19.279914: step 2577, loss 0.122487, acc 0.953125, learning_rate 0.00010013
2017-10-10T14:58:19.454109: step 2578, loss 0.146161, acc 0.96875, learning_rate 0.00010013
2017-10-10T14:58:19.661677: step 2579, loss 0.160298, acc 0.96875, learning_rate 0.000100129
2017-10-10T14:58:19.844095: step 2580, loss 0.0736861, acc 0.984375, learning_rate 0.000100129
2017-10-10T14:58:19.940018: step 2581, loss 0.175177, acc 0.953125, learning_rate 0.000100128
2017-10-10T14:58:20.168911: step 2582, loss 0.121214, acc 0.953125, learning_rate 0.000100128
2017-10-10T14:58:20.391827: step 2583, loss 0.329574, acc 0.921875, learning_rate 0.000100127
2017-10-10T14:58:20.685736: step 2584, loss 0.17462, acc 0.9375, learning_rate 0.000100126
2017-10-10T14:58:20.872922: step 2585, loss 0.201028, acc 0.921875, learning_rate 0.000100126
2017-10-10T14:58:21.140921: step 2586, loss 0.123468, acc 0.9375, learning_rate 0.000100125
2017-10-10T14:58:21.365412: step 2587, loss 0.121652, acc 0.984375, learning_rate 0.000100125
2017-10-10T14:58:21.576820: step 2588, loss 0.127491, acc 0.953125, learning_rate 0.000100124
2017-10-10T14:58:21.811766: step 2589, loss 0.187452, acc 0.9375, learning_rate 0.000100124
2017-10-10T14:58:22.032227: step 2590, loss 0.15503, acc 0.953125, learning_rate 0.000100123
2017-10-10T14:58:22.281063: step 2591, loss 0.173469, acc 0.9375, learning_rate 0.000100123
2017-10-10T14:58:22.500658: step 2592, loss 0.106015, acc 0.953125, learning_rate 0.000100122
2017-10-10T14:58:22.663506: step 2593, loss 0.219071, acc 0.90625, learning_rate 0.000100122
2017-10-10T14:58:22.924260: step 2594, loss 0.123315, acc 0.9375, learning_rate 0.000100121
2017-10-10T14:58:23.160225: step 2595, loss 0.0827078, acc 0.984375, learning_rate 0.000100121
2017-10-10T14:58:23.420112: step 2596, loss 0.171063, acc 0.9375, learning_rate 0.00010012
2017-10-10T14:58:23.671635: step 2597, loss 0.0899173, acc 0.96875, learning_rate 0.00010012
2017-10-10T14:58:23.876546: step 2598, loss 0.256473, acc 0.953125, learning_rate 0.000100119
2017-10-10T14:58:24.134908: step 2599, loss 0.135052, acc 0.96875, learning_rate 0.000100119
2017-10-10T14:58:24.417019: step 2600, loss 0.0656213, acc 0.984375, learning_rate 0.000100118

Evaluation:
2017-10-10T14:58:24.772074: step 2600, loss 0.218563, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2600

2017-10-10T14:58:25.684827: step 2601, loss 0.132013, acc 0.96875, learning_rate 0.000100118
2017-10-10T14:58:25.861391: step 2602, loss 0.0981399, acc 0.96875, learning_rate 0.000100117
2017-10-10T14:58:26.112937: step 2603, loss 0.128157, acc 0.96875, learning_rate 0.000100117
2017-10-10T14:58:26.324234: step 2604, loss 0.119708, acc 0.953125, learning_rate 0.000100117
2017-10-10T14:58:26.566841: step 2605, loss 0.144982, acc 0.953125, learning_rate 0.000100116
2017-10-10T14:58:26.786668: step 2606, loss 0.138047, acc 0.9375, learning_rate 0.000100116
2017-10-10T14:58:26.992910: step 2607, loss 0.226795, acc 0.875, learning_rate 0.000100115
2017-10-10T14:58:27.142483: step 2608, loss 0.14443, acc 0.96875, learning_rate 0.000100115
2017-10-10T14:58:27.287943: step 2609, loss 0.173519, acc 0.96875, learning_rate 0.000100114
2017-10-10T14:58:27.464887: step 2610, loss 0.0943309, acc 0.96875, learning_rate 0.000100114
2017-10-10T14:58:27.658769: step 2611, loss 0.206516, acc 0.953125, learning_rate 0.000100113
2017-10-10T14:58:27.989082: step 2612, loss 0.1174, acc 0.96875, learning_rate 0.000100113
2017-10-10T14:58:28.236510: step 2613, loss 0.256406, acc 0.890625, learning_rate 0.000100112
2017-10-10T14:58:28.432859: step 2614, loss 0.186626, acc 0.90625, learning_rate 0.000100112
2017-10-10T14:58:28.636157: step 2615, loss 0.140698, acc 0.921875, learning_rate 0.000100111
2017-10-10T14:58:28.844575: step 2616, loss 0.21703, acc 0.921875, learning_rate 0.000100111
2017-10-10T14:58:29.061797: step 2617, loss 0.163172, acc 0.921875, learning_rate 0.000100111
2017-10-10T14:58:29.357150: step 2618, loss 0.192546, acc 0.96875, learning_rate 0.00010011
2017-10-10T14:58:29.503482: step 2619, loss 0.17367, acc 0.953125, learning_rate 0.00010011
2017-10-10T14:58:29.656585: step 2620, loss 0.135114, acc 0.9375, learning_rate 0.000100109
2017-10-10T14:58:30.180671: step 2621, loss 0.173986, acc 0.9375, learning_rate 0.000100109
2017-10-10T14:58:30.380814: step 2622, loss 0.123319, acc 0.953125, learning_rate 0.000100108
2017-10-10T14:58:30.595023: step 2623, loss 0.230731, acc 0.875, learning_rate 0.000100108
2017-10-10T14:58:30.756970: step 2624, loss 0.162328, acc 0.9375, learning_rate 0.000100107
2017-10-10T14:58:30.893687: step 2625, loss 0.0885167, acc 0.953125, learning_rate 0.000100107
2017-10-10T14:58:31.138989: step 2626, loss 0.0970893, acc 0.984375, learning_rate 0.000100107
2017-10-10T14:58:31.357215: step 2627, loss 0.123627, acc 0.96875, learning_rate 0.000100106
2017-10-10T14:58:31.533919: step 2628, loss 0.0912864, acc 0.96875, learning_rate 0.000100106
2017-10-10T14:58:31.728571: step 2629, loss 0.0929183, acc 0.984375, learning_rate 0.000100105
2017-10-10T14:58:31.842388: step 2630, loss 0.287836, acc 0.921875, learning_rate 0.000100105
2017-10-10T14:58:31.957981: step 2631, loss 0.201199, acc 0.953125, learning_rate 0.000100104
2017-10-10T14:58:32.135381: step 2632, loss 0.178743, acc 0.9375, learning_rate 0.000100104
2017-10-10T14:58:32.259322: step 2633, loss 0.127857, acc 0.96875, learning_rate 0.000100104
2017-10-10T14:58:32.456292: step 2634, loss 0.160272, acc 0.953125, learning_rate 0.000100103
2017-10-10T14:58:32.793057: step 2635, loss 0.108944, acc 0.984375, learning_rate 0.000100103
2017-10-10T14:58:33.052375: step 2636, loss 0.207166, acc 0.90625, learning_rate 0.000100102
2017-10-10T14:58:33.346507: step 2637, loss 0.093751, acc 0.96875, learning_rate 0.000100102
2017-10-10T14:58:33.558344: step 2638, loss 0.076733, acc 0.984375, learning_rate 0.000100101
2017-10-10T14:58:33.804485: step 2639, loss 0.231777, acc 0.953125, learning_rate 0.000100101
2017-10-10T14:58:34.072863: step 2640, loss 0.12333, acc 0.953125, learning_rate 0.000100101

Evaluation:
2017-10-10T14:58:34.420833: step 2640, loss 0.21575, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2640

2017-10-10T14:58:35.472171: step 2641, loss 0.17365, acc 0.953125, learning_rate 0.0001001
2017-10-10T14:58:35.745035: step 2642, loss 0.191488, acc 0.921875, learning_rate 0.0001001
2017-10-10T14:58:35.978655: step 2643, loss 0.190169, acc 0.921875, learning_rate 0.000100099
2017-10-10T14:58:36.177121: step 2644, loss 0.106928, acc 0.96875, learning_rate 0.000100099
2017-10-10T14:58:36.404857: step 2645, loss 0.148797, acc 0.953125, learning_rate 0.000100099
2017-10-10T14:58:36.534401: step 2646, loss 0.138031, acc 0.980392, learning_rate 0.000100098
2017-10-10T14:58:36.872927: step 2647, loss 0.182465, acc 0.96875, learning_rate 0.000100098
2017-10-10T14:58:37.066895: step 2648, loss 0.0997119, acc 0.96875, learning_rate 0.000100097
2017-10-10T14:58:37.300504: step 2649, loss 0.262121, acc 0.9375, learning_rate 0.000100097
2017-10-10T14:58:37.398899: step 2650, loss 0.163602, acc 0.953125, learning_rate 0.000100097
2017-10-10T14:58:37.604793: step 2651, loss 0.1712, acc 0.9375, learning_rate 0.000100096
2017-10-10T14:58:37.802812: step 2652, loss 0.215388, acc 0.9375, learning_rate 0.000100096
2017-10-10T14:58:38.060787: step 2653, loss 0.186834, acc 0.96875, learning_rate 0.000100095
2017-10-10T14:58:38.376848: step 2654, loss 0.111455, acc 0.96875, learning_rate 0.000100095
2017-10-10T14:58:38.567248: step 2655, loss 0.0917889, acc 0.984375, learning_rate 0.000100095
2017-10-10T14:58:38.750415: step 2656, loss 0.135775, acc 0.9375, learning_rate 0.000100094
2017-10-10T14:58:38.901030: step 2657, loss 0.119792, acc 0.9375, learning_rate 0.000100094
2017-10-10T14:58:39.139198: step 2658, loss 0.302016, acc 0.90625, learning_rate 0.000100093
2017-10-10T14:58:39.380825: step 2659, loss 0.077874, acc 0.984375, learning_rate 0.000100093
2017-10-10T14:58:39.565027: step 2660, loss 0.101626, acc 0.984375, learning_rate 0.000100093
2017-10-10T14:58:39.771410: step 2661, loss 0.207177, acc 0.9375, learning_rate 0.000100092
2017-10-10T14:58:40.000212: step 2662, loss 0.151289, acc 0.9375, learning_rate 0.000100092
2017-10-10T14:58:40.134301: step 2663, loss 0.13242, acc 0.96875, learning_rate 0.000100092
2017-10-10T14:58:40.324828: step 2664, loss 0.290636, acc 0.90625, learning_rate 0.000100091
2017-10-10T14:58:40.560058: step 2665, loss 0.127644, acc 0.96875, learning_rate 0.000100091
2017-10-10T14:58:40.719062: step 2666, loss 0.170743, acc 0.9375, learning_rate 0.00010009
2017-10-10T14:58:40.940055: step 2667, loss 0.128857, acc 0.96875, learning_rate 0.00010009
2017-10-10T14:58:41.096844: step 2668, loss 0.185977, acc 0.953125, learning_rate 0.00010009
2017-10-10T14:58:41.372951: step 2669, loss 0.170189, acc 0.921875, learning_rate 0.000100089
2017-10-10T14:58:41.547016: step 2670, loss 0.2733, acc 0.921875, learning_rate 0.000100089
2017-10-10T14:58:41.713201: step 2671, loss 0.179194, acc 0.9375, learning_rate 0.000100089
2017-10-10T14:58:41.865374: step 2672, loss 0.0806462, acc 0.984375, learning_rate 0.000100088
2017-10-10T14:58:42.000812: step 2673, loss 0.169904, acc 0.9375, learning_rate 0.000100088
2017-10-10T14:58:42.236749: step 2674, loss 0.105909, acc 0.96875, learning_rate 0.000100088
2017-10-10T14:58:42.421770: step 2675, loss 0.124775, acc 0.953125, learning_rate 0.000100087
2017-10-10T14:58:42.644139: step 2676, loss 0.146513, acc 0.953125, learning_rate 0.000100087
2017-10-10T14:58:42.852852: step 2677, loss 0.15306, acc 0.953125, learning_rate 0.000100086
2017-10-10T14:58:42.989825: step 2678, loss 0.14035, acc 0.953125, learning_rate 0.000100086
2017-10-10T14:58:43.288891: step 2679, loss 0.0901503, acc 0.984375, learning_rate 0.000100086
2017-10-10T14:58:43.579719: step 2680, loss 0.161147, acc 0.96875, learning_rate 0.000100085

Evaluation:
2017-10-10T14:58:43.924919: step 2680, loss 0.214148, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2680

2017-10-10T14:58:44.876186: step 2681, loss 0.108631, acc 0.984375, learning_rate 0.000100085
2017-10-10T14:58:45.168836: step 2682, loss 0.0943335, acc 0.984375, learning_rate 0.000100085
2017-10-10T14:58:45.367200: step 2683, loss 0.108392, acc 0.953125, learning_rate 0.000100084
2017-10-10T14:58:45.637862: step 2684, loss 0.111406, acc 0.984375, learning_rate 0.000100084
2017-10-10T14:58:45.848481: step 2685, loss 0.198198, acc 0.921875, learning_rate 0.000100084
2017-10-10T14:58:46.140034: step 2686, loss 0.116537, acc 0.953125, learning_rate 0.000100083
2017-10-10T14:58:46.337900: step 2687, loss 0.156548, acc 0.9375, learning_rate 0.000100083
2017-10-10T14:58:46.560236: step 2688, loss 0.152381, acc 0.9375, learning_rate 0.000100083
2017-10-10T14:58:46.769340: step 2689, loss 0.0732713, acc 1, learning_rate 0.000100082
2017-10-10T14:58:46.969880: step 2690, loss 0.178579, acc 0.953125, learning_rate 0.000100082
2017-10-10T14:58:47.273169: step 2691, loss 0.172618, acc 0.9375, learning_rate 0.000100082
2017-10-10T14:58:47.496832: step 2692, loss 0.111649, acc 0.96875, learning_rate 0.000100081
2017-10-10T14:58:47.757194: step 2693, loss 0.256949, acc 0.9375, learning_rate 0.000100081
2017-10-10T14:58:48.013268: step 2694, loss 0.127132, acc 0.9375, learning_rate 0.000100081
2017-10-10T14:58:48.185992: step 2695, loss 0.231227, acc 0.9375, learning_rate 0.00010008
2017-10-10T14:58:48.432227: step 2696, loss 0.112066, acc 0.96875, learning_rate 0.00010008
2017-10-10T14:58:48.864989: step 2697, loss 0.232766, acc 0.9375, learning_rate 0.00010008
2017-10-10T14:58:49.109145: step 2698, loss 0.227342, acc 0.90625, learning_rate 0.000100079
2017-10-10T14:58:49.366783: step 2699, loss 0.112128, acc 0.984375, learning_rate 0.000100079
2017-10-10T14:58:49.621035: step 2700, loss 0.112471, acc 0.96875, learning_rate 0.000100079
2017-10-10T14:58:49.871428: step 2701, loss 0.114883, acc 0.953125, learning_rate 0.000100078
2017-10-10T14:58:50.024174: step 2702, loss 0.281703, acc 0.90625, learning_rate 0.000100078
2017-10-10T14:58:50.305918: step 2703, loss 0.120991, acc 0.984375, learning_rate 0.000100078
2017-10-10T14:58:50.525088: step 2704, loss 0.221497, acc 0.90625, learning_rate 0.000100077
2017-10-10T14:58:50.818594: step 2705, loss 0.149103, acc 0.96875, learning_rate 0.000100077
2017-10-10T14:58:51.111513: step 2706, loss 0.131772, acc 0.953125, learning_rate 0.000100077
2017-10-10T14:58:51.537275: step 2707, loss 0.13338, acc 0.96875, learning_rate 0.000100076
2017-10-10T14:58:51.796858: step 2708, loss 0.10684, acc 0.984375, learning_rate 0.000100076
2017-10-10T14:58:51.989228: step 2709, loss 0.0631839, acc 0.984375, learning_rate 0.000100076
2017-10-10T14:58:52.208338: step 2710, loss 0.0766495, acc 0.984375, learning_rate 0.000100076
2017-10-10T14:58:52.449065: step 2711, loss 0.207323, acc 0.921875, learning_rate 0.000100075
2017-10-10T14:58:52.693126: step 2712, loss 0.251385, acc 0.953125, learning_rate 0.000100075
2017-10-10T14:58:52.987951: step 2713, loss 0.227269, acc 0.90625, learning_rate 0.000100075
2017-10-10T14:58:53.296322: step 2714, loss 0.141438, acc 0.953125, learning_rate 0.000100074
2017-10-10T14:58:53.549112: step 2715, loss 0.121432, acc 0.953125, learning_rate 0.000100074
2017-10-10T14:58:53.812856: step 2716, loss 0.148242, acc 0.9375, learning_rate 0.000100074
2017-10-10T14:58:54.170761: step 2717, loss 0.102807, acc 0.953125, learning_rate 0.000100073
2017-10-10T14:58:54.401367: step 2718, loss 0.168742, acc 0.9375, learning_rate 0.000100073
2017-10-10T14:58:54.591863: step 2719, loss 0.228157, acc 0.90625, learning_rate 0.000100073
2017-10-10T14:58:54.872820: step 2720, loss 0.133614, acc 0.953125, learning_rate 0.000100073

Evaluation:
2017-10-10T14:58:55.235571: step 2720, loss 0.213729, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2720

2017-10-10T14:58:56.266847: step 2721, loss 0.191567, acc 0.953125, learning_rate 0.000100072
2017-10-10T14:58:56.505502: step 2722, loss 0.167259, acc 0.953125, learning_rate 0.000100072
2017-10-10T14:58:56.700865: step 2723, loss 0.126483, acc 0.96875, learning_rate 0.000100072
2017-10-10T14:58:56.963290: step 2724, loss 0.081483, acc 0.984375, learning_rate 0.000100071
2017-10-10T14:58:57.194482: step 2725, loss 0.106299, acc 0.96875, learning_rate 0.000100071
2017-10-10T14:58:57.440954: step 2726, loss 0.207124, acc 0.9375, learning_rate 0.000100071
2017-10-10T14:58:57.685554: step 2727, loss 0.093487, acc 0.984375, learning_rate 0.00010007
2017-10-10T14:58:57.900900: step 2728, loss 0.0565412, acc 0.984375, learning_rate 0.00010007
2017-10-10T14:58:58.144036: step 2729, loss 0.121246, acc 0.984375, learning_rate 0.00010007
2017-10-10T14:58:58.333157: step 2730, loss 0.366011, acc 0.859375, learning_rate 0.00010007
2017-10-10T14:58:58.561239: step 2731, loss 0.149338, acc 0.9375, learning_rate 0.000100069
2017-10-10T14:58:58.824670: step 2732, loss 0.117849, acc 0.953125, learning_rate 0.000100069
2017-10-10T14:58:59.099818: step 2733, loss 0.105559, acc 0.96875, learning_rate 0.000100069
2017-10-10T14:58:59.400497: step 2734, loss 0.158258, acc 0.96875, learning_rate 0.000100068
2017-10-10T14:58:59.740394: step 2735, loss 0.161838, acc 0.953125, learning_rate 0.000100068
2017-10-10T14:59:00.004872: step 2736, loss 0.219444, acc 0.921875, learning_rate 0.000100068
2017-10-10T14:59:00.267595: step 2737, loss 0.155743, acc 0.9375, learning_rate 0.000100068
2017-10-10T14:59:00.536875: step 2738, loss 0.198776, acc 0.921875, learning_rate 0.000100067
2017-10-10T14:59:00.797931: step 2739, loss 0.159048, acc 0.9375, learning_rate 0.000100067
2017-10-10T14:59:01.020839: step 2740, loss 0.268923, acc 0.90625, learning_rate 0.000100067
2017-10-10T14:59:01.232843: step 2741, loss 0.113473, acc 0.984375, learning_rate 0.000100067
2017-10-10T14:59:01.401898: step 2742, loss 0.215847, acc 0.9375, learning_rate 0.000100066
2017-10-10T14:59:01.609002: step 2743, loss 0.164353, acc 0.96875, learning_rate 0.000100066
2017-10-10T14:59:01.841060: step 2744, loss 0.157065, acc 0.941176, learning_rate 0.000100066
2017-10-10T14:59:02.094383: step 2745, loss 0.113707, acc 0.953125, learning_rate 0.000100065
2017-10-10T14:59:02.405091: step 2746, loss 0.24401, acc 0.875, learning_rate 0.000100065
2017-10-10T14:59:02.637158: step 2747, loss 0.168513, acc 0.96875, learning_rate 0.000100065
2017-10-10T14:59:02.888954: step 2748, loss 0.130002, acc 0.96875, learning_rate 0.000100065
2017-10-10T14:59:03.121675: step 2749, loss 0.103075, acc 0.96875, learning_rate 0.000100064
2017-10-10T14:59:04.139632: step 2750, loss 0.0569246, acc 0.984375, learning_rate 0.000100064
2017-10-10T14:59:04.430079: step 2751, loss 0.297397, acc 0.90625, learning_rate 0.000100064
2017-10-10T14:59:04.653509: step 2752, loss 0.177916, acc 0.96875, learning_rate 0.000100064
2017-10-10T14:59:04.882496: step 2753, loss 0.0914935, acc 0.96875, learning_rate 0.000100063
2017-10-10T14:59:05.031167: step 2754, loss 0.114242, acc 0.96875, learning_rate 0.000100063
2017-10-10T14:59:05.792339: step 2755, loss 0.121936, acc 0.953125, learning_rate 0.000100063
2017-10-10T14:59:06.090836: step 2756, loss 0.179751, acc 0.953125, learning_rate 0.000100063
2017-10-10T14:59:06.302925: step 2757, loss 0.0615813, acc 0.984375, learning_rate 0.000100062
2017-10-10T14:59:06.516479: step 2758, loss 0.143279, acc 0.96875, learning_rate 0.000100062
2017-10-10T14:59:06.750621: step 2759, loss 0.193086, acc 0.984375, learning_rate 0.000100062
2017-10-10T14:59:07.032952: step 2760, loss 0.124421, acc 0.953125, learning_rate 0.000100062

Evaluation:
2017-10-10T14:59:07.397169: step 2760, loss 0.211663, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2760

2017-10-10T14:59:08.316967: step 2761, loss 0.0610226, acc 0.96875, learning_rate 0.000100061
2017-10-10T14:59:08.549230: step 2762, loss 0.113984, acc 0.953125, learning_rate 0.000100061
2017-10-10T14:59:08.808964: step 2763, loss 0.218489, acc 0.90625, learning_rate 0.000100061
2017-10-10T14:59:09.077216: step 2764, loss 0.177705, acc 0.921875, learning_rate 0.000100061
2017-10-10T14:59:09.341003: step 2765, loss 0.0705032, acc 1, learning_rate 0.00010006
2017-10-10T14:59:09.552914: step 2766, loss 0.143775, acc 0.953125, learning_rate 0.00010006
2017-10-10T14:59:09.797027: step 2767, loss 0.176484, acc 0.96875, learning_rate 0.00010006
2017-10-10T14:59:10.049218: step 2768, loss 0.181417, acc 0.921875, learning_rate 0.00010006
2017-10-10T14:59:10.263434: step 2769, loss 0.226525, acc 0.921875, learning_rate 0.000100059
2017-10-10T14:59:10.528092: step 2770, loss 0.252827, acc 0.890625, learning_rate 0.000100059
2017-10-10T14:59:10.732961: step 2771, loss 0.213399, acc 0.90625, learning_rate 0.000100059
2017-10-10T14:59:10.972917: step 2772, loss 0.0652324, acc 0.984375, learning_rate 0.000100059
2017-10-10T14:59:11.211671: step 2773, loss 0.0941011, acc 0.984375, learning_rate 0.000100058
2017-10-10T14:59:11.397143: step 2774, loss 0.142224, acc 0.984375, learning_rate 0.000100058
2017-10-10T14:59:11.651185: step 2775, loss 0.158906, acc 0.9375, learning_rate 0.000100058
2017-10-10T14:59:11.916925: step 2776, loss 0.191429, acc 0.921875, learning_rate 0.000100058
2017-10-10T14:59:12.180659: step 2777, loss 0.143023, acc 0.953125, learning_rate 0.000100057
2017-10-10T14:59:12.448853: step 2778, loss 0.128762, acc 0.953125, learning_rate 0.000100057
2017-10-10T14:59:12.746904: step 2779, loss 0.16953, acc 0.953125, learning_rate 0.000100057
2017-10-10T14:59:12.949153: step 2780, loss 0.179896, acc 0.953125, learning_rate 0.000100057
2017-10-10T14:59:13.184856: step 2781, loss 0.223127, acc 0.90625, learning_rate 0.000100056
2017-10-10T14:59:13.412675: step 2782, loss 0.144157, acc 0.9375, learning_rate 0.000100056
2017-10-10T14:59:13.625419: step 2783, loss 0.238348, acc 0.921875, learning_rate 0.000100056
2017-10-10T14:59:13.908925: step 2784, loss 0.129201, acc 0.953125, learning_rate 0.000100056
2017-10-10T14:59:14.144926: step 2785, loss 0.137533, acc 0.9375, learning_rate 0.000100056
2017-10-10T14:59:14.359086: step 2786, loss 0.140421, acc 0.96875, learning_rate 0.000100055
2017-10-10T14:59:14.621760: step 2787, loss 0.151499, acc 0.984375, learning_rate 0.000100055
2017-10-10T14:59:14.884892: step 2788, loss 0.114827, acc 0.96875, learning_rate 0.000100055
2017-10-10T14:59:15.131318: step 2789, loss 0.0962142, acc 0.96875, learning_rate 0.000100055
2017-10-10T14:59:15.349792: step 2790, loss 0.170276, acc 0.9375, learning_rate 0.000100054
2017-10-10T14:59:15.588255: step 2791, loss 0.124149, acc 0.96875, learning_rate 0.000100054
2017-10-10T14:59:15.857090: step 2792, loss 0.0513167, acc 1, learning_rate 0.000100054
2017-10-10T14:59:16.085608: step 2793, loss 0.146321, acc 0.96875, learning_rate 0.000100054
2017-10-10T14:59:16.318757: step 2794, loss 0.0980398, acc 0.984375, learning_rate 0.000100054
2017-10-10T14:59:16.511806: step 2795, loss 0.182607, acc 0.90625, learning_rate 0.000100053
2017-10-10T14:59:16.780853: step 2796, loss 0.112939, acc 0.984375, learning_rate 0.000100053
2017-10-10T14:59:17.080594: step 2797, loss 0.0365701, acc 1, learning_rate 0.000100053
2017-10-10T14:59:17.339020: step 2798, loss 0.159691, acc 0.9375, learning_rate 0.000100053
2017-10-10T14:59:17.601073: step 2799, loss 0.109846, acc 0.984375, learning_rate 0.000100052
2017-10-10T14:59:17.807503: step 2800, loss 0.0731207, acc 0.984375, learning_rate 0.000100052

Evaluation:
2017-10-10T14:59:18.156753: step 2800, loss 0.21222, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2800

2017-10-10T14:59:19.181741: step 2801, loss 0.069186, acc 0.984375, learning_rate 0.000100052
2017-10-10T14:59:19.411410: step 2802, loss 0.162983, acc 0.96875, learning_rate 0.000100052
2017-10-10T14:59:19.624352: step 2803, loss 0.0919973, acc 0.984375, learning_rate 0.000100052
2017-10-10T14:59:19.874074: step 2804, loss 0.0451935, acc 1, learning_rate 0.000100051
2017-10-10T14:59:20.089390: step 2805, loss 0.185794, acc 0.9375, learning_rate 0.000100051
2017-10-10T14:59:20.297573: step 2806, loss 0.258824, acc 0.90625, learning_rate 0.000100051
2017-10-10T14:59:20.598712: step 2807, loss 0.24435, acc 0.953125, learning_rate 0.000100051
2017-10-10T14:59:20.843283: step 2808, loss 0.223438, acc 0.9375, learning_rate 0.000100051
2017-10-10T14:59:21.070903: step 2809, loss 0.15228, acc 0.953125, learning_rate 0.00010005
2017-10-10T14:59:21.344962: step 2810, loss 0.0841603, acc 0.984375, learning_rate 0.00010005
2017-10-10T14:59:21.549235: step 2811, loss 0.131916, acc 0.953125, learning_rate 0.00010005
2017-10-10T14:59:21.834571: step 2812, loss 0.114277, acc 0.96875, learning_rate 0.00010005
2017-10-10T14:59:22.060791: step 2813, loss 0.145281, acc 0.9375, learning_rate 0.00010005
2017-10-10T14:59:22.293478: step 2814, loss 0.254001, acc 0.859375, learning_rate 0.000100049
2017-10-10T14:59:22.544942: step 2815, loss 0.0986869, acc 0.96875, learning_rate 0.000100049
2017-10-10T14:59:22.790262: step 2816, loss 0.150406, acc 0.921875, learning_rate 0.000100049
2017-10-10T14:59:23.060353: step 2817, loss 0.116433, acc 0.96875, learning_rate 0.000100049
2017-10-10T14:59:23.307949: step 2818, loss 0.189275, acc 0.9375, learning_rate 0.000100049
2017-10-10T14:59:23.514246: step 2819, loss 0.102087, acc 0.984375, learning_rate 0.000100048
2017-10-10T14:59:23.724786: step 2820, loss 0.0537387, acc 1, learning_rate 0.000100048
2017-10-10T14:59:23.923469: step 2821, loss 0.238248, acc 0.90625, learning_rate 0.000100048
2017-10-10T14:59:24.118982: step 2822, loss 0.151753, acc 0.96875, learning_rate 0.000100048
2017-10-10T14:59:24.328939: step 2823, loss 0.0670803, acc 1, learning_rate 0.000100048
2017-10-10T14:59:24.547661: step 2824, loss 0.190058, acc 0.953125, learning_rate 0.000100047
2017-10-10T14:59:24.785089: step 2825, loss 0.17065, acc 0.953125, learning_rate 0.000100047
2017-10-10T14:59:25.105974: step 2826, loss 0.157603, acc 0.953125, learning_rate 0.000100047
2017-10-10T14:59:25.372131: step 2827, loss 0.130372, acc 0.953125, learning_rate 0.000100047
2017-10-10T14:59:25.635064: step 2828, loss 0.181132, acc 0.9375, learning_rate 0.000100047
2017-10-10T14:59:25.876994: step 2829, loss 0.142222, acc 0.984375, learning_rate 0.000100046
2017-10-10T14:59:26.104874: step 2830, loss 0.10756, acc 0.953125, learning_rate 0.000100046
2017-10-10T14:59:26.352922: step 2831, loss 0.160614, acc 0.9375, learning_rate 0.000100046
2017-10-10T14:59:26.601168: step 2832, loss 0.0769411, acc 0.984375, learning_rate 0.000100046
2017-10-10T14:59:26.865701: step 2833, loss 0.195596, acc 0.9375, learning_rate 0.000100046
2017-10-10T14:59:27.124925: step 2834, loss 0.147478, acc 0.953125, learning_rate 0.000100045
2017-10-10T14:59:27.391688: step 2835, loss 0.123697, acc 0.984375, learning_rate 0.000100045
2017-10-10T14:59:27.596988: step 2836, loss 0.162456, acc 0.9375, learning_rate 0.000100045
2017-10-10T14:59:27.855649: step 2837, loss 0.302792, acc 0.921875, learning_rate 0.000100045
2017-10-10T14:59:28.117692: step 2838, loss 0.228169, acc 0.90625, learning_rate 0.000100045
2017-10-10T14:59:28.373319: step 2839, loss 0.166006, acc 0.953125, learning_rate 0.000100045
2017-10-10T14:59:28.711768: step 2840, loss 0.0655759, acc 1, learning_rate 0.000100044

Evaluation:
2017-10-10T14:59:29.184856: step 2840, loss 0.212475, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2840

2017-10-10T14:59:30.244963: step 2841, loss 0.113742, acc 0.984375, learning_rate 0.000100044
2017-10-10T14:59:30.488871: step 2842, loss 0.291319, acc 0.843137, learning_rate 0.000100044
2017-10-10T14:59:30.748830: step 2843, loss 0.139658, acc 0.953125, learning_rate 0.000100044
2017-10-10T14:59:31.015017: step 2844, loss 0.138705, acc 0.953125, learning_rate 0.000100044
2017-10-10T14:59:31.276976: step 2845, loss 0.236953, acc 0.921875, learning_rate 0.000100043
2017-10-10T14:59:31.451729: step 2846, loss 0.0760928, acc 0.984375, learning_rate 0.000100043
2017-10-10T14:59:31.720831: step 2847, loss 0.108186, acc 0.984375, learning_rate 0.000100043
2017-10-10T14:59:31.981204: step 2848, loss 0.153479, acc 0.9375, learning_rate 0.000100043
2017-10-10T14:59:32.234632: step 2849, loss 0.175801, acc 0.921875, learning_rate 0.000100043
2017-10-10T14:59:32.518871: step 2850, loss 0.149266, acc 0.921875, learning_rate 0.000100043
2017-10-10T14:59:32.813066: step 2851, loss 0.128612, acc 0.9375, learning_rate 0.000100042
2017-10-10T14:59:33.025112: step 2852, loss 0.167243, acc 0.96875, learning_rate 0.000100042
2017-10-10T14:59:33.255650: step 2853, loss 0.145916, acc 0.953125, learning_rate 0.000100042
2017-10-10T14:59:33.524906: step 2854, loss 0.0714571, acc 0.984375, learning_rate 0.000100042
2017-10-10T14:59:33.737575: step 2855, loss 0.132272, acc 0.9375, learning_rate 0.000100042
2017-10-10T14:59:33.898464: step 2856, loss 0.231907, acc 0.9375, learning_rate 0.000100042
2017-10-10T14:59:34.095000: step 2857, loss 0.0782966, acc 0.984375, learning_rate 0.000100041
2017-10-10T14:59:34.360904: step 2858, loss 0.198947, acc 0.921875, learning_rate 0.000100041
2017-10-10T14:59:34.653981: step 2859, loss 0.132173, acc 0.953125, learning_rate 0.000100041
2017-10-10T14:59:34.943487: step 2860, loss 0.0833164, acc 0.984375, learning_rate 0.000100041
2017-10-10T14:59:35.166802: step 2861, loss 0.119514, acc 0.984375, learning_rate 0.000100041
2017-10-10T14:59:35.335789: step 2862, loss 0.128065, acc 0.953125, learning_rate 0.000100041
2017-10-10T14:59:35.584844: step 2863, loss 0.167584, acc 0.9375, learning_rate 0.00010004
2017-10-10T14:59:35.768540: step 2864, loss 0.209689, acc 0.9375, learning_rate 0.00010004
2017-10-10T14:59:35.982868: step 2865, loss 0.0741403, acc 0.984375, learning_rate 0.00010004
2017-10-10T14:59:36.229292: step 2866, loss 0.123683, acc 0.96875, learning_rate 0.00010004
2017-10-10T14:59:36.481935: step 2867, loss 0.0637551, acc 0.984375, learning_rate 0.00010004
2017-10-10T14:59:36.771388: step 2868, loss 0.116936, acc 0.953125, learning_rate 0.00010004
2017-10-10T14:59:37.040125: step 2869, loss 0.191211, acc 0.9375, learning_rate 0.000100039
2017-10-10T14:59:37.344920: step 2870, loss 0.0874958, acc 1, learning_rate 0.000100039
2017-10-10T14:59:37.532510: step 2871, loss 0.116268, acc 0.953125, learning_rate 0.000100039
2017-10-10T14:59:37.773948: step 2872, loss 0.0858705, acc 0.984375, learning_rate 0.000100039
2017-10-10T14:59:38.064912: step 2873, loss 0.149991, acc 0.9375, learning_rate 0.000100039
2017-10-10T14:59:38.300583: step 2874, loss 0.128023, acc 0.9375, learning_rate 0.000100039
2017-10-10T14:59:38.528655: step 2875, loss 0.145205, acc 0.96875, learning_rate 0.000100038
2017-10-10T14:59:38.774839: step 2876, loss 0.0664198, acc 0.984375, learning_rate 0.000100038
2017-10-10T14:59:39.014380: step 2877, loss 0.113076, acc 0.96875, learning_rate 0.000100038
2017-10-10T14:59:39.255161: step 2878, loss 0.0770552, acc 0.984375, learning_rate 0.000100038
2017-10-10T14:59:39.496937: step 2879, loss 0.162268, acc 0.921875, learning_rate 0.000100038
2017-10-10T14:59:39.708687: step 2880, loss 0.103436, acc 0.984375, learning_rate 0.000100038

Evaluation:
2017-10-10T14:59:40.071865: step 2880, loss 0.2128, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2880

2017-10-10T14:59:41.032467: step 2881, loss 0.143767, acc 0.96875, learning_rate 0.000100038
2017-10-10T14:59:41.241858: step 2882, loss 0.0929908, acc 0.984375, learning_rate 0.000100037
2017-10-10T14:59:41.480475: step 2883, loss 0.104232, acc 0.96875, learning_rate 0.000100037
2017-10-10T14:59:41.666255: step 2884, loss 0.0717495, acc 1, learning_rate 0.000100037
2017-10-10T14:59:41.840391: step 2885, loss 0.133465, acc 0.953125, learning_rate 0.000100037
2017-10-10T14:59:42.093786: step 2886, loss 0.141373, acc 0.953125, learning_rate 0.000100037
2017-10-10T14:59:42.348996: step 2887, loss 0.212624, acc 0.953125, learning_rate 0.000100037
2017-10-10T14:59:42.606717: step 2888, loss 0.192891, acc 0.9375, learning_rate 0.000100036
2017-10-10T14:59:42.865062: step 2889, loss 0.145436, acc 0.953125, learning_rate 0.000100036
2017-10-10T14:59:43.100955: step 2890, loss 0.114276, acc 0.9375, learning_rate 0.000100036
2017-10-10T14:59:43.371527: step 2891, loss 0.170108, acc 0.90625, learning_rate 0.000100036
2017-10-10T14:59:43.640228: step 2892, loss 0.0819966, acc 0.984375, learning_rate 0.000100036
2017-10-10T14:59:43.892934: step 2893, loss 0.151672, acc 0.953125, learning_rate 0.000100036
2017-10-10T14:59:44.107432: step 2894, loss 0.19532, acc 0.9375, learning_rate 0.000100036
2017-10-10T14:59:44.355948: step 2895, loss 0.115481, acc 0.953125, learning_rate 0.000100035
2017-10-10T14:59:44.604451: step 2896, loss 0.243188, acc 0.921875, learning_rate 0.000100035
2017-10-10T14:59:44.869061: step 2897, loss 0.107655, acc 0.96875, learning_rate 0.000100035
2017-10-10T14:59:45.128961: step 2898, loss 0.166402, acc 0.953125, learning_rate 0.000100035
2017-10-10T14:59:45.392869: step 2899, loss 0.0837352, acc 0.984375, learning_rate 0.000100035
2017-10-10T14:59:45.669873: step 2900, loss 0.155794, acc 0.96875, learning_rate 0.000100035
2017-10-10T14:59:45.952790: step 2901, loss 0.117293, acc 0.96875, learning_rate 0.000100035
2017-10-10T14:59:46.117323: step 2902, loss 0.0648935, acc 0.96875, learning_rate 0.000100034
2017-10-10T14:59:46.374791: step 2903, loss 0.18833, acc 0.921875, learning_rate 0.000100034
2017-10-10T14:59:46.581091: step 2904, loss 0.188254, acc 0.96875, learning_rate 0.000100034
2017-10-10T14:59:46.813170: step 2905, loss 0.257461, acc 0.90625, learning_rate 0.000100034
2017-10-10T14:59:47.056833: step 2906, loss 0.231727, acc 0.9375, learning_rate 0.000100034
2017-10-10T14:59:47.287924: step 2907, loss 0.205048, acc 0.9375, learning_rate 0.000100034
2017-10-10T14:59:47.587048: step 2908, loss 0.151214, acc 0.953125, learning_rate 0.000100034
2017-10-10T14:59:47.872910: step 2909, loss 0.163191, acc 0.96875, learning_rate 0.000100033
2017-10-10T14:59:48.148507: step 2910, loss 0.158643, acc 0.921875, learning_rate 0.000100033
2017-10-10T14:59:48.392258: step 2911, loss 0.169668, acc 0.890625, learning_rate 0.000100033
2017-10-10T14:59:48.610655: step 2912, loss 0.139817, acc 0.953125, learning_rate 0.000100033
2017-10-10T14:59:48.845024: step 2913, loss 0.13709, acc 0.953125, learning_rate 0.000100033
2017-10-10T14:59:49.072857: step 2914, loss 0.130447, acc 0.953125, learning_rate 0.000100033
2017-10-10T14:59:49.313823: step 2915, loss 0.159064, acc 0.953125, learning_rate 0.000100033
2017-10-10T14:59:49.599411: step 2916, loss 0.127473, acc 0.953125, learning_rate 0.000100033
2017-10-10T14:59:49.820781: step 2917, loss 0.145332, acc 0.96875, learning_rate 0.000100032
2017-10-10T14:59:50.044801: step 2918, loss 0.0929292, acc 0.96875, learning_rate 0.000100032
2017-10-10T14:59:50.262511: step 2919, loss 0.144903, acc 0.9375, learning_rate 0.000100032
2017-10-10T14:59:50.542285: step 2920, loss 0.166405, acc 0.953125, learning_rate 0.000100032

Evaluation:
2017-10-10T14:59:50.917426: step 2920, loss 0.211018, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2920

2017-10-10T14:59:52.020391: step 2921, loss 0.13851, acc 0.953125, learning_rate 0.000100032
2017-10-10T14:59:52.213583: step 2922, loss 0.137428, acc 0.9375, learning_rate 0.000100032
2017-10-10T14:59:52.409755: step 2923, loss 0.0994675, acc 0.96875, learning_rate 0.000100032
2017-10-10T14:59:52.659695: step 2924, loss 0.121283, acc 0.96875, learning_rate 0.000100031
2017-10-10T14:59:52.908890: step 2925, loss 0.162506, acc 0.953125, learning_rate 0.000100031
2017-10-10T14:59:53.129910: step 2926, loss 0.25137, acc 0.875, learning_rate 0.000100031
2017-10-10T14:59:53.364525: step 2927, loss 0.115315, acc 0.984375, learning_rate 0.000100031
2017-10-10T14:59:53.585239: step 2928, loss 0.267187, acc 0.84375, learning_rate 0.000100031
2017-10-10T14:59:53.832986: step 2929, loss 0.111887, acc 0.96875, learning_rate 0.000100031
2017-10-10T14:59:54.071154: step 2930, loss 0.255924, acc 0.9375, learning_rate 0.000100031
2017-10-10T14:59:54.329418: step 2931, loss 0.263653, acc 0.90625, learning_rate 0.000100031
2017-10-10T14:59:54.609334: step 2932, loss 0.221266, acc 0.90625, learning_rate 0.00010003
2017-10-10T14:59:54.853151: step 2933, loss 0.122209, acc 0.953125, learning_rate 0.00010003
2017-10-10T14:59:55.076848: step 2934, loss 0.164294, acc 0.953125, learning_rate 0.00010003
2017-10-10T14:59:55.377422: step 2935, loss 0.18148, acc 0.90625, learning_rate 0.00010003
2017-10-10T14:59:55.672568: step 2936, loss 0.125812, acc 0.953125, learning_rate 0.00010003
2017-10-10T14:59:55.893514: step 2937, loss 0.199114, acc 0.921875, learning_rate 0.00010003
2017-10-10T14:59:56.164887: step 2938, loss 0.0520563, acc 1, learning_rate 0.00010003
2017-10-10T14:59:56.404346: step 2939, loss 0.256641, acc 0.921875, learning_rate 0.00010003
2017-10-10T14:59:56.660905: step 2940, loss 0.152726, acc 0.941176, learning_rate 0.000100029
2017-10-10T14:59:56.903088: step 2941, loss 0.14495, acc 0.953125, learning_rate 0.000100029
2017-10-10T14:59:57.140255: step 2942, loss 0.300356, acc 0.875, learning_rate 0.000100029
2017-10-10T14:59:57.394056: step 2943, loss 0.22577, acc 0.9375, learning_rate 0.000100029
2017-10-10T14:59:57.655548: step 2944, loss 0.182239, acc 0.890625, learning_rate 0.000100029
2017-10-10T14:59:57.899695: step 2945, loss 0.187871, acc 0.9375, learning_rate 0.000100029
2017-10-10T14:59:58.125153: step 2946, loss 0.0885682, acc 1, learning_rate 0.000100029
2017-10-10T14:59:58.380821: step 2947, loss 0.176595, acc 0.9375, learning_rate 0.000100029
2017-10-10T14:59:58.676286: step 2948, loss 0.172017, acc 0.9375, learning_rate 0.000100029
2017-10-10T14:59:59.025005: step 2949, loss 0.113853, acc 0.984375, learning_rate 0.000100028
2017-10-10T14:59:59.329994: step 2950, loss 0.164165, acc 0.953125, learning_rate 0.000100028
2017-10-10T14:59:59.465440: step 2951, loss 0.134916, acc 0.9375, learning_rate 0.000100028
2017-10-10T14:59:59.652319: step 2952, loss 0.115402, acc 0.984375, learning_rate 0.000100028
2017-10-10T14:59:59.832838: step 2953, loss 0.156951, acc 0.953125, learning_rate 0.000100028
2017-10-10T15:00:00.025083: step 2954, loss 0.118313, acc 0.953125, learning_rate 0.000100028
2017-10-10T15:00:00.235021: step 2955, loss 0.114367, acc 0.96875, learning_rate 0.000100028
2017-10-10T15:00:00.491809: step 2956, loss 0.22827, acc 0.90625, learning_rate 0.000100028
2017-10-10T15:00:00.676926: step 2957, loss 0.17297, acc 0.9375, learning_rate 0.000100028
2017-10-10T15:00:00.969118: step 2958, loss 0.103563, acc 0.984375, learning_rate 0.000100027
2017-10-10T15:00:01.653012: step 2959, loss 0.146333, acc 0.96875, learning_rate 0.000100027
2017-10-10T15:00:01.892967: step 2960, loss 0.124803, acc 0.9375, learning_rate 0.000100027

Evaluation:
2017-10-10T15:00:02.276920: step 2960, loss 0.212305, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-2960

2017-10-10T15:00:03.376852: step 2961, loss 0.170832, acc 0.9375, learning_rate 0.000100027
2017-10-10T15:00:03.603628: step 2962, loss 0.275589, acc 0.9375, learning_rate 0.000100027
2017-10-10T15:00:03.825872: step 2963, loss 0.132203, acc 0.96875, learning_rate 0.000100027
2017-10-10T15:00:04.079064: step 2964, loss 0.100908, acc 0.984375, learning_rate 0.000100027
2017-10-10T15:00:04.296587: step 2965, loss 0.0991975, acc 0.96875, learning_rate 0.000100027
2017-10-10T15:00:04.548859: step 2966, loss 0.163679, acc 0.9375, learning_rate 0.000100027
2017-10-10T15:00:04.814498: step 2967, loss 0.139959, acc 0.984375, learning_rate 0.000100026
2017-10-10T15:00:05.021727: step 2968, loss 0.11517, acc 0.96875, learning_rate 0.000100026
2017-10-10T15:00:05.320440: step 2969, loss 0.111516, acc 0.984375, learning_rate 0.000100026
2017-10-10T15:00:05.526675: step 2970, loss 0.152323, acc 0.953125, learning_rate 0.000100026
2017-10-10T15:00:05.781100: step 2971, loss 0.130612, acc 0.96875, learning_rate 0.000100026
2017-10-10T15:00:06.035540: step 2972, loss 0.245413, acc 0.9375, learning_rate 0.000100026
2017-10-10T15:00:06.256877: step 2973, loss 0.109021, acc 0.953125, learning_rate 0.000100026
2017-10-10T15:00:06.537947: step 2974, loss 0.177793, acc 0.96875, learning_rate 0.000100026
2017-10-10T15:00:06.791722: step 2975, loss 0.177854, acc 0.921875, learning_rate 0.000100026
2017-10-10T15:00:06.997624: step 2976, loss 0.0580692, acc 0.984375, learning_rate 0.000100025
2017-10-10T15:00:07.277967: step 2977, loss 0.168163, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:00:07.541914: step 2978, loss 0.135438, acc 0.96875, learning_rate 0.000100025
2017-10-10T15:00:07.792926: step 2979, loss 0.187106, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:00:08.061617: step 2980, loss 0.127997, acc 0.984375, learning_rate 0.000100025
2017-10-10T15:00:08.288256: step 2981, loss 0.136549, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:00:08.598468: step 2982, loss 0.0976462, acc 0.96875, learning_rate 0.000100025
2017-10-10T15:00:08.911405: step 2983, loss 0.278829, acc 0.90625, learning_rate 0.000100025
2017-10-10T15:00:09.181006: step 2984, loss 0.0951084, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:00:09.434261: step 2985, loss 0.126737, acc 0.96875, learning_rate 0.000100025
2017-10-10T15:00:09.670243: step 2986, loss 0.0734845, acc 1, learning_rate 0.000100024
2017-10-10T15:00:09.933741: step 2987, loss 0.115329, acc 0.953125, learning_rate 0.000100024
2017-10-10T15:00:10.125102: step 2988, loss 0.182885, acc 0.9375, learning_rate 0.000100024
2017-10-10T15:00:10.429458: step 2989, loss 0.376228, acc 0.859375, learning_rate 0.000100024
2017-10-10T15:00:11.409748: step 2990, loss 0.19313, acc 0.9375, learning_rate 0.000100024
2017-10-10T15:00:11.602366: step 2991, loss 0.120077, acc 0.953125, learning_rate 0.000100024
2017-10-10T15:00:11.831067: step 2992, loss 0.221409, acc 0.90625, learning_rate 0.000100024
2017-10-10T15:00:12.046974: step 2993, loss 0.0719343, acc 0.984375, learning_rate 0.000100024
2017-10-10T15:00:12.339217: step 2994, loss 0.0454973, acc 1, learning_rate 0.000100024
2017-10-10T15:00:13.049119: step 2995, loss 0.0472249, acc 0.984375, learning_rate 0.000100024
2017-10-10T15:00:13.241039: step 2996, loss 0.108981, acc 0.953125, learning_rate 0.000100023
2017-10-10T15:00:13.483821: step 2997, loss 0.0676381, acc 0.984375, learning_rate 0.000100023
2017-10-10T15:00:13.708873: step 2998, loss 0.104247, acc 0.984375, learning_rate 0.000100023
2017-10-10T15:00:13.948847: step 2999, loss 0.121271, acc 0.96875, learning_rate 0.000100023
2017-10-10T15:00:14.219000: step 3000, loss 0.174677, acc 0.9375, learning_rate 0.000100023

Evaluation:
2017-10-10T15:00:14.764184: step 3000, loss 0.211617, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3000

2017-10-10T15:00:15.763701: step 3001, loss 0.16933, acc 0.9375, learning_rate 0.000100023
2017-10-10T15:00:16.033945: step 3002, loss 0.212093, acc 0.90625, learning_rate 0.000100023
2017-10-10T15:00:16.225435: step 3003, loss 0.147897, acc 0.953125, learning_rate 0.000100023
2017-10-10T15:00:16.500861: step 3004, loss 0.115045, acc 0.984375, learning_rate 0.000100023
2017-10-10T15:00:16.745110: step 3005, loss 0.17447, acc 0.921875, learning_rate 0.000100023
2017-10-10T15:00:17.076981: step 3006, loss 0.160218, acc 0.9375, learning_rate 0.000100023
2017-10-10T15:00:17.361121: step 3007, loss 0.198654, acc 0.921875, learning_rate 0.000100022
2017-10-10T15:00:17.652813: step 3008, loss 0.159759, acc 0.921875, learning_rate 0.000100022
2017-10-10T15:00:17.858578: step 3009, loss 0.124311, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:00:18.044874: step 3010, loss 0.257793, acc 0.890625, learning_rate 0.000100022
2017-10-10T15:00:18.240841: step 3011, loss 0.0762294, acc 0.984375, learning_rate 0.000100022
2017-10-10T15:00:18.418111: step 3012, loss 0.166455, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:00:18.949066: step 3013, loss 0.115468, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:00:19.197297: step 3014, loss 0.0850708, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:00:19.480671: step 3015, loss 0.164991, acc 0.953125, learning_rate 0.000100022
2017-10-10T15:00:19.697024: step 3016, loss 0.230689, acc 0.921875, learning_rate 0.000100022
2017-10-10T15:00:19.952877: step 3017, loss 0.136326, acc 0.9375, learning_rate 0.000100022
2017-10-10T15:00:20.164875: step 3018, loss 0.130464, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:00:20.389058: step 3019, loss 0.123608, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:00:20.584838: step 3020, loss 0.131173, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:00:20.858723: step 3021, loss 0.0854228, acc 0.96875, learning_rate 0.000100021
2017-10-10T15:00:21.131136: step 3022, loss 0.137778, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:00:21.408834: step 3023, loss 0.151807, acc 0.96875, learning_rate 0.000100021
2017-10-10T15:00:21.679383: step 3024, loss 0.0797604, acc 0.984375, learning_rate 0.000100021
2017-10-10T15:00:21.968812: step 3025, loss 0.158827, acc 0.9375, learning_rate 0.000100021
2017-10-10T15:00:22.221009: step 3026, loss 0.206346, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:00:22.464993: step 3027, loss 0.0788805, acc 0.984375, learning_rate 0.000100021
2017-10-10T15:00:22.730504: step 3028, loss 0.234083, acc 0.9375, learning_rate 0.000100021
2017-10-10T15:00:22.990638: step 3029, loss 0.0822992, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:00:23.252837: step 3030, loss 0.0952099, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:00:23.563719: step 3031, loss 0.156432, acc 0.953125, learning_rate 0.00010002
2017-10-10T15:00:23.767344: step 3032, loss 0.119096, acc 0.953125, learning_rate 0.00010002
2017-10-10T15:00:23.992377: step 3033, loss 0.127525, acc 0.953125, learning_rate 0.00010002
2017-10-10T15:00:24.216973: step 3034, loss 0.265787, acc 0.953125, learning_rate 0.00010002
2017-10-10T15:00:24.454751: step 3035, loss 0.124815, acc 0.953125, learning_rate 0.00010002
2017-10-10T15:00:24.724823: step 3036, loss 0.286433, acc 0.890625, learning_rate 0.00010002
2017-10-10T15:00:24.973110: step 3037, loss 0.049473, acc 1, learning_rate 0.00010002
2017-10-10T15:00:25.176294: step 3038, loss 0.16335, acc 0.960784, learning_rate 0.00010002
2017-10-10T15:00:25.432200: step 3039, loss 0.156923, acc 0.953125, learning_rate 0.00010002
2017-10-10T15:00:25.761322: step 3040, loss 0.17494, acc 0.96875, learning_rate 0.00010002

Evaluation:
2017-10-10T15:00:26.122221: step 3040, loss 0.209392, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3040

2017-10-10T15:00:26.969051: step 3041, loss 0.136382, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:00:27.213797: step 3042, loss 0.159578, acc 0.921875, learning_rate 0.000100019
2017-10-10T15:00:27.405456: step 3043, loss 0.140051, acc 0.9375, learning_rate 0.000100019
2017-10-10T15:00:27.605426: step 3044, loss 0.129656, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:00:27.944782: step 3045, loss 0.139973, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:00:28.145276: step 3046, loss 0.0753575, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:00:28.417070: step 3047, loss 0.181743, acc 0.890625, learning_rate 0.000100019
2017-10-10T15:00:28.649239: step 3048, loss 0.0588093, acc 1, learning_rate 0.000100019
2017-10-10T15:00:28.888087: step 3049, loss 0.186256, acc 0.921875, learning_rate 0.000100019
2017-10-10T15:00:29.195582: step 3050, loss 0.1104, acc 0.953125, learning_rate 0.000100019
2017-10-10T15:00:29.401202: step 3051, loss 0.11716, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:00:29.648730: step 3052, loss 0.12751, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:00:29.875792: step 3053, loss 0.0700799, acc 1, learning_rate 0.000100019
2017-10-10T15:00:30.123912: step 3054, loss 0.206462, acc 0.90625, learning_rate 0.000100018
2017-10-10T15:00:30.420869: step 3055, loss 0.0754498, acc 0.984375, learning_rate 0.000100018
2017-10-10T15:00:30.648493: step 3056, loss 0.114533, acc 0.953125, learning_rate 0.000100018
2017-10-10T15:00:30.875197: step 3057, loss 0.102372, acc 0.984375, learning_rate 0.000100018
2017-10-10T15:00:31.078545: step 3058, loss 0.152186, acc 0.953125, learning_rate 0.000100018
2017-10-10T15:00:31.310841: step 3059, loss 0.131929, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:00:31.544928: step 3060, loss 0.194415, acc 0.921875, learning_rate 0.000100018
2017-10-10T15:00:31.792178: step 3061, loss 0.188916, acc 0.953125, learning_rate 0.000100018
2017-10-10T15:00:32.104845: step 3062, loss 0.213254, acc 0.90625, learning_rate 0.000100018
2017-10-10T15:00:32.357090: step 3063, loss 0.127494, acc 0.953125, learning_rate 0.000100018
2017-10-10T15:00:32.630995: step 3064, loss 0.264582, acc 0.890625, learning_rate 0.000100018
2017-10-10T15:00:32.876918: step 3065, loss 0.188081, acc 0.9375, learning_rate 0.000100018
2017-10-10T15:00:33.170376: step 3066, loss 0.215166, acc 0.953125, learning_rate 0.000100018
2017-10-10T15:00:33.422373: step 3067, loss 0.195595, acc 0.921875, learning_rate 0.000100018
2017-10-10T15:00:33.637473: step 3068, loss 0.110744, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:00:33.833629: step 3069, loss 0.201047, acc 0.9375, learning_rate 0.000100017
2017-10-10T15:00:34.073717: step 3070, loss 0.117097, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:00:34.297048: step 3071, loss 0.125655, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:00:34.516969: step 3072, loss 0.103292, acc 1, learning_rate 0.000100017
2017-10-10T15:00:34.783067: step 3073, loss 0.108728, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:00:35.068907: step 3074, loss 0.136033, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:00:35.351907: step 3075, loss 0.149853, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:00:35.611947: step 3076, loss 0.105813, acc 0.984375, learning_rate 0.000100017
2017-10-10T15:00:35.773677: step 3077, loss 0.253547, acc 0.890625, learning_rate 0.000100017
2017-10-10T15:00:35.932496: step 3078, loss 0.143669, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:00:36.108497: step 3079, loss 0.158872, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:00:36.353230: step 3080, loss 0.0923772, acc 0.984375, learning_rate 0.000100017

Evaluation:
2017-10-10T15:00:36.773835: step 3080, loss 0.210616, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3080

2017-10-10T15:00:37.783324: step 3081, loss 0.200624, acc 0.921875, learning_rate 0.000100017
2017-10-10T15:00:38.025228: step 3082, loss 0.0673545, acc 0.984375, learning_rate 0.000100016
2017-10-10T15:00:38.260982: step 3083, loss 0.17245, acc 0.9375, learning_rate 0.000100016
2017-10-10T15:00:38.512013: step 3084, loss 0.204267, acc 0.890625, learning_rate 0.000100016
2017-10-10T15:00:38.725001: step 3085, loss 0.100252, acc 0.984375, learning_rate 0.000100016
2017-10-10T15:00:38.993437: step 3086, loss 0.123028, acc 0.9375, learning_rate 0.000100016
2017-10-10T15:00:39.258215: step 3087, loss 0.23333, acc 0.9375, learning_rate 0.000100016
2017-10-10T15:00:39.537731: step 3088, loss 0.208741, acc 0.9375, learning_rate 0.000100016
2017-10-10T15:00:39.795403: step 3089, loss 0.196956, acc 0.90625, learning_rate 0.000100016
2017-10-10T15:00:40.044871: step 3090, loss 0.169106, acc 0.921875, learning_rate 0.000100016
2017-10-10T15:00:40.193830: step 3091, loss 0.259336, acc 0.921875, learning_rate 0.000100016
2017-10-10T15:00:40.437513: step 3092, loss 0.215797, acc 0.921875, learning_rate 0.000100016
2017-10-10T15:00:40.621141: step 3093, loss 0.0890712, acc 0.984375, learning_rate 0.000100016
2017-10-10T15:00:40.880733: step 3094, loss 0.157401, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:00:41.181861: step 3095, loss 0.18748, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:00:41.405023: step 3096, loss 0.169106, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:00:41.595648: step 3097, loss 0.0804635, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:00:41.800961: step 3098, loss 0.143221, acc 0.9375, learning_rate 0.000100015
2017-10-10T15:00:42.068820: step 3099, loss 0.15517, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:00:42.271801: step 3100, loss 0.137387, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:00:42.557593: step 3101, loss 0.244548, acc 0.890625, learning_rate 0.000100015
2017-10-10T15:00:42.842678: step 3102, loss 0.061614, acc 0.984375, learning_rate 0.000100015
2017-10-10T15:00:43.034988: step 3103, loss 0.109267, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:00:43.214605: step 3104, loss 0.13586, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:00:43.432302: step 3105, loss 0.166973, acc 0.921875, learning_rate 0.000100015
2017-10-10T15:00:43.581390: step 3106, loss 0.237338, acc 0.921875, learning_rate 0.000100015
2017-10-10T15:00:43.798864: step 3107, loss 0.186095, acc 0.9375, learning_rate 0.000100015
2017-10-10T15:00:44.041515: step 3108, loss 0.0538952, acc 1, learning_rate 0.000100015
2017-10-10T15:00:44.284843: step 3109, loss 0.121965, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:00:44.585466: step 3110, loss 0.153962, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:00:44.805344: step 3111, loss 0.113513, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:00:45.076577: step 3112, loss 0.204305, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:00:45.383881: step 3113, loss 0.140111, acc 0.9375, learning_rate 0.000100015
2017-10-10T15:00:45.600912: step 3114, loss 0.210013, acc 0.90625, learning_rate 0.000100014
2017-10-10T15:00:45.878822: step 3115, loss 0.12274, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:00:46.121476: step 3116, loss 0.0631419, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:00:46.349999: step 3117, loss 0.24348, acc 0.9375, learning_rate 0.000100014
2017-10-10T15:00:46.564761: step 3118, loss 0.13477, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:00:46.833004: step 3119, loss 0.0805662, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:00:47.117325: step 3120, loss 0.273905, acc 0.90625, learning_rate 0.000100014

Evaluation:
2017-10-10T15:00:47.526840: step 3120, loss 0.209399, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3120

2017-10-10T15:00:48.661293: step 3121, loss 0.135768, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:00:48.888201: step 3122, loss 0.127764, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:00:49.138219: step 3123, loss 0.240717, acc 0.9375, learning_rate 0.000100014
2017-10-10T15:00:49.391278: step 3124, loss 0.218024, acc 0.921875, learning_rate 0.000100014
2017-10-10T15:00:49.622883: step 3125, loss 0.124898, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:00:49.829768: step 3126, loss 0.0966079, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:00:50.050131: step 3127, loss 0.113178, acc 0.9375, learning_rate 0.000100014
2017-10-10T15:00:50.320895: step 3128, loss 0.0494519, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:00:50.596898: step 3129, loss 0.168602, acc 0.90625, learning_rate 0.000100014
2017-10-10T15:00:50.849050: step 3130, loss 0.127001, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:00:51.112983: step 3131, loss 0.0698192, acc 1, learning_rate 0.000100014
2017-10-10T15:00:51.394597: step 3132, loss 0.0996688, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:00:51.666555: step 3133, loss 0.0917856, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:00:51.836943: step 3134, loss 0.0700092, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:00:52.153751: step 3135, loss 0.130076, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:00:52.348878: step 3136, loss 0.130858, acc 0.960784, learning_rate 0.000100013
2017-10-10T15:00:52.541153: step 3137, loss 0.129883, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:00:52.869205: step 3138, loss 0.161823, acc 0.90625, learning_rate 0.000100013
2017-10-10T15:00:53.109281: step 3139, loss 0.198397, acc 0.9375, learning_rate 0.000100013
2017-10-10T15:00:53.396182: step 3140, loss 0.171165, acc 0.9375, learning_rate 0.000100013
2017-10-10T15:00:53.636374: step 3141, loss 0.0880373, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:00:53.885067: step 3142, loss 0.0766608, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:00:54.087724: step 3143, loss 0.120803, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:00:54.346704: step 3144, loss 0.125074, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:00:54.588805: step 3145, loss 0.0506445, acc 1, learning_rate 0.000100013
2017-10-10T15:00:54.795857: step 3146, loss 0.13451, acc 0.9375, learning_rate 0.000100013
2017-10-10T15:00:55.072824: step 3147, loss 0.143067, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:00:55.332836: step 3148, loss 0.117914, acc 0.9375, learning_rate 0.000100013
2017-10-10T15:00:55.584921: step 3149, loss 0.0818413, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:00:55.791773: step 3150, loss 0.0855121, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:00:56.030538: step 3151, loss 0.107561, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:00:56.303628: step 3152, loss 0.120788, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:00:56.528132: step 3153, loss 0.180034, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:00:56.776965: step 3154, loss 0.171314, acc 0.921875, learning_rate 0.000100012
2017-10-10T15:00:57.013491: step 3155, loss 0.101027, acc 1, learning_rate 0.000100012
2017-10-10T15:00:57.314176: step 3156, loss 0.261318, acc 0.90625, learning_rate 0.000100012
2017-10-10T15:00:57.581044: step 3157, loss 0.169399, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:00:57.811758: step 3158, loss 0.14272, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:00:58.120798: step 3159, loss 0.157239, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:00:58.313909: step 3160, loss 0.0886124, acc 0.96875, learning_rate 0.000100012

Evaluation:
2017-10-10T15:00:58.712912: step 3160, loss 0.210694, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3160

2017-10-10T15:00:59.667421: step 3161, loss 0.147608, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:00:59.915116: step 3162, loss 0.153403, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:01:00.130671: step 3163, loss 0.147235, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:01:00.377040: step 3164, loss 0.153231, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:01:00.655842: step 3165, loss 0.143264, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:01:00.904862: step 3166, loss 0.137784, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:01:01.102399: step 3167, loss 0.235398, acc 0.921875, learning_rate 0.000100012
2017-10-10T15:01:01.384523: step 3168, loss 0.174352, acc 0.921875, learning_rate 0.000100012
2017-10-10T15:01:01.603677: step 3169, loss 0.188138, acc 0.90625, learning_rate 0.000100012
2017-10-10T15:01:01.868851: step 3170, loss 0.0856635, acc 1, learning_rate 0.000100012
2017-10-10T15:01:02.139182: step 3171, loss 0.114399, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:01:02.371186: step 3172, loss 0.0557921, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:01:02.684869: step 3173, loss 0.115838, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:01:02.933025: step 3174, loss 0.143586, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:01:03.124053: step 3175, loss 0.0996322, acc 1, learning_rate 0.000100011
2017-10-10T15:01:03.376820: step 3176, loss 0.157803, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:01:03.669319: step 3177, loss 0.0844593, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:01:03.856899: step 3178, loss 0.198829, acc 0.90625, learning_rate 0.000100011
2017-10-10T15:01:04.102995: step 3179, loss 0.236144, acc 0.90625, learning_rate 0.000100011
2017-10-10T15:01:04.372819: step 3180, loss 0.118823, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:01:04.630845: step 3181, loss 0.0777481, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:01:04.812555: step 3182, loss 0.247073, acc 0.921875, learning_rate 0.000100011
2017-10-10T15:01:05.104383: step 3183, loss 0.103291, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:01:05.268888: step 3184, loss 0.184003, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:01:05.584983: step 3185, loss 0.130348, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:01:05.842745: step 3186, loss 0.0832265, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:01:06.091354: step 3187, loss 0.12975, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:01:06.378637: step 3188, loss 0.207216, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:01:06.582833: step 3189, loss 0.160007, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:01:06.816165: step 3190, loss 0.154229, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:01:07.081727: step 3191, loss 0.225423, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:01:07.261967: step 3192, loss 0.137766, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:01:07.540378: step 3193, loss 0.11485, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:01:07.773271: step 3194, loss 0.125164, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:01:08.019432: step 3195, loss 0.250923, acc 0.921875, learning_rate 0.00010001
2017-10-10T15:01:08.293042: step 3196, loss 0.211145, acc 0.90625, learning_rate 0.00010001
2017-10-10T15:01:08.471490: step 3197, loss 0.127246, acc 0.9375, learning_rate 0.00010001
2017-10-10T15:01:08.685763: step 3198, loss 0.238615, acc 0.90625, learning_rate 0.00010001
2017-10-10T15:01:08.935074: step 3199, loss 0.061216, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:01:09.218572: step 3200, loss 0.120022, acc 0.9375, learning_rate 0.00010001

Evaluation:
2017-10-10T15:01:09.644518: step 3200, loss 0.210402, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3200

2017-10-10T15:01:10.835721: step 3201, loss 0.15823, acc 0.9375, learning_rate 0.00010001
2017-10-10T15:01:11.032334: step 3202, loss 0.0567203, acc 1, learning_rate 0.00010001
2017-10-10T15:01:11.264954: step 3203, loss 0.061998, acc 1, learning_rate 0.00010001
2017-10-10T15:01:11.556902: step 3204, loss 0.116379, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:01:11.884220: step 3205, loss 0.203758, acc 0.921875, learning_rate 0.00010001
2017-10-10T15:01:12.109720: step 3206, loss 0.32101, acc 0.9375, learning_rate 0.00010001
2017-10-10T15:01:12.344693: step 3207, loss 0.194436, acc 0.9375, learning_rate 0.00010001
2017-10-10T15:01:13.508848: step 3208, loss 0.115207, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:01:13.725743: step 3209, loss 0.225293, acc 0.90625, learning_rate 0.00010001
2017-10-10T15:01:14.017618: step 3210, loss 0.142801, acc 0.921875, learning_rate 0.00010001
2017-10-10T15:01:14.237985: step 3211, loss 0.19631, acc 0.921875, learning_rate 0.00010001
2017-10-10T15:01:14.461293: step 3212, loss 0.104129, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:01:14.699654: step 3213, loss 0.16141, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:01:14.960685: step 3214, loss 0.300891, acc 0.890625, learning_rate 0.00010001
2017-10-10T15:01:15.255912: step 3215, loss 0.0668262, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:01:15.501060: step 3216, loss 0.0820571, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:01:15.732877: step 3217, loss 0.242952, acc 0.890625, learning_rate 0.000100009
2017-10-10T15:01:15.982094: step 3218, loss 0.163779, acc 0.90625, learning_rate 0.000100009
2017-10-10T15:01:16.236141: step 3219, loss 0.0930014, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:01:16.504989: step 3220, loss 0.190019, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:01:16.820986: step 3221, loss 0.193495, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:01:17.137012: step 3222, loss 0.129855, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:01:17.379857: step 3223, loss 0.111573, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:01:17.624686: step 3224, loss 0.0891152, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:01:17.845456: step 3225, loss 0.211262, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:01:18.060907: step 3226, loss 0.0799818, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:01:18.244879: step 3227, loss 0.249497, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:01:18.460947: step 3228, loss 0.0762167, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:01:18.674520: step 3229, loss 0.122015, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:01:18.939908: step 3230, loss 0.202937, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:01:19.133387: step 3231, loss 0.0916514, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:01:19.380801: step 3232, loss 0.139141, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:01:19.670037: step 3233, loss 0.18084, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:01:19.869336: step 3234, loss 0.17547, acc 0.921569, learning_rate 0.000100009
2017-10-10T15:01:20.144455: step 3235, loss 0.118448, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:01:20.368959: step 3236, loss 0.13328, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:01:20.643772: step 3237, loss 0.158956, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:01:20.871460: step 3238, loss 0.0831024, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:01:21.145411: step 3239, loss 0.0558044, acc 1, learning_rate 0.000100009
2017-10-10T15:01:21.412865: step 3240, loss 0.144326, acc 0.953125, learning_rate 0.000100009

Evaluation:
2017-10-10T15:01:21.786198: step 3240, loss 0.208992, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3240

2017-10-10T15:01:22.853068: step 3241, loss 0.210756, acc 0.921875, learning_rate 0.000100009
2017-10-10T15:01:23.116787: step 3242, loss 0.100636, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:01:23.424893: step 3243, loss 0.215147, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:01:23.683028: step 3244, loss 0.0971187, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:01:23.884627: step 3245, loss 0.158966, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:01:24.134948: step 3246, loss 0.118823, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:01:24.340903: step 3247, loss 0.096209, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:01:24.616859: step 3248, loss 0.18154, acc 0.921875, learning_rate 0.000100008
2017-10-10T15:01:24.917031: step 3249, loss 0.127765, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:01:25.147103: step 3250, loss 0.0986743, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:01:25.401055: step 3251, loss 0.0870555, acc 1, learning_rate 0.000100008
2017-10-10T15:01:25.701371: step 3252, loss 0.130085, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:01:25.935776: step 3253, loss 0.085037, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:01:26.144830: step 3254, loss 0.200058, acc 0.921875, learning_rate 0.000100008
2017-10-10T15:01:26.445278: step 3255, loss 0.134752, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:01:26.752601: step 3256, loss 0.186049, acc 0.921875, learning_rate 0.000100008
2017-10-10T15:01:26.975902: step 3257, loss 0.123417, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:01:27.213127: step 3258, loss 0.12046, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:01:27.419230: step 3259, loss 0.209709, acc 0.90625, learning_rate 0.000100008
2017-10-10T15:01:27.692498: step 3260, loss 0.117184, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:01:27.906919: step 3261, loss 0.155612, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:01:28.122843: step 3262, loss 0.192804, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:01:28.377640: step 3263, loss 0.0512157, acc 1, learning_rate 0.000100008
2017-10-10T15:01:28.621689: step 3264, loss 0.174182, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:01:28.898505: step 3265, loss 0.172316, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:01:29.144365: step 3266, loss 0.142534, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:01:29.454610: step 3267, loss 0.195709, acc 0.921875, learning_rate 0.000100008
2017-10-10T15:01:29.607263: step 3268, loss 0.0812357, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:01:29.829582: step 3269, loss 0.122472, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:01:30.048933: step 3270, loss 0.173263, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:01:30.270261: step 3271, loss 0.148153, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:01:30.536899: step 3272, loss 0.175968, acc 0.921875, learning_rate 0.000100008
2017-10-10T15:01:30.796888: step 3273, loss 0.121536, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:01:30.992889: step 3274, loss 0.0889693, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:01:31.235755: step 3275, loss 0.135221, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:01:31.436822: step 3276, loss 0.0984968, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:01:31.628870: step 3277, loss 0.188395, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:01:31.908436: step 3278, loss 0.156983, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:01:32.126248: step 3279, loss 0.180856, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:01:32.380871: step 3280, loss 0.0869333, acc 0.984375, learning_rate 0.000100007

Evaluation:
2017-10-10T15:01:32.736334: step 3280, loss 0.20955, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3280

2017-10-10T15:01:33.804840: step 3281, loss 0.151878, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:01:34.052768: step 3282, loss 0.158644, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:01:34.297500: step 3283, loss 0.173762, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:01:34.492998: step 3284, loss 0.172607, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:01:34.673455: step 3285, loss 0.136415, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:01:34.909007: step 3286, loss 0.0957851, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:01:35.084820: step 3287, loss 0.0988011, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:01:35.377203: step 3288, loss 0.0821977, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:01:35.593093: step 3289, loss 0.153487, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:01:35.837092: step 3290, loss 0.127768, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:01:36.116843: step 3291, loss 0.193834, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:01:36.336976: step 3292, loss 0.203172, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:01:36.588615: step 3293, loss 0.125467, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:01:36.858690: step 3294, loss 0.158846, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:01:37.103359: step 3295, loss 0.131104, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:01:37.376794: step 3296, loss 0.0721389, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:01:37.665054: step 3297, loss 0.276372, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:01:37.869596: step 3298, loss 0.222544, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:01:38.145050: step 3299, loss 0.170308, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:01:38.387560: step 3300, loss 0.161068, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:01:38.624887: step 3301, loss 0.0874564, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:01:38.828832: step 3302, loss 0.166564, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:01:39.063042: step 3303, loss 0.101815, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:01:39.329214: step 3304, loss 0.129917, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:01:39.592255: step 3305, loss 0.149791, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:01:39.891812: step 3306, loss 0.104913, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:01:40.168913: step 3307, loss 0.0869785, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:01:40.433162: step 3308, loss 0.145658, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:01:40.690353: step 3309, loss 0.248796, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:01:40.924891: step 3310, loss 0.22013, acc 0.921875, learning_rate 0.000100006
2017-10-10T15:01:41.181011: step 3311, loss 0.0900482, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:01:41.523953: step 3312, loss 0.0782992, acc 1, learning_rate 0.000100006
2017-10-10T15:01:41.775606: step 3313, loss 0.132827, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:01:41.959508: step 3314, loss 0.141608, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:01:42.213160: step 3315, loss 0.160765, acc 0.921875, learning_rate 0.000100006
2017-10-10T15:01:42.440992: step 3316, loss 0.138507, acc 0.921875, learning_rate 0.000100006
2017-10-10T15:01:42.767493: step 3317, loss 0.119297, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:01:42.968968: step 3318, loss 0.238145, acc 0.859375, learning_rate 0.000100006
2017-10-10T15:01:43.263691: step 3319, loss 0.0871971, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:01:43.473879: step 3320, loss 0.184453, acc 0.921875, learning_rate 0.000100006

Evaluation:
2017-10-10T15:01:43.861454: step 3320, loss 0.208004, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3320

2017-10-10T15:01:44.784659: step 3321, loss 0.0885791, acc 1, learning_rate 0.000100006
2017-10-10T15:01:45.045037: step 3322, loss 0.145564, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:01:45.234508: step 3323, loss 0.0916921, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:01:45.521120: step 3324, loss 0.162841, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:01:45.812824: step 3325, loss 0.0916377, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:01:46.074382: step 3326, loss 0.132351, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:01:46.380986: step 3327, loss 0.129781, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:01:46.625680: step 3328, loss 0.0962449, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:01:46.835479: step 3329, loss 0.163004, acc 0.921875, learning_rate 0.000100006
2017-10-10T15:01:47.036837: step 3330, loss 0.252905, acc 0.90625, learning_rate 0.000100006
2017-10-10T15:01:47.276810: step 3331, loss 0.132752, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:01:47.532859: step 3332, loss 0.200445, acc 0.921569, learning_rate 0.000100006
2017-10-10T15:01:47.732537: step 3333, loss 0.136455, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:01:48.004830: step 3334, loss 0.117821, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:01:48.237046: step 3335, loss 0.156452, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:01:48.463838: step 3336, loss 0.126531, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:01:48.756365: step 3337, loss 0.197315, acc 0.921875, learning_rate 0.000100006
2017-10-10T15:01:49.055501: step 3338, loss 0.0913676, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:01:49.288694: step 3339, loss 0.0850521, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:01:49.546026: step 3340, loss 0.124274, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:01:49.758763: step 3341, loss 0.0641937, acc 1, learning_rate 0.000100006
2017-10-10T15:01:49.972728: step 3342, loss 0.159465, acc 0.921875, learning_rate 0.000100006
2017-10-10T15:01:50.271262: step 3343, loss 0.160741, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:01:50.617181: step 3344, loss 0.068298, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:01:50.888859: step 3345, loss 0.132655, acc 0.921875, learning_rate 0.000100006
2017-10-10T15:01:51.052148: step 3346, loss 0.154254, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:01:51.315072: step 3347, loss 0.196624, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:01:51.529081: step 3348, loss 0.0700694, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:01:51.773176: step 3349, loss 0.151347, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:01:51.992841: step 3350, loss 0.0870391, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:01:52.203853: step 3351, loss 0.200296, acc 0.921875, learning_rate 0.000100005
2017-10-10T15:01:52.421614: step 3352, loss 0.241673, acc 0.921875, learning_rate 0.000100005
2017-10-10T15:01:52.644949: step 3353, loss 0.0680261, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:01:52.869930: step 3354, loss 0.121515, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:01:53.103856: step 3355, loss 0.127191, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:01:53.360952: step 3356, loss 0.177036, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:01:53.573152: step 3357, loss 0.0584952, acc 1, learning_rate 0.000100005
2017-10-10T15:01:53.812829: step 3358, loss 0.141718, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:01:54.065010: step 3359, loss 0.115131, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:01:54.286743: step 3360, loss 0.118198, acc 0.953125, learning_rate 0.000100005

Evaluation:
2017-10-10T15:01:54.671750: step 3360, loss 0.20831, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3360

2017-10-10T15:01:55.749550: step 3361, loss 0.120548, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:01:55.982878: step 3362, loss 0.148724, acc 0.921875, learning_rate 0.000100005
2017-10-10T15:01:56.265232: step 3363, loss 0.0733091, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:01:56.512691: step 3364, loss 0.204803, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:01:56.746262: step 3365, loss 0.0815156, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:01:57.052923: step 3366, loss 0.191041, acc 0.921875, learning_rate 0.000100005
2017-10-10T15:01:57.264600: step 3367, loss 0.201195, acc 0.921875, learning_rate 0.000100005
2017-10-10T15:01:57.507689: step 3368, loss 0.144555, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:01:57.701407: step 3369, loss 0.110999, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:01:57.959701: step 3370, loss 0.106293, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:01:58.212317: step 3371, loss 0.222181, acc 0.921875, learning_rate 0.000100005
2017-10-10T15:01:58.431384: step 3372, loss 0.106179, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:01:58.631450: step 3373, loss 0.166972, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:01:58.888851: step 3374, loss 0.292756, acc 0.90625, learning_rate 0.000100005
2017-10-10T15:01:59.176921: step 3375, loss 0.25262, acc 0.90625, learning_rate 0.000100005
2017-10-10T15:01:59.412826: step 3376, loss 0.176446, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:01:59.708863: step 3377, loss 0.068539, acc 1, learning_rate 0.000100005
2017-10-10T15:02:00.008973: step 3378, loss 0.264609, acc 0.890625, learning_rate 0.000100005
2017-10-10T15:02:00.217019: step 3379, loss 0.088625, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:02:00.485874: step 3380, loss 0.124237, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:02:00.675583: step 3381, loss 0.250567, acc 0.875, learning_rate 0.000100005
2017-10-10T15:02:00.911627: step 3382, loss 0.16181, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:02:01.164973: step 3383, loss 0.118758, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:02:01.424961: step 3384, loss 0.178973, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:02:01.683768: step 3385, loss 0.17236, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:02:01.944827: step 3386, loss 0.145232, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:02:02.245124: step 3387, loss 0.0435766, acc 1, learning_rate 0.000100005
2017-10-10T15:02:02.509389: step 3388, loss 0.113416, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:02:02.732862: step 3389, loss 0.191467, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:02:03.031720: step 3390, loss 0.204888, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:02:03.259171: step 3391, loss 0.0647724, acc 1, learning_rate 0.000100005
2017-10-10T15:02:03.488721: step 3392, loss 0.142092, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:02:03.789250: step 3393, loss 0.152406, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:02:03.988893: step 3394, loss 0.131999, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:02:04.271714: step 3395, loss 0.0461347, acc 1, learning_rate 0.000100005
2017-10-10T15:02:04.588984: step 3396, loss 0.137524, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:02:04.849034: step 3397, loss 0.211718, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:02:05.059129: step 3398, loss 0.141533, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:02:05.300982: step 3399, loss 0.0698969, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:02:05.543330: step 3400, loss 0.186255, acc 0.953125, learning_rate 0.000100004

Evaluation:
2017-10-10T15:02:05.968855: step 3400, loss 0.208747, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3400

2017-10-10T15:02:07.101159: step 3401, loss 0.0837157, acc 1, learning_rate 0.000100004
2017-10-10T15:02:07.348858: step 3402, loss 0.251494, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:02:07.568601: step 3403, loss 0.0622521, acc 1, learning_rate 0.000100004
2017-10-10T15:02:07.757446: step 3404, loss 0.118175, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:07.932507: step 3405, loss 0.120025, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:02:08.125465: step 3406, loss 0.272767, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:02:08.308974: step 3407, loss 0.209094, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:08.581039: step 3408, loss 0.107414, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:08.828873: step 3409, loss 0.202246, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:02:09.099198: step 3410, loss 0.165356, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:02:09.323155: step 3411, loss 0.103085, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:02:09.567705: step 3412, loss 0.128189, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:09.848870: step 3413, loss 0.0520602, acc 1, learning_rate 0.000100004
2017-10-10T15:02:10.054852: step 3414, loss 0.107187, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:10.309050: step 3415, loss 0.0876577, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:02:10.553211: step 3416, loss 0.21455, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:02:10.804951: step 3417, loss 0.153319, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:11.066184: step 3418, loss 0.122428, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:02:11.292663: step 3419, loss 0.210497, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:02:11.548687: step 3420, loss 0.0836265, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:02:11.742664: step 3421, loss 0.11104, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:02:11.991185: step 3422, loss 0.0914638, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:02:12.205055: step 3423, loss 0.188035, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:02:12.442856: step 3424, loss 0.114303, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:02:12.696111: step 3425, loss 0.135207, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:12.976913: step 3426, loss 0.151822, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:13.252841: step 3427, loss 0.0914149, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:02:13.455680: step 3428, loss 0.0786951, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:13.645351: step 3429, loss 0.179801, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:02:13.873151: step 3430, loss 0.151119, acc 0.921569, learning_rate 0.000100004
2017-10-10T15:02:14.096744: step 3431, loss 0.0927771, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:02:14.368829: step 3432, loss 0.113069, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:14.637849: step 3433, loss 0.173381, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:02:14.859074: step 3434, loss 0.0940695, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:15.119072: step 3435, loss 0.199659, acc 0.90625, learning_rate 0.000100004
2017-10-10T15:02:15.372978: step 3436, loss 0.0764362, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:02:15.672964: step 3437, loss 0.119055, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:02:15.932592: step 3438, loss 0.199018, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:02:16.180886: step 3439, loss 0.0844623, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:02:16.448832: step 3440, loss 0.0972654, acc 0.96875, learning_rate 0.000100004

Evaluation:
2017-10-10T15:02:20.540275: step 3440, loss 0.20864, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3440

2017-10-10T15:02:21.652608: step 3441, loss 0.228604, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:21.876125: step 3442, loss 0.132702, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:22.151610: step 3443, loss 0.22313, acc 0.90625, learning_rate 0.000100004
2017-10-10T15:02:22.424872: step 3444, loss 0.158677, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:02:22.646252: step 3445, loss 0.173991, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:02:22.873066: step 3446, loss 0.264293, acc 0.90625, learning_rate 0.000100004
2017-10-10T15:02:23.157605: step 3447, loss 0.171702, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:02:23.344975: step 3448, loss 0.262331, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:02:23.575940: step 3449, loss 0.124027, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:02:23.832053: step 3450, loss 0.109501, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:24.056808: step 3451, loss 0.0776986, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:02:24.399337: step 3452, loss 0.100278, acc 1, learning_rate 0.000100004
2017-10-10T15:02:24.729965: step 3453, loss 0.112301, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:02:24.915835: step 3454, loss 0.122761, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:02:25.157004: step 3455, loss 0.15952, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:02:25.412987: step 3456, loss 0.140478, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:02:25.590349: step 3457, loss 0.137851, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:25.814132: step 3458, loss 0.149884, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:26.037211: step 3459, loss 0.107898, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:26.247202: step 3460, loss 0.20711, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:02:26.510747: step 3461, loss 0.147223, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:02:26.753007: step 3462, loss 0.175133, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:02:26.996842: step 3463, loss 0.156974, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:27.257984: step 3464, loss 0.179856, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:02:27.488915: step 3465, loss 0.176045, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:02:27.754320: step 3466, loss 0.141054, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:27.980978: step 3467, loss 0.238527, acc 0.875, learning_rate 0.000100003
2017-10-10T15:02:28.272212: step 3468, loss 0.0876636, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:28.451744: step 3469, loss 0.0666967, acc 1, learning_rate 0.000100003
2017-10-10T15:02:28.683676: step 3470, loss 0.148813, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:02:28.938280: step 3471, loss 0.0956555, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:29.160196: step 3472, loss 0.110273, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:29.463659: step 3473, loss 0.0760183, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:29.701051: step 3474, loss 0.177236, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:02:29.975842: step 3475, loss 0.0744333, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:30.257268: step 3476, loss 0.104906, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:02:30.532449: step 3477, loss 0.101688, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:30.764308: step 3478, loss 0.204803, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:02:30.944159: step 3479, loss 0.0803309, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:02:31.236736: step 3480, loss 0.237109, acc 0.9375, learning_rate 0.000100003

Evaluation:
2017-10-10T15:02:31.541251: step 3480, loss 0.206957, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3480

2017-10-10T15:02:32.516836: step 3481, loss 0.115472, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:32.724153: step 3482, loss 0.163972, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:02:32.972817: step 3483, loss 0.052545, acc 1, learning_rate 0.000100003
2017-10-10T15:02:33.217117: step 3484, loss 0.112422, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:33.493068: step 3485, loss 0.118347, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:33.723107: step 3486, loss 0.0482358, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:02:34.016512: step 3487, loss 0.0900913, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:02:34.278701: step 3488, loss 0.0845338, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:02:34.506214: step 3489, loss 0.051517, acc 1, learning_rate 0.000100003
2017-10-10T15:02:34.767141: step 3490, loss 0.106342, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:35.016960: step 3491, loss 0.245158, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:02:35.224502: step 3492, loss 0.148725, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:02:35.469113: step 3493, loss 0.100684, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:35.692308: step 3494, loss 0.114154, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:02:35.932909: step 3495, loss 0.177612, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:36.164034: step 3496, loss 0.0847334, acc 1, learning_rate 0.000100003
2017-10-10T15:02:36.415265: step 3497, loss 0.0613372, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:02:36.664987: step 3498, loss 0.249198, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:02:36.896855: step 3499, loss 0.169267, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:02:37.213151: step 3500, loss 0.188155, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:37.456888: step 3501, loss 0.138607, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:37.689035: step 3502, loss 0.200654, acc 0.890625, learning_rate 0.000100003
2017-10-10T15:02:37.968805: step 3503, loss 0.130756, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:38.248788: step 3504, loss 0.15563, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:38.528895: step 3505, loss 0.153299, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:38.812916: step 3506, loss 0.140896, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:02:39.064957: step 3507, loss 0.12278, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:39.323645: step 3508, loss 0.0906812, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:39.588917: step 3509, loss 0.122047, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:39.850134: step 3510, loss 0.264247, acc 0.875, learning_rate 0.000100003
2017-10-10T15:02:40.032797: step 3511, loss 0.206526, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:40.205400: step 3512, loss 0.130715, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:02:40.474988: step 3513, loss 0.146446, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:40.696721: step 3514, loss 0.129826, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:40.907273: step 3515, loss 0.284653, acc 0.875, learning_rate 0.000100003
2017-10-10T15:02:41.231867: step 3516, loss 0.138656, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:41.457608: step 3517, loss 0.12795, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:41.724926: step 3518, loss 0.0674435, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:42.012951: step 3519, loss 0.134412, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:42.278981: step 3520, loss 0.261583, acc 0.90625, learning_rate 0.000100003

Evaluation:
2017-10-10T15:02:42.616780: step 3520, loss 0.207315, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3520

2017-10-10T15:02:43.695905: step 3521, loss 0.274317, acc 0.90625, learning_rate 0.000100003
2017-10-10T15:02:44.056856: step 3522, loss 0.18694, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:44.276460: step 3523, loss 0.166847, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:02:44.595714: step 3524, loss 0.137518, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:44.861210: step 3525, loss 0.14716, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:45.142866: step 3526, loss 0.0875997, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:45.388891: step 3527, loss 0.102422, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:45.614060: step 3528, loss 0.0475888, acc 0.980392, learning_rate 0.000100003
2017-10-10T15:02:45.904814: step 3529, loss 0.196757, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:02:46.128842: step 3530, loss 0.114653, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:46.417283: step 3531, loss 0.262251, acc 0.890625, learning_rate 0.000100003
2017-10-10T15:02:46.700982: step 3532, loss 0.0663102, acc 1, learning_rate 0.000100003
2017-10-10T15:02:46.960969: step 3533, loss 0.109107, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:47.162453: step 3534, loss 0.197699, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:47.478481: step 3535, loss 0.0717627, acc 1, learning_rate 0.000100003
2017-10-10T15:02:47.676931: step 3536, loss 0.09896, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:02:47.967963: step 3537, loss 0.108803, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:48.240899: step 3538, loss 0.121644, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:48.435419: step 3539, loss 0.16001, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:48.681082: step 3540, loss 0.115094, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:48.870975: step 3541, loss 0.193756, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:02:49.142712: step 3542, loss 0.119127, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:02:49.413222: step 3543, loss 0.106124, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:02:49.701016: step 3544, loss 0.166674, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:02:49.936303: step 3545, loss 0.0840201, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:02:50.148859: step 3546, loss 0.201178, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:02:50.440807: step 3547, loss 0.132325, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:02:50.710648: step 3548, loss 0.143461, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:02:50.976880: step 3549, loss 0.129099, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:02:51.236083: step 3550, loss 0.157477, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:02:51.435037: step 3551, loss 0.210077, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:02:51.672900: step 3552, loss 0.092074, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:02:51.912901: step 3553, loss 0.188071, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:02:52.195750: step 3554, loss 0.0765882, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:02:52.444893: step 3555, loss 0.153264, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:02:52.644061: step 3556, loss 0.058193, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:02:52.856857: step 3557, loss 0.137028, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:02:53.065862: step 3558, loss 0.0800813, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:02:53.320710: step 3559, loss 0.182635, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:02:53.604941: step 3560, loss 0.191904, acc 0.953125, learning_rate 0.000100002

Evaluation:
2017-10-10T15:02:54.103965: step 3560, loss 0.208523, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3560

2017-10-10T15:02:55.085111: step 3561, loss 0.121236, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:02:55.348862: step 3562, loss 0.161399, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:02:55.612324: step 3563, loss 0.148144, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:02:55.829519: step 3564, loss 0.0459996, acc 1, learning_rate 0.000100002
2017-10-10T15:02:56.131889: step 3565, loss 0.069343, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:02:56.397045: step 3566, loss 0.113072, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:02:56.669232: step 3567, loss 0.0853715, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:02:56.960896: step 3568, loss 0.193734, acc 0.90625, learning_rate 0.000100002
2017-10-10T15:02:57.110124: step 3569, loss 0.168912, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:02:57.344846: step 3570, loss 0.15568, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:02:57.511565: step 3571, loss 0.130271, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:02:57.739944: step 3572, loss 0.133544, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:02:57.969429: step 3573, loss 0.189597, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:02:58.226118: step 3574, loss 0.237149, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:02:58.532639: step 3575, loss 0.109802, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:02:58.755696: step 3576, loss 0.210304, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:02:59.008181: step 3577, loss 0.206263, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:02:59.289139: step 3578, loss 0.140987, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:02:59.487566: step 3579, loss 0.127772, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:02:59.748052: step 3580, loss 0.117242, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:00.000913: step 3581, loss 0.212174, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:00.203351: step 3582, loss 0.0990128, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:00.441864: step 3583, loss 0.211684, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:03:00.698239: step 3584, loss 0.161301, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:00.917920: step 3585, loss 0.0851395, acc 1, learning_rate 0.000100002
2017-10-10T15:03:01.148536: step 3586, loss 0.20625, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:03:01.400044: step 3587, loss 0.0844211, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:03:01.705175: step 3588, loss 0.0701195, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:01.889459: step 3589, loss 0.0937468, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:02.118093: step 3590, loss 0.294865, acc 0.875, learning_rate 0.000100002
2017-10-10T15:03:02.394100: step 3591, loss 0.104557, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:02.548000: step 3592, loss 0.140724, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:02.817357: step 3593, loss 0.145823, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:03.016981: step 3594, loss 0.0444284, acc 1, learning_rate 0.000100002
2017-10-10T15:03:03.342910: step 3595, loss 0.112324, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:03.591870: step 3596, loss 0.120673, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:03:03.852943: step 3597, loss 0.2106, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:03:04.092855: step 3598, loss 0.100638, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:04.357422: step 3599, loss 0.10727, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:04.604859: step 3600, loss 0.143569, acc 0.921875, learning_rate 0.000100002

Evaluation:
2017-10-10T15:03:04.974141: step 3600, loss 0.205347, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3600

2017-10-10T15:03:05.933319: step 3601, loss 0.144815, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:06.210481: step 3602, loss 0.152467, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:06.476851: step 3603, loss 0.108228, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:03:06.708947: step 3604, loss 0.070947, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:03:06.932935: step 3605, loss 0.113086, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:07.210083: step 3606, loss 0.116492, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:07.483090: step 3607, loss 0.220892, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:03:07.712913: step 3608, loss 0.113283, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:07.965081: step 3609, loss 0.1207, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:08.208163: step 3610, loss 0.0755449, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:03:08.433039: step 3611, loss 0.193123, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:03:08.724314: step 3612, loss 0.0483742, acc 1, learning_rate 0.000100002
2017-10-10T15:03:08.984206: step 3613, loss 0.142304, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:09.228741: step 3614, loss 0.129296, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:09.477046: step 3615, loss 0.174336, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:09.724013: step 3616, loss 0.181747, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:03:09.976918: step 3617, loss 0.0920866, acc 1, learning_rate 0.000100002
2017-10-10T15:03:10.204915: step 3618, loss 0.100162, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:10.439758: step 3619, loss 0.0979548, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:03:10.674608: step 3620, loss 0.185138, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:03:10.889517: step 3621, loss 0.0925493, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:11.168987: step 3622, loss 0.235961, acc 0.90625, learning_rate 0.000100002
2017-10-10T15:03:11.376329: step 3623, loss 0.133716, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:11.628097: step 3624, loss 0.12008, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:11.868863: step 3625, loss 0.165255, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:12.123957: step 3626, loss 0.222356, acc 0.882353, learning_rate 0.000100002
2017-10-10T15:03:12.401569: step 3627, loss 0.239593, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:03:12.621650: step 3628, loss 0.135207, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:12.877026: step 3629, loss 0.0955461, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:13.143291: step 3630, loss 0.201821, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:03:13.390318: step 3631, loss 0.133769, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:03:13.708887: step 3632, loss 0.150697, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:13.986893: step 3633, loss 0.0657492, acc 1, learning_rate 0.000100002
2017-10-10T15:03:14.153685: step 3634, loss 0.160179, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:14.342060: step 3635, loss 0.117461, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:14.519154: step 3636, loss 0.17031, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:14.730863: step 3637, loss 0.091556, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:14.961062: step 3638, loss 0.160697, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:03:15.201355: step 3639, loss 0.0926986, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:15.441909: step 3640, loss 0.0672306, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-10T15:03:15.816772: step 3640, loss 0.20559, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3640

2017-10-10T15:03:16.749394: step 3641, loss 0.262658, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:16.962769: step 3642, loss 0.245847, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:03:17.215906: step 3643, loss 0.127369, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:17.456981: step 3644, loss 0.084061, acc 1, learning_rate 0.000100002
2017-10-10T15:03:17.734118: step 3645, loss 0.0914327, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:17.994779: step 3646, loss 0.105795, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:03:18.249024: step 3647, loss 0.0681746, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:03:18.472982: step 3648, loss 0.0690693, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:18.737675: step 3649, loss 0.136967, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:03:19.032884: step 3650, loss 0.137257, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:19.189486: step 3651, loss 0.0905112, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:03:19.440398: step 3652, loss 0.195286, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:03:19.669927: step 3653, loss 0.0617771, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:03:19.944130: step 3654, loss 0.230275, acc 0.890625, learning_rate 0.000100002
2017-10-10T15:03:20.140084: step 3655, loss 0.136864, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:20.308726: step 3656, loss 0.108292, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:20.557657: step 3657, loss 0.146592, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:20.786980: step 3658, loss 0.129961, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:21.049104: step 3659, loss 0.111733, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:21.269131: step 3660, loss 0.110118, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:21.508365: step 3661, loss 0.153826, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:21.762073: step 3662, loss 0.17746, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:03:21.970426: step 3663, loss 0.0737552, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:03:22.197147: step 3664, loss 0.217776, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:03:22.440752: step 3665, loss 0.112027, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:03:22.748747: step 3666, loss 0.255837, acc 0.890625, learning_rate 0.000100002
2017-10-10T15:03:22.969096: step 3667, loss 0.0769637, acc 1, learning_rate 0.000100002
2017-10-10T15:03:23.223929: step 3668, loss 0.107657, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:03:23.461086: step 3669, loss 0.0494601, acc 1, learning_rate 0.000100001
2017-10-10T15:03:23.689968: step 3670, loss 0.118409, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:24.014164: step 3671, loss 0.145755, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:24.299763: step 3672, loss 0.150737, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:24.516818: step 3673, loss 0.228709, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:24.778211: step 3674, loss 0.102727, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:24.999926: step 3675, loss 0.0943822, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:25.225423: step 3676, loss 0.076112, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:25.541728: step 3677, loss 0.122076, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:25.780137: step 3678, loss 0.128671, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:26.026909: step 3679, loss 0.0577222, acc 1, learning_rate 0.000100001
2017-10-10T15:03:26.300903: step 3680, loss 0.165901, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T15:03:26.689310: step 3680, loss 0.204925, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3680

2017-10-10T15:03:27.778074: step 3681, loss 0.0794344, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:28.007697: step 3682, loss 0.0630571, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:28.215501: step 3683, loss 0.119003, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:28.485158: step 3684, loss 0.0845438, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:28.717152: step 3685, loss 0.20138, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:28.935382: step 3686, loss 0.143066, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:29.208917: step 3687, loss 0.190333, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:29.400815: step 3688, loss 0.123467, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:29.716944: step 3689, loss 0.116706, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:30.033193: step 3690, loss 0.0635236, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:30.295070: step 3691, loss 0.138394, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:30.556930: step 3692, loss 0.0898452, acc 1, learning_rate 0.000100001
2017-10-10T15:03:30.784168: step 3693, loss 0.130941, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:31.043375: step 3694, loss 0.23828, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:03:31.269067: step 3695, loss 0.0948513, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:31.527804: step 3696, loss 0.222173, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:31.785031: step 3697, loss 0.192737, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:32.079997: step 3698, loss 0.154267, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:32.278487: step 3699, loss 0.189107, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:32.560971: step 3700, loss 0.106301, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:32.748531: step 3701, loss 0.253095, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:33.006177: step 3702, loss 0.260855, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:03:33.318688: step 3703, loss 0.127369, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:33.571032: step 3704, loss 0.17918, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:33.820959: step 3705, loss 0.158596, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:03:34.096964: step 3706, loss 0.0799993, acc 1, learning_rate 0.000100001
2017-10-10T15:03:34.302338: step 3707, loss 0.087504, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:34.528966: step 3708, loss 0.107892, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:34.792866: step 3709, loss 0.0983809, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:35.069530: step 3710, loss 0.0915256, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:35.297589: step 3711, loss 0.16062, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:35.569184: step 3712, loss 0.111711, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:35.835723: step 3713, loss 0.0889619, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:36.051962: step 3714, loss 0.103501, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:36.358284: step 3715, loss 0.0919291, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:36.568750: step 3716, loss 0.0883122, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:36.844661: step 3717, loss 0.158413, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:03:37.072924: step 3718, loss 0.103901, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:37.376880: step 3719, loss 0.154245, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:37.624521: step 3720, loss 0.220185, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-10-10T15:03:38.157208: step 3720, loss 0.204744, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3720

2017-10-10T15:03:39.239807: step 3721, loss 0.129944, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:39.436949: step 3722, loss 0.171149, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:39.601059: step 3723, loss 0.09837, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:39.820302: step 3724, loss 0.147537, acc 0.960784, learning_rate 0.000100001
2017-10-10T15:03:40.070307: step 3725, loss 0.0910991, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:40.312974: step 3726, loss 0.166266, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:40.551258: step 3727, loss 0.159884, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:03:40.776914: step 3728, loss 0.105076, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:41.039424: step 3729, loss 0.0801911, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:41.244995: step 3730, loss 0.193517, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:41.545014: step 3731, loss 0.151284, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:03:41.764814: step 3732, loss 0.2117, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:03:42.017874: step 3733, loss 0.070154, acc 1, learning_rate 0.000100001
2017-10-10T15:03:42.241125: step 3734, loss 0.23335, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:03:42.568392: step 3735, loss 0.105673, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:42.836841: step 3736, loss 0.137326, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:03:43.057797: step 3737, loss 0.119043, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:43.319371: step 3738, loss 0.160952, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:43.539636: step 3739, loss 0.103372, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:43.806108: step 3740, loss 0.289612, acc 0.890625, learning_rate 0.000100001
2017-10-10T15:03:44.054291: step 3741, loss 0.136094, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:44.316848: step 3742, loss 0.192496, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:03:44.584804: step 3743, loss 0.0822232, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:44.860857: step 3744, loss 0.143553, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:45.097083: step 3745, loss 0.179598, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:45.327198: step 3746, loss 0.151573, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:45.526001: step 3747, loss 0.118264, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:45.765084: step 3748, loss 0.102152, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:45.973833: step 3749, loss 0.130922, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:46.222814: step 3750, loss 0.103644, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:46.522879: step 3751, loss 0.113128, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:46.754545: step 3752, loss 0.154204, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:03:47.068320: step 3753, loss 0.0678902, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:47.322636: step 3754, loss 0.0934567, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:47.492940: step 3755, loss 0.0581869, acc 1, learning_rate 0.000100001
2017-10-10T15:03:47.712852: step 3756, loss 0.136457, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:47.914248: step 3757, loss 0.15488, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:48.216105: step 3758, loss 0.0801921, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:48.425378: step 3759, loss 0.20642, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:03:48.623795: step 3760, loss 0.18831, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-10-10T15:03:49.008912: step 3760, loss 0.206979, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3760

2017-10-10T15:03:50.051065: step 3761, loss 0.172099, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:50.311548: step 3762, loss 0.0789606, acc 1, learning_rate 0.000100001
2017-10-10T15:03:50.569381: step 3763, loss 0.0847285, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:50.793100: step 3764, loss 0.166314, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:51.062907: step 3765, loss 0.142514, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:03:51.305825: step 3766, loss 0.074382, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:51.553210: step 3767, loss 0.0645313, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:51.800468: step 3768, loss 0.152801, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:03:52.093471: step 3769, loss 0.16635, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:52.323630: step 3770, loss 0.140758, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:52.537074: step 3771, loss 0.155146, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:52.818817: step 3772, loss 0.204109, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:03:53.035786: step 3773, loss 0.0746583, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:53.326126: step 3774, loss 0.188832, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:53.608376: step 3775, loss 0.114709, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:53.867771: step 3776, loss 0.0945417, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:54.094173: step 3777, loss 0.0357548, acc 1, learning_rate 0.000100001
2017-10-10T15:03:54.382408: step 3778, loss 0.110974, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:54.675329: step 3779, loss 0.0706835, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:54.977612: step 3780, loss 0.0829307, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:55.233953: step 3781, loss 0.063349, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:55.464974: step 3782, loss 0.206939, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:03:55.637016: step 3783, loss 0.107095, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:55.892835: step 3784, loss 0.0837591, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:56.161306: step 3785, loss 0.105942, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:03:56.480959: step 3786, loss 0.0771656, acc 1, learning_rate 0.000100001
2017-10-10T15:03:56.797049: step 3787, loss 0.216832, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:57.040982: step 3788, loss 0.0905434, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:57.266571: step 3789, loss 0.242087, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:57.459618: step 3790, loss 0.203226, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:03:57.653293: step 3791, loss 0.136257, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:57.906982: step 3792, loss 0.166084, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:58.169694: step 3793, loss 0.152415, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:58.408499: step 3794, loss 0.146715, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:58.660694: step 3795, loss 0.175792, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:58.908949: step 3796, loss 0.25917, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:03:59.156985: step 3797, loss 0.175712, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:59.397648: step 3798, loss 0.0951152, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:03:59.613193: step 3799, loss 0.149091, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:03:59.880824: step 3800, loss 0.106824, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T15:04:00.308920: step 3800, loss 0.205414, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3800

2017-10-10T15:04:01.300841: step 3801, loss 0.0619622, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:01.488885: step 3802, loss 0.154447, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:04:01.766863: step 3803, loss 0.121833, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:02.073889: step 3804, loss 0.174511, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:02.282258: step 3805, loss 0.102677, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:02.545285: step 3806, loss 0.11053, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:02.804527: step 3807, loss 0.151657, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:03.050245: step 3808, loss 0.112031, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:03.375570: step 3809, loss 0.148977, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:03.569572: step 3810, loss 0.137273, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:03.788693: step 3811, loss 0.107009, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:04.008867: step 3812, loss 0.125168, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:04.216861: step 3813, loss 0.177748, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:04.477043: step 3814, loss 0.167881, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:04.745643: step 3815, loss 0.066511, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:04.919891: step 3816, loss 0.12047, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:05.172851: step 3817, loss 0.0815449, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:05.453007: step 3818, loss 0.104111, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:05.697402: step 3819, loss 0.16585, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:05.967526: step 3820, loss 0.139163, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:06.231988: step 3821, loss 0.136393, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:06.460362: step 3822, loss 0.135352, acc 0.921569, learning_rate 0.000100001
2017-10-10T15:04:06.716906: step 3823, loss 0.172812, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:07.060856: step 3824, loss 0.117566, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:07.284109: step 3825, loss 0.105244, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:07.540875: step 3826, loss 0.106548, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:07.793373: step 3827, loss 0.156226, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:08.046783: step 3828, loss 0.0749437, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:08.324189: step 3829, loss 0.148742, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:08.568830: step 3830, loss 0.0826142, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:08.820925: step 3831, loss 0.0652368, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:09.077144: step 3832, loss 0.0737941, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:09.294792: step 3833, loss 0.14034, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:09.568849: step 3834, loss 0.10651, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:09.772501: step 3835, loss 0.12092, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:10.066332: step 3836, loss 0.209994, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:10.411571: step 3837, loss 0.222096, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:04:10.623082: step 3838, loss 0.0853608, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:10.883647: step 3839, loss 0.109572, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:11.126809: step 3840, loss 0.064034, acc 1, learning_rate 0.000100001

Evaluation:
2017-10-10T15:04:11.579359: step 3840, loss 0.20412, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3840

2017-10-10T15:04:12.584962: step 3841, loss 0.0931276, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:12.835334: step 3842, loss 0.126706, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:13.096985: step 3843, loss 0.134609, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:13.354576: step 3844, loss 0.103907, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:13.609194: step 3845, loss 0.146219, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:13.900144: step 3846, loss 0.118109, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:14.153093: step 3847, loss 0.183392, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:14.416930: step 3848, loss 0.0835501, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:14.690488: step 3849, loss 0.0782143, acc 1, learning_rate 0.000100001
2017-10-10T15:04:14.877073: step 3850, loss 0.14617, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:15.060278: step 3851, loss 0.123697, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:15.281346: step 3852, loss 0.250783, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:15.461301: step 3853, loss 0.113809, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:15.716959: step 3854, loss 0.160404, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:15.964966: step 3855, loss 0.173372, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:16.163751: step 3856, loss 0.177513, acc 0.890625, learning_rate 0.000100001
2017-10-10T15:04:16.409046: step 3857, loss 0.112397, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:16.670376: step 3858, loss 0.133303, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:16.940819: step 3859, loss 0.0968094, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:17.229117: step 3860, loss 0.0978259, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:17.448963: step 3861, loss 0.137376, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:17.736923: step 3862, loss 0.135647, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:18.033364: step 3863, loss 0.135343, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:18.270882: step 3864, loss 0.188463, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:04:18.508314: step 3865, loss 0.135985, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:18.764166: step 3866, loss 0.0625016, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:18.972155: step 3867, loss 0.227583, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:19.213912: step 3868, loss 0.122508, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:19.516876: step 3869, loss 0.183788, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:19.772436: step 3870, loss 0.0871778, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:20.032831: step 3871, loss 0.176825, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:20.277284: step 3872, loss 0.138692, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:20.460968: step 3873, loss 0.0797808, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:20.672943: step 3874, loss 0.188554, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:20.884818: step 3875, loss 0.112724, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:21.098082: step 3876, loss 0.187789, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:21.361512: step 3877, loss 0.113067, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:21.589168: step 3878, loss 0.0946794, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:21.857591: step 3879, loss 0.249428, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:04:22.044130: step 3880, loss 0.098479, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T15:04:22.481758: step 3880, loss 0.205433, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3880

2017-10-10T15:04:23.573182: step 3881, loss 0.0513844, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:23.829876: step 3882, loss 0.265513, acc 0.859375, learning_rate 0.000100001
2017-10-10T15:04:24.113118: step 3883, loss 0.131162, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:24.377913: step 3884, loss 0.132313, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:24.592033: step 3885, loss 0.141248, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:24.904164: step 3886, loss 0.1073, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:25.116838: step 3887, loss 0.127415, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:25.337503: step 3888, loss 0.0828319, acc 1, learning_rate 0.000100001
2017-10-10T15:04:25.596837: step 3889, loss 0.0744189, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:25.772191: step 3890, loss 0.0692537, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:26.084582: step 3891, loss 0.105586, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:26.297039: step 3892, loss 0.234914, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:26.552978: step 3893, loss 0.133032, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:26.797018: step 3894, loss 0.158321, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:27.017084: step 3895, loss 0.164203, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:27.272849: step 3896, loss 0.156135, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:27.541972: step 3897, loss 0.214002, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:27.784026: step 3898, loss 0.105372, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:28.076827: step 3899, loss 0.11071, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:28.331860: step 3900, loss 0.12172, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:28.594076: step 3901, loss 0.177817, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:28.910877: step 3902, loss 0.186255, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:29.149401: step 3903, loss 0.194388, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:04:29.388836: step 3904, loss 0.172667, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:04:29.668900: step 3905, loss 0.123979, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:29.928094: step 3906, loss 0.145886, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:30.200864: step 3907, loss 0.203779, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:04:30.437072: step 3908, loss 0.144346, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:30.654410: step 3909, loss 0.0341186, acc 1, learning_rate 0.000100001
2017-10-10T15:04:30.900810: step 3910, loss 0.0652447, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:31.124237: step 3911, loss 0.196541, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:04:31.348975: step 3912, loss 0.0967109, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:31.661478: step 3913, loss 0.159389, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:31.957121: step 3914, loss 0.112008, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:32.260664: step 3915, loss 0.199319, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:32.476840: step 3916, loss 0.0711925, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:32.666994: step 3917, loss 0.150972, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:32.860867: step 3918, loss 0.0581194, acc 1, learning_rate 0.000100001
2017-10-10T15:04:33.052815: step 3919, loss 0.139441, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:33.282982: step 3920, loss 0.136511, acc 0.941176, learning_rate 0.000100001

Evaluation:
2017-10-10T15:04:33.787685: step 3920, loss 0.203328, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3920

2017-10-10T15:04:34.816825: step 3921, loss 0.123436, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:35.149486: step 3922, loss 0.111356, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:35.376873: step 3923, loss 0.0819477, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:35.638931: step 3924, loss 0.137608, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:35.885042: step 3925, loss 0.132024, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:36.163369: step 3926, loss 0.128194, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:36.425681: step 3927, loss 0.181669, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:36.644810: step 3928, loss 0.198819, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:36.873490: step 3929, loss 0.141532, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:37.103497: step 3930, loss 0.0866136, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:04:37.312831: step 3931, loss 0.144386, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:37.550363: step 3932, loss 0.148484, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:04:37.835886: step 3933, loss 0.117146, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:04:38.106617: step 3934, loss 0.119711, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:38.358419: step 3935, loss 0.167591, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:38.585245: step 3936, loss 0.105966, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:04:38.852109: step 3937, loss 0.118776, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:39.097256: step 3938, loss 0.0988597, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:39.339061: step 3939, loss 0.111147, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:39.614932: step 3940, loss 0.0939321, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:39.822834: step 3941, loss 0.126216, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:40.086265: step 3942, loss 0.16951, acc 0.9375, learning_rate 0.0001
2017-10-10T15:04:40.333199: step 3943, loss 0.216309, acc 0.9375, learning_rate 0.0001
2017-10-10T15:04:40.574893: step 3944, loss 0.0455335, acc 1, learning_rate 0.0001
2017-10-10T15:04:40.843913: step 3945, loss 0.0919999, acc 0.984375, learning_rate 0.0001
2017-10-10T15:04:41.039871: step 3946, loss 0.129172, acc 0.984375, learning_rate 0.0001
2017-10-10T15:04:41.268187: step 3947, loss 0.0893789, acc 0.984375, learning_rate 0.0001
2017-10-10T15:04:41.444800: step 3948, loss 0.124813, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:41.736164: step 3949, loss 0.117482, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:41.984888: step 3950, loss 0.128466, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:42.256986: step 3951, loss 0.139819, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:42.509876: step 3952, loss 0.08448, acc 0.984375, learning_rate 0.0001
2017-10-10T15:04:42.773253: step 3953, loss 0.0933052, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:43.011377: step 3954, loss 0.0920945, acc 0.984375, learning_rate 0.0001
2017-10-10T15:04:43.260889: step 3955, loss 0.135357, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:43.488859: step 3956, loss 0.0350494, acc 1, learning_rate 0.0001
2017-10-10T15:04:43.765096: step 3957, loss 0.0535629, acc 0.984375, learning_rate 0.0001
2017-10-10T15:04:43.986572: step 3958, loss 0.19847, acc 0.9375, learning_rate 0.0001
2017-10-10T15:04:44.245465: step 3959, loss 0.10481, acc 0.984375, learning_rate 0.0001
2017-10-10T15:04:44.433062: step 3960, loss 0.154051, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:04:44.808792: step 3960, loss 0.203932, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-3960

2017-10-10T15:04:45.780883: step 3961, loss 0.160047, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:46.012946: step 3962, loss 0.125299, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:46.241136: step 3963, loss 0.0998452, acc 0.984375, learning_rate 0.0001
2017-10-10T15:04:46.520542: step 3964, loss 0.144971, acc 0.921875, learning_rate 0.0001
2017-10-10T15:04:46.776874: step 3965, loss 0.160119, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:47.028880: step 3966, loss 0.0754103, acc 1, learning_rate 0.0001
2017-10-10T15:04:47.269750: step 3967, loss 0.0970041, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:47.524956: step 3968, loss 0.0718689, acc 0.984375, learning_rate 0.0001
2017-10-10T15:04:47.772849: step 3969, loss 0.127787, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:48.065056: step 3970, loss 0.118096, acc 0.9375, learning_rate 0.0001
2017-10-10T15:04:48.334294: step 3971, loss 0.202961, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:48.593166: step 3972, loss 0.159096, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:48.824363: step 3973, loss 0.131386, acc 0.9375, learning_rate 0.0001
2017-10-10T15:04:49.086011: step 3974, loss 0.168929, acc 0.921875, learning_rate 0.0001
2017-10-10T15:04:49.330573: step 3975, loss 0.0756242, acc 1, learning_rate 0.0001
2017-10-10T15:04:49.558779: step 3976, loss 0.0690338, acc 1, learning_rate 0.0001
2017-10-10T15:04:49.812862: step 3977, loss 0.0356478, acc 1, learning_rate 0.0001
2017-10-10T15:04:50.056924: step 3978, loss 0.110419, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:50.381057: step 3979, loss 0.0643105, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:50.577965: step 3980, loss 0.148089, acc 0.9375, learning_rate 0.0001
2017-10-10T15:04:50.771134: step 3981, loss 0.0795299, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:50.996935: step 3982, loss 0.0743496, acc 0.984375, learning_rate 0.0001
2017-10-10T15:04:51.251366: step 3983, loss 0.116408, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:51.499347: step 3984, loss 0.091508, acc 1, learning_rate 0.0001
2017-10-10T15:04:51.748847: step 3985, loss 0.0838294, acc 0.984375, learning_rate 0.0001
2017-10-10T15:04:52.036915: step 3986, loss 0.0896805, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:52.323826: step 3987, loss 0.056603, acc 1, learning_rate 0.0001
2017-10-10T15:04:52.518925: step 3988, loss 0.158154, acc 0.9375, learning_rate 0.0001
2017-10-10T15:04:52.788823: step 3989, loss 0.182114, acc 0.921875, learning_rate 0.0001
2017-10-10T15:04:52.991597: step 3990, loss 0.149042, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:53.312029: step 3991, loss 0.116616, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:53.456924: step 3992, loss 0.0758802, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:53.732870: step 3993, loss 0.122106, acc 0.9375, learning_rate 0.0001
2017-10-10T15:04:53.956907: step 3994, loss 0.147041, acc 0.921875, learning_rate 0.0001
2017-10-10T15:04:54.170623: step 3995, loss 0.220223, acc 0.921875, learning_rate 0.0001
2017-10-10T15:04:54.401507: step 3996, loss 0.119198, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:54.664906: step 3997, loss 0.206787, acc 0.90625, learning_rate 0.0001
2017-10-10T15:04:54.859699: step 3998, loss 0.161355, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:55.102341: step 3999, loss 0.135886, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:55.306347: step 4000, loss 0.0627162, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:04:55.735850: step 4000, loss 0.20447, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4000

2017-10-10T15:04:56.760557: step 4001, loss 0.219853, acc 0.9375, learning_rate 0.0001
2017-10-10T15:04:56.959690: step 4002, loss 0.177179, acc 0.9375, learning_rate 0.0001
2017-10-10T15:04:57.224909: step 4003, loss 0.0756911, acc 1, learning_rate 0.0001
2017-10-10T15:04:57.441061: step 4004, loss 0.16241, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:57.701149: step 4005, loss 0.106914, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:57.986346: step 4006, loss 0.11743, acc 0.96875, learning_rate 0.0001
2017-10-10T15:04:58.182092: step 4007, loss 0.139456, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:58.444927: step 4008, loss 0.168344, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:58.713154: step 4009, loss 0.164343, acc 0.921875, learning_rate 0.0001
2017-10-10T15:04:58.912851: step 4010, loss 0.142655, acc 0.9375, learning_rate 0.0001
2017-10-10T15:04:59.200867: step 4011, loss 0.206734, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:59.398613: step 4012, loss 0.193301, acc 0.953125, learning_rate 0.0001
2017-10-10T15:04:59.666942: step 4013, loss 0.107092, acc 0.9375, learning_rate 0.0001
2017-10-10T15:04:59.867217: step 4014, loss 0.169358, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:00.157006: step 4015, loss 0.259256, acc 0.921875, learning_rate 0.0001
2017-10-10T15:05:00.434240: step 4016, loss 0.097205, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:00.671146: step 4017, loss 0.0982079, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:00.908846: step 4018, loss 0.11196, acc 0.941176, learning_rate 0.0001
2017-10-10T15:05:01.131829: step 4019, loss 0.0686273, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:01.337356: step 4020, loss 0.108826, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:01.588829: step 4021, loss 0.141796, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:01.807355: step 4022, loss 0.246999, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:02.098093: step 4023, loss 0.0530274, acc 1, learning_rate 0.0001
2017-10-10T15:05:02.340581: step 4024, loss 0.138662, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:02.547172: step 4025, loss 0.116867, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:02.820314: step 4026, loss 0.242151, acc 0.90625, learning_rate 0.0001
2017-10-10T15:05:03.100896: step 4027, loss 0.161683, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:03.361110: step 4028, loss 0.0741628, acc 1, learning_rate 0.0001
2017-10-10T15:05:03.609001: step 4029, loss 0.0879325, acc 1, learning_rate 0.0001
2017-10-10T15:05:03.888962: step 4030, loss 0.120068, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:04.203317: step 4031, loss 0.168492, acc 0.921875, learning_rate 0.0001
2017-10-10T15:05:04.488824: step 4032, loss 0.187038, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:04.691156: step 4033, loss 0.0688765, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:04.992830: step 4034, loss 0.134614, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:05.301112: step 4035, loss 0.12845, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:05.560818: step 4036, loss 0.167795, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:05.827787: step 4037, loss 0.10458, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:06.019790: step 4038, loss 0.0955835, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:06.259766: step 4039, loss 0.167469, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:06.480526: step 4040, loss 0.111538, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:05:06.860840: step 4040, loss 0.204007, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4040

2017-10-10T15:05:08.077853: step 4041, loss 0.0849007, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:08.305088: step 4042, loss 0.0568859, acc 1, learning_rate 0.0001
2017-10-10T15:05:08.577173: step 4043, loss 0.194935, acc 0.921875, learning_rate 0.0001
2017-10-10T15:05:08.808976: step 4044, loss 0.10123, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:09.000921: step 4045, loss 0.105006, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:09.207774: step 4046, loss 0.22968, acc 0.90625, learning_rate 0.0001
2017-10-10T15:05:09.420345: step 4047, loss 0.140949, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:09.741655: step 4048, loss 0.126603, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:09.977076: step 4049, loss 0.0615224, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:10.236820: step 4050, loss 0.149145, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:10.440923: step 4051, loss 0.125964, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:10.626734: step 4052, loss 0.142263, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:10.815662: step 4053, loss 0.139438, acc 0.921875, learning_rate 0.0001
2017-10-10T15:05:11.036042: step 4054, loss 0.0865921, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:11.258294: step 4055, loss 0.158754, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:11.532866: step 4056, loss 0.127328, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:11.794504: step 4057, loss 0.220013, acc 0.921875, learning_rate 0.0001
2017-10-10T15:05:12.013886: step 4058, loss 0.12291, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:12.310900: step 4059, loss 0.113387, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:12.575499: step 4060, loss 0.0984638, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:12.754068: step 4061, loss 0.146363, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:13.024782: step 4062, loss 0.138446, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:13.344936: step 4063, loss 0.0946198, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:13.580981: step 4064, loss 0.117598, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:13.809041: step 4065, loss 0.092351, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:14.072411: step 4066, loss 0.105634, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:14.337114: step 4067, loss 0.19792, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:14.602431: step 4068, loss 0.0417509, acc 1, learning_rate 0.0001
2017-10-10T15:05:14.885895: step 4069, loss 0.0605292, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:15.115035: step 4070, loss 0.110957, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:15.372942: step 4071, loss 0.219917, acc 0.890625, learning_rate 0.0001
2017-10-10T15:05:15.624847: step 4072, loss 0.179818, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:15.903789: step 4073, loss 0.115216, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:16.129012: step 4074, loss 0.158698, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:16.399659: step 4075, loss 0.21099, acc 0.90625, learning_rate 0.0001
2017-10-10T15:05:16.712900: step 4076, loss 0.135323, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:16.947565: step 4077, loss 0.0889202, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:17.212441: step 4078, loss 0.154377, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:17.438926: step 4079, loss 0.178239, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:17.658835: step 4080, loss 0.176406, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T15:05:18.130629: step 4080, loss 0.205697, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4080

2017-10-10T15:05:19.172872: step 4081, loss 0.147681, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:19.421061: step 4082, loss 0.122221, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:19.647928: step 4083, loss 0.0519887, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:19.838130: step 4084, loss 0.0568394, acc 1, learning_rate 0.0001
2017-10-10T15:05:20.075279: step 4085, loss 0.257743, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:20.329729: step 4086, loss 0.102464, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:20.592879: step 4087, loss 0.0564476, acc 1, learning_rate 0.0001
2017-10-10T15:05:20.833381: step 4088, loss 0.0580151, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:21.036469: step 4089, loss 0.160685, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:21.280077: step 4090, loss 0.190096, acc 0.90625, learning_rate 0.0001
2017-10-10T15:05:21.483099: step 4091, loss 0.172016, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:21.688712: step 4092, loss 0.0820886, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:21.953596: step 4093, loss 0.101527, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:22.173848: step 4094, loss 0.115312, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:22.439847: step 4095, loss 0.120156, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:22.626173: step 4096, loss 0.0968146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:22.914948: step 4097, loss 0.0869715, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:23.212947: step 4098, loss 0.115534, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:23.425178: step 4099, loss 0.0763635, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:23.638333: step 4100, loss 0.0866513, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:23.885085: step 4101, loss 0.169182, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:24.149982: step 4102, loss 0.102578, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:24.374574: step 4103, loss 0.0880366, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:24.612852: step 4104, loss 0.158683, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:24.844969: step 4105, loss 0.23283, acc 0.921875, learning_rate 0.0001
2017-10-10T15:05:25.080174: step 4106, loss 0.123768, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:25.329956: step 4107, loss 0.0931897, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:25.676355: step 4108, loss 0.144781, acc 0.921875, learning_rate 0.0001
2017-10-10T15:05:25.896887: step 4109, loss 0.166881, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:26.220879: step 4110, loss 0.0803381, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:26.484986: step 4111, loss 0.136097, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:26.814677: step 4112, loss 0.0974146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:26.983330: step 4113, loss 0.142594, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:27.168988: step 4114, loss 0.166551, acc 0.921875, learning_rate 0.0001
2017-10-10T15:05:27.346072: step 4115, loss 0.19735, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:27.488920: step 4116, loss 0.136233, acc 0.960784, learning_rate 0.0001
2017-10-10T15:05:27.750872: step 4117, loss 0.0788256, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:27.974872: step 4118, loss 0.0679164, acc 1, learning_rate 0.0001
2017-10-10T15:05:28.271682: step 4119, loss 0.0645265, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:28.469131: step 4120, loss 0.0897174, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:05:28.879213: step 4120, loss 0.203474, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4120

2017-10-10T15:05:29.808902: step 4121, loss 0.163095, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:30.028315: step 4122, loss 0.0811326, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:30.284842: step 4123, loss 0.178508, acc 0.921875, learning_rate 0.0001
2017-10-10T15:05:30.495325: step 4124, loss 0.143073, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:30.720860: step 4125, loss 0.113487, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:31.000873: step 4126, loss 0.129918, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:31.248884: step 4127, loss 0.15775, acc 0.921875, learning_rate 0.0001
2017-10-10T15:05:31.472240: step 4128, loss 0.202165, acc 0.921875, learning_rate 0.0001
2017-10-10T15:05:31.697892: step 4129, loss 0.113955, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:31.956974: step 4130, loss 0.0680412, acc 1, learning_rate 0.0001
2017-10-10T15:05:32.188985: step 4131, loss 0.0766115, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:32.391156: step 4132, loss 0.106457, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:32.584508: step 4133, loss 0.167145, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:32.841828: step 4134, loss 0.13898, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:33.192282: step 4135, loss 0.0864935, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:33.397836: step 4136, loss 0.147887, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:33.655605: step 4137, loss 0.173742, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:33.844111: step 4138, loss 0.232331, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:34.072069: step 4139, loss 0.102929, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:34.413566: step 4140, loss 0.120047, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:34.651080: step 4141, loss 0.125656, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:34.925349: step 4142, loss 0.111644, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:35.188903: step 4143, loss 0.042715, acc 1, learning_rate 0.0001
2017-10-10T15:05:35.404749: step 4144, loss 0.057386, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:35.608959: step 4145, loss 0.0833335, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:35.881596: step 4146, loss 0.137687, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:36.189138: step 4147, loss 0.0710384, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:36.440840: step 4148, loss 0.0991804, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:36.685588: step 4149, loss 0.0916751, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:36.960062: step 4150, loss 0.112241, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:37.219469: step 4151, loss 0.0460607, acc 1, learning_rate 0.0001
2017-10-10T15:05:37.452997: step 4152, loss 0.0932349, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:37.725223: step 4153, loss 0.105336, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:37.974813: step 4154, loss 0.126904, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:38.176858: step 4155, loss 0.100311, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:38.369057: step 4156, loss 0.0494526, acc 1, learning_rate 0.0001
2017-10-10T15:05:38.618101: step 4157, loss 0.0738249, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:38.835615: step 4158, loss 0.134816, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:39.069094: step 4159, loss 0.19482, acc 0.90625, learning_rate 0.0001
2017-10-10T15:05:39.368882: step 4160, loss 0.106148, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:05:39.732422: step 4160, loss 0.202784, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4160

2017-10-10T15:05:40.809794: step 4161, loss 0.122013, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:41.102940: step 4162, loss 0.059577, acc 1, learning_rate 0.0001
2017-10-10T15:05:41.296867: step 4163, loss 0.0535906, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:41.516930: step 4164, loss 0.117526, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:41.800882: step 4165, loss 0.0831289, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:42.036248: step 4166, loss 0.141275, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:42.285912: step 4167, loss 0.0984485, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:42.547183: step 4168, loss 0.191369, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:42.788730: step 4169, loss 0.182436, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:43.039198: step 4170, loss 0.0883147, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:43.347559: step 4171, loss 0.0983354, acc 1, learning_rate 0.0001
2017-10-10T15:05:43.600918: step 4172, loss 0.138958, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:43.880391: step 4173, loss 0.0822218, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:44.104934: step 4174, loss 0.100627, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:44.347401: step 4175, loss 0.0880911, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:44.616311: step 4176, loss 0.201756, acc 0.921875, learning_rate 0.0001
2017-10-10T15:05:44.867897: step 4177, loss 0.113252, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:45.059320: step 4178, loss 0.0883904, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:45.315890: step 4179, loss 0.135328, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:45.572753: step 4180, loss 0.193074, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:45.743348: step 4181, loss 0.165217, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:45.884257: step 4182, loss 0.233124, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:46.101873: step 4183, loss 0.0998276, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:46.353894: step 4184, loss 0.151256, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:46.577665: step 4185, loss 0.162607, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:46.833156: step 4186, loss 0.10863, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:47.064567: step 4187, loss 0.152695, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:47.256179: step 4188, loss 0.172214, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:47.524710: step 4189, loss 0.0946408, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:47.780763: step 4190, loss 0.191933, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:47.992166: step 4191, loss 0.0958999, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:48.203084: step 4192, loss 0.238615, acc 0.875, learning_rate 0.0001
2017-10-10T15:05:48.455865: step 4193, loss 0.148107, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:48.756855: step 4194, loss 0.108871, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:49.086944: step 4195, loss 0.258673, acc 0.921875, learning_rate 0.0001
2017-10-10T15:05:49.318792: step 4196, loss 0.0521022, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:49.541769: step 4197, loss 0.220354, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:49.859203: step 4198, loss 0.139534, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:50.117068: step 4199, loss 0.19495, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:50.368349: step 4200, loss 0.136582, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:05:50.824917: step 4200, loss 0.201538, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4200

2017-10-10T15:05:51.879174: step 4201, loss 0.069078, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:52.071590: step 4202, loss 0.155881, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:52.344942: step 4203, loss 0.100072, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:52.554209: step 4204, loss 0.106322, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:52.840952: step 4205, loss 0.199566, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:53.084122: step 4206, loss 0.265898, acc 0.890625, learning_rate 0.0001
2017-10-10T15:05:53.299517: step 4207, loss 0.0808512, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:53.571011: step 4208, loss 0.0807229, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:53.774798: step 4209, loss 0.187341, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:54.066307: step 4210, loss 0.186635, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:54.304677: step 4211, loss 0.159574, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:54.573144: step 4212, loss 0.0830983, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:54.802243: step 4213, loss 0.136005, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:55.004380: step 4214, loss 0.269121, acc 0.921569, learning_rate 0.0001
2017-10-10T15:05:55.211916: step 4215, loss 0.129343, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:55.474923: step 4216, loss 0.0931684, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:55.752865: step 4217, loss 0.123408, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:55.975557: step 4218, loss 0.110109, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:56.259176: step 4219, loss 0.242139, acc 0.890625, learning_rate 0.0001
2017-10-10T15:05:56.505860: step 4220, loss 0.0526619, acc 1, learning_rate 0.0001
2017-10-10T15:05:56.720776: step 4221, loss 0.21987, acc 0.890625, learning_rate 0.0001
2017-10-10T15:05:57.014175: step 4222, loss 0.102315, acc 1, learning_rate 0.0001
2017-10-10T15:05:57.190700: step 4223, loss 0.061135, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:57.459042: step 4224, loss 0.147297, acc 0.9375, learning_rate 0.0001
2017-10-10T15:05:57.691356: step 4225, loss 0.110766, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:57.956469: step 4226, loss 0.129776, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:58.178881: step 4227, loss 0.203228, acc 0.921875, learning_rate 0.0001
2017-10-10T15:05:58.371456: step 4228, loss 0.13341, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:58.696098: step 4229, loss 0.12542, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:58.903869: step 4230, loss 0.120895, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:59.139070: step 4231, loss 0.0862141, acc 0.96875, learning_rate 0.0001
2017-10-10T15:05:59.366105: step 4232, loss 0.098353, acc 0.984375, learning_rate 0.0001
2017-10-10T15:05:59.572823: step 4233, loss 0.112772, acc 0.953125, learning_rate 0.0001
2017-10-10T15:05:59.957553: step 4234, loss 0.0677801, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:00.244941: step 4235, loss 0.0819747, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:00.472833: step 4236, loss 0.094978, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:00.709208: step 4237, loss 0.116375, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:00.880833: step 4238, loss 0.119286, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:01.109795: step 4239, loss 0.0761946, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:01.407674: step 4240, loss 0.173355, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:06:01.778877: step 4240, loss 0.19955, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4240

2017-10-10T15:06:02.693210: step 4241, loss 0.116368, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:02.983624: step 4242, loss 0.143317, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:03.187997: step 4243, loss 0.148369, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:03.409651: step 4244, loss 0.0899459, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:03.671157: step 4245, loss 0.0717294, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:03.868942: step 4246, loss 0.129258, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:04.122767: step 4247, loss 0.143418, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:04.393006: step 4248, loss 0.05378, acc 1, learning_rate 0.0001
2017-10-10T15:06:04.615358: step 4249, loss 0.125239, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:04.824027: step 4250, loss 0.111378, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:05.105626: step 4251, loss 0.144436, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:05.404870: step 4252, loss 0.0250643, acc 1, learning_rate 0.0001
2017-10-10T15:06:05.602334: step 4253, loss 0.118012, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:05.855657: step 4254, loss 0.197893, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:06.113038: step 4255, loss 0.0762715, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:06.319033: step 4256, loss 0.103542, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:06.560968: step 4257, loss 0.0648191, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:06.776985: step 4258, loss 0.108202, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:07.036244: step 4259, loss 0.160387, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:07.205471: step 4260, loss 0.26192, acc 0.90625, learning_rate 0.0001
2017-10-10T15:06:07.469051: step 4261, loss 0.0701866, acc 1, learning_rate 0.0001
2017-10-10T15:06:07.718313: step 4262, loss 0.118467, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:07.975098: step 4263, loss 0.129343, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:08.208069: step 4264, loss 0.196366, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:08.484870: step 4265, loss 0.0454921, acc 1, learning_rate 0.0001
2017-10-10T15:06:08.764849: step 4266, loss 0.140078, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:08.977904: step 4267, loss 0.219918, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:09.251369: step 4268, loss 0.100162, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:09.502084: step 4269, loss 0.1233, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:09.678625: step 4270, loss 0.139445, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:09.992972: step 4271, loss 0.148356, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:10.244887: step 4272, loss 0.08258, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:10.477903: step 4273, loss 0.276122, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:10.741020: step 4274, loss 0.140131, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:10.941035: step 4275, loss 0.154916, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:11.205221: step 4276, loss 0.0868773, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:11.489927: step 4277, loss 0.181133, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:11.752838: step 4278, loss 0.102748, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:12.092858: step 4279, loss 0.107964, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:12.273546: step 4280, loss 0.234467, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T15:06:12.603331: step 4280, loss 0.20063, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4280

2017-10-10T15:06:13.615522: step 4281, loss 0.121873, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:13.867808: step 4282, loss 0.0837769, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:14.098472: step 4283, loss 0.10857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:14.380843: step 4284, loss 0.209261, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:14.608930: step 4285, loss 0.108093, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:14.844821: step 4286, loss 0.142622, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:15.106467: step 4287, loss 0.0926584, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:15.396950: step 4288, loss 0.126071, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:15.685829: step 4289, loss 0.116853, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:15.908105: step 4290, loss 0.116649, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:16.197258: step 4291, loss 0.115774, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:16.451438: step 4292, loss 0.0944344, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:16.678720: step 4293, loss 0.0665973, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:16.832943: step 4294, loss 0.123028, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:17.066515: step 4295, loss 0.172794, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:17.278161: step 4296, loss 0.11725, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:17.481062: step 4297, loss 0.0966331, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:17.731084: step 4298, loss 0.0772027, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:17.918885: step 4299, loss 0.0578032, acc 1, learning_rate 0.0001
2017-10-10T15:06:18.147389: step 4300, loss 0.173828, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:18.382109: step 4301, loss 0.0620161, acc 1, learning_rate 0.0001
2017-10-10T15:06:18.635498: step 4302, loss 0.125907, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:18.847927: step 4303, loss 0.0396139, acc 1, learning_rate 0.0001
2017-10-10T15:06:19.120845: step 4304, loss 0.175191, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:19.373091: step 4305, loss 0.0937644, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:19.617216: step 4306, loss 0.152398, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:19.892271: step 4307, loss 0.14448, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:20.134224: step 4308, loss 0.122189, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:20.360993: step 4309, loss 0.119635, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:20.561167: step 4310, loss 0.0739054, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:20.844392: step 4311, loss 0.149887, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:21.064901: step 4312, loss 0.146025, acc 0.960784, learning_rate 0.0001
2017-10-10T15:06:21.369020: step 4313, loss 0.111359, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:21.584894: step 4314, loss 0.161829, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:21.749121: step 4315, loss 0.214278, acc 0.90625, learning_rate 0.0001
2017-10-10T15:06:21.996856: step 4316, loss 0.0802366, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:22.242716: step 4317, loss 0.116456, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:22.532464: step 4318, loss 0.169801, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:22.731850: step 4319, loss 0.121042, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:22.957126: step 4320, loss 0.0912178, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:06:23.365001: step 4320, loss 0.201216, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4320

2017-10-10T15:06:24.425111: step 4321, loss 0.096505, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:24.649778: step 4322, loss 0.114473, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:24.914652: step 4323, loss 0.101454, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:25.153024: step 4324, loss 0.172196, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:25.387025: step 4325, loss 0.204352, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:25.724292: step 4326, loss 0.130317, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:25.940992: step 4327, loss 0.0614554, acc 1, learning_rate 0.0001
2017-10-10T15:06:26.191502: step 4328, loss 0.0361407, acc 1, learning_rate 0.0001
2017-10-10T15:06:26.452949: step 4329, loss 0.0888498, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:26.633137: step 4330, loss 0.135161, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:26.896735: step 4331, loss 0.158649, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:27.173025: step 4332, loss 0.164264, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:27.396052: step 4333, loss 0.167228, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:27.665858: step 4334, loss 0.142564, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:27.880177: step 4335, loss 0.147339, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:28.139969: step 4336, loss 0.119322, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:28.417495: step 4337, loss 0.162518, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:28.647295: step 4338, loss 0.236476, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:28.914880: step 4339, loss 0.050277, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:29.192490: step 4340, loss 0.0878082, acc 1, learning_rate 0.0001
2017-10-10T15:06:29.411709: step 4341, loss 0.138364, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:29.685070: step 4342, loss 0.108223, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:29.932914: step 4343, loss 0.0594019, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:30.193823: step 4344, loss 0.129039, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:30.410123: step 4345, loss 0.0500881, acc 1, learning_rate 0.0001
2017-10-10T15:06:30.692979: step 4346, loss 0.141413, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:30.951443: step 4347, loss 0.161633, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:31.249042: step 4348, loss 0.248794, acc 0.890625, learning_rate 0.0001
2017-10-10T15:06:31.482621: step 4349, loss 0.203877, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:31.737508: step 4350, loss 0.132583, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:31.956956: step 4351, loss 0.0897354, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:32.242244: step 4352, loss 0.115942, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:32.487138: step 4353, loss 0.121057, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:32.690785: step 4354, loss 0.0947742, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:32.947824: step 4355, loss 0.18191, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:33.172850: step 4356, loss 0.217554, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:33.471172: step 4357, loss 0.121667, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:33.656893: step 4358, loss 0.126496, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:33.854092: step 4359, loss 0.170124, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:34.036990: step 4360, loss 0.13164, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:06:34.493804: step 4360, loss 0.199976, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4360

2017-10-10T15:06:35.497166: step 4361, loss 0.16346, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:35.736432: step 4362, loss 0.0899687, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:35.988735: step 4363, loss 0.160608, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:36.225508: step 4364, loss 0.0855019, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:36.453196: step 4365, loss 0.199829, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:36.750579: step 4366, loss 0.141082, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:36.977953: step 4367, loss 0.144844, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:37.216601: step 4368, loss 0.0708688, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:37.484819: step 4369, loss 0.0788491, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:37.716926: step 4370, loss 0.1076, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:37.949197: step 4371, loss 0.0809021, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:38.211133: step 4372, loss 0.0800247, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:38.436914: step 4373, loss 0.0909562, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:38.806528: step 4374, loss 0.152837, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:39.140948: step 4375, loss 0.0698727, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:39.340272: step 4376, loss 0.0937195, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:39.512820: step 4377, loss 0.0733638, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:39.718262: step 4378, loss 0.0465039, acc 1, learning_rate 0.0001
2017-10-10T15:06:39.876814: step 4379, loss 0.136788, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:40.030303: step 4380, loss 0.110824, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:40.216674: step 4381, loss 0.177946, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:40.484914: step 4382, loss 0.294551, acc 0.890625, learning_rate 0.0001
2017-10-10T15:06:40.737071: step 4383, loss 0.146036, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:40.973079: step 4384, loss 0.0759908, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:41.260931: step 4385, loss 0.166147, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:41.527633: step 4386, loss 0.0935291, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:41.792876: step 4387, loss 0.165817, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:42.029019: step 4388, loss 0.12433, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:42.313351: step 4389, loss 0.0746146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:42.556897: step 4390, loss 0.0710208, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:42.798101: step 4391, loss 0.269467, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:43.056906: step 4392, loss 0.175387, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:43.320820: step 4393, loss 0.204299, acc 0.90625, learning_rate 0.0001
2017-10-10T15:06:43.620972: step 4394, loss 0.107656, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:43.882050: step 4395, loss 0.145637, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:44.107090: step 4396, loss 0.14957, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:44.396950: step 4397, loss 0.102639, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:44.708531: step 4398, loss 0.142086, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:44.993478: step 4399, loss 0.168387, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:45.204083: step 4400, loss 0.212171, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:06:45.584403: step 4400, loss 0.199396, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4400

2017-10-10T15:06:46.466507: step 4401, loss 0.0761858, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:46.633539: step 4402, loss 0.0801207, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:46.908858: step 4403, loss 0.128205, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:47.145339: step 4404, loss 0.073882, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:47.341618: step 4405, loss 0.0979542, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:47.632825: step 4406, loss 0.116082, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:47.813084: step 4407, loss 0.118962, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:48.033657: step 4408, loss 0.102329, acc 1, learning_rate 0.0001
2017-10-10T15:06:48.324930: step 4409, loss 0.0945053, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:48.514857: step 4410, loss 0.154994, acc 0.941176, learning_rate 0.0001
2017-10-10T15:06:48.724825: step 4411, loss 0.159375, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:48.965659: step 4412, loss 0.0745884, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:49.297509: step 4413, loss 0.140637, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:49.569312: step 4414, loss 0.181079, acc 0.890625, learning_rate 0.0001
2017-10-10T15:06:49.811158: step 4415, loss 0.0944177, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:50.049699: step 4416, loss 0.122478, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:50.227357: step 4417, loss 0.0534071, acc 1, learning_rate 0.0001
2017-10-10T15:06:50.452907: step 4418, loss 0.108808, acc 1, learning_rate 0.0001
2017-10-10T15:06:50.671276: step 4419, loss 0.110864, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:50.926996: step 4420, loss 0.0834796, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:51.173006: step 4421, loss 0.110318, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:51.455269: step 4422, loss 0.114198, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:51.684961: step 4423, loss 0.127308, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:51.944475: step 4424, loss 0.152634, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:52.218224: step 4425, loss 0.0565111, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:52.500994: step 4426, loss 0.0943516, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:52.728863: step 4427, loss 0.0670336, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:52.988868: step 4428, loss 0.160148, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:53.168670: step 4429, loss 0.0867272, acc 1, learning_rate 0.0001
2017-10-10T15:06:53.426768: step 4430, loss 0.164134, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:53.711889: step 4431, loss 0.169297, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:53.926343: step 4432, loss 0.123288, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:54.191101: step 4433, loss 0.101828, acc 1, learning_rate 0.0001
2017-10-10T15:06:54.413074: step 4434, loss 0.162091, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:54.629874: step 4435, loss 0.0973887, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:54.885697: step 4436, loss 0.151776, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:55.106290: step 4437, loss 0.12097, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:55.395105: step 4438, loss 0.0920877, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:55.651876: step 4439, loss 0.188679, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:55.891482: step 4440, loss 0.174905, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:06:56.277087: step 4440, loss 0.20163, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4440

2017-10-10T15:06:57.419401: step 4441, loss 0.0540846, acc 1, learning_rate 0.0001
2017-10-10T15:06:57.648845: step 4442, loss 0.0657428, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:57.833033: step 4443, loss 0.105406, acc 0.984375, learning_rate 0.0001
2017-10-10T15:06:58.060820: step 4444, loss 0.190253, acc 0.9375, learning_rate 0.0001
2017-10-10T15:06:58.304022: step 4445, loss 0.135077, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:58.487160: step 4446, loss 0.130553, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:58.747831: step 4447, loss 0.203585, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:59.015761: step 4448, loss 0.136601, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:59.281002: step 4449, loss 0.108803, acc 0.96875, learning_rate 0.0001
2017-10-10T15:06:59.502380: step 4450, loss 0.114291, acc 0.953125, learning_rate 0.0001
2017-10-10T15:06:59.759108: step 4451, loss 0.174189, acc 0.921875, learning_rate 0.0001
2017-10-10T15:06:59.996588: step 4452, loss 0.140983, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:00.184819: step 4453, loss 0.0901932, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:00.495597: step 4454, loss 0.134282, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:00.774819: step 4455, loss 0.131564, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:01.000382: step 4456, loss 0.081885, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:01.273690: step 4457, loss 0.169432, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:01.505350: step 4458, loss 0.12024, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:01.752353: step 4459, loss 0.112297, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:02.026155: step 4460, loss 0.0479851, acc 1, learning_rate 0.0001
2017-10-10T15:07:02.286499: step 4461, loss 0.191777, acc 0.90625, learning_rate 0.0001
2017-10-10T15:07:02.559301: step 4462, loss 0.0626996, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:02.813056: step 4463, loss 0.102402, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:03.094073: step 4464, loss 0.0839258, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:03.356888: step 4465, loss 0.0900805, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:03.620879: step 4466, loss 0.150113, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:03.893812: step 4467, loss 0.0690287, acc 1, learning_rate 0.0001
2017-10-10T15:07:04.141940: step 4468, loss 0.132336, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:04.348507: step 4469, loss 0.0973375, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:04.582300: step 4470, loss 0.16558, acc 0.90625, learning_rate 0.0001
2017-10-10T15:07:04.861035: step 4471, loss 0.0908373, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:05.096903: step 4472, loss 0.163937, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:05.339938: step 4473, loss 0.0947178, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:05.640200: step 4474, loss 0.124474, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:05.868812: step 4475, loss 0.107604, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:06.196449: step 4476, loss 0.159944, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:06.391704: step 4477, loss 0.168304, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:06.596494: step 4478, loss 0.206786, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:06.793005: step 4479, loss 0.0927356, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:07.006835: step 4480, loss 0.0883612, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:07:07.377050: step 4480, loss 0.199689, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4480

2017-10-10T15:07:08.529587: step 4481, loss 0.125185, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:08.765362: step 4482, loss 0.230875, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:08.997122: step 4483, loss 0.0985094, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:09.241314: step 4484, loss 0.104904, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:09.488965: step 4485, loss 0.19242, acc 0.90625, learning_rate 0.0001
2017-10-10T15:07:09.737238: step 4486, loss 0.104958, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:09.942901: step 4487, loss 0.0960414, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:10.234293: step 4488, loss 0.101331, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:10.452952: step 4489, loss 0.15469, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:10.764978: step 4490, loss 0.0890441, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:11.049370: step 4491, loss 0.145463, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:11.337065: step 4492, loss 0.189858, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:11.548863: step 4493, loss 0.105526, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:11.888302: step 4494, loss 0.142935, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:12.529121: step 4495, loss 0.102557, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:12.717724: step 4496, loss 0.0993554, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:12.956898: step 4497, loss 0.18675, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:13.259786: step 4498, loss 0.119901, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:13.537044: step 4499, loss 0.0742122, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:13.820896: step 4500, loss 0.06118, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:14.070141: step 4501, loss 0.139778, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:14.332430: step 4502, loss 0.144118, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:14.573260: step 4503, loss 0.158782, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:14.780891: step 4504, loss 0.0982255, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:15.076527: step 4505, loss 0.0523444, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:15.269076: step 4506, loss 0.138566, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:15.546927: step 4507, loss 0.106304, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:15.753087: step 4508, loss 0.0865991, acc 1, learning_rate 0.0001
2017-10-10T15:07:16.101090: step 4509, loss 0.0822181, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:16.377125: step 4510, loss 0.120081, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:16.601891: step 4511, loss 0.0670914, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:16.803988: step 4512, loss 0.108463, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:17.047715: step 4513, loss 0.145153, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:17.247843: step 4514, loss 0.13147, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:17.512973: step 4515, loss 0.0505285, acc 1, learning_rate 0.0001
2017-10-10T15:07:17.787224: step 4516, loss 0.154603, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:18.027336: step 4517, loss 0.174044, acc 0.921875, learning_rate 0.0001
2017-10-10T15:07:18.312831: step 4518, loss 0.108371, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:18.521225: step 4519, loss 0.15128, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:18.769957: step 4520, loss 0.0549721, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:07:19.274857: step 4520, loss 0.199428, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4520

2017-10-10T15:07:20.177387: step 4521, loss 0.141215, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:20.469696: step 4522, loss 0.108927, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:20.693107: step 4523, loss 0.211216, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:20.914543: step 4524, loss 0.0285093, acc 1, learning_rate 0.0001
2017-10-10T15:07:21.168593: step 4525, loss 0.0745217, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:21.378718: step 4526, loss 0.12264, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:21.667407: step 4527, loss 0.191062, acc 0.921875, learning_rate 0.0001
2017-10-10T15:07:21.940825: step 4528, loss 0.096424, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:22.135043: step 4529, loss 0.198048, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:22.378015: step 4530, loss 0.146052, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:22.602485: step 4531, loss 0.0775493, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:22.910007: step 4532, loss 0.140781, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:23.194781: step 4533, loss 0.195414, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:23.372985: step 4534, loss 0.115103, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:23.618590: step 4535, loss 0.141803, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:23.806497: step 4536, loss 0.0986586, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:24.078105: step 4537, loss 0.0427209, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:24.326698: step 4538, loss 0.0944731, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:24.518753: step 4539, loss 0.121432, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:24.746299: step 4540, loss 0.116883, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:24.984975: step 4541, loss 0.0402458, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:25.252408: step 4542, loss 0.107272, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:25.544874: step 4543, loss 0.0546218, acc 1, learning_rate 0.0001
2017-10-10T15:07:25.785761: step 4544, loss 0.125444, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:26.044239: step 4545, loss 0.125166, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:26.293210: step 4546, loss 0.0816646, acc 1, learning_rate 0.0001
2017-10-10T15:07:26.537002: step 4547, loss 0.178224, acc 0.921875, learning_rate 0.0001
2017-10-10T15:07:26.742274: step 4548, loss 0.113736, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:27.035770: step 4549, loss 0.104824, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:27.296223: step 4550, loss 0.0918294, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:27.520175: step 4551, loss 0.109054, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:27.785228: step 4552, loss 0.261851, acc 0.90625, learning_rate 0.0001
2017-10-10T15:07:28.012535: step 4553, loss 0.150086, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:28.269529: step 4554, loss 0.113681, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:28.560816: step 4555, loss 0.133242, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:28.807807: step 4556, loss 0.10823, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:29.089782: step 4557, loss 0.122328, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:29.308311: step 4558, loss 0.166155, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:29.552850: step 4559, loss 0.167147, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:29.797076: step 4560, loss 0.218125, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T15:07:30.168858: step 4560, loss 0.199607, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4560

2017-10-10T15:07:31.272837: step 4561, loss 0.0875527, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:31.510103: step 4562, loss 0.112242, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:31.688100: step 4563, loss 0.172737, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:31.967310: step 4564, loss 0.134315, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:32.209024: step 4565, loss 0.0946164, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:32.475133: step 4566, loss 0.135277, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:32.686178: step 4567, loss 0.0651957, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:32.908255: step 4568, loss 0.0746461, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:33.129406: step 4569, loss 0.123827, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:33.318230: step 4570, loss 0.084543, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:33.575618: step 4571, loss 0.1778, acc 0.921875, learning_rate 0.0001
2017-10-10T15:07:33.829414: step 4572, loss 0.127873, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:34.064546: step 4573, loss 0.104911, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:34.358991: step 4574, loss 0.125001, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:34.654714: step 4575, loss 0.103042, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:34.880861: step 4576, loss 0.169303, acc 0.921875, learning_rate 0.0001
2017-10-10T15:07:35.132939: step 4577, loss 0.187331, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:35.359742: step 4578, loss 0.168974, acc 0.921875, learning_rate 0.0001
2017-10-10T15:07:35.603477: step 4579, loss 0.109024, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:35.918071: step 4580, loss 0.0713241, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:36.146889: step 4581, loss 0.0636427, acc 1, learning_rate 0.0001
2017-10-10T15:07:36.456963: step 4582, loss 0.142012, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:36.715350: step 4583, loss 0.137643, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:36.948632: step 4584, loss 0.128186, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:37.177153: step 4585, loss 0.170664, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:37.692080: step 4586, loss 0.0916701, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:37.900980: step 4587, loss 0.112909, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:38.164483: step 4588, loss 0.21092, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:38.418950: step 4589, loss 0.0900049, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:38.623755: step 4590, loss 0.0279902, acc 1, learning_rate 0.0001
2017-10-10T15:07:38.872128: step 4591, loss 0.0869882, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:39.161488: step 4592, loss 0.0992096, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:39.410047: step 4593, loss 0.131286, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:39.664533: step 4594, loss 0.176487, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:39.937894: step 4595, loss 0.175101, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:40.145595: step 4596, loss 0.247099, acc 0.921875, learning_rate 0.0001
2017-10-10T15:07:40.390379: step 4597, loss 0.109793, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:40.641048: step 4598, loss 0.0866548, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:40.877016: step 4599, loss 0.15862, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:41.166870: step 4600, loss 0.106754, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:07:41.602298: step 4600, loss 0.199317, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4600

2017-10-10T15:07:42.713667: step 4601, loss 0.209337, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:42.908920: step 4602, loss 0.166423, acc 0.90625, learning_rate 0.0001
2017-10-10T15:07:43.194990: step 4603, loss 0.0773794, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:43.494049: step 4604, loss 0.0897836, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:43.735736: step 4605, loss 0.0600613, acc 1, learning_rate 0.0001
2017-10-10T15:07:43.974635: step 4606, loss 0.0662103, acc 1, learning_rate 0.0001
2017-10-10T15:07:44.216801: step 4607, loss 0.0852458, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:44.427085: step 4608, loss 0.0839401, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:44.687764: step 4609, loss 0.0839886, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:44.957257: step 4610, loss 0.0813011, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:45.239250: step 4611, loss 0.283282, acc 0.90625, learning_rate 0.0001
2017-10-10T15:07:45.501006: step 4612, loss 0.0957715, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:45.758049: step 4613, loss 0.0982103, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:45.970496: step 4614, loss 0.19786, acc 0.90625, learning_rate 0.0001
2017-10-10T15:07:46.186252: step 4615, loss 0.087319, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:46.424824: step 4616, loss 0.159078, acc 0.921875, learning_rate 0.0001
2017-10-10T15:07:46.642398: step 4617, loss 0.110456, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:46.903362: step 4618, loss 0.16102, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:47.140144: step 4619, loss 0.109317, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:47.373218: step 4620, loss 0.160099, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:47.644932: step 4621, loss 0.192685, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:47.893791: step 4622, loss 0.114546, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:48.148939: step 4623, loss 0.0917047, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:48.372879: step 4624, loss 0.125594, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:48.667734: step 4625, loss 0.117023, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:48.861978: step 4626, loss 0.220353, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:49.072925: step 4627, loss 0.0650292, acc 1, learning_rate 0.0001
2017-10-10T15:07:49.293898: step 4628, loss 0.11541, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:49.545088: step 4629, loss 0.11049, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:49.778071: step 4630, loss 0.155631, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:50.043866: step 4631, loss 0.200267, acc 0.90625, learning_rate 0.0001
2017-10-10T15:07:50.292757: step 4632, loss 0.145636, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:50.529267: step 4633, loss 0.142837, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:50.811825: step 4634, loss 0.159973, acc 0.921875, learning_rate 0.0001
2017-10-10T15:07:50.989349: step 4635, loss 0.0801653, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:51.265734: step 4636, loss 0.133521, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:51.509026: step 4637, loss 0.100499, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:51.809355: step 4638, loss 0.14821, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:52.051686: step 4639, loss 0.0696077, acc 1, learning_rate 0.0001
2017-10-10T15:07:52.281806: step 4640, loss 0.0864296, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:07:52.697262: step 4640, loss 0.198096, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4640

2017-10-10T15:07:53.799150: step 4641, loss 0.145085, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:53.963328: step 4642, loss 0.11, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:54.168862: step 4643, loss 0.161827, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:54.418824: step 4644, loss 0.0710021, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:54.625612: step 4645, loss 0.142889, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:54.908393: step 4646, loss 0.121847, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:55.136502: step 4647, loss 0.102903, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:55.365638: step 4648, loss 0.0764682, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:55.591897: step 4649, loss 0.0641401, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:55.886708: step 4650, loss 0.128192, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:56.092572: step 4651, loss 0.0927771, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:56.327912: step 4652, loss 0.0805353, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:56.508988: step 4653, loss 0.0550777, acc 1, learning_rate 0.0001
2017-10-10T15:07:56.701467: step 4654, loss 0.0588479, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:56.920980: step 4655, loss 0.083151, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:57.177082: step 4656, loss 0.065903, acc 1, learning_rate 0.0001
2017-10-10T15:07:57.451424: step 4657, loss 0.162427, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:57.662478: step 4658, loss 0.0715635, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:57.892893: step 4659, loss 0.180942, acc 0.921875, learning_rate 0.0001
2017-10-10T15:07:58.128936: step 4660, loss 0.0932984, acc 0.96875, learning_rate 0.0001
2017-10-10T15:07:58.390824: step 4661, loss 0.0929082, acc 0.984375, learning_rate 0.0001
2017-10-10T15:07:58.709939: step 4662, loss 0.129684, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:58.974893: step 4663, loss 0.179282, acc 0.921875, learning_rate 0.0001
2017-10-10T15:07:59.183168: step 4664, loss 0.144009, acc 0.9375, learning_rate 0.0001
2017-10-10T15:07:59.389575: step 4665, loss 0.101364, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:59.664863: step 4666, loss 0.083163, acc 0.953125, learning_rate 0.0001
2017-10-10T15:07:59.869144: step 4667, loss 0.164271, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:00.120992: step 4668, loss 0.177223, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:00.372305: step 4669, loss 0.170115, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:00.584207: step 4670, loss 0.122458, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:00.940904: step 4671, loss 0.0858257, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:01.138102: step 4672, loss 0.134972, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:01.371958: step 4673, loss 0.178013, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:01.641284: step 4674, loss 0.108885, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:01.862271: step 4675, loss 0.173288, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:02.107174: step 4676, loss 0.0638008, acc 1, learning_rate 0.0001
2017-10-10T15:08:02.350563: step 4677, loss 0.146452, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:02.562770: step 4678, loss 0.208774, acc 0.90625, learning_rate 0.0001
2017-10-10T15:08:02.821516: step 4679, loss 0.167736, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:03.061410: step 4680, loss 0.0562159, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:08:03.533507: step 4680, loss 0.198016, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4680

2017-10-10T15:08:04.461297: step 4681, loss 0.0784261, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:04.678580: step 4682, loss 0.111601, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:04.925279: step 4683, loss 0.0833349, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:05.170565: step 4684, loss 0.0907436, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:05.420935: step 4685, loss 0.106054, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:05.726544: step 4686, loss 0.0763738, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:05.997182: step 4687, loss 0.123342, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:06.256932: step 4688, loss 0.0469957, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:06.548577: step 4689, loss 0.186705, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:06.787918: step 4690, loss 0.0701819, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:07.006276: step 4691, loss 0.0895867, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:07.319103: step 4692, loss 0.0724619, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:07.534417: step 4693, loss 0.075368, acc 1, learning_rate 0.0001
2017-10-10T15:08:07.784364: step 4694, loss 0.138703, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:08.081902: step 4695, loss 0.107814, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:08.311903: step 4696, loss 0.0894375, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:08.551806: step 4697, loss 0.26807, acc 0.90625, learning_rate 0.0001
2017-10-10T15:08:08.729343: step 4698, loss 0.150226, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:08.991631: step 4699, loss 0.119152, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:09.179369: step 4700, loss 0.123262, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:09.453329: step 4701, loss 0.0462912, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:09.691526: step 4702, loss 0.120312, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:09.942640: step 4703, loss 0.0606302, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:10.168521: step 4704, loss 0.127897, acc 0.941176, learning_rate 0.0001
2017-10-10T15:08:10.421113: step 4705, loss 0.252616, acc 0.859375, learning_rate 0.0001
2017-10-10T15:08:10.672749: step 4706, loss 0.100587, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:10.969812: step 4707, loss 0.104387, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:11.199985: step 4708, loss 0.124042, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:11.461744: step 4709, loss 0.140783, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:11.727869: step 4710, loss 0.110193, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:11.944660: step 4711, loss 0.0885264, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:12.184004: step 4712, loss 0.116429, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:12.445058: step 4713, loss 0.0937948, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:12.729183: step 4714, loss 0.21435, acc 0.90625, learning_rate 0.0001
2017-10-10T15:08:12.996861: step 4715, loss 0.132495, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:13.123504: step 4716, loss 0.0882128, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:13.285666: step 4717, loss 0.14621, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:13.470160: step 4718, loss 0.0825842, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:13.629789: step 4719, loss 0.172668, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:13.937587: step 4720, loss 0.0740638, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:08:14.336824: step 4720, loss 0.198036, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4720

2017-10-10T15:08:15.425749: step 4721, loss 0.0902289, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:15.693149: step 4722, loss 0.151046, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:15.964834: step 4723, loss 0.14457, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:16.276838: step 4724, loss 0.0944616, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:16.520976: step 4725, loss 0.0996454, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:16.748863: step 4726, loss 0.068418, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:17.033624: step 4727, loss 0.112431, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:17.260842: step 4728, loss 0.0680246, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:17.512713: step 4729, loss 0.231553, acc 0.90625, learning_rate 0.0001
2017-10-10T15:08:17.751636: step 4730, loss 0.157318, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:17.980974: step 4731, loss 0.140674, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:18.200952: step 4732, loss 0.129774, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:18.453113: step 4733, loss 0.084955, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:18.698488: step 4734, loss 0.0949832, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:18.929073: step 4735, loss 0.157064, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:19.232923: step 4736, loss 0.0870204, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:19.452394: step 4737, loss 0.108518, acc 1, learning_rate 0.0001
2017-10-10T15:08:19.665075: step 4738, loss 0.156608, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:19.896479: step 4739, loss 0.100148, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:20.141090: step 4740, loss 0.0914528, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:20.394378: step 4741, loss 0.0602698, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:20.605147: step 4742, loss 0.136328, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:20.925984: step 4743, loss 0.150408, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:21.164003: step 4744, loss 0.16431, acc 0.90625, learning_rate 0.0001
2017-10-10T15:08:21.381553: step 4745, loss 0.115166, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:21.594556: step 4746, loss 0.171339, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:21.871904: step 4747, loss 0.155084, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:22.172888: step 4748, loss 0.0564343, acc 1, learning_rate 0.0001
2017-10-10T15:08:22.416858: step 4749, loss 0.218931, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:22.687361: step 4750, loss 0.133074, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:22.984862: step 4751, loss 0.0724102, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:23.237904: step 4752, loss 0.112003, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:23.517768: step 4753, loss 0.0697959, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:23.706850: step 4754, loss 0.144896, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:23.962094: step 4755, loss 0.089873, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:24.261834: step 4756, loss 0.229443, acc 0.90625, learning_rate 0.0001
2017-10-10T15:08:24.466842: step 4757, loss 0.0492795, acc 1, learning_rate 0.0001
2017-10-10T15:08:24.693341: step 4758, loss 0.112717, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:24.954814: step 4759, loss 0.1449, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:25.220415: step 4760, loss 0.166296, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:08:25.612917: step 4760, loss 0.198441, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4760

2017-10-10T15:08:26.599178: step 4761, loss 0.0875241, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:26.780564: step 4762, loss 0.0876849, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:27.068056: step 4763, loss 0.119795, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:27.278311: step 4764, loss 0.0943361, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:27.528688: step 4765, loss 0.0764855, acc 1, learning_rate 0.0001
2017-10-10T15:08:27.798253: step 4766, loss 0.0486243, acc 1, learning_rate 0.0001
2017-10-10T15:08:28.063458: step 4767, loss 0.0840783, acc 1, learning_rate 0.0001
2017-10-10T15:08:28.356935: step 4768, loss 0.0650586, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:28.620935: step 4769, loss 0.16975, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:28.908130: step 4770, loss 0.113863, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:29.204452: step 4771, loss 0.174443, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:29.464848: step 4772, loss 0.134838, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:29.740921: step 4773, loss 0.129511, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:29.904001: step 4774, loss 0.109514, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:30.112108: step 4775, loss 0.050098, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:30.273045: step 4776, loss 0.0681931, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:30.465909: step 4777, loss 0.118224, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:30.714400: step 4778, loss 0.129579, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:30.954836: step 4779, loss 0.0512041, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:31.197044: step 4780, loss 0.139586, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:31.464891: step 4781, loss 0.0727385, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:31.685045: step 4782, loss 0.213057, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:31.992853: step 4783, loss 0.0747789, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:32.285251: step 4784, loss 0.193353, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:32.491528: step 4785, loss 0.101068, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:32.729769: step 4786, loss 0.121452, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:33.008861: step 4787, loss 0.057141, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:33.208840: step 4788, loss 0.104189, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:33.501133: step 4789, loss 0.106578, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:33.711698: step 4790, loss 0.04809, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:33.956793: step 4791, loss 0.0516681, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:34.210178: step 4792, loss 0.158872, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:34.512827: step 4793, loss 0.0745113, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:34.769305: step 4794, loss 0.135807, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:35.053918: step 4795, loss 0.15713, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:35.316183: step 4796, loss 0.0647839, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:35.542278: step 4797, loss 0.165954, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:35.782402: step 4798, loss 0.0832525, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:35.992885: step 4799, loss 0.0807623, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:36.246015: step 4800, loss 0.111573, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:08:36.688878: step 4800, loss 0.196418, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4800

2017-10-10T15:08:37.640886: step 4801, loss 0.118755, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:37.814258: step 4802, loss 0.112774, acc 0.980392, learning_rate 0.0001
2017-10-10T15:08:38.088577: step 4803, loss 0.0644924, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:38.332943: step 4804, loss 0.205929, acc 0.90625, learning_rate 0.0001
2017-10-10T15:08:38.603488: step 4805, loss 0.303962, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:38.849092: step 4806, loss 0.0601043, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:39.095206: step 4807, loss 0.117484, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:39.337828: step 4808, loss 0.137438, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:39.568959: step 4809, loss 0.171373, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:39.849115: step 4810, loss 0.190864, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:40.089133: step 4811, loss 0.096165, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:40.372075: step 4812, loss 0.154756, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:40.644844: step 4813, loss 0.125545, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:40.900864: step 4814, loss 0.0715573, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:41.108930: step 4815, loss 0.0853952, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:41.403914: step 4816, loss 0.173009, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:41.644970: step 4817, loss 0.100776, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:41.886847: step 4818, loss 0.131861, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:42.126025: step 4819, loss 0.130109, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:42.353087: step 4820, loss 0.180063, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:42.615962: step 4821, loss 0.0626991, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:42.852948: step 4822, loss 0.11885, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:43.108934: step 4823, loss 0.141864, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:43.342514: step 4824, loss 0.0945686, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:43.556270: step 4825, loss 0.103862, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:43.817974: step 4826, loss 0.089873, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:44.072328: step 4827, loss 0.117485, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:44.341230: step 4828, loss 0.117969, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:44.579649: step 4829, loss 0.109349, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:44.798756: step 4830, loss 0.148738, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:45.040890: step 4831, loss 0.217949, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:45.326466: step 4832, loss 0.0964382, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:45.555104: step 4833, loss 0.117787, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:45.775913: step 4834, loss 0.149722, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:46.042964: step 4835, loss 0.1479, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:46.368853: step 4836, loss 0.105279, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:46.600922: step 4837, loss 0.118187, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:46.742183: step 4838, loss 0.0504925, acc 1, learning_rate 0.0001
2017-10-10T15:08:46.993768: step 4839, loss 0.182693, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:47.233253: step 4840, loss 0.124979, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:08:47.674027: step 4840, loss 0.198123, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4840

2017-10-10T15:08:48.708184: step 4841, loss 0.0683984, acc 1, learning_rate 0.0001
2017-10-10T15:08:48.992131: step 4842, loss 0.0654096, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:49.213161: step 4843, loss 0.134064, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:49.550599: step 4844, loss 0.0964671, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:49.793051: step 4845, loss 0.0783809, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:49.955820: step 4846, loss 0.127153, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:50.239453: step 4847, loss 0.264775, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:50.515205: step 4848, loss 0.0821213, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:50.756870: step 4849, loss 0.0921047, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:50.996522: step 4850, loss 0.152645, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:51.248591: step 4851, loss 0.127106, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:51.486437: step 4852, loss 0.0822123, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:51.717185: step 4853, loss 0.194723, acc 0.90625, learning_rate 0.0001
2017-10-10T15:08:51.974139: step 4854, loss 0.144482, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:52.194820: step 4855, loss 0.102701, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:52.431924: step 4856, loss 0.0823413, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:52.710246: step 4857, loss 0.127232, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:52.931352: step 4858, loss 0.161104, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:53.170866: step 4859, loss 0.0512452, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:53.424908: step 4860, loss 0.178512, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:53.628974: step 4861, loss 0.0878317, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:53.948905: step 4862, loss 0.165709, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:54.244922: step 4863, loss 0.0578967, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:54.443586: step 4864, loss 0.120351, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:54.733939: step 4865, loss 0.19087, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:54.985003: step 4866, loss 0.100121, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:55.195938: step 4867, loss 0.131868, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:55.472279: step 4868, loss 0.0563326, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:55.685406: step 4869, loss 0.0508468, acc 0.96875, learning_rate 0.0001
2017-10-10T15:08:55.913058: step 4870, loss 0.18564, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:56.121048: step 4871, loss 0.129184, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:56.394888: step 4872, loss 0.12284, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:56.688907: step 4873, loss 0.0571164, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:56.944910: step 4874, loss 0.152737, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:57.190561: step 4875, loss 0.158762, acc 0.9375, learning_rate 0.0001
2017-10-10T15:08:57.381570: step 4876, loss 0.199068, acc 0.921875, learning_rate 0.0001
2017-10-10T15:08:57.624022: step 4877, loss 0.0553003, acc 1, learning_rate 0.0001
2017-10-10T15:08:57.892815: step 4878, loss 0.0585737, acc 0.984375, learning_rate 0.0001
2017-10-10T15:08:58.186050: step 4879, loss 0.0810023, acc 0.953125, learning_rate 0.0001
2017-10-10T15:08:58.399438: step 4880, loss 0.129895, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:08:58.802885: step 4880, loss 0.19709, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4880

2017-10-10T15:09:00.033991: step 4881, loss 0.0413896, acc 1, learning_rate 0.0001
2017-10-10T15:09:00.265134: step 4882, loss 0.128258, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:00.539477: step 4883, loss 0.127429, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:00.801216: step 4884, loss 0.106064, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:01.089019: step 4885, loss 0.057528, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:01.335184: step 4886, loss 0.0648897, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:01.580823: step 4887, loss 0.153871, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:01.877656: step 4888, loss 0.0763444, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:02.135216: step 4889, loss 0.232598, acc 0.90625, learning_rate 0.0001
2017-10-10T15:09:02.348955: step 4890, loss 0.12175, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:02.664980: step 4891, loss 0.0861769, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:02.990387: step 4892, loss 0.177219, acc 0.90625, learning_rate 0.0001
2017-10-10T15:09:03.154393: step 4893, loss 0.122539, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:03.357049: step 4894, loss 0.0666715, acc 1, learning_rate 0.0001
2017-10-10T15:09:04.476995: step 4895, loss 0.0951857, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:04.745001: step 4896, loss 0.0409058, acc 1, learning_rate 0.0001
2017-10-10T15:09:04.953018: step 4897, loss 0.147946, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:05.245462: step 4898, loss 0.0673047, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:05.504174: step 4899, loss 0.081201, acc 1, learning_rate 0.0001
2017-10-10T15:09:05.731153: step 4900, loss 0.113875, acc 0.921569, learning_rate 0.0001
2017-10-10T15:09:06.006088: step 4901, loss 0.0942585, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:06.304926: step 4902, loss 0.111392, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:06.563513: step 4903, loss 0.0914431, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:06.809629: step 4904, loss 0.0421152, acc 1, learning_rate 0.0001
2017-10-10T15:09:07.005999: step 4905, loss 0.105372, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:07.214818: step 4906, loss 0.0851467, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:07.396242: step 4907, loss 0.181613, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:07.595698: step 4908, loss 0.107348, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:07.894739: step 4909, loss 0.223808, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:08.133162: step 4910, loss 0.0920952, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:08.384483: step 4911, loss 0.12762, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:08.670133: step 4912, loss 0.0784662, acc 1, learning_rate 0.0001
2017-10-10T15:09:08.837090: step 4913, loss 0.145883, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:09.154975: step 4914, loss 0.114519, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:09.395528: step 4915, loss 0.0907089, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:09.618941: step 4916, loss 0.218302, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:09.901625: step 4917, loss 0.12733, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:10.095426: step 4918, loss 0.06171, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:10.322572: step 4919, loss 0.0526466, acc 1, learning_rate 0.0001
2017-10-10T15:09:10.541005: step 4920, loss 0.0758799, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:09:10.903615: step 4920, loss 0.196705, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4920

2017-10-10T15:09:11.932808: step 4921, loss 0.037918, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:12.231150: step 4922, loss 0.0607689, acc 1, learning_rate 0.0001
2017-10-10T15:09:12.507569: step 4923, loss 0.117231, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:12.804856: step 4924, loss 0.129744, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:13.012901: step 4925, loss 0.0878438, acc 1, learning_rate 0.0001
2017-10-10T15:09:13.273471: step 4926, loss 0.0735675, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:13.472048: step 4927, loss 0.128525, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:13.709087: step 4928, loss 0.108377, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:13.962388: step 4929, loss 0.158143, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:14.196301: step 4930, loss 0.0923785, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:14.469074: step 4931, loss 0.257909, acc 0.875, learning_rate 0.0001
2017-10-10T15:09:14.714189: step 4932, loss 0.125226, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:14.980103: step 4933, loss 0.0652476, acc 1, learning_rate 0.0001
2017-10-10T15:09:15.192899: step 4934, loss 0.0909114, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:15.381070: step 4935, loss 0.250857, acc 0.875, learning_rate 0.0001
2017-10-10T15:09:15.596218: step 4936, loss 0.0661425, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:15.853904: step 4937, loss 0.100671, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:16.115150: step 4938, loss 0.158774, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:16.366697: step 4939, loss 0.0641873, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:16.648881: step 4940, loss 0.102968, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:16.877195: step 4941, loss 0.155819, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:17.089280: step 4942, loss 0.106779, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:17.365476: step 4943, loss 0.227841, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:17.629081: step 4944, loss 0.20189, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:17.868868: step 4945, loss 0.181374, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:18.218868: step 4946, loss 0.159343, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:18.431279: step 4947, loss 0.122465, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:18.655253: step 4948, loss 0.0825083, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:18.890765: step 4949, loss 0.0292311, acc 1, learning_rate 0.0001
2017-10-10T15:09:19.127124: step 4950, loss 0.0967869, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:19.447287: step 4951, loss 0.154149, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:19.675010: step 4952, loss 0.11195, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:19.921516: step 4953, loss 0.096113, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:20.124889: step 4954, loss 0.175208, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:20.391888: step 4955, loss 0.0792479, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:20.588908: step 4956, loss 0.0867971, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:20.749896: step 4957, loss 0.0542303, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:20.976943: step 4958, loss 0.1022, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:21.204889: step 4959, loss 0.114492, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:21.438925: step 4960, loss 0.126476, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:09:21.807379: step 4960, loss 0.195721, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-4960

2017-10-10T15:09:22.748830: step 4961, loss 0.122984, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:22.927447: step 4962, loss 0.0775752, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:23.221117: step 4963, loss 0.079139, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:23.417006: step 4964, loss 0.0588611, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:23.657552: step 4965, loss 0.178917, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:23.937034: step 4966, loss 0.119555, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:24.180971: step 4967, loss 0.0635, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:24.448701: step 4968, loss 0.0900511, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:24.697482: step 4969, loss 0.102285, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:24.948296: step 4970, loss 0.174879, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:25.176910: step 4971, loss 0.10071, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:25.344820: step 4972, loss 0.0768095, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:25.576936: step 4973, loss 0.108482, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:25.778691: step 4974, loss 0.0934612, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:26.000972: step 4975, loss 0.166894, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:26.270444: step 4976, loss 0.060405, acc 1, learning_rate 0.0001
2017-10-10T15:09:26.574997: step 4977, loss 0.110469, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:26.822446: step 4978, loss 0.0544633, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:27.047172: step 4979, loss 0.154218, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:27.296853: step 4980, loss 0.10081, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:27.491026: step 4981, loss 0.100364, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:27.772896: step 4982, loss 0.134768, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:28.046756: step 4983, loss 0.117513, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:28.245051: step 4984, loss 0.0643099, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:28.554520: step 4985, loss 0.16123, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:28.840832: step 4986, loss 0.126322, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:29.080057: step 4987, loss 0.114367, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:29.328804: step 4988, loss 0.0681572, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:29.592854: step 4989, loss 0.0803475, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:29.884930: step 4990, loss 0.157592, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:30.160387: step 4991, loss 0.121148, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:30.432349: step 4992, loss 0.241021, acc 0.90625, learning_rate 0.0001
2017-10-10T15:09:30.652843: step 4993, loss 0.132404, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:30.880962: step 4994, loss 0.160846, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:31.170412: step 4995, loss 0.223245, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:31.426757: step 4996, loss 0.112957, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:31.696906: step 4997, loss 0.0856682, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:31.908875: step 4998, loss 0.110753, acc 0.941176, learning_rate 0.0001
2017-10-10T15:09:32.234591: step 4999, loss 0.145975, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:32.465402: step 5000, loss 0.0800912, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:09:32.893877: step 5000, loss 0.19593, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5000

2017-10-10T15:09:33.837818: step 5001, loss 0.249486, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:34.055212: step 5002, loss 0.0924431, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:34.241388: step 5003, loss 0.121693, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:34.490013: step 5004, loss 0.0766805, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:34.736840: step 5005, loss 0.108978, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:34.932903: step 5006, loss 0.13759, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:35.234211: step 5007, loss 0.120259, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:35.469367: step 5008, loss 0.0629607, acc 1, learning_rate 0.0001
2017-10-10T15:09:35.700305: step 5009, loss 0.15056, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:35.997213: step 5010, loss 0.0481869, acc 1, learning_rate 0.0001
2017-10-10T15:09:36.254510: step 5011, loss 0.0720645, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:36.517277: step 5012, loss 0.110825, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:36.808081: step 5013, loss 0.117042, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:37.030078: step 5014, loss 0.0942439, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:37.233597: step 5015, loss 0.0532263, acc 1, learning_rate 0.0001
2017-10-10T15:09:37.485837: step 5016, loss 0.0610668, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:37.672424: step 5017, loss 0.145056, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:37.960917: step 5018, loss 0.204854, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:38.201179: step 5019, loss 0.179003, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:38.448170: step 5020, loss 0.159953, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:38.700991: step 5021, loss 0.139892, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:38.911933: step 5022, loss 0.0897495, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:39.194816: step 5023, loss 0.0545235, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:39.424861: step 5024, loss 0.11957, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:39.682821: step 5025, loss 0.150088, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:39.944926: step 5026, loss 0.183882, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:40.168241: step 5027, loss 0.192141, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:40.431443: step 5028, loss 0.127171, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:40.685099: step 5029, loss 0.119725, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:40.932948: step 5030, loss 0.125394, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:41.141004: step 5031, loss 0.203807, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:41.365057: step 5032, loss 0.0609946, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:41.648966: step 5033, loss 0.0919072, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:41.965820: step 5034, loss 0.098624, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:42.204953: step 5035, loss 0.116938, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:42.498798: step 5036, loss 0.0942843, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:42.768987: step 5037, loss 0.0579517, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:43.046937: step 5038, loss 0.110488, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:43.263558: step 5039, loss 0.145254, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:43.486949: step 5040, loss 0.0732981, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:09:43.843726: step 5040, loss 0.195965, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5040

2017-10-10T15:09:44.910716: step 5041, loss 0.14476, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:45.122996: step 5042, loss 0.126408, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:45.376904: step 5043, loss 0.107087, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:45.597734: step 5044, loss 0.144058, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:45.827384: step 5045, loss 0.0680186, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:46.089386: step 5046, loss 0.100683, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:46.284013: step 5047, loss 0.0896321, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:46.535740: step 5048, loss 0.0876499, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:46.805030: step 5049, loss 0.107877, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:47.084857: step 5050, loss 0.114335, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:47.348355: step 5051, loss 0.149941, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:47.606096: step 5052, loss 0.105893, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:47.839363: step 5053, loss 0.155325, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:48.109090: step 5054, loss 0.102881, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:48.385164: step 5055, loss 0.158886, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:48.676958: step 5056, loss 0.0695242, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:48.944919: step 5057, loss 0.0720226, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:49.199796: step 5058, loss 0.167635, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:49.469530: step 5059, loss 0.0530572, acc 1, learning_rate 0.0001
2017-10-10T15:09:49.719010: step 5060, loss 0.184926, acc 0.90625, learning_rate 0.0001
2017-10-10T15:09:49.898385: step 5061, loss 0.0750857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:50.180901: step 5062, loss 0.0946878, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:50.410985: step 5063, loss 0.189979, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:50.675953: step 5064, loss 0.189314, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:50.997003: step 5065, loss 0.147127, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:51.254036: step 5066, loss 0.155105, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:51.515943: step 5067, loss 0.139313, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:51.755429: step 5068, loss 0.0781253, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:51.985827: step 5069, loss 0.0539315, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:52.188853: step 5070, loss 0.0958636, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:52.377617: step 5071, loss 0.149506, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:52.644834: step 5072, loss 0.116876, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:52.897007: step 5073, loss 0.0906916, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:53.168259: step 5074, loss 0.0900125, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:53.376975: step 5075, loss 0.195484, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:53.600385: step 5076, loss 0.096048, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:53.795615: step 5077, loss 0.181036, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:54.021023: step 5078, loss 0.0906253, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:54.292898: step 5079, loss 0.113847, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:54.522941: step 5080, loss 0.13789, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:09:54.911930: step 5080, loss 0.195813, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5080

2017-10-10T15:09:55.974962: step 5081, loss 0.109075, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:56.216307: step 5082, loss 0.0693615, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:56.429019: step 5083, loss 0.128208, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:56.695740: step 5084, loss 0.129818, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:56.983119: step 5085, loss 0.114349, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:57.210239: step 5086, loss 0.123033, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:57.488968: step 5087, loss 0.10522, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:57.740443: step 5088, loss 0.0748002, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:57.976921: step 5089, loss 0.111605, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:58.231357: step 5090, loss 0.229844, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:58.474332: step 5091, loss 0.0533356, acc 1, learning_rate 0.0001
2017-10-10T15:09:58.738134: step 5092, loss 0.131489, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:58.959724: step 5093, loss 0.124523, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:59.239914: step 5094, loss 0.110804, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:59.499979: step 5095, loss 0.118571, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:59.657839: step 5096, loss 0.130457, acc 0.960784, learning_rate 0.0001
2017-10-10T15:09:59.970315: step 5097, loss 0.0551698, acc 1, learning_rate 0.0001
2017-10-10T15:10:00.241165: step 5098, loss 0.179343, acc 0.90625, learning_rate 0.0001
2017-10-10T15:10:00.544986: step 5099, loss 0.0923787, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:00.867514: step 5100, loss 0.125423, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:01.023566: step 5101, loss 0.0401417, acc 1, learning_rate 0.0001
2017-10-10T15:10:01.205368: step 5102, loss 0.100021, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:01.451930: step 5103, loss 0.16921, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:01.657107: step 5104, loss 0.123056, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:01.920846: step 5105, loss 0.130086, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:02.181557: step 5106, loss 0.102997, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:02.440933: step 5107, loss 0.105474, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:02.659993: step 5108, loss 0.0783946, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:02.896610: step 5109, loss 0.0852782, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:03.168858: step 5110, loss 0.183104, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:03.390671: step 5111, loss 0.154493, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:03.625106: step 5112, loss 0.0431467, acc 1, learning_rate 0.0001
2017-10-10T15:10:03.941495: step 5113, loss 0.154918, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:04.146685: step 5114, loss 0.140565, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:04.397082: step 5115, loss 0.0787654, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:04.625494: step 5116, loss 0.0670617, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:04.857124: step 5117, loss 0.0877488, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:05.071806: step 5118, loss 0.140856, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:05.320666: step 5119, loss 0.201913, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:05.544394: step 5120, loss 0.215919, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T15:10:05.989055: step 5120, loss 0.193484, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5120

2017-10-10T15:10:06.922804: step 5121, loss 0.110326, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:07.163258: step 5122, loss 0.199756, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:07.398410: step 5123, loss 0.0707602, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:07.634575: step 5124, loss 0.121645, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:07.908488: step 5125, loss 0.183151, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:08.225045: step 5126, loss 0.306664, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:08.430455: step 5127, loss 0.0889015, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:08.684186: step 5128, loss 0.169531, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:08.868574: step 5129, loss 0.0799982, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:09.130495: step 5130, loss 0.0975033, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:09.397173: step 5131, loss 0.0902868, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:09.640178: step 5132, loss 0.193295, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:09.831092: step 5133, loss 0.0722994, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:10.058711: step 5134, loss 0.0926807, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:10.241741: step 5135, loss 0.12334, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:10.412843: step 5136, loss 0.102316, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:10.669049: step 5137, loss 0.0590602, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:10.882279: step 5138, loss 0.13881, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:11.170669: step 5139, loss 0.0932287, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:11.405071: step 5140, loss 0.0446514, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:11.689212: step 5141, loss 0.0743466, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:11.951407: step 5142, loss 0.107443, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:12.164838: step 5143, loss 0.127156, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:12.448897: step 5144, loss 0.139229, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:12.677597: step 5145, loss 0.0791899, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:12.899314: step 5146, loss 0.159898, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:13.153201: step 5147, loss 0.183789, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:13.356218: step 5148, loss 0.125835, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:13.664828: step 5149, loss 0.0891473, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:13.947460: step 5150, loss 0.0847875, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:14.176138: step 5151, loss 0.0603629, acc 1, learning_rate 0.0001
2017-10-10T15:10:14.399745: step 5152, loss 0.133446, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:14.604681: step 5153, loss 0.0956071, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:14.833106: step 5154, loss 0.196672, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:15.141071: step 5155, loss 0.0676867, acc 1, learning_rate 0.0001
2017-10-10T15:10:15.393518: step 5156, loss 0.093729, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:15.636877: step 5157, loss 0.16435, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:15.908954: step 5158, loss 0.125325, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:16.217343: step 5159, loss 0.0894054, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:16.474835: step 5160, loss 0.0718198, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:10:16.822509: step 5160, loss 0.194853, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5160

2017-10-10T15:10:17.843244: step 5161, loss 0.0894413, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:18.113544: step 5162, loss 0.101709, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:18.339067: step 5163, loss 0.023469, acc 1, learning_rate 0.0001
2017-10-10T15:10:18.603754: step 5164, loss 0.0440135, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:18.964969: step 5165, loss 0.0838915, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:19.124899: step 5166, loss 0.0511692, acc 1, learning_rate 0.0001
2017-10-10T15:10:19.364357: step 5167, loss 0.0480285, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:19.549885: step 5168, loss 0.172701, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:19.749750: step 5169, loss 0.0913921, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:19.986630: step 5170, loss 0.0808142, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:20.273722: step 5171, loss 0.169693, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:20.489058: step 5172, loss 0.0497013, acc 1, learning_rate 0.0001
2017-10-10T15:10:20.780029: step 5173, loss 0.142322, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:21.013319: step 5174, loss 0.0936299, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:21.225565: step 5175, loss 0.160968, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:21.460840: step 5176, loss 0.0535159, acc 1, learning_rate 0.0001
2017-10-10T15:10:21.713247: step 5177, loss 0.158032, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:21.973144: step 5178, loss 0.0699398, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:22.281415: step 5179, loss 0.092967, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:22.516535: step 5180, loss 0.129361, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:22.736120: step 5181, loss 0.0967543, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:23.000498: step 5182, loss 0.110093, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:23.301905: step 5183, loss 0.121534, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:23.535702: step 5184, loss 0.0853612, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:23.747807: step 5185, loss 0.16339, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:24.008947: step 5186, loss 0.118273, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:24.243827: step 5187, loss 0.0994278, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:24.472323: step 5188, loss 0.0669753, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:24.780947: step 5189, loss 0.0621845, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:25.016938: step 5190, loss 0.127471, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:25.257271: step 5191, loss 0.168239, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:25.517485: step 5192, loss 0.0987965, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:25.716678: step 5193, loss 0.0742152, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:26.016896: step 5194, loss 0.104826, acc 0.960784, learning_rate 0.0001
2017-10-10T15:10:26.321527: step 5195, loss 0.135055, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:26.530166: step 5196, loss 0.220494, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:26.726438: step 5197, loss 0.127234, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:26.923365: step 5198, loss 0.170257, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:27.208224: step 5199, loss 0.165567, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:27.412217: step 5200, loss 0.0641635, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:10:27.923299: step 5200, loss 0.19486, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5200

2017-10-10T15:10:28.920440: step 5201, loss 0.0903471, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:29.144844: step 5202, loss 0.06692, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:29.388876: step 5203, loss 0.0700613, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:29.611839: step 5204, loss 0.0984017, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:29.876137: step 5205, loss 0.072125, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:30.100058: step 5206, loss 0.196987, acc 0.90625, learning_rate 0.0001
2017-10-10T15:10:30.342233: step 5207, loss 0.106697, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:30.571909: step 5208, loss 0.108357, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:30.798797: step 5209, loss 0.169356, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:31.064952: step 5210, loss 0.0869285, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:31.231072: step 5211, loss 0.115636, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:31.543035: step 5212, loss 0.0606186, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:31.745933: step 5213, loss 0.119022, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:32.013859: step 5214, loss 0.19837, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:32.313918: step 5215, loss 0.132081, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:32.584668: step 5216, loss 0.21158, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:32.872049: step 5217, loss 0.155957, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:33.064960: step 5218, loss 0.0963431, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:33.344882: step 5219, loss 0.115095, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:33.594344: step 5220, loss 0.0995616, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:33.834817: step 5221, loss 0.0894978, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:34.109868: step 5222, loss 0.0596937, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:34.344927: step 5223, loss 0.11301, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:34.567898: step 5224, loss 0.162722, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:34.840939: step 5225, loss 0.155683, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:35.061022: step 5226, loss 0.1779, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:35.356247: step 5227, loss 0.145525, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:35.638294: step 5228, loss 0.113773, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:35.864947: step 5229, loss 0.187744, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:36.153032: step 5230, loss 0.0755439, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:36.373803: step 5231, loss 0.0411927, acc 1, learning_rate 0.0001
2017-10-10T15:10:36.620940: step 5232, loss 0.0521017, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:36.799393: step 5233, loss 0.0462772, acc 1, learning_rate 0.0001
2017-10-10T15:10:37.193211: step 5234, loss 0.209903, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:37.418182: step 5235, loss 0.164159, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:37.681896: step 5236, loss 0.0846972, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:37.918302: step 5237, loss 0.0727984, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:38.108951: step 5238, loss 0.0896659, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:38.340964: step 5239, loss 0.114277, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:38.577247: step 5240, loss 0.102523, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:10:39.004936: step 5240, loss 0.193453, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5240

2017-10-10T15:10:40.109027: step 5241, loss 0.0796128, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:40.397499: step 5242, loss 0.118058, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:40.700885: step 5243, loss 0.0789326, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:40.989112: step 5244, loss 0.0419005, acc 1, learning_rate 0.0001
2017-10-10T15:10:41.220339: step 5245, loss 0.186528, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:41.424850: step 5246, loss 0.11383, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:41.587443: step 5247, loss 0.163688, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:41.820860: step 5248, loss 0.158331, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:42.168880: step 5249, loss 0.0580359, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:42.430517: step 5250, loss 0.102364, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:42.758462: step 5251, loss 0.0714595, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:42.973083: step 5252, loss 0.0410569, acc 1, learning_rate 0.0001
2017-10-10T15:10:43.170406: step 5253, loss 0.183793, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:43.352879: step 5254, loss 0.0598034, acc 1, learning_rate 0.0001
2017-10-10T15:10:43.568993: step 5255, loss 0.120033, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:43.797072: step 5256, loss 0.283123, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:44.042595: step 5257, loss 0.121271, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:44.321492: step 5258, loss 0.0697137, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:44.526836: step 5259, loss 0.118593, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:44.784938: step 5260, loss 0.0765278, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:45.056938: step 5261, loss 0.142225, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:45.296341: step 5262, loss 0.107082, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:45.529956: step 5263, loss 0.0789033, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:45.745389: step 5264, loss 0.0689365, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:46.005086: step 5265, loss 0.112616, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:46.285368: step 5266, loss 0.0813492, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:46.512765: step 5267, loss 0.0914894, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:46.747489: step 5268, loss 0.111775, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:46.991261: step 5269, loss 0.114577, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:47.188137: step 5270, loss 0.163179, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:47.441658: step 5271, loss 0.132761, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:47.642212: step 5272, loss 0.125805, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:47.843547: step 5273, loss 0.096715, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:48.060041: step 5274, loss 0.0831968, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:48.350936: step 5275, loss 0.166697, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:48.560871: step 5276, loss 0.125276, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:48.864868: step 5277, loss 0.118388, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:49.148541: step 5278, loss 0.150168, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:49.487458: step 5279, loss 0.12128, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:49.725597: step 5280, loss 0.119176, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:10:50.103925: step 5280, loss 0.196561, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5280

2017-10-10T15:10:51.004887: step 5281, loss 0.0861255, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:51.245575: step 5282, loss 0.175793, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:51.513036: step 5283, loss 0.051478, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:51.796873: step 5284, loss 0.0930746, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:52.045669: step 5285, loss 0.0990476, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:52.274682: step 5286, loss 0.120088, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:52.564858: step 5287, loss 0.160853, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:52.810406: step 5288, loss 0.0805312, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:53.061152: step 5289, loss 0.0831272, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:53.340370: step 5290, loss 0.0513849, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:53.563749: step 5291, loss 0.0338474, acc 1, learning_rate 0.0001
2017-10-10T15:10:53.795470: step 5292, loss 0.123383, acc 0.960784, learning_rate 0.0001
2017-10-10T15:10:54.059191: step 5293, loss 0.215122, acc 0.90625, learning_rate 0.0001
2017-10-10T15:10:54.298382: step 5294, loss 0.0693141, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:54.516860: step 5295, loss 0.0723612, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:54.783179: step 5296, loss 0.0388148, acc 1, learning_rate 0.0001
2017-10-10T15:10:55.025220: step 5297, loss 0.157966, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:55.292593: step 5298, loss 0.122853, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:55.527331: step 5299, loss 0.112373, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:55.748830: step 5300, loss 0.0909436, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:55.975437: step 5301, loss 0.0568107, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:56.226410: step 5302, loss 0.138587, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:56.449011: step 5303, loss 0.190953, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:56.680913: step 5304, loss 0.0975868, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:56.905873: step 5305, loss 0.0675195, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:57.161311: step 5306, loss 0.118096, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:57.429531: step 5307, loss 0.164186, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:57.650514: step 5308, loss 0.0829854, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:57.947122: step 5309, loss 0.132106, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:58.197048: step 5310, loss 0.0692254, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:58.416016: step 5311, loss 0.0691354, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:58.646786: step 5312, loss 0.165169, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:58.892830: step 5313, loss 0.152409, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:59.253166: step 5314, loss 0.113166, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:59.440836: step 5315, loss 0.0333318, acc 1, learning_rate 0.0001
2017-10-10T15:10:59.741152: step 5316, loss 0.0411697, acc 1, learning_rate 0.0001
2017-10-10T15:10:59.956860: step 5317, loss 0.11972, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:00.139918: step 5318, loss 0.206672, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:00.370218: step 5319, loss 0.142123, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:00.599788: step 5320, loss 0.110795, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:11:00.978410: step 5320, loss 0.197787, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5320

2017-10-10T15:11:02.007062: step 5321, loss 0.0889331, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:02.273137: step 5322, loss 0.265466, acc 0.90625, learning_rate 0.0001
2017-10-10T15:11:02.500402: step 5323, loss 0.129217, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:02.705147: step 5324, loss 0.0981172, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:02.951577: step 5325, loss 0.145722, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:03.240330: step 5326, loss 0.0532274, acc 1, learning_rate 0.0001
2017-10-10T15:11:03.512992: step 5327, loss 0.174465, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:03.718847: step 5328, loss 0.0900456, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:03.992400: step 5329, loss 0.136978, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:04.209131: step 5330, loss 0.064403, acc 1, learning_rate 0.0001
2017-10-10T15:11:04.514784: step 5331, loss 0.026703, acc 1, learning_rate 0.0001
2017-10-10T15:11:04.789127: step 5332, loss 0.042508, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:05.007416: step 5333, loss 0.110076, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:05.220840: step 5334, loss 0.0707165, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:05.506112: step 5335, loss 0.139328, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:05.751604: step 5336, loss 0.098001, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:06.068960: step 5337, loss 0.154527, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:06.343518: step 5338, loss 0.0682228, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:06.551385: step 5339, loss 0.0794813, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:06.783863: step 5340, loss 0.0598718, acc 1, learning_rate 0.0001
2017-10-10T15:11:07.096905: step 5341, loss 0.145009, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:07.327686: step 5342, loss 0.118173, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:07.528094: step 5343, loss 0.144753, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:07.712901: step 5344, loss 0.123612, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:07.957001: step 5345, loss 0.0969266, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:08.196805: step 5346, loss 0.105208, acc 1, learning_rate 0.0001
2017-10-10T15:11:08.476152: step 5347, loss 0.0654766, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:08.726885: step 5348, loss 0.0705962, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:09.004963: step 5349, loss 0.0573526, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:09.277871: step 5350, loss 0.198311, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:09.484577: step 5351, loss 0.0630429, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:09.759650: step 5352, loss 0.136212, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:09.998734: step 5353, loss 0.0875546, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:10.253368: step 5354, loss 0.0877153, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:10.502517: step 5355, loss 0.0935119, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:10.745192: step 5356, loss 0.159508, acc 0.90625, learning_rate 0.0001
2017-10-10T15:11:11.005012: step 5357, loss 0.123958, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:11.227088: step 5358, loss 0.0611838, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:11.457643: step 5359, loss 0.131067, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:11.704856: step 5360, loss 0.184429, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:11:12.097136: step 5360, loss 0.194455, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5360

2017-10-10T15:11:13.195840: step 5361, loss 0.0551225, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:13.416630: step 5362, loss 0.0443553, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:13.621356: step 5363, loss 0.100639, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:13.801457: step 5364, loss 0.125268, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:14.029520: step 5365, loss 0.0981255, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:14.296443: step 5366, loss 0.0694465, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:14.533542: step 5367, loss 0.0978666, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:14.772018: step 5368, loss 0.186055, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:14.935537: step 5369, loss 0.109125, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:15.208040: step 5370, loss 0.14723, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:15.448916: step 5371, loss 0.0731414, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:15.720895: step 5372, loss 0.139394, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:15.978788: step 5373, loss 0.173542, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:16.264948: step 5374, loss 0.10752, acc 1, learning_rate 0.0001
2017-10-10T15:11:16.528832: step 5375, loss 0.0819012, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:16.745058: step 5376, loss 0.0704664, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:16.958785: step 5377, loss 0.140445, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:17.171651: step 5378, loss 0.0435409, acc 1, learning_rate 0.0001
2017-10-10T15:11:17.428448: step 5379, loss 0.148706, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:17.688265: step 5380, loss 0.0742345, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:17.894436: step 5381, loss 0.172202, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:18.173077: step 5382, loss 0.120213, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:18.425026: step 5383, loss 0.0850368, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:18.652110: step 5384, loss 0.106363, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:18.916927: step 5385, loss 0.107681, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:19.121123: step 5386, loss 0.139374, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:19.332839: step 5387, loss 0.119229, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:19.641012: step 5388, loss 0.150528, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:19.865108: step 5389, loss 0.0378459, acc 1, learning_rate 0.0001
2017-10-10T15:11:20.095422: step 5390, loss 0.106661, acc 0.980392, learning_rate 0.0001
2017-10-10T15:11:20.353169: step 5391, loss 0.0583672, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:20.582560: step 5392, loss 0.140439, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:20.859162: step 5393, loss 0.0970051, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:21.112959: step 5394, loss 0.10583, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:21.294297: step 5395, loss 0.12215, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:21.517854: step 5396, loss 0.104937, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:21.752933: step 5397, loss 0.0777726, acc 1, learning_rate 0.0001
2017-10-10T15:11:22.018517: step 5398, loss 0.14714, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:22.221014: step 5399, loss 0.119456, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:22.516041: step 5400, loss 0.078788, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:11:22.911316: step 5400, loss 0.193312, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5400

2017-10-10T15:11:24.024940: step 5401, loss 0.0770877, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:24.217062: step 5402, loss 0.113394, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:24.512850: step 5403, loss 0.152918, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:24.681502: step 5404, loss 0.122838, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:24.933983: step 5405, loss 0.121073, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:25.125553: step 5406, loss 0.0967605, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:25.422830: step 5407, loss 0.0888376, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:25.643104: step 5408, loss 0.152696, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:25.896524: step 5409, loss 0.0766949, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:26.148913: step 5410, loss 0.125617, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:26.432450: step 5411, loss 0.224045, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:26.666982: step 5412, loss 0.0628356, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:26.852931: step 5413, loss 0.212434, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:27.112909: step 5414, loss 0.12051, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:27.339482: step 5415, loss 0.0607505, acc 1, learning_rate 0.0001
2017-10-10T15:11:27.638095: step 5416, loss 0.0999919, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:27.859104: step 5417, loss 0.035069, acc 1, learning_rate 0.0001
2017-10-10T15:11:28.075293: step 5418, loss 0.144396, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:28.403168: step 5419, loss 0.034674, acc 1, learning_rate 0.0001
2017-10-10T15:11:28.673224: step 5420, loss 0.0746535, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:28.952962: step 5421, loss 0.0368879, acc 1, learning_rate 0.0001
2017-10-10T15:11:29.197143: step 5422, loss 0.109862, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:29.458394: step 5423, loss 0.15295, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:29.641575: step 5424, loss 0.0311502, acc 1, learning_rate 0.0001
2017-10-10T15:11:29.924846: step 5425, loss 0.180642, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:30.153070: step 5426, loss 0.0586775, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:30.373127: step 5427, loss 0.154035, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:30.671583: step 5428, loss 0.262677, acc 0.90625, learning_rate 0.0001
2017-10-10T15:11:30.973279: step 5429, loss 0.144676, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:31.271447: step 5430, loss 0.108112, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:31.462724: step 5431, loss 0.145762, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:31.661813: step 5432, loss 0.160093, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:31.821153: step 5433, loss 0.111552, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:32.018803: step 5434, loss 0.11876, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:32.167569: step 5435, loss 0.0653792, acc 1, learning_rate 0.0001
2017-10-10T15:11:32.425841: step 5436, loss 0.143784, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:32.644955: step 5437, loss 0.0953628, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:32.956406: step 5438, loss 0.0596846, acc 1, learning_rate 0.0001
2017-10-10T15:11:33.136787: step 5439, loss 0.0997416, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:33.395490: step 5440, loss 0.102478, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:11:33.737245: step 5440, loss 0.191533, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5440

2017-10-10T15:11:34.640948: step 5441, loss 0.075645, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:34.856581: step 5442, loss 0.106464, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:35.111293: step 5443, loss 0.0962728, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:35.335223: step 5444, loss 0.151672, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:35.565064: step 5445, loss 0.13858, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:35.867884: step 5446, loss 0.0870087, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:36.105110: step 5447, loss 0.102347, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:36.365011: step 5448, loss 0.116823, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:36.629003: step 5449, loss 0.0483542, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:36.891436: step 5450, loss 0.118744, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:37.127846: step 5451, loss 0.0770402, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:37.388326: step 5452, loss 0.223203, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:37.704821: step 5453, loss 0.133417, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:37.996832: step 5454, loss 0.147057, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:38.225029: step 5455, loss 0.109304, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:38.502183: step 5456, loss 0.195363, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:38.780922: step 5457, loss 0.245549, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:38.964864: step 5458, loss 0.135071, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:39.231744: step 5459, loss 0.064483, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:39.483322: step 5460, loss 0.23117, acc 0.90625, learning_rate 0.0001
2017-10-10T15:11:39.702867: step 5461, loss 0.10465, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:39.967212: step 5462, loss 0.146184, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:40.201165: step 5463, loss 0.0713883, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:40.432721: step 5464, loss 0.0571857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:40.630850: step 5465, loss 0.101595, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:40.890121: step 5466, loss 0.0684855, acc 1, learning_rate 0.0001
2017-10-10T15:11:41.101936: step 5467, loss 0.0916824, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:41.368894: step 5468, loss 0.0920712, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:41.590360: step 5469, loss 0.0937802, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:41.830622: step 5470, loss 0.18279, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:42.093057: step 5471, loss 0.134302, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:42.338045: step 5472, loss 0.0468935, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:42.585004: step 5473, loss 0.133545, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:42.825374: step 5474, loss 0.0529503, acc 1, learning_rate 0.0001
2017-10-10T15:11:43.144767: step 5475, loss 0.116668, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:43.330426: step 5476, loss 0.10286, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:43.601581: step 5477, loss 0.0675829, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:43.814968: step 5478, loss 0.0570317, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:44.036853: step 5479, loss 0.0622521, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:44.312822: step 5480, loss 0.0882272, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:11:44.752944: step 5480, loss 0.193063, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5480

2017-10-10T15:11:45.885004: step 5481, loss 0.058684, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:46.098410: step 5482, loss 0.125996, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:46.365471: step 5483, loss 0.156131, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:46.567360: step 5484, loss 0.103612, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:46.853054: step 5485, loss 0.0843487, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:47.121154: step 5486, loss 0.0997419, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:47.384835: step 5487, loss 0.0940517, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:47.628239: step 5488, loss 0.221762, acc 0.941176, learning_rate 0.0001
2017-10-10T15:11:47.851874: step 5489, loss 0.116561, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:48.068578: step 5490, loss 0.0940164, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:48.344860: step 5491, loss 0.0991459, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:48.603142: step 5492, loss 0.0366469, acc 1, learning_rate 0.0001
2017-10-10T15:11:48.864180: step 5493, loss 0.102338, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:49.139608: step 5494, loss 0.0574767, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:49.444613: step 5495, loss 0.0993401, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:49.705995: step 5496, loss 0.203624, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:49.940732: step 5497, loss 0.0920054, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:50.140838: step 5498, loss 0.204826, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:50.339815: step 5499, loss 0.0656993, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:50.543382: step 5500, loss 0.0618885, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:50.748868: step 5501, loss 0.216341, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:51.044825: step 5502, loss 0.0553555, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:51.277298: step 5503, loss 0.16331, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:51.571393: step 5504, loss 0.214396, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:51.828061: step 5505, loss 0.0517336, acc 1, learning_rate 0.0001
2017-10-10T15:11:52.068823: step 5506, loss 0.151975, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:52.300906: step 5507, loss 0.124178, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:52.551188: step 5508, loss 0.0532443, acc 1, learning_rate 0.0001
2017-10-10T15:11:52.837397: step 5509, loss 0.0569291, acc 1, learning_rate 0.0001
2017-10-10T15:11:53.030024: step 5510, loss 0.119083, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:53.337013: step 5511, loss 0.111433, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:53.545073: step 5512, loss 0.131101, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:53.784678: step 5513, loss 0.0770683, acc 1, learning_rate 0.0001
2017-10-10T15:11:54.012895: step 5514, loss 0.090231, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:54.256544: step 5515, loss 0.0839309, acc 1, learning_rate 0.0001
2017-10-10T15:11:54.488867: step 5516, loss 0.212704, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:54.747798: step 5517, loss 0.104858, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:55.008937: step 5518, loss 0.0435175, acc 1, learning_rate 0.0001
2017-10-10T15:11:55.289149: step 5519, loss 0.13272, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:55.519612: step 5520, loss 0.101399, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:11:55.864457: step 5520, loss 0.193771, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5520

2017-10-10T15:11:56.972838: step 5521, loss 0.0745906, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:57.209039: step 5522, loss 0.122247, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:57.513027: step 5523, loss 0.0825794, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:57.778166: step 5524, loss 0.0446592, acc 1, learning_rate 0.0001
2017-10-10T15:11:58.055564: step 5525, loss 0.0940624, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:58.304538: step 5526, loss 0.0847936, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:58.568902: step 5527, loss 0.123329, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:58.843717: step 5528, loss 0.180386, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:59.077023: step 5529, loss 0.094139, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:59.298912: step 5530, loss 0.220982, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:59.594698: step 5531, loss 0.0722332, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:59.824253: step 5532, loss 0.0589824, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:00.108854: step 5533, loss 0.119449, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:00.436829: step 5534, loss 0.0604449, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:00.625231: step 5535, loss 0.209873, acc 0.890625, learning_rate 0.0001
2017-10-10T15:12:00.875115: step 5536, loss 0.0744654, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:01.151846: step 5537, loss 0.10608, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:01.344567: step 5538, loss 0.0887677, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:01.616693: step 5539, loss 0.062641, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:01.881622: step 5540, loss 0.153165, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:02.101751: step 5541, loss 0.0335005, acc 1, learning_rate 0.0001
2017-10-10T15:12:02.348883: step 5542, loss 0.0598845, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:02.632447: step 5543, loss 0.069805, acc 1, learning_rate 0.0001
2017-10-10T15:12:02.909041: step 5544, loss 0.0622626, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:03.142503: step 5545, loss 0.129423, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:03.392809: step 5546, loss 0.0978809, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:03.628963: step 5547, loss 0.0854377, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:03.862969: step 5548, loss 0.0605256, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:04.147287: step 5549, loss 0.138303, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:04.390933: step 5550, loss 0.142015, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:04.609510: step 5551, loss 0.173888, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:04.832598: step 5552, loss 0.1425, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:05.072053: step 5553, loss 0.136386, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:05.368895: step 5554, loss 0.0671687, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:05.600932: step 5555, loss 0.0781339, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:05.900604: step 5556, loss 0.0602855, acc 1, learning_rate 0.0001
2017-10-10T15:12:06.120236: step 5557, loss 0.0632111, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:06.322538: step 5558, loss 0.0984393, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:06.520614: step 5559, loss 0.193605, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:06.742426: step 5560, loss 0.0478906, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:07.140015: step 5560, loss 0.193114, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5560

2017-10-10T15:12:08.258502: step 5561, loss 0.108499, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:08.489871: step 5562, loss 0.175525, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:08.692468: step 5563, loss 0.138076, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:08.907243: step 5564, loss 0.106711, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:09.178687: step 5565, loss 0.169287, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:09.390510: step 5566, loss 0.0931953, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:09.616889: step 5567, loss 0.223653, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:09.904859: step 5568, loss 0.0580152, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:10.164895: step 5569, loss 0.132183, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:10.427484: step 5570, loss 0.0997831, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:10.703667: step 5571, loss 0.189684, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:10.948761: step 5572, loss 0.0787324, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:11.189044: step 5573, loss 0.0327524, acc 1, learning_rate 0.0001
2017-10-10T15:12:11.400624: step 5574, loss 0.10768, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:11.727294: step 5575, loss 0.0654279, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:11.939887: step 5576, loss 0.0895694, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:12.218916: step 5577, loss 0.185021, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:12.492971: step 5578, loss 0.264772, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:12.704966: step 5579, loss 0.0744052, acc 1, learning_rate 0.0001
2017-10-10T15:12:12.984202: step 5580, loss 0.037647, acc 1, learning_rate 0.0001
2017-10-10T15:12:13.234356: step 5581, loss 0.178313, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:13.488980: step 5582, loss 0.132931, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:13.800597: step 5583, loss 0.0574223, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:14.047237: step 5584, loss 0.0715288, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:14.252563: step 5585, loss 0.0772742, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:14.413743: step 5586, loss 0.177838, acc 0.960784, learning_rate 0.0001
2017-10-10T15:12:14.681129: step 5587, loss 0.0977276, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:14.898668: step 5588, loss 0.200085, acc 0.890625, learning_rate 0.0001
2017-10-10T15:12:15.175436: step 5589, loss 0.111714, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:15.448832: step 5590, loss 0.109932, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:15.718625: step 5591, loss 0.111119, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:15.977274: step 5592, loss 0.0486678, acc 1, learning_rate 0.0001
2017-10-10T15:12:16.253842: step 5593, loss 0.132503, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:16.552853: step 5594, loss 0.0791267, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:16.751658: step 5595, loss 0.134223, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:17.048541: step 5596, loss 0.134832, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:17.305012: step 5597, loss 0.0994665, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:17.495874: step 5598, loss 0.103697, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:17.774061: step 5599, loss 0.101491, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:17.994183: step 5600, loss 0.16038, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:18.372928: step 5600, loss 0.192254, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5600

2017-10-10T15:12:19.314363: step 5601, loss 0.0860512, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:19.546896: step 5602, loss 0.146765, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:19.871697: step 5603, loss 0.12217, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:20.145971: step 5604, loss 0.0382348, acc 1, learning_rate 0.0001
2017-10-10T15:12:20.328817: step 5605, loss 0.117215, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:20.565302: step 5606, loss 0.176363, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:20.813242: step 5607, loss 0.0570555, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:21.047206: step 5608, loss 0.137505, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:21.255525: step 5609, loss 0.0842528, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:21.510380: step 5610, loss 0.116154, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:21.768910: step 5611, loss 0.135531, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:21.973717: step 5612, loss 0.0995823, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:22.243152: step 5613, loss 0.173758, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:22.530788: step 5614, loss 0.1336, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:22.820329: step 5615, loss 0.0652855, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:23.001156: step 5616, loss 0.0886762, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:23.229486: step 5617, loss 0.215645, acc 0.90625, learning_rate 0.0001
2017-10-10T15:12:23.473646: step 5618, loss 0.152531, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:23.682997: step 5619, loss 0.0952421, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:23.940969: step 5620, loss 0.113743, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:24.210273: step 5621, loss 0.103058, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:24.418996: step 5622, loss 0.0651329, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:24.733088: step 5623, loss 0.0910757, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:24.969337: step 5624, loss 0.21056, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:25.206769: step 5625, loss 0.0682346, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:25.460366: step 5626, loss 0.0632938, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:25.667014: step 5627, loss 0.0445629, acc 1, learning_rate 0.0001
2017-10-10T15:12:25.938646: step 5628, loss 0.195877, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:26.195359: step 5629, loss 0.120588, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:26.464850: step 5630, loss 0.0361657, acc 1, learning_rate 0.0001
2017-10-10T15:12:26.703240: step 5631, loss 0.134973, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:26.994074: step 5632, loss 0.0861663, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:27.214389: step 5633, loss 0.0291552, acc 1, learning_rate 0.0001
2017-10-10T15:12:27.404826: step 5634, loss 0.0870534, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:27.664825: step 5635, loss 0.0896971, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:27.876581: step 5636, loss 0.0705138, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:28.150211: step 5637, loss 0.140984, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:28.399290: step 5638, loss 0.125359, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:28.661836: step 5639, loss 0.0509689, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:28.920168: step 5640, loss 0.142404, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:29.365020: step 5640, loss 0.191295, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5640

2017-10-10T15:12:30.334721: step 5641, loss 0.269138, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:30.607169: step 5642, loss 0.0275954, acc 1, learning_rate 0.0001
2017-10-10T15:12:30.833252: step 5643, loss 0.0780759, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:31.088839: step 5644, loss 0.138954, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:31.380977: step 5645, loss 0.0708339, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:31.633167: step 5646, loss 0.0946836, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:31.905023: step 5647, loss 0.259472, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:32.183441: step 5648, loss 0.0900012, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:32.432164: step 5649, loss 0.128002, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:32.680828: step 5650, loss 0.095042, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:32.940744: step 5651, loss 0.0342601, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:33.225709: step 5652, loss 0.15913, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:33.484918: step 5653, loss 0.0704459, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:33.726270: step 5654, loss 0.15034, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:33.995291: step 5655, loss 0.0987211, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:34.222210: step 5656, loss 0.0715521, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:34.476878: step 5657, loss 0.0841803, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:34.714147: step 5658, loss 0.119137, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:34.940881: step 5659, loss 0.0923209, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:35.194046: step 5660, loss 0.170021, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:35.498994: step 5661, loss 0.034885, acc 1, learning_rate 0.0001
2017-10-10T15:12:35.707969: step 5662, loss 0.0654218, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:35.993050: step 5663, loss 0.126351, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:36.260948: step 5664, loss 0.122181, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:36.492867: step 5665, loss 0.124904, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:36.765689: step 5666, loss 0.0763737, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:37.052873: step 5667, loss 0.10038, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:37.230620: step 5668, loss 0.0757414, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:37.504837: step 5669, loss 0.0980433, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:37.745303: step 5670, loss 0.119537, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:38.000548: step 5671, loss 0.103948, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:38.207925: step 5672, loss 0.113102, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:38.452483: step 5673, loss 0.0558627, acc 1, learning_rate 0.0001
2017-10-10T15:12:38.698832: step 5674, loss 0.0660969, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:38.915503: step 5675, loss 0.0895814, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:39.200988: step 5676, loss 0.0616883, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:39.479292: step 5677, loss 0.117888, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:39.730167: step 5678, loss 0.0799621, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:39.960801: step 5679, loss 0.083089, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:40.224815: step 5680, loss 0.1785, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:40.619007: step 5680, loss 0.191281, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5680

2017-10-10T15:12:41.829181: step 5681, loss 0.0872949, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:42.085485: step 5682, loss 0.0693914, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:42.336866: step 5683, loss 0.0465606, acc 1, learning_rate 0.0001
2017-10-10T15:12:42.540340: step 5684, loss 0.0907512, acc 0.960784, learning_rate 0.0001
2017-10-10T15:12:42.745094: step 5685, loss 0.135572, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:43.021108: step 5686, loss 0.123295, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:43.284928: step 5687, loss 0.084862, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:43.508297: step 5688, loss 0.0743911, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:43.735358: step 5689, loss 0.151036, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:44.029055: step 5690, loss 0.0942774, acc 1, learning_rate 0.0001
2017-10-10T15:12:44.300872: step 5691, loss 0.0643447, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:44.520937: step 5692, loss 0.0761622, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:44.765499: step 5693, loss 0.168799, acc 0.90625, learning_rate 0.0001
2017-10-10T15:12:44.927235: step 5694, loss 0.0512292, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:45.116815: step 5695, loss 0.0502351, acc 1, learning_rate 0.0001
2017-10-10T15:12:45.355555: step 5696, loss 0.108646, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:45.552840: step 5697, loss 0.159621, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:45.812524: step 5698, loss 0.141303, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:46.075182: step 5699, loss 0.122423, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:46.291264: step 5700, loss 0.109561, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:46.528227: step 5701, loss 0.167677, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:46.752840: step 5702, loss 0.210078, acc 0.890625, learning_rate 0.0001
2017-10-10T15:12:46.989608: step 5703, loss 0.0896262, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:47.243596: step 5704, loss 0.0726571, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:47.502453: step 5705, loss 0.0428466, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:47.743705: step 5706, loss 0.082261, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:47.971966: step 5707, loss 0.124524, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:48.193048: step 5708, loss 0.10432, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:48.449242: step 5709, loss 0.180961, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:48.695050: step 5710, loss 0.0934983, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:48.913118: step 5711, loss 0.109089, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:49.184872: step 5712, loss 0.172847, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:49.396876: step 5713, loss 0.125219, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:49.669769: step 5714, loss 0.124206, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:49.851548: step 5715, loss 0.0935167, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:50.202088: step 5716, loss 0.10768, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:50.443043: step 5717, loss 0.110694, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:50.716916: step 5718, loss 0.0995958, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:51.004954: step 5719, loss 0.17313, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:51.296890: step 5720, loss 0.16183, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:51.698771: step 5720, loss 0.191701, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5720

2017-10-10T15:12:52.824870: step 5721, loss 0.0967566, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:53.044884: step 5722, loss 0.0725274, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:53.348867: step 5723, loss 0.061367, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:53.583895: step 5724, loss 0.133772, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:53.813664: step 5725, loss 0.138479, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:54.017913: step 5726, loss 0.0638479, acc 1, learning_rate 0.0001
2017-10-10T15:12:54.258522: step 5727, loss 0.065074, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:54.521980: step 5728, loss 0.0662622, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:54.789136: step 5729, loss 0.150946, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:55.007815: step 5730, loss 0.143339, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:55.257997: step 5731, loss 0.1873, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:55.465013: step 5732, loss 0.0289705, acc 1, learning_rate 0.0001
2017-10-10T15:12:55.795763: step 5733, loss 0.0744891, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:56.113769: step 5734, loss 0.0469898, acc 1, learning_rate 0.0001
2017-10-10T15:12:56.324604: step 5735, loss 0.14222, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:56.584892: step 5736, loss 0.104571, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:56.821211: step 5737, loss 0.0623085, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:57.060478: step 5738, loss 0.145402, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:57.345626: step 5739, loss 0.146876, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:57.544514: step 5740, loss 0.116069, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:57.821539: step 5741, loss 0.0677719, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:58.081548: step 5742, loss 0.125993, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:58.281025: step 5743, loss 0.162043, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:58.534471: step 5744, loss 0.198769, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:58.769766: step 5745, loss 0.160205, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:59.026295: step 5746, loss 0.0947542, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:59.241083: step 5747, loss 0.176689, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:59.512240: step 5748, loss 0.15763, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:59.768933: step 5749, loss 0.120094, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:00.010182: step 5750, loss 0.134244, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:00.229142: step 5751, loss 0.150829, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:00.480836: step 5752, loss 0.107053, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:00.787376: step 5753, loss 0.067869, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:00.971237: step 5754, loss 0.0854006, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:01.251215: step 5755, loss 0.159004, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:01.504905: step 5756, loss 0.168844, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:01.724944: step 5757, loss 0.127734, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:02.007059: step 5758, loss 0.0430164, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:02.205340: step 5759, loss 0.101476, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:02.516974: step 5760, loss 0.0850232, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:03.003925: step 5760, loss 0.191724, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5760

2017-10-10T15:13:04.146913: step 5761, loss 0.107167, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:04.457630: step 5762, loss 0.0714994, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:04.707981: step 5763, loss 0.113015, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:04.951621: step 5764, loss 0.182176, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:05.198565: step 5765, loss 0.12454, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:05.481390: step 5766, loss 0.0718188, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:05.729768: step 5767, loss 0.100507, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:05.956292: step 5768, loss 0.145945, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:06.211663: step 5769, loss 0.0631496, acc 1, learning_rate 0.0001
2017-10-10T15:13:06.472919: step 5770, loss 0.123605, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:06.749170: step 5771, loss 0.0467446, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:06.972977: step 5772, loss 0.111601, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:07.163225: step 5773, loss 0.124264, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:07.388919: step 5774, loss 0.101821, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:07.609073: step 5775, loss 0.111669, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:07.848206: step 5776, loss 0.0772421, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:08.113516: step 5777, loss 0.127036, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:08.388863: step 5778, loss 0.0446069, acc 1, learning_rate 0.0001
2017-10-10T15:13:08.696997: step 5779, loss 0.0954485, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:08.886000: step 5780, loss 0.0811253, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:09.112058: step 5781, loss 0.0617131, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:09.301321: step 5782, loss 0.040967, acc 1, learning_rate 0.0001
2017-10-10T15:13:09.514580: step 5783, loss 0.060684, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:09.821492: step 5784, loss 0.140782, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:10.032961: step 5785, loss 0.150087, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:10.300514: step 5786, loss 0.172942, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:10.592851: step 5787, loss 0.0474421, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:10.842066: step 5788, loss 0.0916964, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:11.120352: step 5789, loss 0.128844, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:11.356996: step 5790, loss 0.103738, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:11.587304: step 5791, loss 0.123722, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:11.846750: step 5792, loss 0.102877, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:12.136840: step 5793, loss 0.0855335, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:12.456633: step 5794, loss 0.117723, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:12.668251: step 5795, loss 0.089054, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:12.846276: step 5796, loss 0.0653674, acc 1, learning_rate 0.0001
2017-10-10T15:13:13.077115: step 5797, loss 0.0599754, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:13.276876: step 5798, loss 0.0615509, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:13.579781: step 5799, loss 0.07777, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:13.848916: step 5800, loss 0.06458, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:14.300226: step 5800, loss 0.19301, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5800

2017-10-10T15:13:15.446342: step 5801, loss 0.0814024, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:15.621002: step 5802, loss 0.11255, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:15.921679: step 5803, loss 0.0890557, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:16.137190: step 5804, loss 0.0606209, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:16.433028: step 5805, loss 0.060306, acc 1, learning_rate 0.0001
2017-10-10T15:13:16.680212: step 5806, loss 0.0597782, acc 1, learning_rate 0.0001
2017-10-10T15:13:16.887030: step 5807, loss 0.0834835, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:17.140910: step 5808, loss 0.15638, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:17.409073: step 5809, loss 0.145292, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:17.693768: step 5810, loss 0.0991448, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:17.921545: step 5811, loss 0.104945, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:18.180925: step 5812, loss 0.0567721, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:18.468352: step 5813, loss 0.0564465, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:18.722521: step 5814, loss 0.10461, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:18.960932: step 5815, loss 0.0646584, acc 1, learning_rate 0.0001
2017-10-10T15:13:19.235732: step 5816, loss 0.153807, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:19.482959: step 5817, loss 0.108323, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:19.700873: step 5818, loss 0.10868, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:20.036886: step 5819, loss 0.101216, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:20.208837: step 5820, loss 0.17589, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:20.474197: step 5821, loss 0.131673, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:20.775143: step 5822, loss 0.0998981, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:21.064983: step 5823, loss 0.168747, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:21.337007: step 5824, loss 0.120293, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:21.566786: step 5825, loss 0.158988, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:21.766137: step 5826, loss 0.0830542, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:21.966530: step 5827, loss 0.062589, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:22.213120: step 5828, loss 0.15519, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:22.478813: step 5829, loss 0.101305, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:22.668874: step 5830, loss 0.0694892, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:22.960474: step 5831, loss 0.0982921, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:23.285064: step 5832, loss 0.135889, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:23.483915: step 5833, loss 0.152649, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:23.736225: step 5834, loss 0.124264, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:23.943432: step 5835, loss 0.0704242, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:24.215982: step 5836, loss 0.18324, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:24.434970: step 5837, loss 0.103858, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:24.692710: step 5838, loss 0.0585212, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:24.952935: step 5839, loss 0.0534385, acc 1, learning_rate 0.0001
2017-10-10T15:13:25.212920: step 5840, loss 0.137828, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:25.528550: step 5840, loss 0.192019, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5840

2017-10-10T15:13:26.626396: step 5841, loss 0.0682173, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:26.875939: step 5842, loss 0.0444379, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:27.146587: step 5843, loss 0.0978167, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:27.396997: step 5844, loss 0.0721721, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:27.586620: step 5845, loss 0.155231, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:27.889375: step 5846, loss 0.0937252, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:28.164945: step 5847, loss 0.088091, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:28.383971: step 5848, loss 0.085199, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:28.700818: step 5849, loss 0.0436848, acc 1, learning_rate 0.0001
2017-10-10T15:13:28.988633: step 5850, loss 0.0958394, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:29.236156: step 5851, loss 0.120796, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:29.440805: step 5852, loss 0.131556, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:29.666145: step 5853, loss 0.113142, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:29.872885: step 5854, loss 0.0590166, acc 1, learning_rate 0.0001
2017-10-10T15:13:30.109087: step 5855, loss 0.0935522, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:30.340595: step 5856, loss 0.0249975, acc 1, learning_rate 0.0001
2017-10-10T15:13:30.624868: step 5857, loss 0.111208, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:30.853115: step 5858, loss 0.082948, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:31.190707: step 5859, loss 0.0389296, acc 1, learning_rate 0.0001
2017-10-10T15:13:31.469989: step 5860, loss 0.0826608, acc 1, learning_rate 0.0001
2017-10-10T15:13:31.689424: step 5861, loss 0.104934, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:31.989064: step 5862, loss 0.0713172, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:32.153214: step 5863, loss 0.107459, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:32.411386: step 5864, loss 0.0721958, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:32.744868: step 5865, loss 0.0403472, acc 1, learning_rate 0.0001
2017-10-10T15:13:33.029010: step 5866, loss 0.12339, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:33.321117: step 5867, loss 0.0976186, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:33.586638: step 5868, loss 0.138494, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:33.821448: step 5869, loss 0.0583441, acc 1, learning_rate 0.0001
2017-10-10T15:13:34.028964: step 5870, loss 0.0866068, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:34.258588: step 5871, loss 0.0866412, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:34.518335: step 5872, loss 0.146425, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:34.745131: step 5873, loss 0.155674, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:34.978295: step 5874, loss 0.101807, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:35.225889: step 5875, loss 0.0934495, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:35.435459: step 5876, loss 0.170126, acc 0.90625, learning_rate 0.0001
2017-10-10T15:13:35.649122: step 5877, loss 0.0311181, acc 1, learning_rate 0.0001
2017-10-10T15:13:35.912621: step 5878, loss 0.0674889, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:36.265060: step 5879, loss 0.102465, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:36.481416: step 5880, loss 0.085284, acc 0.980392, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:36.868386: step 5880, loss 0.191914, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5880

2017-10-10T15:13:37.835596: step 5881, loss 0.127962, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:38.100953: step 5882, loss 0.0836022, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:38.364825: step 5883, loss 0.0598229, acc 1, learning_rate 0.0001
2017-10-10T15:13:38.665338: step 5884, loss 0.108888, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:38.944931: step 5885, loss 0.107183, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:39.231711: step 5886, loss 0.0798261, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:39.458269: step 5887, loss 0.0322866, acc 1, learning_rate 0.0001
2017-10-10T15:13:39.712841: step 5888, loss 0.111968, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:39.899030: step 5889, loss 0.0413126, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:40.095516: step 5890, loss 0.0764948, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:40.324466: step 5891, loss 0.0761567, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:40.530409: step 5892, loss 0.112028, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:40.820557: step 5893, loss 0.0556877, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:41.044053: step 5894, loss 0.0696769, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:41.308967: step 5895, loss 0.0310534, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:41.569264: step 5896, loss 0.085214, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:41.808959: step 5897, loss 0.0811135, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:42.068913: step 5898, loss 0.0412925, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:42.291341: step 5899, loss 0.0709084, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:42.548871: step 5900, loss 0.0907678, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:42.794904: step 5901, loss 0.083299, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:43.056557: step 5902, loss 0.103912, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:43.228046: step 5903, loss 0.140107, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:43.488761: step 5904, loss 0.135559, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:43.745122: step 5905, loss 0.0944887, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:44.001871: step 5906, loss 0.0945019, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:44.258683: step 5907, loss 0.0837413, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:44.532347: step 5908, loss 0.137044, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:44.775820: step 5909, loss 0.0594171, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:45.051818: step 5910, loss 0.105965, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:45.325789: step 5911, loss 0.113671, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:45.649126: step 5912, loss 0.199078, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:45.916496: step 5913, loss 0.0852477, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:46.105888: step 5914, loss 0.0936897, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:46.347924: step 5915, loss 0.104556, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:46.499392: step 5916, loss 0.136932, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:46.659763: step 5917, loss 0.101361, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:46.893960: step 5918, loss 0.0949134, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:47.091833: step 5919, loss 0.0657934, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:47.417190: step 5920, loss 0.149827, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:47.821835: step 5920, loss 0.191636, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5920

2017-10-10T15:13:48.929834: step 5921, loss 0.119042, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:49.160768: step 5922, loss 0.140077, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:49.420850: step 5923, loss 0.102594, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:49.743724: step 5924, loss 0.0833471, acc 1, learning_rate 0.0001
2017-10-10T15:13:50.008892: step 5925, loss 0.124185, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:50.253043: step 5926, loss 0.0939753, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:50.510918: step 5927, loss 0.141451, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:50.779931: step 5928, loss 0.100916, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:51.077422: step 5929, loss 0.0910143, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:51.288881: step 5930, loss 0.256314, acc 0.90625, learning_rate 0.0001
2017-10-10T15:13:51.541318: step 5931, loss 0.0725502, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:51.838341: step 5932, loss 0.192462, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:52.132987: step 5933, loss 0.139893, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:52.383116: step 5934, loss 0.0553261, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:52.670828: step 5935, loss 0.222534, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:52.933178: step 5936, loss 0.161782, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:53.188373: step 5937, loss 0.0843451, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:53.456921: step 5938, loss 0.236112, acc 0.90625, learning_rate 0.0001
2017-10-10T15:13:53.720169: step 5939, loss 0.0921775, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:53.944920: step 5940, loss 0.119136, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:54.173030: step 5941, loss 0.177362, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:54.441058: step 5942, loss 0.0781374, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:54.713018: step 5943, loss 0.178251, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:54.920871: step 5944, loss 0.0519911, acc 1, learning_rate 0.0001
2017-10-10T15:13:55.235753: step 5945, loss 0.146233, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:55.530091: step 5946, loss 0.0488644, acc 1, learning_rate 0.0001
2017-10-10T15:13:55.740951: step 5947, loss 0.0988122, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:56.076869: step 5948, loss 0.180894, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:56.349067: step 5949, loss 0.157665, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:56.573061: step 5950, loss 0.0972795, acc 1, learning_rate 0.0001
2017-10-10T15:13:56.895467: step 5951, loss 0.0569308, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:57.193853: step 5952, loss 0.138931, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:57.464982: step 5953, loss 0.114681, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:57.696851: step 5954, loss 0.111876, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:57.932970: step 5955, loss 0.0641806, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:58.151560: step 5956, loss 0.0573857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:58.518447: step 5957, loss 0.117726, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:58.792806: step 5958, loss 0.130758, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:58.996988: step 5959, loss 0.1352, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:59.268848: step 5960, loss 0.015497, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:59.673991: step 5960, loss 0.189975, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-5960

2017-10-10T15:14:00.769838: step 5961, loss 0.0913653, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:01.025757: step 5962, loss 0.0744797, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:01.245014: step 5963, loss 0.153029, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:01.471093: step 5964, loss 0.179631, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:01.688860: step 5965, loss 0.161308, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:02.013024: step 5966, loss 0.101564, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:02.320798: step 5967, loss 0.151478, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:02.526043: step 5968, loss 0.0640325, acc 1, learning_rate 0.0001
2017-10-10T15:14:02.776830: step 5969, loss 0.108026, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:03.005108: step 5970, loss 0.0941928, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:03.208965: step 5971, loss 0.046191, acc 1, learning_rate 0.0001
2017-10-10T15:14:03.450965: step 5972, loss 0.173325, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:03.703201: step 5973, loss 0.0676185, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:03.977151: step 5974, loss 0.0559476, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:04.244935: step 5975, loss 0.0933451, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:04.451074: step 5976, loss 0.186485, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:04.764847: step 5977, loss 0.0681442, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:04.971456: step 5978, loss 0.0979194, acc 0.960784, learning_rate 0.0001
2017-10-10T15:14:05.234194: step 5979, loss 0.0932284, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:05.540932: step 5980, loss 0.116556, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:05.768960: step 5981, loss 0.158065, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:06.036803: step 5982, loss 0.0428982, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:06.296943: step 5983, loss 0.0494936, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:06.537733: step 5984, loss 0.0993203, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:06.780912: step 5985, loss 0.132524, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:07.020911: step 5986, loss 0.0565171, acc 1, learning_rate 0.0001
2017-10-10T15:14:07.290270: step 5987, loss 0.194814, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:07.509277: step 5988, loss 0.0519325, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:07.771785: step 5989, loss 0.085181, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:08.006451: step 5990, loss 0.0638148, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:08.248958: step 5991, loss 0.0546184, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:08.504861: step 5992, loss 0.0713285, acc 1, learning_rate 0.0001
2017-10-10T15:14:08.725961: step 5993, loss 0.0534266, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:08.962644: step 5994, loss 0.0513062, acc 1, learning_rate 0.0001
2017-10-10T15:14:09.164550: step 5995, loss 0.207535, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:09.411046: step 5996, loss 0.137146, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:09.703534: step 5997, loss 0.089165, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:09.975143: step 5998, loss 0.0626662, acc 1, learning_rate 0.0001
2017-10-10T15:14:10.228035: step 5999, loss 0.102987, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:10.415033: step 6000, loss 0.0765408, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:10.787225: step 6000, loss 0.191702, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6000

2017-10-10T15:14:11.814499: step 6001, loss 0.119775, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:12.092897: step 6002, loss 0.0788776, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:12.342924: step 6003, loss 0.110488, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:12.620895: step 6004, loss 0.105724, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:12.859511: step 6005, loss 0.130798, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:13.152830: step 6006, loss 0.0432615, acc 1, learning_rate 0.0001
2017-10-10T15:14:13.459270: step 6007, loss 0.0610195, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:13.664803: step 6008, loss 0.237832, acc 0.90625, learning_rate 0.0001
2017-10-10T15:14:13.947669: step 6009, loss 0.0985019, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:14.137068: step 6010, loss 0.154306, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:14.412671: step 6011, loss 0.113068, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:14.665067: step 6012, loss 0.0965884, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:14.916827: step 6013, loss 0.115143, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:15.176922: step 6014, loss 0.0828665, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:15.412144: step 6015, loss 0.0631389, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:15.704350: step 6016, loss 0.0572819, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:15.914638: step 6017, loss 0.0898528, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:16.134592: step 6018, loss 0.103287, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:16.329715: step 6019, loss 0.113434, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:16.511772: step 6020, loss 0.0809543, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:16.768738: step 6021, loss 0.0748911, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:17.007174: step 6022, loss 0.0988136, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:17.209564: step 6023, loss 0.105857, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:17.437211: step 6024, loss 0.0391979, acc 1, learning_rate 0.0001
2017-10-10T15:14:17.645119: step 6025, loss 0.118681, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:17.956902: step 6026, loss 0.192101, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:18.208812: step 6027, loss 0.196127, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:18.484882: step 6028, loss 0.102371, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:18.792874: step 6029, loss 0.104583, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:19.124821: step 6030, loss 0.083279, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:19.317864: step 6031, loss 0.10502, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:19.504776: step 6032, loss 0.0985178, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:19.746214: step 6033, loss 0.110738, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:19.905135: step 6034, loss 0.0664587, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:20.170564: step 6035, loss 0.0490118, acc 1, learning_rate 0.0001
2017-10-10T15:14:20.476912: step 6036, loss 0.158986, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:20.733300: step 6037, loss 0.0398261, acc 1, learning_rate 0.0001
2017-10-10T15:14:21.019729: step 6038, loss 0.17323, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:21.292857: step 6039, loss 0.115445, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:21.529254: step 6040, loss 0.114299, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:21.870368: step 6040, loss 0.188292, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6040

2017-10-10T15:14:23.001034: step 6041, loss 0.141148, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:23.255099: step 6042, loss 0.0912435, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:23.479936: step 6043, loss 0.0962921, acc 1, learning_rate 0.0001
2017-10-10T15:14:23.776915: step 6044, loss 0.142808, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:24.009140: step 6045, loss 0.0554055, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:24.277650: step 6046, loss 0.073912, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:24.533338: step 6047, loss 0.0631675, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:24.745062: step 6048, loss 0.141247, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:25.042936: step 6049, loss 0.0707453, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:25.260909: step 6050, loss 0.0671968, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:25.524865: step 6051, loss 0.0472604, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:25.754845: step 6052, loss 0.0931539, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:25.996873: step 6053, loss 0.103164, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:26.280944: step 6054, loss 0.105511, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:26.537236: step 6055, loss 0.0875203, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:26.791923: step 6056, loss 0.10602, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:27.025599: step 6057, loss 0.0453178, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:27.202111: step 6058, loss 0.0751687, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:27.438760: step 6059, loss 0.0491765, acc 1, learning_rate 0.0001
2017-10-10T15:14:27.752848: step 6060, loss 0.106141, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:27.984259: step 6061, loss 0.0801697, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:28.261374: step 6062, loss 0.119308, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:28.541119: step 6063, loss 0.0663799, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:28.792969: step 6064, loss 0.174752, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:28.996849: step 6065, loss 0.0863931, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:29.241317: step 6066, loss 0.059163, acc 1, learning_rate 0.0001
2017-10-10T15:14:29.424869: step 6067, loss 0.0685043, acc 1, learning_rate 0.0001
2017-10-10T15:14:29.729251: step 6068, loss 0.0719111, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:29.951283: step 6069, loss 0.0366398, acc 1, learning_rate 0.0001
2017-10-10T15:14:30.201506: step 6070, loss 0.120164, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:30.465115: step 6071, loss 0.0803398, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:30.784279: step 6072, loss 0.0846305, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:31.001199: step 6073, loss 0.108397, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:31.277899: step 6074, loss 0.0968641, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:31.553385: step 6075, loss 0.0803275, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:31.760985: step 6076, loss 0.0864902, acc 0.980392, learning_rate 0.0001
2017-10-10T15:14:31.977790: step 6077, loss 0.0803749, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:32.204146: step 6078, loss 0.105379, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:32.483510: step 6079, loss 0.0362132, acc 1, learning_rate 0.0001
2017-10-10T15:14:32.756344: step 6080, loss 0.0525793, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:33.162088: step 6080, loss 0.189189, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6080

2017-10-10T15:14:34.257061: step 6081, loss 0.0940197, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:34.509182: step 6082, loss 0.0989168, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:34.728835: step 6083, loss 0.107476, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:34.900996: step 6084, loss 0.0964056, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:35.102537: step 6085, loss 0.115136, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:35.422034: step 6086, loss 0.0898587, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:35.681825: step 6087, loss 0.09267, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:35.881490: step 6088, loss 0.0722449, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:36.100391: step 6089, loss 0.0240375, acc 1, learning_rate 0.0001
2017-10-10T15:14:36.374438: step 6090, loss 0.0559766, acc 1, learning_rate 0.0001
2017-10-10T15:14:36.637240: step 6091, loss 0.066982, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:36.929030: step 6092, loss 0.181615, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:37.166952: step 6093, loss 0.155603, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:37.420862: step 6094, loss 0.1471, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:37.668894: step 6095, loss 0.0689765, acc 1, learning_rate 0.0001
2017-10-10T15:14:37.883848: step 6096, loss 0.117535, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:38.145533: step 6097, loss 0.0875012, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:38.437647: step 6098, loss 0.126577, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:38.696959: step 6099, loss 0.125489, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:38.940915: step 6100, loss 0.0624451, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:39.212884: step 6101, loss 0.140938, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:39.440561: step 6102, loss 0.171903, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:39.732987: step 6103, loss 0.103275, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:39.968217: step 6104, loss 0.0610218, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:40.212682: step 6105, loss 0.0990326, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:40.406440: step 6106, loss 0.0681251, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:40.642630: step 6107, loss 0.168906, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:40.893077: step 6108, loss 0.0850103, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:41.097153: step 6109, loss 0.0878665, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:41.409291: step 6110, loss 0.115475, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:41.652136: step 6111, loss 0.0884157, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:41.910829: step 6112, loss 0.162932, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:42.196011: step 6113, loss 0.11033, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:42.445133: step 6114, loss 0.152346, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:42.643898: step 6115, loss 0.120446, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:42.932896: step 6116, loss 0.083403, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:43.220969: step 6117, loss 0.12538, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:43.428669: step 6118, loss 0.102492, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:43.740870: step 6119, loss 0.0968577, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:43.974385: step 6120, loss 0.181145, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:44.422955: step 6120, loss 0.189336, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6120

2017-10-10T15:14:45.834118: step 6121, loss 0.147606, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:46.141843: step 6122, loss 0.0998768, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:46.400807: step 6123, loss 0.0739751, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:46.689510: step 6124, loss 0.0579571, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:46.991813: step 6125, loss 0.135275, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:47.160966: step 6126, loss 0.0731897, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:47.417608: step 6127, loss 0.0969333, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:47.649088: step 6128, loss 0.0617064, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:47.921114: step 6129, loss 0.105091, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:48.158019: step 6130, loss 0.116067, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:48.462741: step 6131, loss 0.148587, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:48.745019: step 6132, loss 0.205983, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:48.991685: step 6133, loss 0.11513, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:49.261941: step 6134, loss 0.0938129, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:49.524881: step 6135, loss 0.06892, acc 1, learning_rate 0.0001
2017-10-10T15:14:49.743051: step 6136, loss 0.0668705, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:50.027802: step 6137, loss 0.163498, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:50.271800: step 6138, loss 0.101521, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:50.606194: step 6139, loss 0.22052, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:50.860879: step 6140, loss 0.0871058, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:51.096160: step 6141, loss 0.0606511, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:51.368599: step 6142, loss 0.0948669, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:51.614672: step 6143, loss 0.066231, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:51.848842: step 6144, loss 0.145449, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:52.108842: step 6145, loss 0.132326, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:52.331028: step 6146, loss 0.148832, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:52.596556: step 6147, loss 0.158191, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:52.850999: step 6148, loss 0.0841725, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:53.020043: step 6149, loss 0.041113, acc 1, learning_rate 0.0001
2017-10-10T15:14:53.146896: step 6150, loss 0.0821098, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:53.332337: step 6151, loss 0.0851512, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:53.503118: step 6152, loss 0.116791, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:53.741137: step 6153, loss 0.170099, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:53.963974: step 6154, loss 0.116946, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:54.245159: step 6155, loss 0.1155, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:54.457896: step 6156, loss 0.185611, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:54.720218: step 6157, loss 0.0302898, acc 1, learning_rate 0.0001
2017-10-10T15:14:54.951186: step 6158, loss 0.0967149, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:55.195093: step 6159, loss 0.102941, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:55.454340: step 6160, loss 0.0980616, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:55.788139: step 6160, loss 0.189097, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6160

2017-10-10T15:14:56.656789: step 6161, loss 0.103281, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:56.911405: step 6162, loss 0.0824413, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:57.232844: step 6163, loss 0.0588689, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:57.468989: step 6164, loss 0.110832, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:57.728925: step 6165, loss 0.171638, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:57.960737: step 6166, loss 0.179151, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:58.240800: step 6167, loss 0.0681228, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:58.604833: step 6168, loss 0.0386871, acc 1, learning_rate 0.0001
2017-10-10T15:14:58.860888: step 6169, loss 0.0597666, acc 1, learning_rate 0.0001
2017-10-10T15:14:59.065489: step 6170, loss 0.0822107, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:59.288867: step 6171, loss 0.173224, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:59.542476: step 6172, loss 0.0950879, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:59.777182: step 6173, loss 0.0683061, acc 1, learning_rate 0.0001
2017-10-10T15:14:59.989493: step 6174, loss 0.0585268, acc 0.980392, learning_rate 0.0001
2017-10-10T15:15:00.256946: step 6175, loss 0.0654432, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:00.496947: step 6176, loss 0.111246, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:00.744834: step 6177, loss 0.0354291, acc 1, learning_rate 0.0001
2017-10-10T15:15:01.045071: step 6178, loss 0.114558, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:01.352864: step 6179, loss 0.110001, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:01.563349: step 6180, loss 0.113762, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:01.790830: step 6181, loss 0.160582, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:02.028972: step 6182, loss 0.0725136, acc 1, learning_rate 0.0001
2017-10-10T15:15:02.270748: step 6183, loss 0.124808, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:02.503360: step 6184, loss 0.117136, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:02.781329: step 6185, loss 0.104034, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:03.036839: step 6186, loss 0.0929266, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:03.325238: step 6187, loss 0.0968819, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:03.603225: step 6188, loss 0.109604, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:03.849181: step 6189, loss 0.117877, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:04.069455: step 6190, loss 0.147148, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:04.332859: step 6191, loss 0.0546618, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:04.616124: step 6192, loss 0.0617601, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:04.884022: step 6193, loss 0.136948, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:05.124057: step 6194, loss 0.126758, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:05.396989: step 6195, loss 0.0683729, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:05.630112: step 6196, loss 0.126879, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:05.932943: step 6197, loss 0.106972, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:06.188891: step 6198, loss 0.123548, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:06.453026: step 6199, loss 0.0540612, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:06.620997: step 6200, loss 0.0606932, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:06.990822: step 6200, loss 0.188335, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6200

2017-10-10T15:15:07.996898: step 6201, loss 0.0702585, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:08.280930: step 6202, loss 0.143753, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:08.472914: step 6203, loss 0.0970828, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:08.790934: step 6204, loss 0.193995, acc 0.90625, learning_rate 0.0001
2017-10-10T15:15:09.124840: step 6205, loss 0.0765297, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:09.309039: step 6206, loss 0.17871, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:09.503057: step 6207, loss 0.0844561, acc 1, learning_rate 0.0001
2017-10-10T15:15:09.788858: step 6208, loss 0.153421, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:10.072319: step 6209, loss 0.184249, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:10.381219: step 6210, loss 0.101796, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:10.576905: step 6211, loss 0.049366, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:10.792847: step 6212, loss 0.08513, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:11.028937: step 6213, loss 0.127348, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:11.289209: step 6214, loss 0.0542623, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:11.509239: step 6215, loss 0.0755772, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:11.730086: step 6216, loss 0.0679044, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:11.906449: step 6217, loss 0.121418, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:12.176966: step 6218, loss 0.0542674, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:12.397087: step 6219, loss 0.232044, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:12.670939: step 6220, loss 0.165775, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:12.940880: step 6221, loss 0.125819, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:13.145875: step 6222, loss 0.0712973, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:13.411906: step 6223, loss 0.109911, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:13.622521: step 6224, loss 0.108336, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:13.862065: step 6225, loss 0.0764914, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:14.136968: step 6226, loss 0.0447971, acc 1, learning_rate 0.0001
2017-10-10T15:15:14.356715: step 6227, loss 0.148183, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:14.600901: step 6228, loss 0.140901, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:14.870082: step 6229, loss 0.198035, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:15.144016: step 6230, loss 0.0586109, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:15.412863: step 6231, loss 0.0832127, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:15.649037: step 6232, loss 0.0538874, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:15.879198: step 6233, loss 0.103174, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:16.146534: step 6234, loss 0.0760475, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:16.381328: step 6235, loss 0.0828297, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:16.675479: step 6236, loss 0.0806915, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:16.879298: step 6237, loss 0.076006, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:17.156395: step 6238, loss 0.160386, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:17.436913: step 6239, loss 0.0803895, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:17.662410: step 6240, loss 0.123038, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:18.044416: step 6240, loss 0.187717, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6240

2017-10-10T15:15:19.091164: step 6241, loss 0.0740897, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:19.340595: step 6242, loss 0.165463, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:19.607786: step 6243, loss 0.0426254, acc 1, learning_rate 0.0001
2017-10-10T15:15:19.812962: step 6244, loss 0.0800017, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:20.076832: step 6245, loss 0.113578, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:20.314789: step 6246, loss 0.0449611, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:20.590293: step 6247, loss 0.0818394, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:20.817317: step 6248, loss 0.194514, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:21.081670: step 6249, loss 0.116531, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:21.357642: step 6250, loss 0.115211, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:21.624893: step 6251, loss 0.0609215, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:21.901315: step 6252, loss 0.115285, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:22.169082: step 6253, loss 0.110227, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:22.426141: step 6254, loss 0.16825, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:22.685347: step 6255, loss 0.171121, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:22.956858: step 6256, loss 0.118187, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:23.238913: step 6257, loss 0.0876788, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:23.511183: step 6258, loss 0.056132, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:23.733116: step 6259, loss 0.047965, acc 1, learning_rate 0.0001
2017-10-10T15:15:24.024786: step 6260, loss 0.0446481, acc 1, learning_rate 0.0001
2017-10-10T15:15:24.316449: step 6261, loss 0.107686, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:24.524872: step 6262, loss 0.171707, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:24.726356: step 6263, loss 0.0760057, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:25.028831: step 6264, loss 0.118229, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:25.282043: step 6265, loss 0.0684793, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:25.584854: step 6266, loss 0.101961, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:25.897054: step 6267, loss 0.0711623, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:26.058045: step 6268, loss 0.0728063, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:26.327705: step 6269, loss 0.164851, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:26.494012: step 6270, loss 0.151623, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:26.724149: step 6271, loss 0.0952879, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:26.965054: step 6272, loss 0.130215, acc 0.960784, learning_rate 0.0001
2017-10-10T15:15:27.157806: step 6273, loss 0.109971, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:27.420937: step 6274, loss 0.132663, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:27.720919: step 6275, loss 0.0757493, acc 1, learning_rate 0.0001
2017-10-10T15:15:28.001036: step 6276, loss 0.161399, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:28.207413: step 6277, loss 0.105482, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:28.384835: step 6278, loss 0.114768, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:28.599133: step 6279, loss 0.0536363, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:28.784904: step 6280, loss 0.104629, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:29.227410: step 6280, loss 0.18814, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6280

2017-10-10T15:15:30.346006: step 6281, loss 0.150404, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:30.608891: step 6282, loss 0.113214, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:30.846180: step 6283, loss 0.101721, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:31.077536: step 6284, loss 0.0640891, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:31.366712: step 6285, loss 0.162474, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:31.644102: step 6286, loss 0.0551216, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:31.911687: step 6287, loss 0.1211, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:32.167259: step 6288, loss 0.127792, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:32.400024: step 6289, loss 0.136355, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:32.646180: step 6290, loss 0.110591, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:32.848833: step 6291, loss 0.0777967, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:33.069223: step 6292, loss 0.0420619, acc 1, learning_rate 0.0001
2017-10-10T15:15:33.290840: step 6293, loss 0.0715789, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:33.495495: step 6294, loss 0.174181, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:33.749188: step 6295, loss 0.0434752, acc 1, learning_rate 0.0001
2017-10-10T15:15:34.021967: step 6296, loss 0.114947, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:34.276879: step 6297, loss 0.236002, acc 0.890625, learning_rate 0.0001
2017-10-10T15:15:34.531799: step 6298, loss 0.0851224, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:34.797140: step 6299, loss 0.106659, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:35.028965: step 6300, loss 0.0997451, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:35.324953: step 6301, loss 0.09928, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:35.560232: step 6302, loss 0.0355537, acc 1, learning_rate 0.0001
2017-10-10T15:15:35.757650: step 6303, loss 0.0627188, acc 1, learning_rate 0.0001
2017-10-10T15:15:36.057255: step 6304, loss 0.0691034, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:36.285083: step 6305, loss 0.0583224, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:36.520815: step 6306, loss 0.146182, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:36.760821: step 6307, loss 0.0686972, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:37.000698: step 6308, loss 0.114352, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:37.253267: step 6309, loss 0.104503, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:37.493074: step 6310, loss 0.0796738, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:37.740707: step 6311, loss 0.0908735, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:37.977992: step 6312, loss 0.0654243, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:38.192404: step 6313, loss 0.0252039, acc 1, learning_rate 0.0001
2017-10-10T15:15:38.409231: step 6314, loss 0.0727715, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:38.664866: step 6315, loss 0.0828218, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:38.965124: step 6316, loss 0.143343, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:39.221177: step 6317, loss 0.0861367, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:39.520916: step 6318, loss 0.0798629, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:39.765035: step 6319, loss 0.0540229, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:39.982061: step 6320, loss 0.0293506, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:40.380496: step 6320, loss 0.186473, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6320

2017-10-10T15:15:41.325254: step 6321, loss 0.1789, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:41.549083: step 6322, loss 0.0291165, acc 1, learning_rate 0.0001
2017-10-10T15:15:41.812840: step 6323, loss 0.118524, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:42.144897: step 6324, loss 0.0833712, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:42.460146: step 6325, loss 0.0712942, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:42.660885: step 6326, loss 0.0807509, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:42.888591: step 6327, loss 0.16078, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:43.087383: step 6328, loss 0.0526761, acc 1, learning_rate 0.0001
2017-10-10T15:15:43.294130: step 6329, loss 0.0396327, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:43.573074: step 6330, loss 0.168116, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:43.800772: step 6331, loss 0.0587637, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:44.084020: step 6332, loss 0.084145, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:44.344979: step 6333, loss 0.169639, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:44.587097: step 6334, loss 0.0886523, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:44.842160: step 6335, loss 0.0949693, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:45.064838: step 6336, loss 0.0835223, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:45.352979: step 6337, loss 0.101684, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:45.613751: step 6338, loss 0.0206884, acc 1, learning_rate 0.0001
2017-10-10T15:15:45.873203: step 6339, loss 0.0771231, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:46.127161: step 6340, loss 0.103785, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:46.380129: step 6341, loss 0.0500355, acc 1, learning_rate 0.0001
2017-10-10T15:15:46.528119: step 6342, loss 0.132281, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:46.712766: step 6343, loss 0.143728, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:46.967863: step 6344, loss 0.115786, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:47.173460: step 6345, loss 0.11935, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:47.385434: step 6346, loss 0.0582277, acc 1, learning_rate 0.0001
2017-10-10T15:15:47.620968: step 6347, loss 0.0763624, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:47.840179: step 6348, loss 0.0908584, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:48.126120: step 6349, loss 0.112503, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:48.348798: step 6350, loss 0.111348, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:48.576884: step 6351, loss 0.109775, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:48.817525: step 6352, loss 0.115784, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:49.040855: step 6353, loss 0.181532, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:49.303784: step 6354, loss 0.135663, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:49.565125: step 6355, loss 0.0823692, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:49.816452: step 6356, loss 0.161511, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:49.983863: step 6357, loss 0.094116, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:50.248908: step 6358, loss 0.0917628, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:50.492934: step 6359, loss 0.0920454, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:50.752754: step 6360, loss 0.0893737, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:51.168021: step 6360, loss 0.187783, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6360

2017-10-10T15:15:52.283460: step 6361, loss 0.142838, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:52.560204: step 6362, loss 0.122825, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:52.824815: step 6363, loss 0.119764, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:53.094744: step 6364, loss 0.074947, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:53.376873: step 6365, loss 0.152528, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:53.616533: step 6366, loss 0.0404611, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:53.885725: step 6367, loss 0.0694741, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:54.128786: step 6368, loss 0.0823539, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:54.368243: step 6369, loss 0.0664515, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:54.573411: step 6370, loss 0.0422481, acc 0.980392, learning_rate 0.0001
2017-10-10T15:15:54.821024: step 6371, loss 0.101264, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:55.111837: step 6372, loss 0.0659605, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:55.310749: step 6373, loss 0.0883287, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:55.569272: step 6374, loss 0.111634, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:55.864076: step 6375, loss 0.0766179, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:56.096926: step 6376, loss 0.140004, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:56.348680: step 6377, loss 0.102913, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:56.663465: step 6378, loss 0.0671018, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:56.901196: step 6379, loss 0.169956, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:57.157334: step 6380, loss 0.129861, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:57.458861: step 6381, loss 0.0781544, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:57.683109: step 6382, loss 0.086619, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:57.924108: step 6383, loss 0.0585886, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:58.172400: step 6384, loss 0.0409986, acc 1, learning_rate 0.0001
2017-10-10T15:15:58.524311: step 6385, loss 0.153574, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:58.823159: step 6386, loss 0.0535234, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:59.111003: step 6387, loss 0.074881, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:59.294165: step 6388, loss 0.0946321, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:59.466997: step 6389, loss 0.119793, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:59.658966: step 6390, loss 0.123373, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:59.870523: step 6391, loss 0.143517, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:00.105788: step 6392, loss 0.0627071, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:00.385293: step 6393, loss 0.061427, acc 1, learning_rate 0.0001
2017-10-10T15:16:00.607094: step 6394, loss 0.0625594, acc 1, learning_rate 0.0001
2017-10-10T15:16:00.814070: step 6395, loss 0.0815241, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:01.070062: step 6396, loss 0.198454, acc 0.90625, learning_rate 0.0001
2017-10-10T15:16:01.244031: step 6397, loss 0.0730453, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:01.500879: step 6398, loss 0.0908202, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:01.765268: step 6399, loss 0.0733238, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:01.972476: step 6400, loss 0.148066, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:02.357879: step 6400, loss 0.1895, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6400

2017-10-10T15:16:03.388837: step 6401, loss 0.0811325, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:03.642615: step 6402, loss 0.0700385, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:03.901168: step 6403, loss 0.123102, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:04.216192: step 6404, loss 0.13981, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:04.497043: step 6405, loss 0.103217, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:04.684532: step 6406, loss 0.175147, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:04.921832: step 6407, loss 0.199397, acc 0.90625, learning_rate 0.0001
2017-10-10T15:16:05.148599: step 6408, loss 0.0977096, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:05.305736: step 6409, loss 0.0668724, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:05.541393: step 6410, loss 0.176506, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:05.812851: step 6411, loss 0.116925, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:06.055113: step 6412, loss 0.035808, acc 1, learning_rate 0.0001
2017-10-10T15:16:06.274077: step 6413, loss 0.137948, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:06.464949: step 6414, loss 0.0478102, acc 1, learning_rate 0.0001
2017-10-10T15:16:06.703804: step 6415, loss 0.0968505, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:06.977056: step 6416, loss 0.155097, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:07.217325: step 6417, loss 0.0550416, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:07.445090: step 6418, loss 0.259976, acc 0.90625, learning_rate 0.0001
2017-10-10T15:16:07.645139: step 6419, loss 0.105897, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:07.941913: step 6420, loss 0.136709, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:08.200100: step 6421, loss 0.0498375, acc 1, learning_rate 0.0001
2017-10-10T15:16:08.420859: step 6422, loss 0.0930033, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:08.655172: step 6423, loss 0.100047, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:08.890325: step 6424, loss 0.0269822, acc 1, learning_rate 0.0001
2017-10-10T15:16:09.196857: step 6425, loss 0.175706, acc 0.890625, learning_rate 0.0001
2017-10-10T15:16:09.377114: step 6426, loss 0.106553, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:09.637022: step 6427, loss 0.0551098, acc 1, learning_rate 0.0001
2017-10-10T15:16:09.836119: step 6428, loss 0.0305242, acc 1, learning_rate 0.0001
2017-10-10T15:16:10.094057: step 6429, loss 0.0505898, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:10.317115: step 6430, loss 0.118381, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:10.548877: step 6431, loss 0.108667, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:10.796925: step 6432, loss 0.0509673, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:11.095386: step 6433, loss 0.0426774, acc 1, learning_rate 0.0001
2017-10-10T15:16:11.346557: step 6434, loss 0.0686532, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:11.595127: step 6435, loss 0.106792, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:11.894328: step 6436, loss 0.105381, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:12.165065: step 6437, loss 0.0902814, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:12.448669: step 6438, loss 0.0572502, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:12.728633: step 6439, loss 0.0363301, acc 1, learning_rate 0.0001
2017-10-10T15:16:12.948839: step 6440, loss 0.122521, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:13.209208: step 6440, loss 0.187484, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6440

2017-10-10T15:16:14.138000: step 6441, loss 0.0774313, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:14.415912: step 6442, loss 0.18623, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:14.636460: step 6443, loss 0.144118, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:14.886866: step 6444, loss 0.108288, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:15.168863: step 6445, loss 0.100452, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:15.399611: step 6446, loss 0.0511485, acc 1, learning_rate 0.0001
2017-10-10T15:16:15.671838: step 6447, loss 0.0536364, acc 1, learning_rate 0.0001
2017-10-10T15:16:15.916740: step 6448, loss 0.109113, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:16.097202: step 6449, loss 0.113942, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:16.307664: step 6450, loss 0.0725332, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:16.524656: step 6451, loss 0.0827299, acc 1, learning_rate 0.0001
2017-10-10T15:16:16.725227: step 6452, loss 0.115916, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:16.992807: step 6453, loss 0.0533227, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:17.223802: step 6454, loss 0.0637189, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:17.502446: step 6455, loss 0.1572, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:17.716907: step 6456, loss 0.119616, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:17.938689: step 6457, loss 0.0773522, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:18.172158: step 6458, loss 0.0384609, acc 1, learning_rate 0.0001
2017-10-10T15:16:18.407563: step 6459, loss 0.115413, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:18.678554: step 6460, loss 0.176126, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:18.925058: step 6461, loss 0.186919, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:19.200757: step 6462, loss 0.106924, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:19.449172: step 6463, loss 0.122442, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:19.676151: step 6464, loss 0.103532, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:19.968879: step 6465, loss 0.0716716, acc 1, learning_rate 0.0001
2017-10-10T15:16:20.216592: step 6466, loss 0.0657263, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:20.513024: step 6467, loss 0.0974877, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:20.788871: step 6468, loss 0.0432813, acc 0.980392, learning_rate 0.0001
2017-10-10T15:16:21.068968: step 6469, loss 0.0817033, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:21.298430: step 6470, loss 0.172601, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:21.532877: step 6471, loss 0.0427653, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:21.818545: step 6472, loss 0.165173, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:22.080049: step 6473, loss 0.0709031, acc 1, learning_rate 0.0001
2017-10-10T15:16:22.292864: step 6474, loss 0.131342, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:22.601185: step 6475, loss 0.179736, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:22.892997: step 6476, loss 0.15186, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:23.176998: step 6477, loss 0.0825898, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:23.428842: step 6478, loss 0.126418, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:23.608698: step 6479, loss 0.0414105, acc 1, learning_rate 0.0001
2017-10-10T15:16:23.824167: step 6480, loss 0.124112, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:24.295710: step 6480, loss 0.186957, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6480

2017-10-10T15:16:25.306682: step 6481, loss 0.107431, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:25.544109: step 6482, loss 0.158859, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:25.785766: step 6483, loss 0.0919836, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:26.105089: step 6484, loss 0.0484911, acc 1, learning_rate 0.0001
2017-10-10T15:16:26.324843: step 6485, loss 0.0745007, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:26.532848: step 6486, loss 0.116001, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:26.743851: step 6487, loss 0.0965587, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:27.002882: step 6488, loss 0.154695, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:27.229731: step 6489, loss 0.0649026, acc 1, learning_rate 0.0001
2017-10-10T15:16:27.484082: step 6490, loss 0.148708, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:27.753457: step 6491, loss 0.206694, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:27.984958: step 6492, loss 0.0792812, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:28.241030: step 6493, loss 0.0516105, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:28.458903: step 6494, loss 0.0703625, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:28.696206: step 6495, loss 0.122879, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:28.963026: step 6496, loss 0.216579, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:29.176949: step 6497, loss 0.0846289, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:29.467486: step 6498, loss 0.0468175, acc 1, learning_rate 0.0001
2017-10-10T15:16:29.692900: step 6499, loss 0.0528521, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:29.972564: step 6500, loss 0.111339, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:30.212601: step 6501, loss 0.0470579, acc 1, learning_rate 0.0001
2017-10-10T15:16:30.434878: step 6502, loss 0.0337472, acc 1, learning_rate 0.0001
2017-10-10T15:16:30.649213: step 6503, loss 0.0936943, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:30.894804: step 6504, loss 0.0810281, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:31.164950: step 6505, loss 0.0918308, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:31.426706: step 6506, loss 0.0937592, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:31.665115: step 6507, loss 0.0746099, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:31.930856: step 6508, loss 0.107168, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:32.189658: step 6509, loss 0.0815199, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:32.476952: step 6510, loss 0.0796497, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:32.680844: step 6511, loss 0.136621, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:32.926598: step 6512, loss 0.0649006, acc 1, learning_rate 0.0001
2017-10-10T15:16:33.132857: step 6513, loss 0.142728, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:33.348852: step 6514, loss 0.111484, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:33.633962: step 6515, loss 0.0729571, acc 1, learning_rate 0.0001
2017-10-10T15:16:33.881172: step 6516, loss 0.0658269, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:34.151763: step 6517, loss 0.107129, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:34.473661: step 6518, loss 0.180768, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:34.800885: step 6519, loss 0.0408241, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:35.021307: step 6520, loss 0.0911664, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:35.484962: step 6520, loss 0.187366, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6520

2017-10-10T15:16:36.509025: step 6521, loss 0.112402, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:36.728148: step 6522, loss 0.180868, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:37.019899: step 6523, loss 0.0233241, acc 1, learning_rate 0.0001
2017-10-10T15:16:37.239857: step 6524, loss 0.248936, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:37.476945: step 6525, loss 0.104553, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:37.701136: step 6526, loss 0.170504, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:37.893202: step 6527, loss 0.102677, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:38.157318: step 6528, loss 0.0417702, acc 1, learning_rate 0.0001
2017-10-10T15:16:38.361109: step 6529, loss 0.0390724, acc 1, learning_rate 0.0001
2017-10-10T15:16:38.635002: step 6530, loss 0.060571, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:38.877192: step 6531, loss 0.147845, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:39.185120: step 6532, loss 0.126284, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:39.428882: step 6533, loss 0.0583825, acc 1, learning_rate 0.0001
2017-10-10T15:16:39.679052: step 6534, loss 0.150921, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:39.893437: step 6535, loss 0.0556169, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:40.141939: step 6536, loss 0.101158, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:40.408900: step 6537, loss 0.050972, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:40.657219: step 6538, loss 0.0753564, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:40.945594: step 6539, loss 0.0276355, acc 1, learning_rate 0.0001
2017-10-10T15:16:41.232835: step 6540, loss 0.114796, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:41.537147: step 6541, loss 0.0505205, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:41.757871: step 6542, loss 0.0267108, acc 1, learning_rate 0.0001
2017-10-10T15:16:41.976879: step 6543, loss 0.0701401, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:42.384358: step 6544, loss 0.153126, acc 0.90625, learning_rate 0.0001
2017-10-10T15:16:42.569908: step 6545, loss 0.113309, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:42.773913: step 6546, loss 0.0963609, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:42.975458: step 6547, loss 0.0565489, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:43.155631: step 6548, loss 0.0527927, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:43.429140: step 6549, loss 0.127406, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:43.743157: step 6550, loss 0.104055, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:44.016865: step 6551, loss 0.0719154, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:44.301356: step 6552, loss 0.254529, acc 0.90625, learning_rate 0.0001
2017-10-10T15:16:44.580590: step 6553, loss 0.115974, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:44.844436: step 6554, loss 0.0583039, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:45.108965: step 6555, loss 0.136077, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:45.385199: step 6556, loss 0.0353414, acc 1, learning_rate 0.0001
2017-10-10T15:16:45.639417: step 6557, loss 0.0957417, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:45.912917: step 6558, loss 0.08028, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:46.147207: step 6559, loss 0.0456347, acc 1, learning_rate 0.0001
2017-10-10T15:16:46.396335: step 6560, loss 0.0767398, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:46.769435: step 6560, loss 0.187736, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6560

2017-10-10T15:16:47.897452: step 6561, loss 0.121538, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:48.137080: step 6562, loss 0.136273, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:48.413160: step 6563, loss 0.0811826, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:48.716981: step 6564, loss 0.140067, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:49.022769: step 6565, loss 0.128873, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:49.250608: step 6566, loss 0.0914501, acc 0.960784, learning_rate 0.0001
2017-10-10T15:16:49.422806: step 6567, loss 0.100388, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:49.656805: step 6568, loss 0.0946934, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:49.850898: step 6569, loss 0.0819944, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:50.098979: step 6570, loss 0.142631, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:50.338854: step 6571, loss 0.193765, acc 0.90625, learning_rate 0.0001
2017-10-10T15:16:50.547600: step 6572, loss 0.0764661, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:50.809100: step 6573, loss 0.123047, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:51.126787: step 6574, loss 0.0926297, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:51.407594: step 6575, loss 0.0211645, acc 1, learning_rate 0.0001
2017-10-10T15:16:51.692967: step 6576, loss 0.0678184, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:51.975511: step 6577, loss 0.126549, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:52.241250: step 6578, loss 0.0567653, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:52.572963: step 6579, loss 0.0555921, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:52.862783: step 6580, loss 0.111096, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:53.164597: step 6581, loss 0.0931836, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:53.370276: step 6582, loss 0.108694, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:53.573925: step 6583, loss 0.0943891, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:53.861689: step 6584, loss 0.208718, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:54.017899: step 6585, loss 0.0479014, acc 1, learning_rate 0.0001
2017-10-10T15:16:54.319643: step 6586, loss 0.0940799, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:54.498960: step 6587, loss 0.136273, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:54.777714: step 6588, loss 0.13491, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:55.092930: step 6589, loss 0.0638938, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:55.358533: step 6590, loss 0.0791958, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:55.628906: step 6591, loss 0.210194, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:55.855905: step 6592, loss 0.162128, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:56.122918: step 6593, loss 0.0778487, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:56.373176: step 6594, loss 0.13692, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:56.609041: step 6595, loss 0.167525, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:56.813159: step 6596, loss 0.138868, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:57.053241: step 6597, loss 0.0664213, acc 1, learning_rate 0.0001
2017-10-10T15:16:57.328827: step 6598, loss 0.116383, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:57.572427: step 6599, loss 0.0678001, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:57.819499: step 6600, loss 0.127797, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:58.207222: step 6600, loss 0.187496, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6600

2017-10-10T15:16:59.129255: step 6601, loss 0.063535, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:59.375704: step 6602, loss 0.0700245, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:59.606106: step 6603, loss 0.0397067, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:59.854140: step 6604, loss 0.0741856, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:00.124852: step 6605, loss 0.0530371, acc 1, learning_rate 0.0001
2017-10-10T15:17:00.308570: step 6606, loss 0.123723, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:00.612844: step 6607, loss 0.10897, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:00.784917: step 6608, loss 0.149432, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:01.004518: step 6609, loss 0.101232, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:01.220868: step 6610, loss 0.0475095, acc 1, learning_rate 0.0001
2017-10-10T15:17:01.420995: step 6611, loss 0.0903261, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:01.638973: step 6612, loss 0.0732858, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:01.891481: step 6613, loss 0.130985, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:02.117215: step 6614, loss 0.104752, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:02.364351: step 6615, loss 0.0922589, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:02.660775: step 6616, loss 0.0551493, acc 1, learning_rate 0.0001
2017-10-10T15:17:02.924964: step 6617, loss 0.145159, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:03.188840: step 6618, loss 0.070166, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:03.418907: step 6619, loss 0.0480761, acc 1, learning_rate 0.0001
2017-10-10T15:17:03.640839: step 6620, loss 0.0359457, acc 1, learning_rate 0.0001
2017-10-10T15:17:03.905324: step 6621, loss 0.0790682, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:04.114808: step 6622, loss 0.0698889, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:04.397969: step 6623, loss 0.0671079, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:04.694241: step 6624, loss 0.0677537, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:04.928895: step 6625, loss 0.13688, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:05.229179: step 6626, loss 0.100437, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:05.545641: step 6627, loss 0.0939949, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:05.801418: step 6628, loss 0.0526978, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:06.068889: step 6629, loss 0.0510274, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:06.226936: step 6630, loss 0.0725586, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:06.452430: step 6631, loss 0.118688, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:06.622978: step 6632, loss 0.0795036, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:06.801620: step 6633, loss 0.0747462, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:06.992854: step 6634, loss 0.207668, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:07.200987: step 6635, loss 0.111606, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:07.493591: step 6636, loss 0.131584, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:07.729042: step 6637, loss 0.0902758, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:07.941501: step 6638, loss 0.120869, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:08.180534: step 6639, loss 0.0474568, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:08.457424: step 6640, loss 0.0459643, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:08.835153: step 6640, loss 0.185644, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6640

2017-10-10T15:17:09.953261: step 6641, loss 0.0811133, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:10.186131: step 6642, loss 0.180333, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:10.417098: step 6643, loss 0.0936636, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:10.705517: step 6644, loss 0.0909462, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:10.944750: step 6645, loss 0.140417, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:11.236631: step 6646, loss 0.0446551, acc 1, learning_rate 0.0001
2017-10-10T15:17:11.529942: step 6647, loss 0.351718, acc 0.890625, learning_rate 0.0001
2017-10-10T15:17:11.778467: step 6648, loss 0.0579247, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:12.080818: step 6649, loss 0.117108, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:12.293236: step 6650, loss 0.0705492, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:12.551141: step 6651, loss 0.101995, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:12.810476: step 6652, loss 0.0716678, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:13.093053: step 6653, loss 0.136736, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:13.398377: step 6654, loss 0.0845144, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:13.672831: step 6655, loss 0.0563585, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:13.876821: step 6656, loss 0.118237, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:14.123929: step 6657, loss 0.112227, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:14.337116: step 6658, loss 0.0925893, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:14.570423: step 6659, loss 0.139445, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:14.880291: step 6660, loss 0.0454729, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:15.168206: step 6661, loss 0.0783872, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:15.436940: step 6662, loss 0.0646592, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:15.700466: step 6663, loss 0.0705423, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:15.929546: step 6664, loss 0.0539786, acc 1, learning_rate 0.0001
2017-10-10T15:17:16.216860: step 6665, loss 0.129666, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:16.451689: step 6666, loss 0.0662287, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:16.728680: step 6667, loss 0.0406349, acc 1, learning_rate 0.0001
2017-10-10T15:17:17.054493: step 6668, loss 0.134604, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:17.264248: step 6669, loss 0.0455375, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:17.508005: step 6670, loss 0.0875497, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:17.792866: step 6671, loss 0.130563, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:18.037569: step 6672, loss 0.0448668, acc 1, learning_rate 0.0001
2017-10-10T15:17:18.385087: step 6673, loss 0.111924, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:18.684177: step 6674, loss 0.145281, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:18.917525: step 6675, loss 0.0695438, acc 1, learning_rate 0.0001
2017-10-10T15:17:19.123661: step 6676, loss 0.0950199, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:19.361205: step 6677, loss 0.0754779, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:19.592830: step 6678, loss 0.149851, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:19.780927: step 6679, loss 0.093533, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:20.024814: step 6680, loss 0.0772856, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:20.492672: step 6680, loss 0.187181, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6680

2017-10-10T15:17:21.536927: step 6681, loss 0.147436, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:21.776910: step 6682, loss 0.148769, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:22.100846: step 6683, loss 0.104457, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:22.372020: step 6684, loss 0.191837, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:22.633451: step 6685, loss 0.0848506, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:22.837163: step 6686, loss 0.143546, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:23.062774: step 6687, loss 0.167877, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:23.268933: step 6688, loss 0.0634321, acc 1, learning_rate 0.0001
2017-10-10T15:17:23.513077: step 6689, loss 0.0977516, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:23.785223: step 6690, loss 0.0585514, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:24.038912: step 6691, loss 0.0599953, acc 1, learning_rate 0.0001
2017-10-10T15:17:24.270822: step 6692, loss 0.176786, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:24.561019: step 6693, loss 0.0373656, acc 1, learning_rate 0.0001
2017-10-10T15:17:24.801582: step 6694, loss 0.0931069, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:25.098157: step 6695, loss 0.0386835, acc 1, learning_rate 0.0001
2017-10-10T15:17:25.412858: step 6696, loss 0.130802, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:25.653019: step 6697, loss 0.150233, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:25.905423: step 6698, loss 0.100323, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:26.117284: step 6699, loss 0.0820519, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:26.333091: step 6700, loss 0.0909886, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:26.588956: step 6701, loss 0.130356, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:26.876628: step 6702, loss 0.0865557, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:27.080751: step 6703, loss 0.0858935, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:27.384951: step 6704, loss 0.125779, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:27.644817: step 6705, loss 0.088111, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:27.916849: step 6706, loss 0.0797989, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:28.193025: step 6707, loss 0.0267749, acc 1, learning_rate 0.0001
2017-10-10T15:17:28.421212: step 6708, loss 0.0581001, acc 1, learning_rate 0.0001
2017-10-10T15:17:28.722241: step 6709, loss 0.199895, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:28.936844: step 6710, loss 0.0642455, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:29.182980: step 6711, loss 0.108003, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:29.410216: step 6712, loss 0.133532, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:29.680898: step 6713, loss 0.101885, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:29.932845: step 6714, loss 0.112534, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:30.158464: step 6715, loss 0.117501, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:30.452823: step 6716, loss 0.071359, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:30.744834: step 6717, loss 0.159383, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:30.961440: step 6718, loss 0.0801858, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:31.237855: step 6719, loss 0.033357, acc 1, learning_rate 0.0001
2017-10-10T15:17:31.489087: step 6720, loss 0.0428836, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:31.864030: step 6720, loss 0.186398, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6720

2017-10-10T15:17:33.046981: step 6721, loss 0.0191801, acc 1, learning_rate 0.0001
2017-10-10T15:17:33.296903: step 6722, loss 0.0611224, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:33.498224: step 6723, loss 0.0794239, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:33.740945: step 6724, loss 0.194976, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:34.016517: step 6725, loss 0.0306154, acc 1, learning_rate 0.0001
2017-10-10T15:17:34.276251: step 6726, loss 0.0480896, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:34.544856: step 6727, loss 0.0464074, acc 1, learning_rate 0.0001
2017-10-10T15:17:34.792919: step 6728, loss 0.0646095, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:35.061336: step 6729, loss 0.041887, acc 1, learning_rate 0.0001
2017-10-10T15:17:35.331305: step 6730, loss 0.136562, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:35.585608: step 6731, loss 0.133648, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:35.836190: step 6732, loss 0.122273, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:36.098348: step 6733, loss 0.138127, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:36.312832: step 6734, loss 0.034882, acc 1, learning_rate 0.0001
2017-10-10T15:17:36.624884: step 6735, loss 0.13695, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:36.945072: step 6736, loss 0.1376, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:37.219716: step 6737, loss 0.19201, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:37.435158: step 6738, loss 0.0958441, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:37.728570: step 6739, loss 0.0892781, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:37.940894: step 6740, loss 0.167519, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:38.184982: step 6741, loss 0.0345224, acc 1, learning_rate 0.0001
2017-10-10T15:17:38.408824: step 6742, loss 0.0712782, acc 1, learning_rate 0.0001
2017-10-10T15:17:38.648823: step 6743, loss 0.0711116, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:38.888838: step 6744, loss 0.122848, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:39.154713: step 6745, loss 0.0779946, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:39.363792: step 6746, loss 0.0760857, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:39.539246: step 6747, loss 0.148681, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:39.812822: step 6748, loss 0.0535848, acc 1, learning_rate 0.0001
2017-10-10T15:17:40.121114: step 6749, loss 0.107619, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:40.384429: step 6750, loss 0.133107, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:40.624888: step 6751, loss 0.177885, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:40.952328: step 6752, loss 0.0444935, acc 1, learning_rate 0.0001
2017-10-10T15:17:41.252872: step 6753, loss 0.0550377, acc 1, learning_rate 0.0001
2017-10-10T15:17:41.504902: step 6754, loss 0.143793, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:41.779614: step 6755, loss 0.0436288, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:42.065026: step 6756, loss 0.109924, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:42.295668: step 6757, loss 0.100704, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:42.536634: step 6758, loss 0.0802149, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:42.804832: step 6759, loss 0.110683, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:43.029420: step 6760, loss 0.0883268, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:43.384804: step 6760, loss 0.186113, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6760

2017-10-10T15:17:44.260883: step 6761, loss 0.0731504, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:44.491851: step 6762, loss 0.110409, acc 0.941176, learning_rate 0.0001
2017-10-10T15:17:44.728295: step 6763, loss 0.0584042, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:44.944403: step 6764, loss 0.112933, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:45.196850: step 6765, loss 0.171648, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:45.465225: step 6766, loss 0.185489, acc 0.90625, learning_rate 0.0001
2017-10-10T15:17:45.773102: step 6767, loss 0.077879, acc 1, learning_rate 0.0001
2017-10-10T15:17:46.037109: step 6768, loss 0.103349, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:46.256873: step 6769, loss 0.204351, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:46.446977: step 6770, loss 0.104441, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:46.660689: step 6771, loss 0.0408461, acc 1, learning_rate 0.0001
2017-10-10T15:17:46.917478: step 6772, loss 0.0772919, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:47.165030: step 6773, loss 0.0563734, acc 1, learning_rate 0.0001
2017-10-10T15:17:47.406038: step 6774, loss 0.10729, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:47.651304: step 6775, loss 0.0333201, acc 1, learning_rate 0.0001
2017-10-10T15:17:47.886775: step 6776, loss 0.121759, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:48.109193: step 6777, loss 0.112239, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:48.325858: step 6778, loss 0.106622, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:48.552874: step 6779, loss 0.0445682, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:48.788981: step 6780, loss 0.107904, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:49.025176: step 6781, loss 0.0533793, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:49.281881: step 6782, loss 0.0775563, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:49.551576: step 6783, loss 0.161875, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:49.757811: step 6784, loss 0.038914, acc 1, learning_rate 0.0001
2017-10-10T15:17:49.975213: step 6785, loss 0.0754066, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:50.204877: step 6786, loss 0.088622, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:50.447522: step 6787, loss 0.0706684, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:50.710601: step 6788, loss 0.0738858, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:50.857993: step 6789, loss 0.131704, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:51.064028: step 6790, loss 0.0623666, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:51.304128: step 6791, loss 0.132297, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:51.537144: step 6792, loss 0.161532, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:51.794489: step 6793, loss 0.135944, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:52.028883: step 6794, loss 0.124722, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:52.261863: step 6795, loss 0.0871743, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:52.540537: step 6796, loss 0.0485605, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:52.755683: step 6797, loss 0.124669, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:53.012938: step 6798, loss 0.0618144, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:53.258818: step 6799, loss 0.0951013, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:53.467585: step 6800, loss 0.091921, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:53.868015: step 6800, loss 0.185225, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6800

2017-10-10T15:17:54.960888: step 6801, loss 0.0832052, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:55.297664: step 6802, loss 0.172126, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:55.572176: step 6803, loss 0.0562404, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:55.737688: step 6804, loss 0.0755187, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:55.958012: step 6805, loss 0.0669575, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:56.141982: step 6806, loss 0.143742, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:56.336807: step 6807, loss 0.0812142, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:56.547629: step 6808, loss 0.103903, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:56.746429: step 6809, loss 0.14982, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:57.020269: step 6810, loss 0.116991, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:57.229560: step 6811, loss 0.0993932, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:57.513614: step 6812, loss 0.0886605, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:57.737031: step 6813, loss 0.143212, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:58.017795: step 6814, loss 0.0584445, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:58.304851: step 6815, loss 0.0636231, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:58.467994: step 6816, loss 0.0691292, acc 1, learning_rate 0.0001
2017-10-10T15:17:58.793878: step 6817, loss 0.102259, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:59.041088: step 6818, loss 0.0567966, acc 1, learning_rate 0.0001
2017-10-10T15:17:59.243761: step 6819, loss 0.147046, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:59.506056: step 6820, loss 0.0727096, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:59.684911: step 6821, loss 0.175625, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:59.972841: step 6822, loss 0.0855297, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:00.214192: step 6823, loss 0.140095, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:00.454829: step 6824, loss 0.116853, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:00.680574: step 6825, loss 0.0769093, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:00.913567: step 6826, loss 0.0961171, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:01.140907: step 6827, loss 0.137637, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:01.418133: step 6828, loss 0.0736775, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:01.700944: step 6829, loss 0.0628538, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:01.937741: step 6830, loss 0.0481937, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:02.213176: step 6831, loss 0.0774669, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:02.460695: step 6832, loss 0.131191, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:02.726123: step 6833, loss 0.0992547, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:02.904033: step 6834, loss 0.0905875, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:03.164858: step 6835, loss 0.0527618, acc 1, learning_rate 0.0001
2017-10-10T15:18:03.380848: step 6836, loss 0.054165, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:03.648696: step 6837, loss 0.0879652, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:03.932852: step 6838, loss 0.0625455, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:04.146504: step 6839, loss 0.144472, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:04.391997: step 6840, loss 0.124881, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:04.769836: step 6840, loss 0.184792, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6840

2017-10-10T15:18:05.851088: step 6841, loss 0.0861275, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:06.071360: step 6842, loss 0.0313249, acc 1, learning_rate 0.0001
2017-10-10T15:18:06.336387: step 6843, loss 0.114073, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:06.576401: step 6844, loss 0.0735898, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:06.808157: step 6845, loss 0.154937, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:07.092986: step 6846, loss 0.172832, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:07.308867: step 6847, loss 0.041404, acc 1, learning_rate 0.0001
2017-10-10T15:18:07.594875: step 6848, loss 0.148722, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:07.796837: step 6849, loss 0.0950412, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:08.075466: step 6850, loss 0.116311, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:08.304893: step 6851, loss 0.0906922, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:08.552221: step 6852, loss 0.0478939, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:08.828880: step 6853, loss 0.083265, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:09.048859: step 6854, loss 0.115365, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:09.340287: step 6855, loss 0.105883, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:09.615540: step 6856, loss 0.142814, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:09.818809: step 6857, loss 0.0786383, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:10.098072: step 6858, loss 0.110561, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:10.292323: step 6859, loss 0.092649, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:10.548168: step 6860, loss 0.0826059, acc 0.980392, learning_rate 0.0001
2017-10-10T15:18:10.812882: step 6861, loss 0.123508, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:11.036852: step 6862, loss 0.092436, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:11.287868: step 6863, loss 0.0651579, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:11.542836: step 6864, loss 0.103825, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:11.792216: step 6865, loss 0.0596542, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:12.113112: step 6866, loss 0.0638696, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:12.338250: step 6867, loss 0.065576, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:12.578067: step 6868, loss 0.096541, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:12.831658: step 6869, loss 0.0592577, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:13.036930: step 6870, loss 0.167638, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:13.260591: step 6871, loss 0.06768, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:13.546735: step 6872, loss 0.0793867, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:13.785192: step 6873, loss 0.116209, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:13.976835: step 6874, loss 0.125221, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:14.184922: step 6875, loss 0.0914474, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:14.425076: step 6876, loss 0.0818257, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:14.646666: step 6877, loss 0.107322, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:14.816382: step 6878, loss 0.112789, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:15.056846: step 6879, loss 0.102827, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:15.264937: step 6880, loss 0.115414, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:15.683187: step 6880, loss 0.184773, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6880

2017-10-10T15:18:17.121022: step 6881, loss 0.0842001, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:17.388875: step 6882, loss 0.139701, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:17.663343: step 6883, loss 0.0943019, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:17.931606: step 6884, loss 0.118831, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:18.117820: step 6885, loss 0.155782, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:18.392481: step 6886, loss 0.127869, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:18.650275: step 6887, loss 0.219827, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:18.864253: step 6888, loss 0.147304, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:19.160824: step 6889, loss 0.179437, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:19.379292: step 6890, loss 0.19101, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:19.615005: step 6891, loss 0.0892742, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:19.849149: step 6892, loss 0.0337422, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:20.101473: step 6893, loss 0.0271988, acc 1, learning_rate 0.0001
2017-10-10T15:18:20.316885: step 6894, loss 0.117255, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:20.580901: step 6895, loss 0.0405019, acc 1, learning_rate 0.0001
2017-10-10T15:18:20.851606: step 6896, loss 0.0453267, acc 1, learning_rate 0.0001
2017-10-10T15:18:21.064840: step 6897, loss 0.0642754, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:21.339176: step 6898, loss 0.0855064, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:21.548381: step 6899, loss 0.127305, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:21.817079: step 6900, loss 0.0521442, acc 1, learning_rate 0.0001
2017-10-10T15:18:22.085039: step 6901, loss 0.12878, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:22.362329: step 6902, loss 0.10497, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:22.623808: step 6903, loss 0.15452, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:22.896811: step 6904, loss 0.0877035, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:23.130778: step 6905, loss 0.11581, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:23.378260: step 6906, loss 0.102237, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:23.522745: step 6907, loss 0.0898134, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:23.768150: step 6908, loss 0.115097, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:23.988559: step 6909, loss 0.0626356, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:24.216955: step 6910, loss 0.0630455, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:24.480840: step 6911, loss 0.0844676, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:24.765027: step 6912, loss 0.135757, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:24.973890: step 6913, loss 0.0982344, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:25.284636: step 6914, loss 0.0994287, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:25.504068: step 6915, loss 0.0843857, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:25.760924: step 6916, loss 0.046364, acc 1, learning_rate 0.0001
2017-10-10T15:18:26.033722: step 6917, loss 0.0654072, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:26.268882: step 6918, loss 0.0670195, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:26.482609: step 6919, loss 0.0500883, acc 1, learning_rate 0.0001
2017-10-10T15:18:26.783649: step 6920, loss 0.103278, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:27.219531: step 6920, loss 0.183593, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6920

2017-10-10T15:18:28.099321: step 6921, loss 0.167829, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:28.334412: step 6922, loss 0.101117, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:28.605725: step 6923, loss 0.073823, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:28.851090: step 6924, loss 0.0884898, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:29.093843: step 6925, loss 0.134374, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:29.396873: step 6926, loss 0.13141, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:29.683825: step 6927, loss 0.212144, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:29.911515: step 6928, loss 0.109354, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:30.144952: step 6929, loss 0.0380236, acc 1, learning_rate 0.0001
2017-10-10T15:18:30.384933: step 6930, loss 0.0579686, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:30.645530: step 6931, loss 0.0326357, acc 1, learning_rate 0.0001
2017-10-10T15:18:30.858938: step 6932, loss 0.0813622, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:31.144925: step 6933, loss 0.125959, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:31.412083: step 6934, loss 0.081268, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:31.677629: step 6935, loss 0.0904066, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:31.884969: step 6936, loss 0.0728659, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:32.141012: step 6937, loss 0.0889088, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:32.348919: step 6938, loss 0.121639, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:32.551917: step 6939, loss 0.0796581, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:32.755160: step 6940, loss 0.122091, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:32.954978: step 6941, loss 0.143138, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:33.124817: step 6942, loss 0.119078, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:33.447364: step 6943, loss 0.104179, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:33.621913: step 6944, loss 0.0731741, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:33.885493: step 6945, loss 0.09237, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:34.172840: step 6946, loss 0.0625188, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:34.442103: step 6947, loss 0.0830059, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:34.667887: step 6948, loss 0.0647284, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:34.936349: step 6949, loss 0.124205, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:35.188083: step 6950, loss 0.0867828, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:35.462608: step 6951, loss 0.104203, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:35.744854: step 6952, loss 0.0566179, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:35.936098: step 6953, loss 0.107257, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:36.192809: step 6954, loss 0.0731334, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:36.448818: step 6955, loss 0.123935, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:36.717758: step 6956, loss 0.0857511, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:36.966633: step 6957, loss 0.0936816, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:37.168984: step 6958, loss 0.105966, acc 0.960784, learning_rate 0.0001
2017-10-10T15:18:37.437125: step 6959, loss 0.0545781, acc 1, learning_rate 0.0001
2017-10-10T15:18:37.706871: step 6960, loss 0.13528, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:38.117507: step 6960, loss 0.183585, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-6960

2017-10-10T15:18:39.145119: step 6961, loss 0.0619562, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:39.323840: step 6962, loss 0.0740728, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:39.500892: step 6963, loss 0.0854559, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:39.733039: step 6964, loss 0.125443, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:39.984806: step 6965, loss 0.0995325, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:40.268315: step 6966, loss 0.184359, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:40.867016: step 6967, loss 0.0756038, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:41.127307: step 6968, loss 0.0355625, acc 1, learning_rate 0.0001
2017-10-10T15:18:41.391913: step 6969, loss 0.128664, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:41.640515: step 6970, loss 0.0801829, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:41.896950: step 6971, loss 0.0856591, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:42.126856: step 6972, loss 0.135877, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:42.389288: step 6973, loss 0.162122, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:42.600668: step 6974, loss 0.0832187, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:42.843377: step 6975, loss 0.0886622, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:43.090773: step 6976, loss 0.0336494, acc 1, learning_rate 0.0001
2017-10-10T15:18:43.332830: step 6977, loss 0.125539, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:43.593059: step 6978, loss 0.0343758, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:43.848813: step 6979, loss 0.107723, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:44.108833: step 6980, loss 0.130793, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:44.344436: step 6981, loss 0.104406, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:44.628811: step 6982, loss 0.0806273, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:44.904937: step 6983, loss 0.0691588, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:45.088854: step 6984, loss 0.0874814, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:45.367275: step 6985, loss 0.0861191, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:45.835647: step 6986, loss 0.0517962, acc 1, learning_rate 0.0001
2017-10-10T15:18:46.179049: step 6987, loss 0.0399236, acc 1, learning_rate 0.0001
2017-10-10T15:18:46.389673: step 6988, loss 0.0948077, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:46.683585: step 6989, loss 0.0898668, acc 1, learning_rate 0.0001
2017-10-10T15:18:46.932928: step 6990, loss 0.130759, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:47.204825: step 6991, loss 0.075639, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:47.434679: step 6992, loss 0.0620927, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:47.612921: step 6993, loss 0.0505413, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:47.872680: step 6994, loss 0.078292, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:48.078686: step 6995, loss 0.0732953, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:48.329263: step 6996, loss 0.0786327, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:48.577052: step 6997, loss 0.168256, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:48.821341: step 6998, loss 0.0410875, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:49.090175: step 6999, loss 0.154193, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:49.382307: step 7000, loss 0.178712, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:49.805879: step 7000, loss 0.184495, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7000

2017-10-10T15:18:50.874606: step 7001, loss 0.0750047, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:51.108900: step 7002, loss 0.0571949, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:51.296959: step 7003, loss 0.137014, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:51.541469: step 7004, loss 0.146331, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:51.776084: step 7005, loss 0.0551432, acc 1, learning_rate 0.0001
2017-10-10T15:18:51.997750: step 7006, loss 0.14023, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:52.272445: step 7007, loss 0.030792, acc 1, learning_rate 0.0001
2017-10-10T15:18:52.555094: step 7008, loss 0.113833, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:52.765246: step 7009, loss 0.11976, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:52.949109: step 7010, loss 0.093462, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:53.135008: step 7011, loss 0.124186, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:53.296809: step 7012, loss 0.0890078, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:53.491646: step 7013, loss 0.077315, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:53.740747: step 7014, loss 0.15229, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:54.021334: step 7015, loss 0.134422, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:54.244985: step 7016, loss 0.0720115, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:54.496899: step 7017, loss 0.0548521, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:54.774545: step 7018, loss 0.140817, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:55.085971: step 7019, loss 0.0555679, acc 1, learning_rate 0.0001
2017-10-10T15:18:55.313187: step 7020, loss 0.103101, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:55.559066: step 7021, loss 0.0820313, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:55.803736: step 7022, loss 0.0797088, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:56.024364: step 7023, loss 0.0579429, acc 1, learning_rate 0.0001
2017-10-10T15:18:56.256359: step 7024, loss 0.0944851, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:56.479964: step 7025, loss 0.147723, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:56.662699: step 7026, loss 0.0399097, acc 1, learning_rate 0.0001
2017-10-10T15:18:56.953678: step 7027, loss 0.154987, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:57.181029: step 7028, loss 0.137384, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:57.420926: step 7029, loss 0.0615015, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:57.702302: step 7030, loss 0.0804992, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:58.024878: step 7031, loss 0.0822261, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:58.268299: step 7032, loss 0.123948, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:58.534029: step 7033, loss 0.11134, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:58.764214: step 7034, loss 0.149785, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:58.957133: step 7035, loss 0.0449967, acc 1, learning_rate 0.0001
2017-10-10T15:18:59.200537: step 7036, loss 0.0543421, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:59.453586: step 7037, loss 0.0575543, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:59.638297: step 7038, loss 0.10826, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:59.885130: step 7039, loss 0.0838955, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:00.120514: step 7040, loss 0.0388209, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:00.496261: step 7040, loss 0.184145, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7040

2017-10-10T15:19:01.560815: step 7041, loss 0.0385282, acc 1, learning_rate 0.0001
2017-10-10T15:19:01.844941: step 7042, loss 0.12656, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:02.105336: step 7043, loss 0.052708, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:02.346370: step 7044, loss 0.0715672, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:02.613052: step 7045, loss 0.0888107, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:02.839980: step 7046, loss 0.0843028, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:03.071351: step 7047, loss 0.0945964, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:03.300240: step 7048, loss 0.139521, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:03.563325: step 7049, loss 0.161523, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:03.787549: step 7050, loss 0.0414215, acc 1, learning_rate 0.0001
2017-10-10T15:19:04.057219: step 7051, loss 0.165375, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:04.245418: step 7052, loss 0.0740012, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:04.460326: step 7053, loss 0.040991, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:04.691496: step 7054, loss 0.0808116, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:04.931520: step 7055, loss 0.128046, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:05.155472: step 7056, loss 0.0522166, acc 1, learning_rate 0.0001
2017-10-10T15:19:05.351339: step 7057, loss 0.0262958, acc 1, learning_rate 0.0001
2017-10-10T15:19:05.604861: step 7058, loss 0.11164, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:05.865842: step 7059, loss 0.102247, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:06.103152: step 7060, loss 0.110681, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:06.359803: step 7061, loss 0.0328435, acc 1, learning_rate 0.0001
2017-10-10T15:19:06.585088: step 7062, loss 0.0965058, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:06.816893: step 7063, loss 0.0613443, acc 1, learning_rate 0.0001
2017-10-10T15:19:07.041041: step 7064, loss 0.0505792, acc 1, learning_rate 0.0001
2017-10-10T15:19:07.316898: step 7065, loss 0.0595389, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:07.585280: step 7066, loss 0.0854141, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:07.824570: step 7067, loss 0.0659849, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:08.094595: step 7068, loss 0.0486424, acc 1, learning_rate 0.0001
2017-10-10T15:19:08.392391: step 7069, loss 0.0738448, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:08.644955: step 7070, loss 0.098179, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:08.900898: step 7071, loss 0.0569116, acc 1, learning_rate 0.0001
2017-10-10T15:19:09.168180: step 7072, loss 0.039231, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:09.323682: step 7073, loss 0.109921, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:09.636866: step 7074, loss 0.0798931, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:09.876815: step 7075, loss 0.0974735, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:10.085002: step 7076, loss 0.0994285, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:10.314965: step 7077, loss 0.133242, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:10.562188: step 7078, loss 0.13128, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:10.752963: step 7079, loss 0.142257, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:10.964352: step 7080, loss 0.12441, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:11.424760: step 7080, loss 0.183636, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7080

2017-10-10T15:19:12.353638: step 7081, loss 0.0928061, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:12.700870: step 7082, loss 0.0918663, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:12.896958: step 7083, loss 0.0645676, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:13.147757: step 7084, loss 0.0333994, acc 1, learning_rate 0.0001
2017-10-10T15:19:13.360920: step 7085, loss 0.0977512, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:13.641046: step 7086, loss 0.0665881, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:13.864876: step 7087, loss 0.0621178, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:14.132500: step 7088, loss 0.106014, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:14.358597: step 7089, loss 0.0942097, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:14.594546: step 7090, loss 0.124892, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:14.852948: step 7091, loss 0.0834765, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:15.080971: step 7092, loss 0.0659352, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:15.359350: step 7093, loss 0.0807224, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:15.636825: step 7094, loss 0.0490517, acc 1, learning_rate 0.0001
2017-10-10T15:19:15.853883: step 7095, loss 0.0849816, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:16.082935: step 7096, loss 0.111682, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:16.323541: step 7097, loss 0.062568, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:16.557661: step 7098, loss 0.0845508, acc 1, learning_rate 0.0001
2017-10-10T15:19:16.868910: step 7099, loss 0.0793959, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:17.100182: step 7100, loss 0.0611102, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:17.373378: step 7101, loss 0.118843, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:17.648856: step 7102, loss 0.0774724, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:17.844935: step 7103, loss 0.0834042, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:18.115133: step 7104, loss 0.0960274, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:18.321218: step 7105, loss 0.0431802, acc 1, learning_rate 0.0001
2017-10-10T15:19:18.512206: step 7106, loss 0.157465, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:18.747031: step 7107, loss 0.168321, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:18.998972: step 7108, loss 0.0566976, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:19.237130: step 7109, loss 0.100131, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:19.453759: step 7110, loss 0.0997912, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:19.656565: step 7111, loss 0.0977083, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:19.890376: step 7112, loss 0.0925976, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:20.158211: step 7113, loss 0.127338, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:20.484062: step 7114, loss 0.116534, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:20.672825: step 7115, loss 0.114, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:20.911499: step 7116, loss 0.16981, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:21.081434: step 7117, loss 0.0858266, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:21.311486: step 7118, loss 0.0702679, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:21.620928: step 7119, loss 0.109282, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:21.876994: step 7120, loss 0.089501, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:22.351178: step 7120, loss 0.182266, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7120

2017-10-10T15:19:23.356839: step 7121, loss 0.236476, acc 0.90625, learning_rate 0.0001
2017-10-10T15:19:23.648834: step 7122, loss 0.113388, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:23.855254: step 7123, loss 0.119531, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:24.138403: step 7124, loss 0.0855776, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:24.427450: step 7125, loss 0.105771, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:24.650445: step 7126, loss 0.120564, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:24.879168: step 7127, loss 0.0536313, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:25.148928: step 7128, loss 0.0743665, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:25.418657: step 7129, loss 0.0681967, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:25.639195: step 7130, loss 0.0874255, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:25.870768: step 7131, loss 0.0595822, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:26.108963: step 7132, loss 0.0760485, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:26.328664: step 7133, loss 0.0530427, acc 1, learning_rate 0.0001
2017-10-10T15:19:26.617475: step 7134, loss 0.0999387, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:26.827373: step 7135, loss 0.11912, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:27.106293: step 7136, loss 0.137406, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:27.364728: step 7137, loss 0.0604635, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:27.640952: step 7138, loss 0.0913028, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:27.918410: step 7139, loss 0.0546775, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:28.204924: step 7140, loss 0.077999, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:28.409740: step 7141, loss 0.0802277, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:28.616251: step 7142, loss 0.162782, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:28.852877: step 7143, loss 0.0926109, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:29.048883: step 7144, loss 0.0898395, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:29.297002: step 7145, loss 0.08115, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:29.600953: step 7146, loss 0.139864, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:29.947714: step 7147, loss 0.0866153, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:30.125406: step 7148, loss 0.0687482, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:30.349297: step 7149, loss 0.124859, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:30.619430: step 7150, loss 0.0408598, acc 1, learning_rate 0.0001
2017-10-10T15:19:30.830295: step 7151, loss 0.112107, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:31.064850: step 7152, loss 0.251629, acc 0.890625, learning_rate 0.0001
2017-10-10T15:19:31.316678: step 7153, loss 0.0946291, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:31.574093: step 7154, loss 0.0707737, acc 0.980392, learning_rate 0.0001
2017-10-10T15:19:31.833006: step 7155, loss 0.12943, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:32.056750: step 7156, loss 0.0447325, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:32.334164: step 7157, loss 0.0748444, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:32.586718: step 7158, loss 0.0454063, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:32.793910: step 7159, loss 0.149786, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:32.993790: step 7160, loss 0.102389, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:33.463709: step 7160, loss 0.182171, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7160

2017-10-10T15:19:34.472393: step 7161, loss 0.0527135, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:34.725084: step 7162, loss 0.16864, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:35.040046: step 7163, loss 0.0940794, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:35.216874: step 7164, loss 0.0667082, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:35.486438: step 7165, loss 0.0859018, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:35.692499: step 7166, loss 0.098842, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:35.912868: step 7167, loss 0.0761483, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:36.129029: step 7168, loss 0.0720389, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:36.416844: step 7169, loss 0.118881, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:36.672803: step 7170, loss 0.073639, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:37.039397: step 7171, loss 0.129567, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:37.329372: step 7172, loss 0.0652863, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:37.574676: step 7173, loss 0.0561191, acc 1, learning_rate 0.0001
2017-10-10T15:19:37.790614: step 7174, loss 0.0599618, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:38.013127: step 7175, loss 0.0773447, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:38.256483: step 7176, loss 0.057791, acc 1, learning_rate 0.0001
2017-10-10T15:19:38.438802: step 7177, loss 0.0374899, acc 1, learning_rate 0.0001
2017-10-10T15:19:38.761470: step 7178, loss 0.0597687, acc 1, learning_rate 0.0001
2017-10-10T15:19:39.032870: step 7179, loss 0.107241, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:39.240897: step 7180, loss 0.0370192, acc 1, learning_rate 0.0001
2017-10-10T15:19:39.567115: step 7181, loss 0.123771, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:39.824826: step 7182, loss 0.0349724, acc 1, learning_rate 0.0001
2017-10-10T15:19:40.027267: step 7183, loss 0.123063, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:40.277188: step 7184, loss 0.0502675, acc 1, learning_rate 0.0001
2017-10-10T15:19:40.525023: step 7185, loss 0.173901, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:40.809335: step 7186, loss 0.133871, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:41.055102: step 7187, loss 0.0977226, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:41.287885: step 7188, loss 0.0595097, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:41.552175: step 7189, loss 0.0336698, acc 1, learning_rate 0.0001
2017-10-10T15:19:41.796907: step 7190, loss 0.0831585, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:42.068773: step 7191, loss 0.117779, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:42.260913: step 7192, loss 0.148033, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:42.533311: step 7193, loss 0.147797, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:42.730295: step 7194, loss 0.0634527, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:42.988006: step 7195, loss 0.084891, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:43.288036: step 7196, loss 0.15858, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:43.470019: step 7197, loss 0.0868286, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:43.758245: step 7198, loss 0.126467, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:44.064906: step 7199, loss 0.0778188, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:44.264579: step 7200, loss 0.0800235, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:44.714806: step 7200, loss 0.183273, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7200

2017-10-10T15:19:45.779543: step 7201, loss 0.166622, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:45.956840: step 7202, loss 0.161897, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:46.255613: step 7203, loss 0.150664, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:46.505001: step 7204, loss 0.161549, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:46.824808: step 7205, loss 0.136814, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:47.064475: step 7206, loss 0.217058, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:47.295305: step 7207, loss 0.0818187, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:47.485797: step 7208, loss 0.132557, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:47.674066: step 7209, loss 0.0441978, acc 1, learning_rate 0.0001
2017-10-10T15:19:47.897044: step 7210, loss 0.119729, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:48.182754: step 7211, loss 0.152483, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:48.492914: step 7212, loss 0.0457094, acc 1, learning_rate 0.0001
2017-10-10T15:19:48.710450: step 7213, loss 0.152189, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:48.932793: step 7214, loss 0.089118, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:49.216913: step 7215, loss 0.159839, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:49.453276: step 7216, loss 0.0886718, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:49.744867: step 7217, loss 0.223583, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:50.036809: step 7218, loss 0.135292, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:50.328994: step 7219, loss 0.0656097, acc 1, learning_rate 0.0001
2017-10-10T15:19:50.573317: step 7220, loss 0.108004, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:50.828677: step 7221, loss 0.0962542, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:51.039895: step 7222, loss 0.131627, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:51.270653: step 7223, loss 0.0318387, acc 1, learning_rate 0.0001
2017-10-10T15:19:51.513160: step 7224, loss 0.0774641, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:51.728925: step 7225, loss 0.0658995, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:51.952862: step 7226, loss 0.0523316, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:52.236446: step 7227, loss 0.114036, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:52.469247: step 7228, loss 0.109927, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:52.719582: step 7229, loss 0.0996468, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:52.971429: step 7230, loss 0.204926, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:53.241559: step 7231, loss 0.0490915, acc 1, learning_rate 0.0001
2017-10-10T15:19:53.449400: step 7232, loss 0.0998499, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:53.708054: step 7233, loss 0.0762488, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:53.968974: step 7234, loss 0.0559278, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:54.276855: step 7235, loss 0.135364, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:54.510971: step 7236, loss 0.101986, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:54.660036: step 7237, loss 0.160531, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:54.888819: step 7238, loss 0.101036, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:55.078227: step 7239, loss 0.0834797, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:55.329138: step 7240, loss 0.0481422, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:55.789731: step 7240, loss 0.182802, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7240

2017-10-10T15:19:56.676093: step 7241, loss 0.0525373, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:56.914175: step 7242, loss 0.0580768, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:57.192759: step 7243, loss 0.0952404, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:57.453742: step 7244, loss 0.0570609, acc 1, learning_rate 0.0001
2017-10-10T15:19:57.720548: step 7245, loss 0.0730083, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:57.927849: step 7246, loss 0.06571, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:58.132239: step 7247, loss 0.0623939, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:58.368445: step 7248, loss 0.0575945, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:58.630724: step 7249, loss 0.0634328, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:58.803518: step 7250, loss 0.0644614, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:58.970454: step 7251, loss 0.059174, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:59.117284: step 7252, loss 0.109513, acc 0.960784, learning_rate 0.0001
2017-10-10T15:19:59.304759: step 7253, loss 0.0966408, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:59.552923: step 7254, loss 0.0998076, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:59.783466: step 7255, loss 0.106628, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:00.009017: step 7256, loss 0.0787024, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:00.305187: step 7257, loss 0.253077, acc 0.90625, learning_rate 0.0001
2017-10-10T15:20:00.557559: step 7258, loss 0.0720229, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:00.781079: step 7259, loss 0.091341, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:01.058051: step 7260, loss 0.0519525, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:01.296998: step 7261, loss 0.115708, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:01.540900: step 7262, loss 0.125992, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:01.767499: step 7263, loss 0.108358, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:02.000318: step 7264, loss 0.0232633, acc 1, learning_rate 0.0001
2017-10-10T15:20:02.248212: step 7265, loss 0.178113, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:02.546458: step 7266, loss 0.127208, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:02.775652: step 7267, loss 0.0792825, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:03.032102: step 7268, loss 0.0761433, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:03.260976: step 7269, loss 0.0418285, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:03.493195: step 7270, loss 0.110474, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:03.744107: step 7271, loss 0.0807925, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:03.997103: step 7272, loss 0.0445561, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:04.224961: step 7273, loss 0.0694259, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:04.415374: step 7274, loss 0.066677, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:04.723624: step 7275, loss 0.0909617, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:04.907706: step 7276, loss 0.0726297, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:05.180902: step 7277, loss 0.0627093, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:05.497747: step 7278, loss 0.0449028, acc 1, learning_rate 0.0001
2017-10-10T15:20:05.746692: step 7279, loss 0.0289344, acc 1, learning_rate 0.0001
2017-10-10T15:20:05.999845: step 7280, loss 0.124946, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:06.375683: step 7280, loss 0.182332, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7280

2017-10-10T15:20:07.416080: step 7281, loss 0.104117, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:07.665255: step 7282, loss 0.101331, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:07.861664: step 7283, loss 0.0427103, acc 1, learning_rate 0.0001
2017-10-10T15:20:08.120988: step 7284, loss 0.0540177, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:08.353890: step 7285, loss 0.103032, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:08.588890: step 7286, loss 0.278207, acc 0.90625, learning_rate 0.0001
2017-10-10T15:20:08.812807: step 7287, loss 0.0754652, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:09.116118: step 7288, loss 0.0735735, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:09.397488: step 7289, loss 0.0774783, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:09.619168: step 7290, loss 0.0588598, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:09.921113: step 7291, loss 0.0997418, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:10.190336: step 7292, loss 0.11233, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:10.480973: step 7293, loss 0.117849, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:10.717133: step 7294, loss 0.03178, acc 1, learning_rate 0.0001
2017-10-10T15:20:10.928921: step 7295, loss 0.154485, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:11.152848: step 7296, loss 0.0327672, acc 1, learning_rate 0.0001
2017-10-10T15:20:11.357225: step 7297, loss 0.127774, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:11.625516: step 7298, loss 0.0895708, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:11.932891: step 7299, loss 0.0747955, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:12.216864: step 7300, loss 0.118468, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:12.475675: step 7301, loss 0.102812, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:12.695199: step 7302, loss 0.0897633, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:12.926606: step 7303, loss 0.0410803, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:13.151140: step 7304, loss 0.0585051, acc 1, learning_rate 0.0001
2017-10-10T15:20:13.449049: step 7305, loss 0.0516863, acc 1, learning_rate 0.0001
2017-10-10T15:20:13.674916: step 7306, loss 0.172236, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:13.912980: step 7307, loss 0.0587372, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:14.170031: step 7308, loss 0.0667589, acc 1, learning_rate 0.0001
2017-10-10T15:20:14.437048: step 7309, loss 0.0727199, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:14.644400: step 7310, loss 0.115335, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:14.948021: step 7311, loss 0.12136, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:15.176872: step 7312, loss 0.0619458, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:15.439906: step 7313, loss 0.0583411, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:15.703194: step 7314, loss 0.0981354, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:15.925038: step 7315, loss 0.0865565, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:16.179663: step 7316, loss 0.0663388, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:16.351146: step 7317, loss 0.177452, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:16.659019: step 7318, loss 0.080558, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:16.896954: step 7319, loss 0.0929259, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:17.126136: step 7320, loss 0.069523, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:17.546817: step 7320, loss 0.183571, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7320

2017-10-10T15:20:18.600999: step 7321, loss 0.0675035, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:18.889042: step 7322, loss 0.0504432, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:19.177075: step 7323, loss 0.0566255, acc 1, learning_rate 0.0001
2017-10-10T15:20:19.397403: step 7324, loss 0.138818, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:19.653742: step 7325, loss 0.0646841, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:19.903078: step 7326, loss 0.0483358, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:20.093097: step 7327, loss 0.0872808, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:20.335069: step 7328, loss 0.0787859, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:20.524890: step 7329, loss 0.103284, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:20.804331: step 7330, loss 0.0457968, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:21.080907: step 7331, loss 0.11773, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:21.289121: step 7332, loss 0.14496, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:21.564859: step 7333, loss 0.0536202, acc 1, learning_rate 0.0001
2017-10-10T15:20:21.855473: step 7334, loss 0.0537418, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:22.072831: step 7335, loss 0.0863276, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:22.348876: step 7336, loss 0.0886278, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:22.537150: step 7337, loss 0.0914755, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:22.777729: step 7338, loss 0.152458, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:23.020913: step 7339, loss 0.0909655, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:23.248838: step 7340, loss 0.0889751, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:23.476873: step 7341, loss 0.0659659, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:23.769863: step 7342, loss 0.237693, acc 0.90625, learning_rate 0.0001
2017-10-10T15:20:24.034984: step 7343, loss 0.084233, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:24.280560: step 7344, loss 0.0632761, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:24.488032: step 7345, loss 0.0804479, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:24.728881: step 7346, loss 0.119126, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:24.972856: step 7347, loss 0.198178, acc 0.890625, learning_rate 0.0001
2017-10-10T15:20:25.278180: step 7348, loss 0.0574247, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:25.511592: step 7349, loss 0.0354367, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:25.694079: step 7350, loss 0.082401, acc 0.960784, learning_rate 0.0001
2017-10-10T15:20:25.874576: step 7351, loss 0.0515677, acc 1, learning_rate 0.0001
2017-10-10T15:20:26.088058: step 7352, loss 0.195527, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:26.324303: step 7353, loss 0.0516397, acc 1, learning_rate 0.0001
2017-10-10T15:20:26.589392: step 7354, loss 0.110278, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:26.845039: step 7355, loss 0.0535206, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:27.148320: step 7356, loss 0.0819339, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:27.389833: step 7357, loss 0.101891, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:27.588783: step 7358, loss 0.0987863, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:27.824837: step 7359, loss 0.122466, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:28.037831: step 7360, loss 0.0220662, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:28.488953: step 7360, loss 0.183953, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7360

2017-10-10T15:20:29.750527: step 7361, loss 0.1089, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:29.969711: step 7362, loss 0.0299254, acc 1, learning_rate 0.0001
2017-10-10T15:20:30.253543: step 7363, loss 0.0649097, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:30.493244: step 7364, loss 0.0874868, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:30.720844: step 7365, loss 0.105705, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:30.953043: step 7366, loss 0.0910414, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:31.209058: step 7367, loss 0.131476, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:31.476436: step 7368, loss 0.152038, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:31.705028: step 7369, loss 0.0597614, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:31.940918: step 7370, loss 0.0366544, acc 1, learning_rate 0.0001
2017-10-10T15:20:32.134154: step 7371, loss 0.136111, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:32.379560: step 7372, loss 0.0626501, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:32.548930: step 7373, loss 0.165572, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:32.801904: step 7374, loss 0.161586, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:33.024040: step 7375, loss 0.051241, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:33.262391: step 7376, loss 0.0833666, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:33.562096: step 7377, loss 0.0967188, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:33.793240: step 7378, loss 0.0918255, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:34.073634: step 7379, loss 0.105665, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:34.384984: step 7380, loss 0.0695667, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:34.688931: step 7381, loss 0.0952955, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:34.929175: step 7382, loss 0.0954634, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:35.199048: step 7383, loss 0.128482, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:35.437130: step 7384, loss 0.0658876, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:35.663770: step 7385, loss 0.0419789, acc 1, learning_rate 0.0001
2017-10-10T15:20:35.956812: step 7386, loss 0.108271, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:36.241008: step 7387, loss 0.0814435, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:36.477264: step 7388, loss 0.0669423, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:36.776429: step 7389, loss 0.0196784, acc 1, learning_rate 0.0001
2017-10-10T15:20:37.009022: step 7390, loss 0.0469864, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:37.236433: step 7391, loss 0.0887033, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:37.497473: step 7392, loss 0.152656, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:37.765195: step 7393, loss 0.136372, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:38.072917: step 7394, loss 0.12102, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:38.336943: step 7395, loss 0.0731198, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:38.602503: step 7396, loss 0.175725, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:38.849482: step 7397, loss 0.118178, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:39.046571: step 7398, loss 0.229904, acc 0.90625, learning_rate 0.0001
2017-10-10T15:20:39.285617: step 7399, loss 0.0558603, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:39.592953: step 7400, loss 0.0916353, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:39.984899: step 7400, loss 0.182527, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7400

2017-10-10T15:20:40.882430: step 7401, loss 0.0624466, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:41.172952: step 7402, loss 0.0523572, acc 1, learning_rate 0.0001
2017-10-10T15:20:41.444876: step 7403, loss 0.0694328, acc 1, learning_rate 0.0001
2017-10-10T15:20:41.688563: step 7404, loss 0.105701, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:41.946997: step 7405, loss 0.11278, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:42.218080: step 7406, loss 0.0647058, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:42.450488: step 7407, loss 0.0725052, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:42.634940: step 7408, loss 0.0242349, acc 1, learning_rate 0.0001
2017-10-10T15:20:42.859546: step 7409, loss 0.0557517, acc 1, learning_rate 0.0001
2017-10-10T15:20:43.064862: step 7410, loss 0.0388812, acc 1, learning_rate 0.0001
2017-10-10T15:20:43.299799: step 7411, loss 0.0700567, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:43.605112: step 7412, loss 0.110419, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:43.859913: step 7413, loss 0.0519503, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:44.152875: step 7414, loss 0.160112, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:44.485491: step 7415, loss 0.0779614, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:44.652989: step 7416, loss 0.0647101, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:44.897200: step 7417, loss 0.0882662, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:45.061047: step 7418, loss 0.0515905, acc 1, learning_rate 0.0001
2017-10-10T15:20:45.260832: step 7419, loss 0.110322, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:45.556858: step 7420, loss 0.0196936, acc 1, learning_rate 0.0001
2017-10-10T15:20:45.836566: step 7421, loss 0.0770706, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:46.052857: step 7422, loss 0.115358, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:46.268238: step 7423, loss 0.14462, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:46.486288: step 7424, loss 0.0815515, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:46.792290: step 7425, loss 0.0501435, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:47.036876: step 7426, loss 0.111292, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:47.246728: step 7427, loss 0.0894378, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:47.473220: step 7428, loss 0.0653685, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:47.767704: step 7429, loss 0.0977883, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:47.955893: step 7430, loss 0.0375089, acc 1, learning_rate 0.0001
2017-10-10T15:20:48.228673: step 7431, loss 0.0709544, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:48.492930: step 7432, loss 0.057358, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:48.779503: step 7433, loss 0.0624021, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:49.016980: step 7434, loss 0.122129, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:49.292902: step 7435, loss 0.0448985, acc 1, learning_rate 0.0001
2017-10-10T15:20:49.512408: step 7436, loss 0.117933, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:49.784897: step 7437, loss 0.107487, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:50.029143: step 7438, loss 0.0615347, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:50.261496: step 7439, loss 0.133099, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:50.481951: step 7440, loss 0.0757167, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:50.929051: step 7440, loss 0.181392, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7440

2017-10-10T15:20:52.126046: step 7441, loss 0.0211184, acc 1, learning_rate 0.0001
2017-10-10T15:20:52.312801: step 7442, loss 0.0669454, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:52.563628: step 7443, loss 0.0735862, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:52.804788: step 7444, loss 0.0884709, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:53.077299: step 7445, loss 0.0505873, acc 1, learning_rate 0.0001
2017-10-10T15:20:53.330482: step 7446, loss 0.0459995, acc 1, learning_rate 0.0001
2017-10-10T15:20:53.560540: step 7447, loss 0.0562151, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:53.806620: step 7448, loss 0.109108, acc 0.960784, learning_rate 0.0001
2017-10-10T15:20:54.028856: step 7449, loss 0.0310213, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:54.290723: step 7450, loss 0.0573358, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:54.527751: step 7451, loss 0.051076, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:54.783566: step 7452, loss 0.0977792, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:55.054742: step 7453, loss 0.0943602, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:55.296919: step 7454, loss 0.0918287, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:55.544969: step 7455, loss 0.137056, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:55.797883: step 7456, loss 0.0994073, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:56.059508: step 7457, loss 0.130072, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:56.293021: step 7458, loss 0.0881415, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:56.587022: step 7459, loss 0.0880017, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:56.898333: step 7460, loss 0.0219398, acc 1, learning_rate 0.0001
2017-10-10T15:20:57.121343: step 7461, loss 0.0980591, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:57.381112: step 7462, loss 0.0693203, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:57.593140: step 7463, loss 0.21375, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:57.897670: step 7464, loss 0.0611661, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:58.178555: step 7465, loss 0.0415376, acc 1, learning_rate 0.0001
2017-10-10T15:20:58.400967: step 7466, loss 0.0430897, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:58.655525: step 7467, loss 0.148129, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:58.897267: step 7468, loss 0.0535008, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:59.125325: step 7469, loss 0.0700395, acc 1, learning_rate 0.0001
2017-10-10T15:20:59.393533: step 7470, loss 0.0211034, acc 1, learning_rate 0.0001
2017-10-10T15:20:59.589734: step 7471, loss 0.0731807, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:59.864537: step 7472, loss 0.035761, acc 1, learning_rate 0.0001
2017-10-10T15:21:00.061442: step 7473, loss 0.107973, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:00.340563: step 7474, loss 0.0520305, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:00.604964: step 7475, loss 0.0737166, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:00.803054: step 7476, loss 0.0638167, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:01.064843: step 7477, loss 0.0533938, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:01.374438: step 7478, loss 0.196765, acc 0.90625, learning_rate 0.0001
2017-10-10T15:21:01.578692: step 7479, loss 0.129811, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:01.837061: step 7480, loss 0.0650934, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:02.231344: step 7480, loss 0.181227, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7480

2017-10-10T15:21:03.383297: step 7481, loss 0.114513, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:03.676880: step 7482, loss 0.0662651, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:03.913202: step 7483, loss 0.0339878, acc 1, learning_rate 0.0001
2017-10-10T15:21:04.131922: step 7484, loss 0.0822659, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:04.381500: step 7485, loss 0.0795014, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:04.627327: step 7486, loss 0.0398777, acc 1, learning_rate 0.0001
2017-10-10T15:21:04.884096: step 7487, loss 0.055035, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:05.062924: step 7488, loss 0.0665508, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:05.310401: step 7489, loss 0.0635592, acc 1, learning_rate 0.0001
2017-10-10T15:21:05.554449: step 7490, loss 0.0644312, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:05.791304: step 7491, loss 0.12038, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:06.072070: step 7492, loss 0.182124, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:06.326062: step 7493, loss 0.0984566, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:06.597260: step 7494, loss 0.0935422, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:06.820291: step 7495, loss 0.154581, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:07.032358: step 7496, loss 0.0969809, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:07.376853: step 7497, loss 0.0749651, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:07.613531: step 7498, loss 0.0619247, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:07.852397: step 7499, loss 0.094378, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:08.076849: step 7500, loss 0.107807, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:08.311563: step 7501, loss 0.119571, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:08.563763: step 7502, loss 0.124195, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:08.889024: step 7503, loss 0.173987, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:09.161086: step 7504, loss 0.0764396, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:09.367782: step 7505, loss 0.0546194, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:09.657061: step 7506, loss 0.117555, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:09.889746: step 7507, loss 0.0827836, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:10.187013: step 7508, loss 0.116687, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:10.422674: step 7509, loss 0.0999488, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:10.653098: step 7510, loss 0.09994, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:10.942968: step 7511, loss 0.146226, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:11.164021: step 7512, loss 0.0587461, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:11.447187: step 7513, loss 0.0703399, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:11.643633: step 7514, loss 0.0582861, acc 1, learning_rate 0.0001
2017-10-10T15:21:11.957777: step 7515, loss 0.0437206, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:12.205173: step 7516, loss 0.115039, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:12.496283: step 7517, loss 0.141108, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:12.746757: step 7518, loss 0.0442592, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:12.976469: step 7519, loss 0.0503606, acc 1, learning_rate 0.0001
2017-10-10T15:21:13.233063: step 7520, loss 0.0300341, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:13.701641: step 7520, loss 0.181162, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7520

2017-10-10T15:21:14.624670: step 7521, loss 0.0477401, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:14.897897: step 7522, loss 0.153303, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:15.124148: step 7523, loss 0.108231, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:15.358096: step 7524, loss 0.116954, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:15.608705: step 7525, loss 0.0541843, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:15.853209: step 7526, loss 0.0734374, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:16.145829: step 7527, loss 0.0811885, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:16.405885: step 7528, loss 0.0615711, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:16.728948: step 7529, loss 0.169661, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:16.922346: step 7530, loss 0.166896, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:17.180836: step 7531, loss 0.0987365, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:17.444910: step 7532, loss 0.0921108, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:17.760820: step 7533, loss 0.0624647, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:18.004879: step 7534, loss 0.1025, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:18.261312: step 7535, loss 0.0880845, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:18.371218: step 7536, loss 0.0509523, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:18.470220: step 7537, loss 0.122134, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:18.599586: step 7538, loss 0.0911303, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:18.881191: step 7539, loss 0.0417789, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:18.988880: step 7540, loss 0.12299, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:19.136697: step 7541, loss 0.164919, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:19.266276: step 7542, loss 0.0695551, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:19.416533: step 7543, loss 0.117608, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:19.568849: step 7544, loss 0.0534473, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:19.836150: step 7545, loss 0.0959403, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:20.013931: step 7546, loss 0.0927184, acc 0.960784, learning_rate 0.0001
2017-10-10T15:21:20.257071: step 7547, loss 0.161868, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:20.541664: step 7548, loss 0.153746, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:20.800952: step 7549, loss 0.10938, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:21.008871: step 7550, loss 0.042388, acc 1, learning_rate 0.0001
2017-10-10T15:21:21.213506: step 7551, loss 0.0804866, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:21.426488: step 7552, loss 0.114211, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:21.641077: step 7553, loss 0.0572439, acc 1, learning_rate 0.0001
2017-10-10T15:21:21.929657: step 7554, loss 0.0763224, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:22.149071: step 7555, loss 0.0567589, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:22.350942: step 7556, loss 0.0486869, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:22.618199: step 7557, loss 0.145725, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:22.866707: step 7558, loss 0.0768595, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:23.158725: step 7559, loss 0.0781161, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:23.357320: step 7560, loss 0.117967, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:23.751210: step 7560, loss 0.179808, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7560

2017-10-10T15:21:24.799402: step 7561, loss 0.0948216, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:25.084893: step 7562, loss 0.0568864, acc 1, learning_rate 0.0001
2017-10-10T15:21:25.323140: step 7563, loss 0.0822278, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:25.568897: step 7564, loss 0.113563, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:25.765039: step 7565, loss 0.0510727, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:26.028025: step 7566, loss 0.0958495, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:26.284249: step 7567, loss 0.0434264, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:26.500458: step 7568, loss 0.0394734, acc 1, learning_rate 0.0001
2017-10-10T15:21:26.748263: step 7569, loss 0.0966828, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:26.992963: step 7570, loss 0.0769243, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:27.256989: step 7571, loss 0.21632, acc 0.90625, learning_rate 0.0001
2017-10-10T15:21:27.586082: step 7572, loss 0.13764, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:27.789012: step 7573, loss 0.0532446, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:28.099778: step 7574, loss 0.0407057, acc 1, learning_rate 0.0001
2017-10-10T15:21:28.329431: step 7575, loss 0.0835108, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:28.531038: step 7576, loss 0.138656, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:28.795674: step 7577, loss 0.0925221, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:29.061975: step 7578, loss 0.125885, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:29.332798: step 7579, loss 0.0661815, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:29.604843: step 7580, loss 0.0765122, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:29.867251: step 7581, loss 0.0712117, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:30.147464: step 7582, loss 0.0433449, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:30.380389: step 7583, loss 0.0579915, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:30.603091: step 7584, loss 0.0353132, acc 1, learning_rate 0.0001
2017-10-10T15:21:30.853159: step 7585, loss 0.091797, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:31.150842: step 7586, loss 0.185689, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:31.401201: step 7587, loss 0.109707, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:31.671043: step 7588, loss 0.0885746, acc 1, learning_rate 0.0001
2017-10-10T15:21:31.932884: step 7589, loss 0.0793546, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:32.157717: step 7590, loss 0.0831609, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:32.451581: step 7591, loss 0.0596552, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:32.649678: step 7592, loss 0.0858457, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:32.852914: step 7593, loss 0.104025, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:33.088382: step 7594, loss 0.0553425, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:33.322553: step 7595, loss 0.170802, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:33.597543: step 7596, loss 0.18176, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:33.778692: step 7597, loss 0.05592, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:33.969010: step 7598, loss 0.0829919, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:34.206522: step 7599, loss 0.0678518, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:34.455239: step 7600, loss 0.0394254, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:34.793057: step 7600, loss 0.181069, acc 0.928058

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7600

2017-10-10T15:21:35.844833: step 7601, loss 0.0832992, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:36.094158: step 7602, loss 0.17786, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:36.316865: step 7603, loss 0.0553708, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:36.552716: step 7604, loss 0.128175, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:36.724604: step 7605, loss 0.0309188, acc 1, learning_rate 0.0001
2017-10-10T15:21:36.922069: step 7606, loss 0.0815473, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:37.126152: step 7607, loss 0.0470703, acc 1, learning_rate 0.0001
2017-10-10T15:21:37.299522: step 7608, loss 0.0525142, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:37.505038: step 7609, loss 0.111765, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:37.740833: step 7610, loss 0.132337, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:37.973209: step 7611, loss 0.054404, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:38.272828: step 7612, loss 0.0400585, acc 1, learning_rate 0.0001
2017-10-10T15:21:38.536367: step 7613, loss 0.062583, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:38.802724: step 7614, loss 0.0714712, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:39.095564: step 7615, loss 0.0576934, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:39.333722: step 7616, loss 0.1219, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:39.576732: step 7617, loss 0.0329341, acc 1, learning_rate 0.0001
2017-10-10T15:21:39.909666: step 7618, loss 0.118626, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:40.192838: step 7619, loss 0.0411135, acc 1, learning_rate 0.0001
2017-10-10T15:21:40.461893: step 7620, loss 0.0446925, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:40.747531: step 7621, loss 0.117218, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:41.028910: step 7622, loss 0.126208, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:41.268917: step 7623, loss 0.163893, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:41.540498: step 7624, loss 0.0717173, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:41.820658: step 7625, loss 0.0276013, acc 1, learning_rate 0.0001
2017-10-10T15:21:42.080880: step 7626, loss 0.0815523, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:42.353525: step 7627, loss 0.169808, acc 0.90625, learning_rate 0.0001
2017-10-10T15:21:42.639923: step 7628, loss 0.126651, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:42.891706: step 7629, loss 0.0938547, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:43.113185: step 7630, loss 0.0815106, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:43.362699: step 7631, loss 0.0951338, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:43.677223: step 7632, loss 0.0806568, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:43.936980: step 7633, loss 0.143815, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:44.160830: step 7634, loss 0.113112, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:44.437954: step 7635, loss 0.116077, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:44.691891: step 7636, loss 0.109338, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:44.956208: step 7637, loss 0.156572, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:45.272364: step 7638, loss 0.0482387, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:45.576186: step 7639, loss 0.0895169, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:45.784825: step 7640, loss 0.0837348, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:46.172834: step 7640, loss 0.182462, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7640

2017-10-10T15:21:47.292833: step 7641, loss 0.109488, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:47.533911: step 7642, loss 0.0368091, acc 1, learning_rate 0.0001
2017-10-10T15:21:47.764018: step 7643, loss 0.0957623, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:47.985054: step 7644, loss 0.1446, acc 0.941176, learning_rate 0.0001
2017-10-10T15:21:48.201470: step 7645, loss 0.0773342, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:48.487401: step 7646, loss 0.120607, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:48.730892: step 7647, loss 0.104164, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:48.980412: step 7648, loss 0.0365185, acc 1, learning_rate 0.0001
2017-10-10T15:21:49.198442: step 7649, loss 0.165851, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:49.486727: step 7650, loss 0.112091, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:49.701132: step 7651, loss 0.11175, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:49.943285: step 7652, loss 0.0575922, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:50.244860: step 7653, loss 0.037503, acc 1, learning_rate 0.0001
2017-10-10T15:21:50.408990: step 7654, loss 0.0785919, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:50.708890: step 7655, loss 0.0687115, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:50.928939: step 7656, loss 0.089872, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:51.208838: step 7657, loss 0.0957015, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:51.481011: step 7658, loss 0.0997208, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:51.742089: step 7659, loss 0.109445, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:52.048032: step 7660, loss 0.0157633, acc 1, learning_rate 0.0001
2017-10-10T15:21:52.323282: step 7661, loss 0.0983022, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:52.512840: step 7662, loss 0.0430508, acc 1, learning_rate 0.0001
2017-10-10T15:21:52.794346: step 7663, loss 0.0934111, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:52.992849: step 7664, loss 0.0619192, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:53.214454: step 7665, loss 0.139004, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:53.503006: step 7666, loss 0.0765984, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:53.655397: step 7667, loss 0.0319787, acc 1, learning_rate 0.0001
2017-10-10T15:21:53.968009: step 7668, loss 0.0563488, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:54.240899: step 7669, loss 0.168691, acc 0.90625, learning_rate 0.0001
2017-10-10T15:21:54.511883: step 7670, loss 0.0545542, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:54.825730: step 7671, loss 0.122731, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:55.007431: step 7672, loss 0.14173, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:55.220333: step 7673, loss 0.0654569, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:55.486409: step 7674, loss 0.0558918, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:55.694830: step 7675, loss 0.0722252, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:55.912900: step 7676, loss 0.0470181, acc 1, learning_rate 0.0001
2017-10-10T15:21:56.173499: step 7677, loss 0.0928194, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:56.427540: step 7678, loss 0.0577012, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:56.631513: step 7679, loss 0.137206, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:56.938994: step 7680, loss 0.0659383, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:57.432717: step 7680, loss 0.181653, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7680

2017-10-10T15:21:58.302224: step 7681, loss 0.0758861, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:58.563549: step 7682, loss 0.132425, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:58.809140: step 7683, loss 0.11447, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:59.052028: step 7684, loss 0.125357, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:59.241988: step 7685, loss 0.0390335, acc 1, learning_rate 0.0001
2017-10-10T15:21:59.471359: step 7686, loss 0.0349237, acc 1, learning_rate 0.0001
2017-10-10T15:21:59.681003: step 7687, loss 0.0353542, acc 1, learning_rate 0.0001
2017-10-10T15:21:59.905795: step 7688, loss 0.0687576, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:00.152920: step 7689, loss 0.157831, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:00.356970: step 7690, loss 0.0905285, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:00.630461: step 7691, loss 0.0994406, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:00.865311: step 7692, loss 0.0541508, acc 1, learning_rate 0.0001
2017-10-10T15:22:01.100788: step 7693, loss 0.128921, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:01.424889: step 7694, loss 0.257486, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:01.649247: step 7695, loss 0.134952, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:01.893011: step 7696, loss 0.0891119, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:02.136904: step 7697, loss 0.0714099, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:02.387031: step 7698, loss 0.0732471, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:02.631810: step 7699, loss 0.0960683, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:02.929238: step 7700, loss 0.134011, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:03.146129: step 7701, loss 0.0789835, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:03.411918: step 7702, loss 0.12594, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:03.665078: step 7703, loss 0.120002, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:03.944384: step 7704, loss 0.150644, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:04.164926: step 7705, loss 0.176435, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:04.380296: step 7706, loss 0.0633967, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:04.640723: step 7707, loss 0.0798307, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:04.858541: step 7708, loss 0.0925279, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:05.140852: step 7709, loss 0.109186, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:05.325114: step 7710, loss 0.0888704, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:05.558993: step 7711, loss 0.0566023, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:05.869128: step 7712, loss 0.0521194, acc 1, learning_rate 0.0001
2017-10-10T15:22:06.101238: step 7713, loss 0.121438, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:06.369368: step 7714, loss 0.096574, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:06.636682: step 7715, loss 0.0995606, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:06.921656: step 7716, loss 0.0804778, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:07.117805: step 7717, loss 0.0490508, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:07.393919: step 7718, loss 0.0439103, acc 1, learning_rate 0.0001
2017-10-10T15:22:07.678370: step 7719, loss 0.0705309, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:07.933554: step 7720, loss 0.0910916, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:08.371863: step 7720, loss 0.181673, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7720

2017-10-10T15:22:09.504956: step 7721, loss 0.0918819, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:09.716337: step 7722, loss 0.0472656, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:09.934941: step 7723, loss 0.0626364, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:10.130786: step 7724, loss 0.107418, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:10.365992: step 7725, loss 0.0416718, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:10.573105: step 7726, loss 0.108236, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:10.819656: step 7727, loss 0.114744, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:11.061059: step 7728, loss 0.104467, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:11.325960: step 7729, loss 0.0239284, acc 1, learning_rate 0.0001
2017-10-10T15:22:11.592971: step 7730, loss 0.0275564, acc 1, learning_rate 0.0001
2017-10-10T15:22:11.852344: step 7731, loss 0.0416325, acc 1, learning_rate 0.0001
2017-10-10T15:22:12.147485: step 7732, loss 0.0616286, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:12.380833: step 7733, loss 0.0919534, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:12.640469: step 7734, loss 0.111285, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:12.916636: step 7735, loss 0.0387024, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:13.172684: step 7736, loss 0.164831, acc 0.90625, learning_rate 0.0001
2017-10-10T15:22:13.361391: step 7737, loss 0.0530767, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:13.568201: step 7738, loss 0.0383612, acc 1, learning_rate 0.0001
2017-10-10T15:22:13.737389: step 7739, loss 0.0471575, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:13.945150: step 7740, loss 0.12099, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:14.141091: step 7741, loss 0.0527134, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:14.336504: step 7742, loss 0.0547317, acc 1, learning_rate 0.0001
2017-10-10T15:22:14.637727: step 7743, loss 0.0950961, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:14.864839: step 7744, loss 0.0755867, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:15.109020: step 7745, loss 0.0939659, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:15.345056: step 7746, loss 0.123, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:15.599040: step 7747, loss 0.105334, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:15.828941: step 7748, loss 0.100923, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:16.112842: step 7749, loss 0.0994641, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:16.393122: step 7750, loss 0.0748108, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:16.598095: step 7751, loss 0.0931676, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:16.884991: step 7752, loss 0.075434, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:17.096667: step 7753, loss 0.0945827, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:17.362041: step 7754, loss 0.0651718, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:17.612961: step 7755, loss 0.0757111, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:17.892493: step 7756, loss 0.0383286, acc 1, learning_rate 0.0001
2017-10-10T15:22:18.175983: step 7757, loss 0.140514, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:18.372471: step 7758, loss 0.0677724, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:18.608914: step 7759, loss 0.0717842, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:18.861015: step 7760, loss 0.131191, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:19.243662: step 7760, loss 0.180731, acc 0.926619

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7760

2017-10-10T15:22:20.303030: step 7761, loss 0.0637849, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:20.577019: step 7762, loss 0.0696242, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:20.840944: step 7763, loss 0.0437779, acc 1, learning_rate 0.0001
2017-10-10T15:22:21.067721: step 7764, loss 0.0924007, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:21.381125: step 7765, loss 0.125216, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:21.585483: step 7766, loss 0.128513, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:21.832820: step 7767, loss 0.180397, acc 0.90625, learning_rate 0.0001
2017-10-10T15:22:22.075978: step 7768, loss 0.0757768, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:22.298283: step 7769, loss 0.112608, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:22.544864: step 7770, loss 0.0573229, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:22.727181: step 7771, loss 0.0314733, acc 1, learning_rate 0.0001
2017-10-10T15:22:23.001420: step 7772, loss 0.0410585, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:23.232983: step 7773, loss 0.0909452, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:23.457006: step 7774, loss 0.0947306, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:23.684324: step 7775, loss 0.10555, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:23.942774: step 7776, loss 0.0780444, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:24.181121: step 7777, loss 0.0739141, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:24.447161: step 7778, loss 0.178823, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:24.768089: step 7779, loss 0.152384, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:25.085207: step 7780, loss 0.152051, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:25.301156: step 7781, loss 0.0666883, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:25.505031: step 7782, loss 0.0757661, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:25.720918: step 7783, loss 0.0360752, acc 1, learning_rate 0.0001
2017-10-10T15:22:26.017023: step 7784, loss 0.0749218, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:26.304762: step 7785, loss 0.0412514, acc 1, learning_rate 0.0001
2017-10-10T15:22:26.520865: step 7786, loss 0.0809033, acc 1, learning_rate 0.0001
2017-10-10T15:22:26.744672: step 7787, loss 0.0826429, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:26.995278: step 7788, loss 0.124781, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:27.268630: step 7789, loss 0.0328253, acc 1, learning_rate 0.0001
2017-10-10T15:22:27.521515: step 7790, loss 0.0677441, acc 1, learning_rate 0.0001
2017-10-10T15:22:27.701092: step 7791, loss 0.153136, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:28.012996: step 7792, loss 0.0899687, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:28.220681: step 7793, loss 0.0952301, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:28.489656: step 7794, loss 0.158513, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:28.752244: step 7795, loss 0.0959307, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:28.956913: step 7796, loss 0.101095, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:29.248962: step 7797, loss 0.114271, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:29.410091: step 7798, loss 0.0584002, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:29.683963: step 7799, loss 0.053097, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:29.924974: step 7800, loss 0.108259, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:30.363975: step 7800, loss 0.180615, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7800

2017-10-10T15:22:31.500160: step 7801, loss 0.0496957, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:31.696842: step 7802, loss 0.0684181, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:31.905097: step 7803, loss 0.138888, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:32.120123: step 7804, loss 0.0987146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:32.295571: step 7805, loss 0.0710391, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:32.581438: step 7806, loss 0.059969, acc 1, learning_rate 0.0001
2017-10-10T15:22:32.825125: step 7807, loss 0.059342, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:33.071523: step 7808, loss 0.155027, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:33.312869: step 7809, loss 0.0641011, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:33.540457: step 7810, loss 0.0593035, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:33.823752: step 7811, loss 0.126557, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:34.025060: step 7812, loss 0.0830272, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:34.330253: step 7813, loss 0.0582446, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:34.585041: step 7814, loss 0.0455153, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:34.809376: step 7815, loss 0.060614, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:35.043399: step 7816, loss 0.057658, acc 1, learning_rate 0.0001
2017-10-10T15:22:35.202580: step 7817, loss 0.0659222, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:35.461052: step 7818, loss 0.0865368, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:35.749232: step 7819, loss 0.176792, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:35.977107: step 7820, loss 0.152962, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:36.244170: step 7821, loss 0.0644967, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:36.465091: step 7822, loss 0.0562702, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:36.749009: step 7823, loss 0.0854587, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:37.015918: step 7824, loss 0.121456, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:37.257183: step 7825, loss 0.0276675, acc 1, learning_rate 0.0001
2017-10-10T15:22:37.552906: step 7826, loss 0.0526097, acc 1, learning_rate 0.0001
2017-10-10T15:22:37.844850: step 7827, loss 0.156315, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:38.085500: step 7828, loss 0.047731, acc 1, learning_rate 0.0001
2017-10-10T15:22:38.329449: step 7829, loss 0.0427096, acc 1, learning_rate 0.0001
2017-10-10T15:22:38.549065: step 7830, loss 0.0352261, acc 1, learning_rate 0.0001
2017-10-10T15:22:38.810182: step 7831, loss 0.0578373, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:39.148941: step 7832, loss 0.0661713, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:39.373459: step 7833, loss 0.0407269, acc 1, learning_rate 0.0001
2017-10-10T15:22:39.648875: step 7834, loss 0.0547184, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:39.836930: step 7835, loss 0.146677, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:40.077258: step 7836, loss 0.0409241, acc 1, learning_rate 0.0001
2017-10-10T15:22:40.340816: step 7837, loss 0.174857, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:40.645797: step 7838, loss 0.163845, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:40.900846: step 7839, loss 0.051642, acc 1, learning_rate 0.0001
2017-10-10T15:22:41.088562: step 7840, loss 0.10354, acc 0.960784, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:41.502497: step 7840, loss 0.180625, acc 0.929496

Saved model checkpoint to /home/sheep/bigdata/runs/1507664462/checkpoints/model-7840

