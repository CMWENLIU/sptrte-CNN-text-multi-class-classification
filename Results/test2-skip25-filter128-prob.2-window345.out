
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.2
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=128

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507657355

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-10T12:42:42.699536: step 1, loss 12.8613, acc 0.09375, learning_rate 0.005
2017-10-10T12:42:43.124933: step 2, loss 10.397, acc 0.25, learning_rate 0.00498
2017-10-10T12:42:43.562592: step 3, loss 8.67746, acc 0.34375, learning_rate 0.00496008
2017-10-10T12:42:43.949839: step 4, loss 11.034, acc 0.296875, learning_rate 0.00494024
2017-10-10T12:42:44.264829: step 5, loss 10.7127, acc 0.234375, learning_rate 0.00492049
2017-10-10T12:42:44.586042: step 6, loss 7.28541, acc 0.390625, learning_rate 0.00490081
2017-10-10T12:42:45.000024: step 7, loss 8.47009, acc 0.25, learning_rate 0.00488121
2017-10-10T12:42:45.428875: step 8, loss 7.82941, acc 0.296875, learning_rate 0.0048617
2017-10-10T12:42:45.840488: step 9, loss 8.46718, acc 0.296875, learning_rate 0.00484226
2017-10-10T12:42:46.264206: step 10, loss 7.67086, acc 0.3125, learning_rate 0.00482291
2017-10-10T12:42:46.685809: step 11, loss 7.17317, acc 0.328125, learning_rate 0.00480363
2017-10-10T12:42:47.104953: step 12, loss 6.08115, acc 0.375, learning_rate 0.00478443
2017-10-10T12:42:47.512845: step 13, loss 5.87972, acc 0.453125, learning_rate 0.00476531
2017-10-10T12:42:47.909364: step 14, loss 6.36109, acc 0.515625, learning_rate 0.00474627
2017-10-10T12:42:48.260820: step 15, loss 7.31642, acc 0.296875, learning_rate 0.0047273
2017-10-10T12:42:48.694385: step 16, loss 4.10967, acc 0.484375, learning_rate 0.00470841
2017-10-10T12:42:49.092436: step 17, loss 5.02374, acc 0.5, learning_rate 0.0046896
2017-10-10T12:42:49.492863: step 18, loss 6.01872, acc 0.4375, learning_rate 0.00467087
2017-10-10T12:42:49.909342: step 19, loss 6.06028, acc 0.46875, learning_rate 0.00465221
2017-10-10T12:42:50.329916: step 20, loss 5.37366, acc 0.484375, learning_rate 0.00463363
2017-10-10T12:42:50.824857: step 21, loss 3.10924, acc 0.609375, learning_rate 0.00461513
2017-10-10T12:42:51.271823: step 22, loss 4.74118, acc 0.359375, learning_rate 0.0045967
2017-10-10T12:42:51.549515: step 23, loss 3.78193, acc 0.453125, learning_rate 0.00457834
2017-10-10T12:42:51.880850: step 24, loss 5.5064, acc 0.359375, learning_rate 0.00456006
2017-10-10T12:42:52.188861: step 25, loss 4.92511, acc 0.40625, learning_rate 0.00454186
2017-10-10T12:42:52.597934: step 26, loss 4.58195, acc 0.484375, learning_rate 0.00452373
2017-10-10T12:42:53.018004: step 27, loss 5.1431, acc 0.4375, learning_rate 0.00450567
2017-10-10T12:42:53.448379: step 28, loss 3.54138, acc 0.53125, learning_rate 0.00448769
2017-10-10T12:42:53.886144: step 29, loss 4.14705, acc 0.515625, learning_rate 0.00446978
2017-10-10T12:42:54.316839: step 30, loss 3.97059, acc 0.5625, learning_rate 0.00445194
2017-10-10T12:42:54.727404: step 31, loss 3.34351, acc 0.546875, learning_rate 0.00443418
2017-10-10T12:42:55.151678: step 32, loss 4.68811, acc 0.46875, learning_rate 0.00441649
2017-10-10T12:42:55.568876: step 33, loss 4.15711, acc 0.515625, learning_rate 0.00439887
2017-10-10T12:42:56.004867: step 34, loss 2.81418, acc 0.578125, learning_rate 0.00438132
2017-10-10T12:42:56.496876: step 35, loss 3.67227, acc 0.53125, learning_rate 0.00436385
2017-10-10T12:42:56.879466: step 36, loss 3.97574, acc 0.40625, learning_rate 0.00434644
2017-10-10T12:42:57.275967: step 37, loss 2.81415, acc 0.546875, learning_rate 0.00432911
2017-10-10T12:42:57.664869: step 38, loss 3.80683, acc 0.46875, learning_rate 0.00431185
2017-10-10T12:42:58.088275: step 39, loss 2.82963, acc 0.53125, learning_rate 0.00429465
2017-10-10T12:42:58.494396: step 40, loss 3.32727, acc 0.609375, learning_rate 0.00427753

Evaluation:
2017-10-10T12:42:59.512901: step 40, loss 0.488984, acc 0.828777

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-40

2017-10-10T12:43:01.061298: step 41, loss 3.23128, acc 0.609375, learning_rate 0.00426048
2017-10-10T12:43:01.432898: step 42, loss 2.40186, acc 0.5625, learning_rate 0.0042435
2017-10-10T12:43:01.755262: step 43, loss 2.93312, acc 0.5625, learning_rate 0.00422659
2017-10-10T12:43:02.052345: step 44, loss 3.24915, acc 0.53125, learning_rate 0.00420974
2017-10-10T12:43:02.369511: step 45, loss 2.83857, acc 0.546875, learning_rate 0.00419297
2017-10-10T12:43:02.813113: step 46, loss 2.94849, acc 0.53125, learning_rate 0.00417626
2017-10-10T12:43:03.209566: step 47, loss 2.28237, acc 0.65625, learning_rate 0.00415962
2017-10-10T12:43:03.628972: step 48, loss 2.77852, acc 0.515625, learning_rate 0.00414305
2017-10-10T12:43:04.069076: step 49, loss 2.39233, acc 0.59375, learning_rate 0.00412655
2017-10-10T12:43:04.493348: step 50, loss 1.96295, acc 0.59375, learning_rate 0.00411011
2017-10-10T12:43:04.907381: step 51, loss 1.95183, acc 0.625, learning_rate 0.00409375
2017-10-10T12:43:05.264938: step 52, loss 2.22212, acc 0.59375, learning_rate 0.00407744
2017-10-10T12:43:05.600933: step 53, loss 2.26614, acc 0.625, learning_rate 0.00406121
2017-10-10T12:43:05.977888: step 54, loss 1.29585, acc 0.703125, learning_rate 0.00404504
2017-10-10T12:43:06.343032: step 55, loss 1.48413, acc 0.78125, learning_rate 0.00402894
2017-10-10T12:43:06.746349: step 56, loss 1.84761, acc 0.6875, learning_rate 0.0040129
2017-10-10T12:43:07.187118: step 57, loss 2.44443, acc 0.59375, learning_rate 0.00399693
2017-10-10T12:43:07.560401: step 58, loss 1.76955, acc 0.671875, learning_rate 0.00398102
2017-10-10T12:43:07.960333: step 59, loss 1.4334, acc 0.65625, learning_rate 0.00396518
2017-10-10T12:43:08.440888: step 60, loss 2.14163, acc 0.53125, learning_rate 0.00394941
2017-10-10T12:43:08.927137: step 61, loss 1.74918, acc 0.609375, learning_rate 0.00393369
2017-10-10T12:43:09.241449: step 62, loss 1.48994, acc 0.703125, learning_rate 0.00391804
2017-10-10T12:43:09.548051: step 63, loss 1.90551, acc 0.609375, learning_rate 0.00390246
2017-10-10T12:43:09.859268: step 64, loss 1.77862, acc 0.6875, learning_rate 0.00388694
2017-10-10T12:43:10.298334: step 65, loss 1.16603, acc 0.75, learning_rate 0.00387148
2017-10-10T12:43:10.720852: step 66, loss 1.4878, acc 0.625, learning_rate 0.00385609
2017-10-10T12:43:11.150862: step 67, loss 0.914382, acc 0.6875, learning_rate 0.00384076
2017-10-10T12:43:11.545568: step 68, loss 1.50916, acc 0.796875, learning_rate 0.00382549
2017-10-10T12:43:11.989236: step 69, loss 1.24377, acc 0.65625, learning_rate 0.00381028
2017-10-10T12:43:12.378333: step 70, loss 2.39174, acc 0.59375, learning_rate 0.00379514
2017-10-10T12:43:12.804960: step 71, loss 1.10561, acc 0.734375, learning_rate 0.00378005
2017-10-10T12:43:13.250854: step 72, loss 0.939436, acc 0.828125, learning_rate 0.00376503
2017-10-10T12:43:13.668064: step 73, loss 1.94804, acc 0.640625, learning_rate 0.00375007
2017-10-10T12:43:14.041789: step 74, loss 0.983205, acc 0.71875, learning_rate 0.00373517
2017-10-10T12:43:14.427077: step 75, loss 1.20613, acc 0.75, learning_rate 0.00372034
2017-10-10T12:43:14.845120: step 76, loss 1.22407, acc 0.6875, learning_rate 0.00370556
2017-10-10T12:43:15.224863: step 77, loss 1.73727, acc 0.6875, learning_rate 0.00369084
2017-10-10T12:43:15.618782: step 78, loss 0.989005, acc 0.75, learning_rate 0.00367619
2017-10-10T12:43:16.012790: step 79, loss 1.25909, acc 0.796875, learning_rate 0.00366159
2017-10-10T12:43:16.408333: step 80, loss 1.68758, acc 0.59375, learning_rate 0.00364705

Evaluation:
2017-10-10T12:43:17.317352: step 80, loss 0.389934, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-80

2017-10-10T12:43:18.825125: step 81, loss 1.39137, acc 0.671875, learning_rate 0.00363257
2017-10-10T12:43:19.103680: step 82, loss 1.28346, acc 0.703125, learning_rate 0.00361815
2017-10-10T12:43:19.400888: step 83, loss 0.950558, acc 0.71875, learning_rate 0.00360379
2017-10-10T12:43:19.710654: step 84, loss 1.20911, acc 0.765625, learning_rate 0.00358949
2017-10-10T12:43:20.048907: step 85, loss 1.17046, acc 0.71875, learning_rate 0.00357525
2017-10-10T12:43:20.515355: step 86, loss 1.84454, acc 0.6875, learning_rate 0.00356106
2017-10-10T12:43:21.031141: step 87, loss 1.46162, acc 0.75, learning_rate 0.00354694
2017-10-10T12:43:21.391247: step 88, loss 1.20822, acc 0.734375, learning_rate 0.00353287
2017-10-10T12:43:21.816848: step 89, loss 0.886879, acc 0.75, learning_rate 0.00351885
2017-10-10T12:43:22.204845: step 90, loss 1.38372, acc 0.75, learning_rate 0.0035049
2017-10-10T12:43:22.588860: step 91, loss 0.974445, acc 0.75, learning_rate 0.003491
2017-10-10T12:43:22.986138: step 92, loss 0.75006, acc 0.765625, learning_rate 0.00347716
2017-10-10T12:43:23.303845: step 93, loss 1.0139, acc 0.8125, learning_rate 0.00346338
2017-10-10T12:43:23.644082: step 94, loss 0.952918, acc 0.765625, learning_rate 0.00344965
2017-10-10T12:43:24.066660: step 95, loss 1.1288, acc 0.734375, learning_rate 0.00343597
2017-10-10T12:43:24.540845: step 96, loss 0.529745, acc 0.828125, learning_rate 0.00342236
2017-10-10T12:43:24.950169: step 97, loss 1.55519, acc 0.734375, learning_rate 0.0034088
2017-10-10T12:43:25.264923: step 98, loss 1.01686, acc 0.745098, learning_rate 0.00339529
2017-10-10T12:43:25.702256: step 99, loss 0.761149, acc 0.796875, learning_rate 0.00338184
2017-10-10T12:43:26.116830: step 100, loss 1.3536, acc 0.734375, learning_rate 0.00336844
2017-10-10T12:43:26.437638: step 101, loss 0.836162, acc 0.765625, learning_rate 0.0033551
2017-10-10T12:43:26.740241: step 102, loss 0.971512, acc 0.671875, learning_rate 0.00334182
2017-10-10T12:43:27.063713: step 103, loss 1.11549, acc 0.71875, learning_rate 0.00332858
2017-10-10T12:43:27.497281: step 104, loss 0.916973, acc 0.8125, learning_rate 0.00331541
2017-10-10T12:43:27.930506: step 105, loss 0.906955, acc 0.765625, learning_rate 0.00330228
2017-10-10T12:43:28.328848: step 106, loss 1.48406, acc 0.6875, learning_rate 0.00328921
2017-10-10T12:43:28.768916: step 107, loss 1.19342, acc 0.71875, learning_rate 0.00327619
2017-10-10T12:43:29.180912: step 108, loss 1.5039, acc 0.75, learning_rate 0.00326323
2017-10-10T12:43:29.554624: step 109, loss 0.780146, acc 0.8125, learning_rate 0.00325032
2017-10-10T12:43:29.956867: step 110, loss 1.18509, acc 0.765625, learning_rate 0.00323746
2017-10-10T12:43:30.387481: step 111, loss 0.847864, acc 0.734375, learning_rate 0.00322465
2017-10-10T12:43:30.766670: step 112, loss 0.911457, acc 0.8125, learning_rate 0.0032119
2017-10-10T12:43:31.165249: step 113, loss 0.502764, acc 0.8125, learning_rate 0.0031992
2017-10-10T12:43:31.585478: step 114, loss 1.18014, acc 0.78125, learning_rate 0.00318655
2017-10-10T12:43:31.951804: step 115, loss 0.707302, acc 0.765625, learning_rate 0.00317395
2017-10-10T12:43:32.344852: step 116, loss 1.98912, acc 0.5625, learning_rate 0.0031614
2017-10-10T12:43:32.763801: step 117, loss 0.862438, acc 0.796875, learning_rate 0.0031489
2017-10-10T12:43:33.143570: step 118, loss 1.11444, acc 0.796875, learning_rate 0.00313646
2017-10-10T12:43:33.548829: step 119, loss 1.15938, acc 0.75, learning_rate 0.00312407
2017-10-10T12:43:33.952692: step 120, loss 1.04285, acc 0.671875, learning_rate 0.00311172

Evaluation:
2017-10-10T12:43:34.889003: step 120, loss 0.35641, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-120

2017-10-10T12:43:36.308865: step 121, loss 0.692961, acc 0.796875, learning_rate 0.00309943
2017-10-10T12:43:36.589722: step 122, loss 0.592428, acc 0.8125, learning_rate 0.00308719
2017-10-10T12:43:36.872851: step 123, loss 0.593711, acc 0.8125, learning_rate 0.00307499
2017-10-10T12:43:37.249209: step 124, loss 0.852534, acc 0.78125, learning_rate 0.00306285
2017-10-10T12:43:37.637621: step 125, loss 0.609161, acc 0.84375, learning_rate 0.00305076
2017-10-10T12:43:38.000929: step 126, loss 0.782669, acc 0.84375, learning_rate 0.00303871
2017-10-10T12:43:38.360874: step 127, loss 1.04741, acc 0.765625, learning_rate 0.00302672
2017-10-10T12:43:38.748675: step 128, loss 0.97955, acc 0.765625, learning_rate 0.00301477
2017-10-10T12:43:39.133903: step 129, loss 0.739411, acc 0.765625, learning_rate 0.00300287
2017-10-10T12:43:39.492995: step 130, loss 0.512598, acc 0.859375, learning_rate 0.00299102
2017-10-10T12:43:39.845781: step 131, loss 0.55589, acc 0.84375, learning_rate 0.00297922
2017-10-10T12:43:40.225010: step 132, loss 0.898471, acc 0.8125, learning_rate 0.00296747
2017-10-10T12:43:40.608882: step 133, loss 1.0204, acc 0.796875, learning_rate 0.00295577
2017-10-10T12:43:41.064891: step 134, loss 0.520601, acc 0.859375, learning_rate 0.00294411
2017-10-10T12:43:41.511906: step 135, loss 0.549162, acc 0.828125, learning_rate 0.0029325
2017-10-10T12:43:41.918133: step 136, loss 0.766969, acc 0.8125, learning_rate 0.00292094
2017-10-10T12:43:42.347238: step 137, loss 0.838923, acc 0.78125, learning_rate 0.00290943
2017-10-10T12:43:42.777228: step 138, loss 1.13229, acc 0.71875, learning_rate 0.00289796
2017-10-10T12:43:43.212538: step 139, loss 0.875131, acc 0.78125, learning_rate 0.00288654
2017-10-10T12:43:43.692408: step 140, loss 0.580787, acc 0.796875, learning_rate 0.00287516
2017-10-10T12:43:44.015711: step 141, loss 1.1294, acc 0.71875, learning_rate 0.00286384
2017-10-10T12:43:44.329366: step 142, loss 0.786175, acc 0.796875, learning_rate 0.00285256
2017-10-10T12:43:44.641385: step 143, loss 0.988261, acc 0.734375, learning_rate 0.00284132
2017-10-10T12:43:45.009203: step 144, loss 0.498379, acc 0.78125, learning_rate 0.00283013
2017-10-10T12:43:45.397275: step 145, loss 0.50697, acc 0.8125, learning_rate 0.00281899
2017-10-10T12:43:45.763879: step 146, loss 1.2191, acc 0.734375, learning_rate 0.00280789
2017-10-10T12:43:46.108879: step 147, loss 0.87172, acc 0.71875, learning_rate 0.00279684
2017-10-10T12:43:46.512835: step 148, loss 0.74696, acc 0.78125, learning_rate 0.00278583
2017-10-10T12:43:46.952320: step 149, loss 0.478516, acc 0.8125, learning_rate 0.00277486
2017-10-10T12:43:47.365080: step 150, loss 0.972776, acc 0.734375, learning_rate 0.00276395
2017-10-10T12:43:47.720223: step 151, loss 1.02199, acc 0.734375, learning_rate 0.00275307
2017-10-10T12:43:48.118331: step 152, loss 0.752974, acc 0.796875, learning_rate 0.00274224
2017-10-10T12:43:48.559744: step 153, loss 0.724266, acc 0.78125, learning_rate 0.00273146
2017-10-10T12:43:48.966590: step 154, loss 0.661317, acc 0.828125, learning_rate 0.00272072
2017-10-10T12:43:49.384562: step 155, loss 0.543913, acc 0.90625, learning_rate 0.00271002
2017-10-10T12:43:49.741188: step 156, loss 1.23421, acc 0.796875, learning_rate 0.00269937
2017-10-10T12:43:50.192832: step 157, loss 0.241306, acc 0.90625, learning_rate 0.00268876
2017-10-10T12:43:50.632385: step 158, loss 0.419519, acc 0.859375, learning_rate 0.00267819
2017-10-10T12:43:51.053444: step 159, loss 1.13968, acc 0.78125, learning_rate 0.00266767
2017-10-10T12:43:51.466209: step 160, loss 0.579763, acc 0.8125, learning_rate 0.00265719

Evaluation:
2017-10-10T12:43:52.368815: step 160, loss 0.368387, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-160

2017-10-10T12:43:53.773465: step 161, loss 0.373238, acc 0.859375, learning_rate 0.00264675
2017-10-10T12:43:54.092099: step 162, loss 0.641055, acc 0.828125, learning_rate 0.00263635
2017-10-10T12:43:54.400303: step 163, loss 0.988551, acc 0.78125, learning_rate 0.002626
2017-10-10T12:43:54.709797: step 164, loss 0.546686, acc 0.84375, learning_rate 0.00261569
2017-10-10T12:43:55.074292: step 165, loss 0.789645, acc 0.796875, learning_rate 0.00260542
2017-10-10T12:43:55.436936: step 166, loss 0.660761, acc 0.859375, learning_rate 0.0025952
2017-10-10T12:43:55.859057: step 167, loss 0.401643, acc 0.890625, learning_rate 0.00258501
2017-10-10T12:43:56.224917: step 168, loss 0.475868, acc 0.84375, learning_rate 0.00257487
2017-10-10T12:43:56.656888: step 169, loss 0.667725, acc 0.828125, learning_rate 0.00256477
2017-10-10T12:43:56.957138: step 170, loss 0.712271, acc 0.796875, learning_rate 0.0025547
2017-10-10T12:43:57.404859: step 171, loss 0.862483, acc 0.828125, learning_rate 0.00254469
2017-10-10T12:43:57.816948: step 172, loss 1.12914, acc 0.703125, learning_rate 0.00253471
2017-10-10T12:43:58.169181: step 173, loss 0.670437, acc 0.78125, learning_rate 0.00252477
2017-10-10T12:43:58.540958: step 174, loss 0.6599, acc 0.828125, learning_rate 0.00251487
2017-10-10T12:43:58.945081: step 175, loss 0.594096, acc 0.796875, learning_rate 0.00250501
2017-10-10T12:43:59.329190: step 176, loss 0.51262, acc 0.84375, learning_rate 0.0024952
2017-10-10T12:43:59.705025: step 177, loss 0.60951, acc 0.828125, learning_rate 0.00248542
2017-10-10T12:44:00.121080: step 178, loss 0.773473, acc 0.796875, learning_rate 0.00247568
2017-10-10T12:44:00.441590: step 179, loss 0.639201, acc 0.8125, learning_rate 0.00246599
2017-10-10T12:44:00.913311: step 180, loss 0.8895, acc 0.78125, learning_rate 0.00245633
2017-10-10T12:44:01.320902: step 181, loss 0.964399, acc 0.78125, learning_rate 0.00244671
2017-10-10T12:44:01.621652: step 182, loss 0.866884, acc 0.8125, learning_rate 0.00243713
2017-10-10T12:44:01.884173: step 183, loss 0.585885, acc 0.8125, learning_rate 0.00242759
2017-10-10T12:44:02.180799: step 184, loss 0.855661, acc 0.78125, learning_rate 0.00241809
2017-10-10T12:44:02.605820: step 185, loss 0.576968, acc 0.796875, learning_rate 0.00240863
2017-10-10T12:44:03.031684: step 186, loss 0.411277, acc 0.84375, learning_rate 0.00239921
2017-10-10T12:44:03.467348: step 187, loss 0.553177, acc 0.8125, learning_rate 0.00238982
2017-10-10T12:44:03.887793: step 188, loss 0.519258, acc 0.8125, learning_rate 0.00238048
2017-10-10T12:44:04.284877: step 189, loss 0.509647, acc 0.84375, learning_rate 0.00237117
2017-10-10T12:44:04.686581: step 190, loss 0.702144, acc 0.8125, learning_rate 0.0023619
2017-10-10T12:44:05.108890: step 191, loss 1.0457, acc 0.765625, learning_rate 0.00235267
2017-10-10T12:44:05.504185: step 192, loss 0.638054, acc 0.828125, learning_rate 0.00234347
2017-10-10T12:44:05.885209: step 193, loss 0.643287, acc 0.796875, learning_rate 0.00233431
2017-10-10T12:44:06.307100: step 194, loss 0.456051, acc 0.875, learning_rate 0.00232519
2017-10-10T12:44:06.783755: step 195, loss 0.619927, acc 0.84375, learning_rate 0.00231611
2017-10-10T12:44:07.159137: step 196, loss 0.675143, acc 0.803922, learning_rate 0.00230707
2017-10-10T12:44:07.582140: step 197, loss 0.512552, acc 0.796875, learning_rate 0.00229806
2017-10-10T12:44:08.009103: step 198, loss 0.514205, acc 0.8125, learning_rate 0.00228908
2017-10-10T12:44:08.415831: step 199, loss 0.477005, acc 0.875, learning_rate 0.00228015
2017-10-10T12:44:08.812390: step 200, loss 0.558943, acc 0.78125, learning_rate 0.00227125

Evaluation:
2017-10-10T12:44:09.716857: step 200, loss 0.33649, acc 0.876259

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-200

2017-10-10T12:44:11.152425: step 201, loss 0.643598, acc 0.796875, learning_rate 0.00226239
2017-10-10T12:44:11.612830: step 202, loss 0.550241, acc 0.84375, learning_rate 0.00225356
2017-10-10T12:44:11.978276: step 203, loss 0.557121, acc 0.8125, learning_rate 0.00224477
2017-10-10T12:44:12.293668: step 204, loss 0.7572, acc 0.78125, learning_rate 0.00223602
2017-10-10T12:44:12.602247: step 205, loss 0.41721, acc 0.8125, learning_rate 0.0022273
2017-10-10T12:44:12.919126: step 206, loss 0.743383, acc 0.78125, learning_rate 0.00221862
2017-10-10T12:44:13.331027: step 207, loss 1.02674, acc 0.796875, learning_rate 0.00220997
2017-10-10T12:44:13.736986: step 208, loss 0.311952, acc 0.90625, learning_rate 0.00220136
2017-10-10T12:44:14.149356: step 209, loss 0.258427, acc 0.875, learning_rate 0.00219278
2017-10-10T12:44:14.567498: step 210, loss 0.550834, acc 0.8125, learning_rate 0.00218424
2017-10-10T12:44:15.008827: step 211, loss 0.564938, acc 0.84375, learning_rate 0.00217573
2017-10-10T12:44:15.433166: step 212, loss 0.281724, acc 0.875, learning_rate 0.00216726
2017-10-10T12:44:15.864297: step 213, loss 0.395759, acc 0.875, learning_rate 0.00215882
2017-10-10T12:44:16.343464: step 214, loss 0.797612, acc 0.765625, learning_rate 0.00215041
2017-10-10T12:44:16.734457: step 215, loss 0.360191, acc 0.84375, learning_rate 0.00214204
2017-10-10T12:44:17.150846: step 216, loss 0.319954, acc 0.859375, learning_rate 0.00213371
2017-10-10T12:44:17.555791: step 217, loss 0.574865, acc 0.78125, learning_rate 0.00212541
2017-10-10T12:44:17.994560: step 218, loss 0.678653, acc 0.796875, learning_rate 0.00211714
2017-10-10T12:44:18.419261: step 219, loss 0.801341, acc 0.796875, learning_rate 0.00210891
2017-10-10T12:44:18.961839: step 220, loss 0.851465, acc 0.71875, learning_rate 0.00210071
2017-10-10T12:44:19.398833: step 221, loss 0.501871, acc 0.828125, learning_rate 0.00209254
2017-10-10T12:44:19.670176: step 222, loss 0.381957, acc 0.875, learning_rate 0.00208441
2017-10-10T12:44:19.988853: step 223, loss 0.414707, acc 0.859375, learning_rate 0.00207631
2017-10-10T12:44:20.289335: step 224, loss 0.659203, acc 0.84375, learning_rate 0.00206824
2017-10-10T12:44:20.698848: step 225, loss 0.355896, acc 0.890625, learning_rate 0.00206021
2017-10-10T12:44:21.084266: step 226, loss 0.265216, acc 0.90625, learning_rate 0.00205221
2017-10-10T12:44:21.456830: step 227, loss 0.747729, acc 0.84375, learning_rate 0.00204424
2017-10-10T12:44:21.880173: step 228, loss 0.678552, acc 0.796875, learning_rate 0.0020363
2017-10-10T12:44:22.305831: step 229, loss 0.358926, acc 0.859375, learning_rate 0.0020284
2017-10-10T12:44:22.721616: step 230, loss 0.409024, acc 0.875, learning_rate 0.00202053
2017-10-10T12:44:23.125706: step 231, loss 0.501091, acc 0.796875, learning_rate 0.00201269
2017-10-10T12:44:23.552422: step 232, loss 0.672448, acc 0.8125, learning_rate 0.00200488
2017-10-10T12:44:23.994698: step 233, loss 0.553118, acc 0.8125, learning_rate 0.00199711
2017-10-10T12:44:24.422711: step 234, loss 0.392395, acc 0.859375, learning_rate 0.00198936
2017-10-10T12:44:24.808235: step 235, loss 0.605055, acc 0.828125, learning_rate 0.00198165
2017-10-10T12:44:25.235836: step 236, loss 0.345215, acc 0.875, learning_rate 0.00197397
2017-10-10T12:44:25.608730: step 237, loss 0.933231, acc 0.671875, learning_rate 0.00196632
2017-10-10T12:44:25.938471: step 238, loss 0.667221, acc 0.828125, learning_rate 0.0019587
2017-10-10T12:44:26.317657: step 239, loss 0.696037, acc 0.8125, learning_rate 0.00195112
2017-10-10T12:44:26.722525: step 240, loss 0.728064, acc 0.8125, learning_rate 0.00194356

Evaluation:
2017-10-10T12:44:27.737123: step 240, loss 0.305615, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-240

2017-10-10T12:44:29.217014: step 241, loss 0.394828, acc 0.828125, learning_rate 0.00193604
2017-10-10T12:44:29.544855: step 242, loss 0.406743, acc 0.84375, learning_rate 0.00192854
2017-10-10T12:44:29.860930: step 243, loss 0.571521, acc 0.765625, learning_rate 0.00192108
2017-10-10T12:44:30.164487: step 244, loss 0.245744, acc 0.890625, learning_rate 0.00191364
2017-10-10T12:44:30.554536: step 245, loss 0.488813, acc 0.859375, learning_rate 0.00190624
2017-10-10T12:44:30.922668: step 246, loss 0.232127, acc 0.921875, learning_rate 0.00189887
2017-10-10T12:44:31.315458: step 247, loss 0.825029, acc 0.828125, learning_rate 0.00189153
2017-10-10T12:44:31.736253: step 248, loss 0.357354, acc 0.859375, learning_rate 0.00188421
2017-10-10T12:44:32.156894: step 249, loss 0.667814, acc 0.796875, learning_rate 0.00187693
2017-10-10T12:44:32.560945: step 250, loss 0.415762, acc 0.890625, learning_rate 0.00186968
2017-10-10T12:44:32.977008: step 251, loss 0.454439, acc 0.859375, learning_rate 0.00186245
2017-10-10T12:44:33.420675: step 252, loss 0.343135, acc 0.859375, learning_rate 0.00185526
2017-10-10T12:44:33.807007: step 253, loss 0.56604, acc 0.8125, learning_rate 0.0018481
2017-10-10T12:44:34.251174: step 254, loss 0.603909, acc 0.78125, learning_rate 0.00184096
2017-10-10T12:44:34.656827: step 255, loss 0.340109, acc 0.828125, learning_rate 0.00183385
2017-10-10T12:44:35.058317: step 256, loss 0.669575, acc 0.734375, learning_rate 0.00182678
2017-10-10T12:44:35.486910: step 257, loss 0.586868, acc 0.828125, learning_rate 0.00181973
2017-10-10T12:44:35.959805: step 258, loss 0.344587, acc 0.859375, learning_rate 0.00181271
2017-10-10T12:44:36.449921: step 259, loss 0.349896, acc 0.921875, learning_rate 0.00180572
2017-10-10T12:44:36.958230: step 260, loss 0.332238, acc 0.921875, learning_rate 0.00179876
2017-10-10T12:44:37.279861: step 261, loss 0.880185, acc 0.796875, learning_rate 0.00179182
2017-10-10T12:44:37.601974: step 262, loss 0.617624, acc 0.796875, learning_rate 0.00178492
2017-10-10T12:44:37.905353: step 263, loss 0.690228, acc 0.84375, learning_rate 0.00177804
2017-10-10T12:44:38.295285: step 264, loss 0.471469, acc 0.875, learning_rate 0.00177119
2017-10-10T12:44:38.716828: step 265, loss 0.512262, acc 0.859375, learning_rate 0.00176437
2017-10-10T12:44:39.133783: step 266, loss 0.235728, acc 0.90625, learning_rate 0.00175758
2017-10-10T12:44:39.542423: step 267, loss 0.585434, acc 0.8125, learning_rate 0.00175081
2017-10-10T12:44:39.904843: step 268, loss 0.407613, acc 0.875, learning_rate 0.00174407
2017-10-10T12:44:40.316927: step 269, loss 0.884728, acc 0.703125, learning_rate 0.00173736
2017-10-10T12:44:40.649490: step 270, loss 0.317675, acc 0.875, learning_rate 0.00173068
2017-10-10T12:44:41.028296: step 271, loss 0.237504, acc 0.90625, learning_rate 0.00172402
2017-10-10T12:44:41.428076: step 272, loss 0.543155, acc 0.78125, learning_rate 0.00171739
2017-10-10T12:44:41.788958: step 273, loss 0.270509, acc 0.890625, learning_rate 0.00171079
2017-10-10T12:44:42.216190: step 274, loss 0.764258, acc 0.796875, learning_rate 0.00170422
2017-10-10T12:44:42.632342: step 275, loss 0.573642, acc 0.828125, learning_rate 0.00169767
2017-10-10T12:44:43.048603: step 276, loss 0.461097, acc 0.859375, learning_rate 0.00169115
2017-10-10T12:44:43.479919: step 277, loss 0.782367, acc 0.796875, learning_rate 0.00168465
2017-10-10T12:44:43.889642: step 278, loss 0.465642, acc 0.84375, learning_rate 0.00167818
2017-10-10T12:44:44.336927: step 279, loss 0.728825, acc 0.796875, learning_rate 0.00167174
2017-10-10T12:44:44.728529: step 280, loss 0.352858, acc 0.859375, learning_rate 0.00166533

Evaluation:
2017-10-10T12:44:45.663544: step 280, loss 0.294121, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-280

2017-10-10T12:44:47.149131: step 281, loss 0.547622, acc 0.796875, learning_rate 0.00165894
2017-10-10T12:44:47.560268: step 282, loss 0.640112, acc 0.75, learning_rate 0.00165257
2017-10-10T12:44:47.845357: step 283, loss 0.357901, acc 0.875, learning_rate 0.00164624
2017-10-10T12:44:48.127072: step 284, loss 0.620302, acc 0.78125, learning_rate 0.00163993
2017-10-10T12:44:48.374581: step 285, loss 0.530519, acc 0.75, learning_rate 0.00163364
2017-10-10T12:44:48.758357: step 286, loss 0.293745, acc 0.875, learning_rate 0.00162738
2017-10-10T12:44:49.216601: step 287, loss 0.341284, acc 0.921875, learning_rate 0.00162115
2017-10-10T12:44:49.678397: step 288, loss 0.51529, acc 0.765625, learning_rate 0.00161494
2017-10-10T12:44:50.112520: step 289, loss 0.43949, acc 0.828125, learning_rate 0.00160875
2017-10-10T12:44:50.534528: step 290, loss 0.645454, acc 0.78125, learning_rate 0.00160259
2017-10-10T12:44:50.952835: step 291, loss 0.308042, acc 0.921875, learning_rate 0.00159646
2017-10-10T12:44:51.388063: step 292, loss 0.4211, acc 0.890625, learning_rate 0.00159035
2017-10-10T12:44:51.791811: step 293, loss 0.344418, acc 0.90625, learning_rate 0.00158427
2017-10-10T12:44:52.183484: step 294, loss 0.34703, acc 0.862745, learning_rate 0.00157821
2017-10-10T12:44:52.590915: step 295, loss 0.226859, acc 0.90625, learning_rate 0.00157218
2017-10-10T12:44:53.009971: step 296, loss 0.452603, acc 0.859375, learning_rate 0.00156617
2017-10-10T12:44:53.448820: step 297, loss 0.456146, acc 0.859375, learning_rate 0.00156018
2017-10-10T12:44:53.896820: step 298, loss 0.514821, acc 0.84375, learning_rate 0.00155422
2017-10-10T12:44:54.352394: step 299, loss 0.616799, acc 0.796875, learning_rate 0.00154829
2017-10-10T12:44:54.876880: step 300, loss 0.460003, acc 0.875, learning_rate 0.00154238
2017-10-10T12:44:55.393096: step 301, loss 0.564797, acc 0.828125, learning_rate 0.00153649
2017-10-10T12:44:55.768975: step 302, loss 0.460766, acc 0.859375, learning_rate 0.00153063
2017-10-10T12:44:56.160829: step 303, loss 0.32921, acc 0.90625, learning_rate 0.00152479
2017-10-10T12:44:56.545970: step 304, loss 0.383294, acc 0.90625, learning_rate 0.00151897
2017-10-10T12:44:56.988053: step 305, loss 0.293688, acc 0.90625, learning_rate 0.00151318
2017-10-10T12:44:57.412902: step 306, loss 0.731201, acc 0.765625, learning_rate 0.00150741
2017-10-10T12:44:57.801637: step 307, loss 0.486125, acc 0.828125, learning_rate 0.00150167
2017-10-10T12:44:58.248611: step 308, loss 0.344891, acc 0.875, learning_rate 0.00149594
2017-10-10T12:44:58.681619: step 309, loss 0.394483, acc 0.84375, learning_rate 0.00149025
2017-10-10T12:44:59.089494: step 310, loss 0.284524, acc 0.890625, learning_rate 0.00148457
2017-10-10T12:44:59.487419: step 311, loss 0.380322, acc 0.859375, learning_rate 0.00147892
2017-10-10T12:44:59.972850: step 312, loss 0.270875, acc 0.90625, learning_rate 0.00147329
2017-10-10T12:45:00.377981: step 313, loss 0.527506, acc 0.828125, learning_rate 0.00146769
2017-10-10T12:45:00.768894: step 314, loss 0.61159, acc 0.8125, learning_rate 0.0014621
2017-10-10T12:45:01.200990: step 315, loss 0.382927, acc 0.921875, learning_rate 0.00145654
2017-10-10T12:45:01.578782: step 316, loss 0.600659, acc 0.84375, learning_rate 0.00145101
2017-10-10T12:45:01.977574: step 317, loss 0.49387, acc 0.828125, learning_rate 0.00144549
2017-10-10T12:45:02.348532: step 318, loss 0.677091, acc 0.796875, learning_rate 0.00144
2017-10-10T12:45:02.799086: step 319, loss 0.514979, acc 0.828125, learning_rate 0.00143453
2017-10-10T12:45:03.253225: step 320, loss 0.400507, acc 0.84375, learning_rate 0.00142908

Evaluation:
2017-10-10T12:45:04.170350: step 320, loss 0.286908, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-320

2017-10-10T12:45:05.568995: step 321, loss 0.642974, acc 0.796875, learning_rate 0.00142366
2017-10-10T12:45:06.068975: step 322, loss 0.337961, acc 0.859375, learning_rate 0.00141826
2017-10-10T12:45:06.411697: step 323, loss 0.447727, acc 0.859375, learning_rate 0.00141288
2017-10-10T12:45:06.739833: step 324, loss 0.659835, acc 0.828125, learning_rate 0.00140752
2017-10-10T12:45:07.041220: step 325, loss 0.768921, acc 0.796875, learning_rate 0.00140218
2017-10-10T12:45:07.408105: step 326, loss 0.484428, acc 0.828125, learning_rate 0.00139686
2017-10-10T12:45:07.792775: step 327, loss 0.347707, acc 0.875, learning_rate 0.00139157
2017-10-10T12:45:08.230261: step 328, loss 0.559986, acc 0.8125, learning_rate 0.0013863
2017-10-10T12:45:08.656850: step 329, loss 0.399615, acc 0.90625, learning_rate 0.00138105
2017-10-10T12:45:09.064843: step 330, loss 0.509537, acc 0.828125, learning_rate 0.00137582
2017-10-10T12:45:09.428853: step 331, loss 0.509631, acc 0.796875, learning_rate 0.00137061
2017-10-10T12:45:09.781044: step 332, loss 0.41997, acc 0.875, learning_rate 0.00136543
2017-10-10T12:45:10.204868: step 333, loss 0.532023, acc 0.859375, learning_rate 0.00136026
2017-10-10T12:45:10.652996: step 334, loss 0.554022, acc 0.8125, learning_rate 0.00135512
2017-10-10T12:45:11.083911: step 335, loss 0.291812, acc 0.890625, learning_rate 0.00134999
2017-10-10T12:45:11.436297: step 336, loss 0.736414, acc 0.8125, learning_rate 0.00134489
2017-10-10T12:45:11.814704: step 337, loss 0.711331, acc 0.734375, learning_rate 0.00133981
2017-10-10T12:45:12.247804: step 338, loss 0.70387, acc 0.8125, learning_rate 0.00133475
2017-10-10T12:45:12.629322: step 339, loss 0.424602, acc 0.8125, learning_rate 0.00132971
2017-10-10T12:45:13.142723: step 340, loss 0.631191, acc 0.84375, learning_rate 0.00132469
2017-10-10T12:45:13.657161: step 341, loss 0.337024, acc 0.875, learning_rate 0.00131969
2017-10-10T12:45:14.010954: step 342, loss 0.484045, acc 0.890625, learning_rate 0.00131471
2017-10-10T12:45:14.300837: step 343, loss 0.363448, acc 0.859375, learning_rate 0.00130975
2017-10-10T12:45:14.622725: step 344, loss 0.371309, acc 0.875, learning_rate 0.00130482
2017-10-10T12:45:15.082513: step 345, loss 0.381095, acc 0.875, learning_rate 0.0012999
2017-10-10T12:45:15.514674: step 346, loss 0.467784, acc 0.796875, learning_rate 0.001295
2017-10-10T12:45:15.910804: step 347, loss 0.47103, acc 0.84375, learning_rate 0.00129012
2017-10-10T12:45:16.362423: step 348, loss 0.357611, acc 0.890625, learning_rate 0.00128527
2017-10-10T12:45:16.826662: step 349, loss 0.392911, acc 0.890625, learning_rate 0.00128043
2017-10-10T12:45:17.273199: step 350, loss 0.350798, acc 0.875, learning_rate 0.00127561
2017-10-10T12:45:17.732844: step 351, loss 0.288572, acc 0.875, learning_rate 0.00127081
2017-10-10T12:45:18.184855: step 352, loss 0.288239, acc 0.84375, learning_rate 0.00126603
2017-10-10T12:45:18.653029: step 353, loss 0.684, acc 0.765625, learning_rate 0.00126127
2017-10-10T12:45:19.141019: step 354, loss 0.533982, acc 0.8125, learning_rate 0.00125653
2017-10-10T12:45:19.613036: step 355, loss 0.243541, acc 0.9375, learning_rate 0.00125181
2017-10-10T12:45:20.040966: step 356, loss 0.312874, acc 0.859375, learning_rate 0.00124711
2017-10-10T12:45:20.480934: step 357, loss 0.359129, acc 0.84375, learning_rate 0.00124243
2017-10-10T12:45:20.955090: step 358, loss 0.410821, acc 0.828125, learning_rate 0.00123777
2017-10-10T12:45:21.325487: step 359, loss 0.430501, acc 0.84375, learning_rate 0.00123312
2017-10-10T12:45:21.738125: step 360, loss 0.363547, acc 0.875, learning_rate 0.0012285

Evaluation:
2017-10-10T12:45:22.697733: step 360, loss 0.279126, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-360

2017-10-10T12:45:24.084962: step 361, loss 0.313794, acc 0.84375, learning_rate 0.00122389
2017-10-10T12:45:24.562303: step 362, loss 0.510096, acc 0.8125, learning_rate 0.0012193
2017-10-10T12:45:25.004154: step 363, loss 0.422185, acc 0.828125, learning_rate 0.00121473
2017-10-10T12:45:25.341184: step 364, loss 0.366848, acc 0.890625, learning_rate 0.00121018
2017-10-10T12:45:25.665793: step 365, loss 0.4868, acc 0.828125, learning_rate 0.00120565
2017-10-10T12:45:26.119689: step 366, loss 0.559499, acc 0.84375, learning_rate 0.00120114
2017-10-10T12:45:26.665937: step 367, loss 0.429297, acc 0.8125, learning_rate 0.00119664
2017-10-10T12:45:27.221384: step 368, loss 0.333735, acc 0.859375, learning_rate 0.00119217
2017-10-10T12:45:27.747173: step 369, loss 0.47651, acc 0.828125, learning_rate 0.00118771
2017-10-10T12:45:28.286855: step 370, loss 0.443269, acc 0.890625, learning_rate 0.00118327
2017-10-10T12:45:28.820885: step 371, loss 0.220127, acc 0.90625, learning_rate 0.00117885
2017-10-10T12:45:29.377887: step 372, loss 0.444871, acc 0.828125, learning_rate 0.00117445
2017-10-10T12:45:29.896947: step 373, loss 0.305953, acc 0.890625, learning_rate 0.00117006
2017-10-10T12:45:30.337160: step 374, loss 0.457478, acc 0.875, learning_rate 0.00116569
2017-10-10T12:45:30.836934: step 375, loss 0.49936, acc 0.859375, learning_rate 0.00116134
2017-10-10T12:45:31.396673: step 376, loss 0.470219, acc 0.875, learning_rate 0.00115701
2017-10-10T12:45:32.017183: step 377, loss 0.52533, acc 0.796875, learning_rate 0.0011527
2017-10-10T12:45:32.557169: step 378, loss 0.277715, acc 0.875, learning_rate 0.0011484
2017-10-10T12:45:33.036386: step 379, loss 0.198596, acc 0.90625, learning_rate 0.00114412
2017-10-10T12:45:33.607192: step 380, loss 0.306548, acc 0.921875, learning_rate 0.00113986
2017-10-10T12:45:34.140938: step 381, loss 0.483241, acc 0.84375, learning_rate 0.00113561
2017-10-10T12:45:34.533137: step 382, loss 0.388305, acc 0.84375, learning_rate 0.00113139
2017-10-10T12:45:34.959775: step 383, loss 0.352123, acc 0.90625, learning_rate 0.00112718
2017-10-10T12:45:35.372844: step 384, loss 0.194635, acc 0.96875, learning_rate 0.00112298
2017-10-10T12:45:35.780929: step 385, loss 0.337838, acc 0.875, learning_rate 0.00111881
2017-10-10T12:45:36.171424: step 386, loss 0.403777, acc 0.90625, learning_rate 0.00111465
2017-10-10T12:45:36.733093: step 387, loss 0.196338, acc 0.90625, learning_rate 0.00111051
2017-10-10T12:45:37.337027: step 388, loss 0.426387, acc 0.859375, learning_rate 0.00110638
2017-10-10T12:45:37.817153: step 389, loss 0.436594, acc 0.875, learning_rate 0.00110228
2017-10-10T12:45:38.400930: step 390, loss 0.385348, acc 0.875, learning_rate 0.00109818
2017-10-10T12:45:38.837124: step 391, loss 0.33268, acc 0.921875, learning_rate 0.00109411
2017-10-10T12:45:39.300276: step 392, loss 0.609062, acc 0.784314, learning_rate 0.00109005
2017-10-10T12:45:39.825205: step 393, loss 0.356015, acc 0.890625, learning_rate 0.00108601
2017-10-10T12:45:40.341996: step 394, loss 0.469998, acc 0.875, learning_rate 0.00108199
2017-10-10T12:45:40.915176: step 395, loss 0.333936, acc 0.84375, learning_rate 0.00107798
2017-10-10T12:45:41.423588: step 396, loss 0.312674, acc 0.84375, learning_rate 0.00107399
2017-10-10T12:45:42.005826: step 397, loss 0.485899, acc 0.90625, learning_rate 0.00107001
2017-10-10T12:45:42.528261: step 398, loss 0.321246, acc 0.875, learning_rate 0.00106605
2017-10-10T12:45:43.100783: step 399, loss 0.440371, acc 0.84375, learning_rate 0.00106211
2017-10-10T12:45:43.593395: step 400, loss 0.483223, acc 0.875, learning_rate 0.00105818

Evaluation:
2017-10-10T12:45:44.800854: step 400, loss 0.27575, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-400

2017-10-10T12:45:46.221168: step 401, loss 0.301552, acc 0.90625, learning_rate 0.00105427
2017-10-10T12:45:46.816963: step 402, loss 0.405689, acc 0.875, learning_rate 0.00105037
2017-10-10T12:45:47.341956: step 403, loss 0.307569, acc 0.9375, learning_rate 0.0010465
2017-10-10T12:45:47.858141: step 404, loss 0.405207, acc 0.84375, learning_rate 0.00104263
2017-10-10T12:45:48.291362: step 405, loss 0.556945, acc 0.84375, learning_rate 0.00103878
2017-10-10T12:45:48.750009: step 406, loss 0.446329, acc 0.84375, learning_rate 0.00103495
2017-10-10T12:45:49.161532: step 407, loss 0.502901, acc 0.78125, learning_rate 0.00103114
2017-10-10T12:45:49.816387: step 408, loss 0.182778, acc 0.921875, learning_rate 0.00102734
2017-10-10T12:45:50.248009: step 409, loss 0.396086, acc 0.84375, learning_rate 0.00102355
2017-10-10T12:45:50.648872: step 410, loss 0.510764, acc 0.8125, learning_rate 0.00101978
2017-10-10T12:45:51.049477: step 411, loss 0.421685, acc 0.859375, learning_rate 0.00101603
2017-10-10T12:45:51.457048: step 412, loss 0.429039, acc 0.84375, learning_rate 0.00101229
2017-10-10T12:45:51.846698: step 413, loss 0.373405, acc 0.859375, learning_rate 0.00100856
2017-10-10T12:45:52.343733: step 414, loss 0.290751, acc 0.875, learning_rate 0.00100486
2017-10-10T12:45:52.893149: step 415, loss 0.2538, acc 0.9375, learning_rate 0.00100116
2017-10-10T12:45:53.397176: step 416, loss 0.356562, acc 0.859375, learning_rate 0.000997483
2017-10-10T12:45:53.933140: step 417, loss 0.318471, acc 0.90625, learning_rate 0.00099382
2017-10-10T12:45:54.530334: step 418, loss 0.451505, acc 0.8125, learning_rate 0.000990172
2017-10-10T12:45:54.987191: step 419, loss 0.231346, acc 0.90625, learning_rate 0.000986538
2017-10-10T12:45:55.463461: step 420, loss 0.316101, acc 0.859375, learning_rate 0.00098292
2017-10-10T12:45:55.961078: step 421, loss 0.311899, acc 0.90625, learning_rate 0.000979316
2017-10-10T12:45:56.502620: step 422, loss 0.367884, acc 0.859375, learning_rate 0.000975727
2017-10-10T12:45:57.073040: step 423, loss 0.420437, acc 0.875, learning_rate 0.000972152
2017-10-10T12:45:57.661883: step 424, loss 0.362082, acc 0.90625, learning_rate 0.000968592
2017-10-10T12:45:58.099988: step 425, loss 0.50571, acc 0.828125, learning_rate 0.000965047
2017-10-10T12:45:58.522826: step 426, loss 0.272107, acc 0.9375, learning_rate 0.000961516
2017-10-10T12:45:59.049000: step 427, loss 0.324184, acc 0.890625, learning_rate 0.000958
2017-10-10T12:45:59.545049: step 428, loss 0.431426, acc 0.875, learning_rate 0.000954497
2017-10-10T12:46:00.058036: step 429, loss 0.462473, acc 0.84375, learning_rate 0.00095101
2017-10-10T12:46:00.628620: step 430, loss 0.403213, acc 0.859375, learning_rate 0.000947536
2017-10-10T12:46:01.175995: step 431, loss 0.379177, acc 0.90625, learning_rate 0.000944076
2017-10-10T12:46:01.768782: step 432, loss 0.44946, acc 0.859375, learning_rate 0.000940631
2017-10-10T12:46:02.334643: step 433, loss 0.387017, acc 0.90625, learning_rate 0.0009372
2017-10-10T12:46:02.908928: step 434, loss 0.585273, acc 0.859375, learning_rate 0.000933783
2017-10-10T12:46:03.431200: step 435, loss 0.311033, acc 0.890625, learning_rate 0.000930379
2017-10-10T12:46:03.956941: step 436, loss 0.302919, acc 0.90625, learning_rate 0.00092699
2017-10-10T12:46:04.520869: step 437, loss 0.235343, acc 0.875, learning_rate 0.000923614
2017-10-10T12:46:05.012847: step 438, loss 0.249238, acc 0.875, learning_rate 0.000920253
2017-10-10T12:46:05.520872: step 439, loss 0.304572, acc 0.90625, learning_rate 0.000916905
2017-10-10T12:46:06.072855: step 440, loss 0.487371, acc 0.796875, learning_rate 0.00091357

Evaluation:
2017-10-10T12:46:07.656982: step 440, loss 0.271534, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-440

2017-10-10T12:46:09.457234: step 441, loss 0.27652, acc 0.890625, learning_rate 0.000910249
2017-10-10T12:46:09.979945: step 442, loss 0.382002, acc 0.859375, learning_rate 0.000906942
2017-10-10T12:46:10.504828: step 443, loss 0.423529, acc 0.859375, learning_rate 0.000903648
2017-10-10T12:46:10.954598: step 444, loss 0.503992, acc 0.828125, learning_rate 0.000900368
2017-10-10T12:46:11.416746: step 445, loss 0.551835, acc 0.859375, learning_rate 0.000897101
2017-10-10T12:46:11.940923: step 446, loss 0.473191, acc 0.890625, learning_rate 0.000893848
2017-10-10T12:46:12.512843: step 447, loss 0.289287, acc 0.90625, learning_rate 0.000890607
2017-10-10T12:46:13.148960: step 448, loss 0.561755, acc 0.828125, learning_rate 0.00088738
2017-10-10T12:46:13.855095: step 449, loss 0.383688, acc 0.859375, learning_rate 0.000884166
2017-10-10T12:46:14.360593: step 450, loss 0.38044, acc 0.875, learning_rate 0.000880966
2017-10-10T12:46:14.787263: step 451, loss 0.258047, acc 0.90625, learning_rate 0.000877778
2017-10-10T12:46:15.344624: step 452, loss 0.230002, acc 0.921875, learning_rate 0.000874603
2017-10-10T12:46:15.860838: step 453, loss 0.452447, acc 0.859375, learning_rate 0.000871441
2017-10-10T12:46:16.417347: step 454, loss 0.28674, acc 0.921875, learning_rate 0.000868293
2017-10-10T12:46:16.935495: step 455, loss 0.578013, acc 0.78125, learning_rate 0.000865157
2017-10-10T12:46:17.408876: step 456, loss 0.339388, acc 0.890625, learning_rate 0.000862033
2017-10-10T12:46:17.992973: step 457, loss 0.236733, acc 0.9375, learning_rate 0.000858923
2017-10-10T12:46:18.543732: step 458, loss 0.438834, acc 0.828125, learning_rate 0.000855825
2017-10-10T12:46:19.105060: step 459, loss 0.267903, acc 0.875, learning_rate 0.00085274
2017-10-10T12:46:19.648919: step 460, loss 0.2163, acc 0.9375, learning_rate 0.000849668
2017-10-10T12:46:20.269267: step 461, loss 0.355707, acc 0.90625, learning_rate 0.000846608
2017-10-10T12:46:20.654116: step 462, loss 0.448185, acc 0.859375, learning_rate 0.00084356
2017-10-10T12:46:20.961486: step 463, loss 0.72582, acc 0.765625, learning_rate 0.000840525
2017-10-10T12:46:21.156887: step 464, loss 0.387409, acc 0.90625, learning_rate 0.000837502
2017-10-10T12:46:21.601352: step 465, loss 0.352702, acc 0.859375, learning_rate 0.000834492
2017-10-10T12:46:22.054285: step 466, loss 0.358513, acc 0.84375, learning_rate 0.000831494
2017-10-10T12:46:22.616793: step 467, loss 0.341302, acc 0.828125, learning_rate 0.000828508
2017-10-10T12:46:23.120853: step 468, loss 0.43193, acc 0.84375, learning_rate 0.000825535
2017-10-10T12:46:23.630935: step 469, loss 0.371141, acc 0.890625, learning_rate 0.000822573
2017-10-10T12:46:24.148951: step 470, loss 0.254344, acc 0.90625, learning_rate 0.000819624
2017-10-10T12:46:24.651446: step 471, loss 0.281718, acc 0.921875, learning_rate 0.000816687
2017-10-10T12:46:25.084006: step 472, loss 0.240033, acc 0.90625, learning_rate 0.000813761
2017-10-10T12:46:25.668305: step 473, loss 0.340134, acc 0.890625, learning_rate 0.000810848
2017-10-10T12:46:26.268873: step 474, loss 0.260416, acc 0.921875, learning_rate 0.000807946
2017-10-10T12:46:26.763783: step 475, loss 0.741406, acc 0.796875, learning_rate 0.000805057
2017-10-10T12:46:27.261925: step 476, loss 0.276178, acc 0.921875, learning_rate 0.000802179
2017-10-10T12:46:27.769188: step 477, loss 0.380045, acc 0.875, learning_rate 0.000799313
2017-10-10T12:46:28.286080: step 478, loss 0.521106, acc 0.890625, learning_rate 0.000796458
2017-10-10T12:46:28.804871: step 479, loss 0.466638, acc 0.8125, learning_rate 0.000793616
2017-10-10T12:46:29.350663: step 480, loss 0.348537, acc 0.90625, learning_rate 0.000790784

Evaluation:
2017-10-10T12:46:30.573623: step 480, loss 0.273517, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-480

2017-10-10T12:46:32.415730: step 481, loss 0.294692, acc 0.921875, learning_rate 0.000787965
2017-10-10T12:46:33.080919: step 482, loss 0.26696, acc 0.90625, learning_rate 0.000785157
2017-10-10T12:46:33.574750: step 483, loss 0.272474, acc 0.90625, learning_rate 0.00078236
2017-10-10T12:46:33.796296: step 484, loss 0.255663, acc 0.90625, learning_rate 0.000779575
2017-10-10T12:46:34.220842: step 485, loss 0.211142, acc 0.875, learning_rate 0.000776801
2017-10-10T12:46:34.533173: step 486, loss 0.336603, acc 0.890625, learning_rate 0.000774038
2017-10-10T12:46:34.961089: step 487, loss 0.282885, acc 0.859375, learning_rate 0.000771287
2017-10-10T12:46:35.508917: step 488, loss 0.360446, acc 0.859375, learning_rate 0.000768547
2017-10-10T12:46:36.027381: step 489, loss 0.515905, acc 0.875, learning_rate 0.000765818
2017-10-10T12:46:36.499157: step 490, loss 0.343261, acc 0.901961, learning_rate 0.000763101
2017-10-10T12:46:37.094559: step 491, loss 0.376821, acc 0.890625, learning_rate 0.000760394
2017-10-10T12:46:37.664843: step 492, loss 0.264035, acc 0.921875, learning_rate 0.000757698
2017-10-10T12:46:38.118178: step 493, loss 0.246068, acc 0.9375, learning_rate 0.000755014
2017-10-10T12:46:38.544297: step 494, loss 0.467505, acc 0.84375, learning_rate 0.00075234
2017-10-10T12:46:39.073473: step 495, loss 0.310587, acc 0.859375, learning_rate 0.000749677
2017-10-10T12:46:39.575561: step 496, loss 0.309071, acc 0.921875, learning_rate 0.000747026
2017-10-10T12:46:40.141600: step 497, loss 0.5072, acc 0.8125, learning_rate 0.000744385
2017-10-10T12:46:40.711731: step 498, loss 0.308124, acc 0.890625, learning_rate 0.000741754
2017-10-10T12:46:41.293106: step 499, loss 0.243444, acc 0.890625, learning_rate 0.000739135
2017-10-10T12:46:41.845527: step 500, loss 0.234361, acc 0.90625, learning_rate 0.000736526
2017-10-10T12:46:42.380328: step 501, loss 0.264716, acc 0.90625, learning_rate 0.000733928
2017-10-10T12:46:42.915955: step 502, loss 0.397473, acc 0.875, learning_rate 0.00073134
2017-10-10T12:46:43.513010: step 503, loss 0.571554, acc 0.765625, learning_rate 0.000728763
2017-10-10T12:46:44.105745: step 504, loss 0.348003, acc 0.890625, learning_rate 0.000726197
2017-10-10T12:46:44.552617: step 505, loss 0.266156, acc 0.90625, learning_rate 0.000723641
2017-10-10T12:46:45.024852: step 506, loss 0.363883, acc 0.84375, learning_rate 0.000721095
2017-10-10T12:46:45.484789: step 507, loss 0.356476, acc 0.828125, learning_rate 0.00071856
2017-10-10T12:46:46.041183: step 508, loss 0.247701, acc 0.875, learning_rate 0.000716036
2017-10-10T12:46:46.601158: step 509, loss 0.496724, acc 0.8125, learning_rate 0.000713521
2017-10-10T12:46:47.184942: step 510, loss 0.513647, acc 0.875, learning_rate 0.000711017
2017-10-10T12:46:47.725048: step 511, loss 0.417102, acc 0.890625, learning_rate 0.000708523
2017-10-10T12:46:48.223871: step 512, loss 0.313138, acc 0.875, learning_rate 0.000706039
2017-10-10T12:46:48.742673: step 513, loss 0.478373, acc 0.828125, learning_rate 0.000703565
2017-10-10T12:46:49.248864: step 514, loss 0.2058, acc 0.9375, learning_rate 0.000701102
2017-10-10T12:46:49.793002: step 515, loss 0.485083, acc 0.84375, learning_rate 0.000698648
2017-10-10T12:46:50.400900: step 516, loss 0.459432, acc 0.859375, learning_rate 0.000696204
2017-10-10T12:46:50.880944: step 517, loss 0.412358, acc 0.859375, learning_rate 0.000693771
2017-10-10T12:46:51.460922: step 518, loss 0.350635, acc 0.890625, learning_rate 0.000691347
2017-10-10T12:46:51.974257: step 519, loss 0.305266, acc 0.859375, learning_rate 0.000688934
2017-10-10T12:46:52.448919: step 520, loss 0.420277, acc 0.890625, learning_rate 0.00068653

Evaluation:
2017-10-10T12:46:53.684843: step 520, loss 0.265277, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-520

2017-10-10T12:46:55.148837: step 521, loss 0.361942, acc 0.828125, learning_rate 0.000684136
2017-10-10T12:46:55.676103: step 522, loss 0.367532, acc 0.890625, learning_rate 0.000681751
2017-10-10T12:46:56.252961: step 523, loss 0.367548, acc 0.875, learning_rate 0.000679377
2017-10-10T12:46:56.834757: step 524, loss 0.444887, acc 0.859375, learning_rate 0.000677012
2017-10-10T12:46:57.308816: step 525, loss 0.32641, acc 0.890625, learning_rate 0.000674657
2017-10-10T12:46:57.759598: step 526, loss 0.533763, acc 0.84375, learning_rate 0.000672311
2017-10-10T12:46:58.265094: step 527, loss 0.216172, acc 0.90625, learning_rate 0.000669975
2017-10-10T12:46:58.800161: step 528, loss 0.38912, acc 0.84375, learning_rate 0.000667648
2017-10-10T12:46:59.320559: step 529, loss 0.197233, acc 0.921875, learning_rate 0.000665331
2017-10-10T12:46:59.844969: step 530, loss 0.181139, acc 0.9375, learning_rate 0.000663024
2017-10-10T12:47:00.481469: step 531, loss 0.286055, acc 0.875, learning_rate 0.000660726
2017-10-10T12:47:01.099410: step 532, loss 0.386514, acc 0.9375, learning_rate 0.000658437
2017-10-10T12:47:01.544944: step 533, loss 0.331194, acc 0.90625, learning_rate 0.000656158
2017-10-10T12:47:01.969719: step 534, loss 0.476812, acc 0.8125, learning_rate 0.000653888
2017-10-10T12:47:02.524486: step 535, loss 0.296945, acc 0.859375, learning_rate 0.000651627
2017-10-10T12:47:03.032925: step 536, loss 0.35674, acc 0.90625, learning_rate 0.000649375
2017-10-10T12:47:03.560624: step 537, loss 0.324186, acc 0.890625, learning_rate 0.000647133
2017-10-10T12:47:04.064592: step 538, loss 0.157134, acc 0.9375, learning_rate 0.000644899
2017-10-10T12:47:04.584860: step 539, loss 0.309727, acc 0.90625, learning_rate 0.000642675
2017-10-10T12:47:05.177029: step 540, loss 0.205517, acc 0.9375, learning_rate 0.00064046
2017-10-10T12:47:05.744860: step 541, loss 0.379247, acc 0.875, learning_rate 0.000638254
2017-10-10T12:47:06.401044: step 542, loss 0.294955, acc 0.890625, learning_rate 0.000636057
2017-10-10T12:47:06.902710: step 543, loss 0.159123, acc 0.9375, learning_rate 0.000633869
2017-10-10T12:47:07.344959: step 544, loss 0.490838, acc 0.8125, learning_rate 0.00063169
2017-10-10T12:47:07.808868: step 545, loss 0.304786, acc 0.921875, learning_rate 0.00062952
2017-10-10T12:47:08.320967: step 546, loss 0.3315, acc 0.90625, learning_rate 0.000627358
2017-10-10T12:47:08.770862: step 547, loss 0.290996, acc 0.890625, learning_rate 0.000625206
2017-10-10T12:47:09.182149: step 548, loss 0.470811, acc 0.859375, learning_rate 0.000623062
2017-10-10T12:47:09.702199: step 549, loss 0.283522, acc 0.890625, learning_rate 0.000620927
2017-10-10T12:47:10.191910: step 550, loss 0.173502, acc 0.9375, learning_rate 0.000618801
2017-10-10T12:47:10.733610: step 551, loss 0.320922, acc 0.921875, learning_rate 0.000616683
2017-10-10T12:47:11.272049: step 552, loss 0.18976, acc 0.953125, learning_rate 0.000614574
2017-10-10T12:47:11.825947: step 553, loss 0.237245, acc 0.890625, learning_rate 0.000612474
2017-10-10T12:47:12.396145: step 554, loss 0.257091, acc 0.9375, learning_rate 0.000610382
2017-10-10T12:47:12.934498: step 555, loss 0.102283, acc 0.96875, learning_rate 0.000608299
2017-10-10T12:47:13.511788: step 556, loss 0.350658, acc 0.875, learning_rate 0.000606224
2017-10-10T12:47:14.089136: step 557, loss 0.280909, acc 0.9375, learning_rate 0.000604158
2017-10-10T12:47:14.668899: step 558, loss 0.30208, acc 0.921875, learning_rate 0.0006021
2017-10-10T12:47:15.232853: step 559, loss 0.32577, acc 0.875, learning_rate 0.00060005
2017-10-10T12:47:15.740060: step 560, loss 0.339787, acc 0.875, learning_rate 0.000598009

Evaluation:
2017-10-10T12:47:16.885657: step 560, loss 0.26774, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-560

2017-10-10T12:47:18.500985: step 561, loss 0.283359, acc 0.9375, learning_rate 0.000595977
2017-10-10T12:47:19.007833: step 562, loss 0.486616, acc 0.828125, learning_rate 0.000593952
2017-10-10T12:47:19.580947: step 563, loss 0.310758, acc 0.890625, learning_rate 0.000591936
2017-10-10T12:47:20.149119: step 564, loss 0.352603, acc 0.875, learning_rate 0.000589928
2017-10-10T12:47:20.537930: step 565, loss 0.3091, acc 0.890625, learning_rate 0.000587928
2017-10-10T12:47:20.981011: step 566, loss 0.343648, acc 0.875, learning_rate 0.000585937
2017-10-10T12:47:21.445357: step 567, loss 0.332542, acc 0.859375, learning_rate 0.000583953
2017-10-10T12:47:21.941581: step 568, loss 0.426053, acc 0.875, learning_rate 0.000581978
2017-10-10T12:47:22.459100: step 569, loss 0.31591, acc 0.875, learning_rate 0.00058001
2017-10-10T12:47:22.980989: step 570, loss 0.241288, acc 0.921875, learning_rate 0.000578051
2017-10-10T12:47:23.486365: step 571, loss 0.253482, acc 0.890625, learning_rate 0.0005761
2017-10-10T12:47:24.171250: step 572, loss 0.411489, acc 0.875, learning_rate 0.000574157
2017-10-10T12:47:24.661259: step 573, loss 0.335636, acc 0.890625, learning_rate 0.000572221
2017-10-10T12:47:25.104835: step 574, loss 0.209664, acc 0.90625, learning_rate 0.000570294
2017-10-10T12:47:25.632852: step 575, loss 0.247368, acc 0.859375, learning_rate 0.000568374
2017-10-10T12:47:26.163808: step 576, loss 0.396327, acc 0.875, learning_rate 0.000566462
2017-10-10T12:47:26.661080: step 577, loss 0.362679, acc 0.84375, learning_rate 0.000564558
2017-10-10T12:47:27.223514: step 578, loss 0.271546, acc 0.859375, learning_rate 0.000562662
2017-10-10T12:47:27.730446: step 579, loss 0.2374, acc 0.90625, learning_rate 0.000560774
2017-10-10T12:47:28.290188: step 580, loss 0.435065, acc 0.890625, learning_rate 0.000558893
2017-10-10T12:47:28.833108: step 581, loss 0.53938, acc 0.875, learning_rate 0.00055702
2017-10-10T12:47:29.496812: step 582, loss 0.195938, acc 0.953125, learning_rate 0.000555154
2017-10-10T12:47:29.953223: step 583, loss 0.251779, acc 0.921875, learning_rate 0.000553296
2017-10-10T12:47:30.390497: step 584, loss 0.398906, acc 0.875, learning_rate 0.000551446
2017-10-10T12:47:30.876866: step 585, loss 0.135382, acc 0.984375, learning_rate 0.000549604
2017-10-10T12:47:31.320867: step 586, loss 0.349266, acc 0.84375, learning_rate 0.000547768
2017-10-10T12:47:31.809530: step 587, loss 0.193277, acc 0.96875, learning_rate 0.000545941
2017-10-10T12:47:32.197192: step 588, loss 0.702404, acc 0.862745, learning_rate 0.00054412
2017-10-10T12:47:32.702286: step 589, loss 0.326273, acc 0.890625, learning_rate 0.000542308
2017-10-10T12:47:33.205096: step 590, loss 0.280553, acc 0.921875, learning_rate 0.000540502
2017-10-10T12:47:33.744957: step 591, loss 0.344263, acc 0.9375, learning_rate 0.000538704
2017-10-10T12:47:34.328940: step 592, loss 0.287051, acc 0.890625, learning_rate 0.000536914
2017-10-10T12:47:34.818011: step 593, loss 0.447206, acc 0.84375, learning_rate 0.00053513
2017-10-10T12:47:35.418954: step 594, loss 0.158675, acc 0.96875, learning_rate 0.000533354
2017-10-10T12:47:36.000880: step 595, loss 0.215908, acc 0.953125, learning_rate 0.000531585
2017-10-10T12:47:36.546335: step 596, loss 0.294956, acc 0.890625, learning_rate 0.000529824
2017-10-10T12:47:37.084846: step 597, loss 0.445939, acc 0.859375, learning_rate 0.000528069
2017-10-10T12:47:37.622625: step 598, loss 0.300063, acc 0.890625, learning_rate 0.000526322
2017-10-10T12:47:38.205171: step 599, loss 0.55543, acc 0.78125, learning_rate 0.000524582
2017-10-10T12:47:38.716883: step 600, loss 0.199408, acc 0.921875, learning_rate 0.000522849

Evaluation:
2017-10-10T12:47:39.862366: step 600, loss 0.264186, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-600

2017-10-10T12:47:41.685106: step 601, loss 0.309107, acc 0.890625, learning_rate 0.000521123
2017-10-10T12:47:42.305037: step 602, loss 0.288244, acc 0.875, learning_rate 0.000519404
2017-10-10T12:47:42.775309: step 603, loss 0.254782, acc 0.921875, learning_rate 0.000517692
2017-10-10T12:47:43.204353: step 604, loss 0.400154, acc 0.890625, learning_rate 0.000515987
2017-10-10T12:47:43.615066: step 605, loss 0.231529, acc 0.9375, learning_rate 0.000514289
2017-10-10T12:47:44.203781: step 606, loss 0.27672, acc 0.84375, learning_rate 0.000512598
2017-10-10T12:47:44.764966: step 607, loss 0.377753, acc 0.890625, learning_rate 0.000510914
2017-10-10T12:47:45.257027: step 608, loss 0.451577, acc 0.828125, learning_rate 0.000509237
2017-10-10T12:47:45.701237: step 609, loss 0.425994, acc 0.859375, learning_rate 0.000507566
2017-10-10T12:47:46.136353: step 610, loss 0.346186, acc 0.890625, learning_rate 0.000505903
2017-10-10T12:47:46.700873: step 611, loss 0.204092, acc 0.9375, learning_rate 0.000504246
2017-10-10T12:47:47.233314: step 612, loss 0.287385, acc 0.90625, learning_rate 0.000502596
2017-10-10T12:47:47.676302: step 613, loss 0.390622, acc 0.828125, learning_rate 0.000500953
2017-10-10T12:47:48.133048: step 614, loss 0.438585, acc 0.84375, learning_rate 0.000499316
2017-10-10T12:47:48.652846: step 615, loss 0.252802, acc 0.90625, learning_rate 0.000497686
2017-10-10T12:47:49.123603: step 616, loss 0.241934, acc 0.953125, learning_rate 0.000496063
2017-10-10T12:47:49.666419: step 617, loss 0.240916, acc 0.9375, learning_rate 0.000494446
2017-10-10T12:47:50.240902: step 618, loss 0.348762, acc 0.875, learning_rate 0.000492836
2017-10-10T12:47:50.784890: step 619, loss 0.324587, acc 0.890625, learning_rate 0.000491233
2017-10-10T12:47:51.313401: step 620, loss 0.23887, acc 0.9375, learning_rate 0.000489636
2017-10-10T12:47:51.853245: step 621, loss 0.223408, acc 0.921875, learning_rate 0.000488045
2017-10-10T12:47:52.448965: step 622, loss 0.356801, acc 0.796875, learning_rate 0.000486461
2017-10-10T12:47:52.932835: step 623, loss 0.297499, acc 0.875, learning_rate 0.000484884
2017-10-10T12:47:53.436962: step 624, loss 0.23563, acc 0.96875, learning_rate 0.000483313
2017-10-10T12:47:53.933717: step 625, loss 0.347166, acc 0.90625, learning_rate 0.000481748
2017-10-10T12:47:54.416994: step 626, loss 0.354591, acc 0.90625, learning_rate 0.00048019
2017-10-10T12:47:54.912593: step 627, loss 0.241811, acc 0.9375, learning_rate 0.000478638
2017-10-10T12:47:55.417165: step 628, loss 0.435278, acc 0.828125, learning_rate 0.000477093
2017-10-10T12:47:55.984999: step 629, loss 0.207931, acc 0.90625, learning_rate 0.000475554
2017-10-10T12:47:56.495821: step 630, loss 0.2831, acc 0.9375, learning_rate 0.000474021
2017-10-10T12:47:57.006834: step 631, loss 0.360178, acc 0.90625, learning_rate 0.000472494
2017-10-10T12:47:57.524366: step 632, loss 0.322459, acc 0.90625, learning_rate 0.000470974
2017-10-10T12:47:58.101159: step 633, loss 0.237173, acc 0.90625, learning_rate 0.000469459
2017-10-10T12:47:58.597088: step 634, loss 0.29614, acc 0.875, learning_rate 0.000467951
2017-10-10T12:47:59.076794: step 635, loss 0.372177, acc 0.875, learning_rate 0.000466449
2017-10-10T12:47:59.602943: step 636, loss 0.195235, acc 0.953125, learning_rate 0.000464954
2017-10-10T12:48:00.131835: step 637, loss 0.288437, acc 0.890625, learning_rate 0.000463464
2017-10-10T12:48:00.639527: step 638, loss 0.193605, acc 0.9375, learning_rate 0.00046198
2017-10-10T12:48:01.167812: step 639, loss 0.297557, acc 0.859375, learning_rate 0.000460503
2017-10-10T12:48:01.737035: step 640, loss 0.386815, acc 0.90625, learning_rate 0.000459031

Evaluation:
2017-10-10T12:48:02.842119: step 640, loss 0.263477, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-640

2017-10-10T12:48:04.286251: step 641, loss 0.28132, acc 0.90625, learning_rate 0.000457566
2017-10-10T12:48:04.917011: step 642, loss 0.387682, acc 0.875, learning_rate 0.000456106
2017-10-10T12:48:05.492554: step 643, loss 0.325559, acc 0.875, learning_rate 0.000454653
2017-10-10T12:48:05.960755: step 644, loss 0.318264, acc 0.875, learning_rate 0.000453205
2017-10-10T12:48:06.492444: step 645, loss 0.399861, acc 0.84375, learning_rate 0.000451764
2017-10-10T12:48:07.071455: step 646, loss 0.490308, acc 0.859375, learning_rate 0.000450328
2017-10-10T12:48:07.613030: step 647, loss 0.223879, acc 0.90625, learning_rate 0.000448898
2017-10-10T12:48:08.192562: step 648, loss 0.513429, acc 0.875, learning_rate 0.000447474
2017-10-10T12:48:08.736189: step 649, loss 0.28322, acc 0.9375, learning_rate 0.000446055
2017-10-10T12:48:09.225719: step 650, loss 0.261029, acc 0.90625, learning_rate 0.000444643
2017-10-10T12:48:09.848846: step 651, loss 0.534235, acc 0.859375, learning_rate 0.000443236
2017-10-10T12:48:10.352975: step 652, loss 0.306402, acc 0.890625, learning_rate 0.000441835
2017-10-10T12:48:10.781500: step 653, loss 0.342201, acc 0.921875, learning_rate 0.00044044
2017-10-10T12:48:11.220773: step 654, loss 0.265802, acc 0.890625, learning_rate 0.00043905
2017-10-10T12:48:11.744246: step 655, loss 0.258416, acc 0.90625, learning_rate 0.000437666
2017-10-10T12:48:12.332866: step 656, loss 0.184869, acc 0.921875, learning_rate 0.000436288
2017-10-10T12:48:12.892967: step 657, loss 0.329952, acc 0.84375, learning_rate 0.000434915
2017-10-10T12:48:13.413237: step 658, loss 0.344378, acc 0.921875, learning_rate 0.000433548
2017-10-10T12:48:13.936859: step 659, loss 0.238613, acc 0.9375, learning_rate 0.000432187
2017-10-10T12:48:14.464996: step 660, loss 0.382208, acc 0.859375, learning_rate 0.000430831
2017-10-10T12:48:14.979863: step 661, loss 0.219082, acc 0.921875, learning_rate 0.000429481
2017-10-10T12:48:15.453078: step 662, loss 0.221837, acc 0.9375, learning_rate 0.000428136
2017-10-10T12:48:16.019057: step 663, loss 0.387412, acc 0.90625, learning_rate 0.000426796
2017-10-10T12:48:16.529969: step 664, loss 0.366955, acc 0.828125, learning_rate 0.000425463
2017-10-10T12:48:16.968094: step 665, loss 0.123505, acc 0.96875, learning_rate 0.000424134
2017-10-10T12:48:17.444068: step 666, loss 0.349375, acc 0.90625, learning_rate 0.000422811
2017-10-10T12:48:18.046001: step 667, loss 0.348921, acc 0.90625, learning_rate 0.000421493
2017-10-10T12:48:18.550576: step 668, loss 0.304898, acc 0.859375, learning_rate 0.000420181
2017-10-10T12:48:19.118826: step 669, loss 0.30048, acc 0.921875, learning_rate 0.000418874
2017-10-10T12:48:19.604244: step 670, loss 0.175714, acc 0.953125, learning_rate 0.000417573
2017-10-10T12:48:20.144530: step 671, loss 0.303673, acc 0.890625, learning_rate 0.000416276
2017-10-10T12:48:20.672511: step 672, loss 0.359065, acc 0.859375, learning_rate 0.000414985
2017-10-10T12:48:21.200853: step 673, loss 0.544364, acc 0.796875, learning_rate 0.0004137
2017-10-10T12:48:21.727182: step 674, loss 0.318462, acc 0.890625, learning_rate 0.000412419
2017-10-10T12:48:22.282798: step 675, loss 0.214501, acc 0.9375, learning_rate 0.000411144
2017-10-10T12:48:22.788956: step 676, loss 0.250702, acc 0.921875, learning_rate 0.000409874
2017-10-10T12:48:23.340465: step 677, loss 0.329478, acc 0.890625, learning_rate 0.000408609
2017-10-10T12:48:23.894656: step 678, loss 0.25268, acc 0.90625, learning_rate 0.00040735
2017-10-10T12:48:24.447253: step 679, loss 0.43958, acc 0.84375, learning_rate 0.000406095
2017-10-10T12:48:24.974768: step 680, loss 0.235704, acc 0.9375, learning_rate 0.000404846

Evaluation:
2017-10-10T12:48:26.139919: step 680, loss 0.265362, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-680

2017-10-10T12:48:27.820851: step 681, loss 0.336442, acc 0.859375, learning_rate 0.000403601
2017-10-10T12:48:28.261999: step 682, loss 0.471121, acc 0.859375, learning_rate 0.000402362
2017-10-10T12:48:28.704830: step 683, loss 0.293559, acc 0.890625, learning_rate 0.000401128
2017-10-10T12:48:29.168859: step 684, loss 0.290449, acc 0.890625, learning_rate 0.000399899
2017-10-10T12:48:29.718080: step 685, loss 0.364575, acc 0.875, learning_rate 0.000398675
2017-10-10T12:48:30.168419: step 686, loss 0.464723, acc 0.901961, learning_rate 0.000397456
2017-10-10T12:48:30.702687: step 687, loss 0.241476, acc 0.921875, learning_rate 0.000396241
2017-10-10T12:48:31.235131: step 688, loss 0.339638, acc 0.890625, learning_rate 0.000395032
2017-10-10T12:48:31.753692: step 689, loss 0.275098, acc 0.953125, learning_rate 0.000393828
2017-10-10T12:48:32.309035: step 690, loss 0.178735, acc 0.953125, learning_rate 0.000392629
2017-10-10T12:48:32.871271: step 691, loss 0.38162, acc 0.84375, learning_rate 0.000391434
2017-10-10T12:48:33.404922: step 692, loss 0.327338, acc 0.890625, learning_rate 0.000390245
2017-10-10T12:48:33.802360: step 693, loss 0.262904, acc 0.90625, learning_rate 0.00038906
2017-10-10T12:48:34.210806: step 694, loss 0.145835, acc 0.96875, learning_rate 0.00038788
2017-10-10T12:48:34.652872: step 695, loss 0.342037, acc 0.859375, learning_rate 0.000386705
2017-10-10T12:48:35.243310: step 696, loss 0.269775, acc 0.890625, learning_rate 0.000385535
2017-10-10T12:48:35.736992: step 697, loss 0.140319, acc 0.96875, learning_rate 0.000384369
2017-10-10T12:48:36.257274: step 698, loss 0.382277, acc 0.90625, learning_rate 0.000383209
2017-10-10T12:48:36.777754: step 699, loss 0.248821, acc 0.921875, learning_rate 0.000382053
2017-10-10T12:48:37.317058: step 700, loss 0.508627, acc 0.828125, learning_rate 0.000380901
2017-10-10T12:48:37.855171: step 701, loss 0.199657, acc 0.921875, learning_rate 0.000379755
2017-10-10T12:48:38.455369: step 702, loss 0.280375, acc 0.90625, learning_rate 0.000378613
2017-10-10T12:48:39.045195: step 703, loss 0.319637, acc 0.875, learning_rate 0.000377476
2017-10-10T12:48:39.473576: step 704, loss 0.334358, acc 0.921875, learning_rate 0.000376343
2017-10-10T12:48:39.876853: step 705, loss 0.397884, acc 0.875, learning_rate 0.000375215
2017-10-10T12:48:40.324137: step 706, loss 0.241233, acc 0.90625, learning_rate 0.000374092
2017-10-10T12:48:40.800831: step 707, loss 0.457891, acc 0.828125, learning_rate 0.000372973
2017-10-10T12:48:41.348699: step 708, loss 0.379732, acc 0.890625, learning_rate 0.000371859
2017-10-10T12:48:41.892450: step 709, loss 0.305767, acc 0.921875, learning_rate 0.000370749
2017-10-10T12:48:42.418424: step 710, loss 0.437529, acc 0.84375, learning_rate 0.000369644
2017-10-10T12:48:42.946829: step 711, loss 0.313031, acc 0.875, learning_rate 0.000368543
2017-10-10T12:48:43.486647: step 712, loss 0.299643, acc 0.890625, learning_rate 0.000367447
2017-10-10T12:48:44.029462: step 713, loss 0.233886, acc 0.890625, learning_rate 0.000366356
2017-10-10T12:48:44.572997: step 714, loss 0.325832, acc 0.90625, learning_rate 0.000365268
2017-10-10T12:48:45.092281: step 715, loss 0.291601, acc 0.890625, learning_rate 0.000364186
2017-10-10T12:48:45.536882: step 716, loss 0.540331, acc 0.765625, learning_rate 0.000363107
2017-10-10T12:48:46.040953: step 717, loss 0.216907, acc 0.921875, learning_rate 0.000362033
2017-10-10T12:48:46.473035: step 718, loss 0.496268, acc 0.84375, learning_rate 0.000360964
2017-10-10T12:48:46.951035: step 719, loss 0.287729, acc 0.890625, learning_rate 0.000359899
2017-10-10T12:48:47.396971: step 720, loss 0.220895, acc 0.90625, learning_rate 0.000358838

Evaluation:
2017-10-10T12:48:48.652624: step 720, loss 0.262132, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-720

2017-10-10T12:48:50.545184: step 721, loss 0.379697, acc 0.875, learning_rate 0.000357781
2017-10-10T12:48:51.074068: step 722, loss 0.437105, acc 0.890625, learning_rate 0.000356729
2017-10-10T12:48:51.520659: step 723, loss 0.31143, acc 0.90625, learning_rate 0.000355681
2017-10-10T12:48:51.927946: step 724, loss 0.27293, acc 0.90625, learning_rate 0.000354637
2017-10-10T12:48:52.321705: step 725, loss 0.340713, acc 0.90625, learning_rate 0.000353598
2017-10-10T12:48:52.878255: step 726, loss 0.37003, acc 0.875, learning_rate 0.000352563
2017-10-10T12:48:53.375885: step 727, loss 0.247646, acc 0.9375, learning_rate 0.000351532
2017-10-10T12:48:53.884872: step 728, loss 0.532405, acc 0.828125, learning_rate 0.000350505
2017-10-10T12:48:54.418924: step 729, loss 0.194903, acc 0.953125, learning_rate 0.000349483
2017-10-10T12:48:54.964928: step 730, loss 0.258292, acc 0.921875, learning_rate 0.000348465
2017-10-10T12:48:55.468970: step 731, loss 0.358357, acc 0.90625, learning_rate 0.00034745
2017-10-10T12:48:56.056849: step 732, loss 0.156241, acc 0.96875, learning_rate 0.00034644
2017-10-10T12:48:56.585103: step 733, loss 0.227749, acc 0.921875, learning_rate 0.000345434
2017-10-10T12:48:57.016925: step 734, loss 0.290373, acc 0.90625, learning_rate 0.000344433
2017-10-10T12:48:57.468880: step 735, loss 0.267341, acc 0.875, learning_rate 0.000343435
2017-10-10T12:48:57.872590: step 736, loss 0.259158, acc 0.9375, learning_rate 0.000342441
2017-10-10T12:48:58.378172: step 737, loss 0.196427, acc 0.921875, learning_rate 0.000341452
2017-10-10T12:48:58.897934: step 738, loss 0.191542, acc 0.90625, learning_rate 0.000340466
2017-10-10T12:48:59.383477: step 739, loss 0.244059, acc 0.9375, learning_rate 0.000339485
2017-10-10T12:48:59.960887: step 740, loss 0.200059, acc 0.90625, learning_rate 0.000338507
2017-10-10T12:49:00.438083: step 741, loss 0.312364, acc 0.890625, learning_rate 0.000337534
2017-10-10T12:49:00.973016: step 742, loss 0.282989, acc 0.953125, learning_rate 0.000336564
2017-10-10T12:49:01.581047: step 743, loss 0.274341, acc 0.9375, learning_rate 0.000335598
2017-10-10T12:49:02.161545: step 744, loss 0.27241, acc 0.921875, learning_rate 0.000334637
2017-10-10T12:49:02.637723: step 745, loss 0.546693, acc 0.8125, learning_rate 0.000333679
2017-10-10T12:49:03.112822: step 746, loss 0.194218, acc 0.953125, learning_rate 0.000332725
2017-10-10T12:49:03.582615: step 747, loss 0.236275, acc 0.9375, learning_rate 0.000331775
2017-10-10T12:49:04.088954: step 748, loss 0.361037, acc 0.828125, learning_rate 0.000330829
2017-10-10T12:49:04.572564: step 749, loss 0.301399, acc 0.875, learning_rate 0.000329887
2017-10-10T12:49:05.144893: step 750, loss 0.115024, acc 0.984375, learning_rate 0.000328949
2017-10-10T12:49:05.669786: step 751, loss 0.270963, acc 0.90625, learning_rate 0.000328014
2017-10-10T12:49:06.149135: step 752, loss 0.235597, acc 0.90625, learning_rate 0.000327083
2017-10-10T12:49:06.733484: step 753, loss 0.339855, acc 0.84375, learning_rate 0.000326157
2017-10-10T12:49:07.259636: step 754, loss 0.369458, acc 0.875, learning_rate 0.000325233
2017-10-10T12:49:07.787827: step 755, loss 0.244959, acc 0.890625, learning_rate 0.000324314
2017-10-10T12:49:08.255738: step 756, loss 0.16283, acc 0.9375, learning_rate 0.000323399
2017-10-10T12:49:08.716999: step 757, loss 0.264768, acc 0.890625, learning_rate 0.000322487
2017-10-10T12:49:09.230189: step 758, loss 0.48475, acc 0.84375, learning_rate 0.000321579
2017-10-10T12:49:09.805180: step 759, loss 0.300566, acc 0.90625, learning_rate 0.000320674
2017-10-10T12:49:10.268914: step 760, loss 0.232025, acc 0.9375, learning_rate 0.000319773

Evaluation:
2017-10-10T12:49:11.484937: step 760, loss 0.261809, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-760

2017-10-10T12:49:13.012073: step 761, loss 0.217111, acc 0.921875, learning_rate 0.000318876
2017-10-10T12:49:13.512963: step 762, loss 0.239045, acc 0.921875, learning_rate 0.000317983
2017-10-10T12:49:14.080894: step 763, loss 0.29416, acc 0.875, learning_rate 0.000317093
2017-10-10T12:49:14.629222: step 764, loss 0.164643, acc 0.9375, learning_rate 0.000316207
2017-10-10T12:49:15.058694: step 765, loss 0.37062, acc 0.828125, learning_rate 0.000315325
2017-10-10T12:49:15.517051: step 766, loss 0.402075, acc 0.875, learning_rate 0.000314446
2017-10-10T12:49:16.096928: step 767, loss 0.215285, acc 0.890625, learning_rate 0.00031357
2017-10-10T12:49:16.668853: step 768, loss 0.27638, acc 0.890625, learning_rate 0.000312699
2017-10-10T12:49:17.204874: step 769, loss 0.541975, acc 0.84375, learning_rate 0.00031183
2017-10-10T12:49:17.720903: step 770, loss 0.323864, acc 0.890625, learning_rate 0.000310966
2017-10-10T12:49:18.163411: step 771, loss 0.373907, acc 0.875, learning_rate 0.000310105
2017-10-10T12:49:18.550497: step 772, loss 0.316443, acc 0.84375, learning_rate 0.000309247
2017-10-10T12:49:19.160154: step 773, loss 0.524684, acc 0.828125, learning_rate 0.000308393
2017-10-10T12:49:19.749169: step 774, loss 0.353042, acc 0.859375, learning_rate 0.000307542
2017-10-10T12:49:20.257028: step 775, loss 0.326815, acc 0.890625, learning_rate 0.000306695
2017-10-10T12:49:20.801008: step 776, loss 0.254406, acc 0.921875, learning_rate 0.000305852
2017-10-10T12:49:21.393988: step 777, loss 0.291126, acc 0.859375, learning_rate 0.000305011
2017-10-10T12:49:21.968218: step 778, loss 0.405041, acc 0.875, learning_rate 0.000304174
2017-10-10T12:49:22.496944: step 779, loss 0.309948, acc 0.9375, learning_rate 0.000303341
2017-10-10T12:49:23.064821: step 780, loss 0.230051, acc 0.921875, learning_rate 0.000302511
2017-10-10T12:49:23.566726: step 781, loss 0.290498, acc 0.890625, learning_rate 0.000301684
2017-10-10T12:49:24.127624: step 782, loss 0.335471, acc 0.90625, learning_rate 0.000300861
2017-10-10T12:49:24.747043: step 783, loss 0.189663, acc 0.953125, learning_rate 0.000300041
2017-10-10T12:49:25.261008: step 784, loss 0.234096, acc 0.882353, learning_rate 0.000299225
2017-10-10T12:49:25.697877: step 785, loss 0.383713, acc 0.828125, learning_rate 0.000298412
2017-10-10T12:49:26.157707: step 786, loss 0.316863, acc 0.921875, learning_rate 0.000297602
2017-10-10T12:49:26.624811: step 787, loss 0.213466, acc 0.9375, learning_rate 0.000296795
2017-10-10T12:49:27.135117: step 788, loss 0.372614, acc 0.84375, learning_rate 0.000295992
2017-10-10T12:49:27.672837: step 789, loss 0.191126, acc 0.96875, learning_rate 0.000295192
2017-10-10T12:49:28.188112: step 790, loss 0.336543, acc 0.875, learning_rate 0.000294395
2017-10-10T12:49:28.706368: step 791, loss 0.172756, acc 0.953125, learning_rate 0.000293602
2017-10-10T12:49:29.172837: step 792, loss 0.217648, acc 0.921875, learning_rate 0.000292812
2017-10-10T12:49:29.645522: step 793, loss 0.315463, acc 0.859375, learning_rate 0.000292025
2017-10-10T12:49:30.105248: step 794, loss 0.287836, acc 0.9375, learning_rate 0.000291241
2017-10-10T12:49:30.644873: step 795, loss 0.286532, acc 0.90625, learning_rate 0.00029046
2017-10-10T12:49:31.136881: step 796, loss 0.254214, acc 0.90625, learning_rate 0.000289683
2017-10-10T12:49:31.709041: step 797, loss 0.21721, acc 0.9375, learning_rate 0.000288908
2017-10-10T12:49:32.276900: step 798, loss 0.262289, acc 0.921875, learning_rate 0.000288137
2017-10-10T12:49:32.779568: step 799, loss 0.242189, acc 0.921875, learning_rate 0.000287369
2017-10-10T12:49:33.256853: step 800, loss 0.168069, acc 0.9375, learning_rate 0.000286605

Evaluation:
2017-10-10T12:49:34.352967: step 800, loss 0.258237, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-800

2017-10-10T12:49:36.016875: step 801, loss 0.295907, acc 0.875, learning_rate 0.000285843
2017-10-10T12:49:36.636859: step 802, loss 0.229911, acc 0.90625, learning_rate 0.000285084
2017-10-10T12:49:37.225052: step 803, loss 0.226337, acc 0.9375, learning_rate 0.000284329
2017-10-10T12:49:37.700836: step 804, loss 0.371765, acc 0.875, learning_rate 0.000283577
2017-10-10T12:49:38.128982: step 805, loss 0.434428, acc 0.84375, learning_rate 0.000282827
2017-10-10T12:49:38.755181: step 806, loss 0.281607, acc 0.890625, learning_rate 0.000282081
2017-10-10T12:49:39.296849: step 807, loss 0.22174, acc 0.921875, learning_rate 0.000281338
2017-10-10T12:49:39.835219: step 808, loss 0.229872, acc 0.890625, learning_rate 0.000280598
2017-10-10T12:49:40.316904: step 809, loss 0.424793, acc 0.875, learning_rate 0.00027986
2017-10-10T12:49:40.889096: step 810, loss 0.411465, acc 0.890625, learning_rate 0.000279126
2017-10-10T12:49:41.436928: step 811, loss 0.176103, acc 0.9375, learning_rate 0.000278395
2017-10-10T12:49:42.052107: step 812, loss 0.303105, acc 0.875, learning_rate 0.000277667
2017-10-10T12:49:42.548528: step 813, loss 0.425355, acc 0.859375, learning_rate 0.000276942
2017-10-10T12:49:43.008830: step 814, loss 0.261706, acc 0.90625, learning_rate 0.00027622
2017-10-10T12:49:43.423638: step 815, loss 0.213838, acc 0.921875, learning_rate 0.0002755
2017-10-10T12:49:43.973019: step 816, loss 0.268187, acc 0.875, learning_rate 0.000274784
2017-10-10T12:49:44.517383: step 817, loss 0.210827, acc 0.921875, learning_rate 0.000274071
2017-10-10T12:49:45.083492: step 818, loss 0.337964, acc 0.84375, learning_rate 0.00027336
2017-10-10T12:49:45.619040: step 819, loss 0.274756, acc 0.90625, learning_rate 0.000272652
2017-10-10T12:49:46.158962: step 820, loss 0.287908, acc 0.859375, learning_rate 0.000271948
2017-10-10T12:49:46.653287: step 821, loss 0.359727, acc 0.875, learning_rate 0.000271246
2017-10-10T12:49:47.152094: step 822, loss 0.331517, acc 0.90625, learning_rate 0.000270547
2017-10-10T12:49:47.610321: step 823, loss 0.46794, acc 0.859375, learning_rate 0.000269851
2017-10-10T12:49:48.176244: step 824, loss 0.235327, acc 0.921875, learning_rate 0.000269157
2017-10-10T12:49:48.673640: step 825, loss 0.25832, acc 0.90625, learning_rate 0.000268467
2017-10-10T12:49:49.113745: step 826, loss 0.42812, acc 0.875, learning_rate 0.000267779
2017-10-10T12:49:49.664914: step 827, loss 0.302094, acc 0.90625, learning_rate 0.000267094
2017-10-10T12:49:50.172845: step 828, loss 0.362809, acc 0.90625, learning_rate 0.000266412
2017-10-10T12:49:50.647284: step 829, loss 0.131216, acc 0.9375, learning_rate 0.000265733
2017-10-10T12:49:51.128793: step 830, loss 0.308334, acc 0.90625, learning_rate 0.000265057
2017-10-10T12:49:51.604837: step 831, loss 0.461789, acc 0.890625, learning_rate 0.000264383
2017-10-10T12:49:52.113614: step 832, loss 0.180426, acc 0.953125, learning_rate 0.000263712
2017-10-10T12:49:52.584858: step 833, loss 0.31015, acc 0.90625, learning_rate 0.000263044
2017-10-10T12:49:53.057048: step 834, loss 0.38777, acc 0.875, learning_rate 0.000262378
2017-10-10T12:49:53.437122: step 835, loss 0.204809, acc 0.953125, learning_rate 0.000261715
2017-10-10T12:49:53.986688: step 836, loss 0.431583, acc 0.828125, learning_rate 0.000261055
2017-10-10T12:49:54.544865: step 837, loss 0.330482, acc 0.90625, learning_rate 0.000260398
2017-10-10T12:49:55.100137: step 838, loss 0.382541, acc 0.890625, learning_rate 0.000259743
2017-10-10T12:49:55.655373: step 839, loss 0.253274, acc 0.9375, learning_rate 0.000259091
2017-10-10T12:49:56.264848: step 840, loss 0.395768, acc 0.890625, learning_rate 0.000258442

Evaluation:
2017-10-10T12:49:57.425607: step 840, loss 0.260525, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-840

2017-10-10T12:49:59.284842: step 841, loss 0.446554, acc 0.8125, learning_rate 0.000257795
2017-10-10T12:49:59.848898: step 842, loss 0.378101, acc 0.84375, learning_rate 0.000257151
2017-10-10T12:50:00.368893: step 843, loss 0.469081, acc 0.796875, learning_rate 0.00025651
2017-10-10T12:50:00.892899: step 844, loss 0.199729, acc 0.953125, learning_rate 0.000255871
2017-10-10T12:50:01.352999: step 845, loss 0.15564, acc 0.953125, learning_rate 0.000255235
2017-10-10T12:50:01.787245: step 846, loss 0.448994, acc 0.84375, learning_rate 0.000254601
2017-10-10T12:50:02.311612: step 847, loss 0.261125, acc 0.890625, learning_rate 0.00025397
2017-10-10T12:50:02.843498: step 848, loss 0.448119, acc 0.8125, learning_rate 0.000253341
2017-10-10T12:50:03.410611: step 849, loss 0.261216, acc 0.9375, learning_rate 0.000252716
2017-10-10T12:50:03.985356: step 850, loss 0.289957, acc 0.90625, learning_rate 0.000252092
2017-10-10T12:50:04.544205: step 851, loss 0.253655, acc 0.9375, learning_rate 0.000251471
2017-10-10T12:50:05.112869: step 852, loss 0.252296, acc 0.90625, learning_rate 0.000250853
2017-10-10T12:50:05.657082: step 853, loss 0.30393, acc 0.90625, learning_rate 0.000250237
2017-10-10T12:50:06.269096: step 854, loss 0.224365, acc 0.921875, learning_rate 0.000249624
2017-10-10T12:50:06.677790: step 855, loss 0.225401, acc 0.953125, learning_rate 0.000249013
2017-10-10T12:50:07.080845: step 856, loss 0.217454, acc 0.90625, learning_rate 0.000248405
2017-10-10T12:50:07.541759: step 857, loss 0.382096, acc 0.875, learning_rate 0.000247799
2017-10-10T12:50:08.060140: step 858, loss 0.210143, acc 0.953125, learning_rate 0.000247196
2017-10-10T12:50:08.640608: step 859, loss 0.0977073, acc 0.953125, learning_rate 0.000246595
2017-10-10T12:50:09.226133: step 860, loss 0.28413, acc 0.90625, learning_rate 0.000245997
2017-10-10T12:50:09.759258: step 861, loss 0.304944, acc 0.875, learning_rate 0.000245401
2017-10-10T12:50:10.320283: step 862, loss 0.454522, acc 0.828125, learning_rate 0.000244808
2017-10-10T12:50:10.947086: step 863, loss 0.205328, acc 0.921875, learning_rate 0.000244216
2017-10-10T12:50:11.348331: step 864, loss 0.382565, acc 0.859375, learning_rate 0.000243628
2017-10-10T12:50:11.735622: step 865, loss 0.269994, acc 0.921875, learning_rate 0.000243042
2017-10-10T12:50:12.125405: step 866, loss 0.313233, acc 0.921875, learning_rate 0.000242458
2017-10-10T12:50:12.633567: step 867, loss 0.235078, acc 0.921875, learning_rate 0.000241876
2017-10-10T12:50:13.180402: step 868, loss 0.118942, acc 0.984375, learning_rate 0.000241297
2017-10-10T12:50:13.712573: step 869, loss 0.202163, acc 0.96875, learning_rate 0.00024072
2017-10-10T12:50:14.164858: step 870, loss 0.454864, acc 0.828125, learning_rate 0.000240146
2017-10-10T12:50:14.713636: step 871, loss 0.195223, acc 0.9375, learning_rate 0.000239574
2017-10-10T12:50:15.243173: step 872, loss 0.289365, acc 0.921875, learning_rate 0.000239004
2017-10-10T12:50:15.825456: step 873, loss 0.326688, acc 0.875, learning_rate 0.000238437
2017-10-10T12:50:16.404932: step 874, loss 0.13398, acc 0.96875, learning_rate 0.000237872
2017-10-10T12:50:16.882435: step 875, loss 0.350414, acc 0.859375, learning_rate 0.000237309
2017-10-10T12:50:17.401175: step 876, loss 0.194852, acc 0.96875, learning_rate 0.000236749
2017-10-10T12:50:18.001121: step 877, loss 0.347634, acc 0.890625, learning_rate 0.00023619
2017-10-10T12:50:18.542082: step 878, loss 0.287882, acc 0.90625, learning_rate 0.000235635
2017-10-10T12:50:19.128993: step 879, loss 0.276257, acc 0.890625, learning_rate 0.000235081
2017-10-10T12:50:19.660985: step 880, loss 0.316219, acc 0.9375, learning_rate 0.00023453

Evaluation:
2017-10-10T12:50:20.920192: step 880, loss 0.258199, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-880

2017-10-10T12:50:22.277032: step 881, loss 0.260283, acc 0.890625, learning_rate 0.00023398
2017-10-10T12:50:22.668879: step 882, loss 0.197621, acc 0.960784, learning_rate 0.000233434
2017-10-10T12:50:23.373052: step 883, loss 0.236342, acc 0.890625, learning_rate 0.000232889
2017-10-10T12:50:23.790375: step 884, loss 0.453388, acc 0.828125, learning_rate 0.000232346
2017-10-10T12:50:24.209075: step 885, loss 0.146856, acc 0.9375, learning_rate 0.000231806
2017-10-10T12:50:24.756941: step 886, loss 0.194303, acc 0.953125, learning_rate 0.000231268
2017-10-10T12:50:25.266846: step 887, loss 0.155518, acc 0.953125, learning_rate 0.000230732
2017-10-10T12:50:25.765168: step 888, loss 0.452828, acc 0.859375, learning_rate 0.000230199
2017-10-10T12:50:26.276918: step 889, loss 0.451357, acc 0.859375, learning_rate 0.000229667
2017-10-10T12:50:26.776278: step 890, loss 0.236498, acc 0.921875, learning_rate 0.000229138
2017-10-10T12:50:27.272995: step 891, loss 0.311775, acc 0.875, learning_rate 0.000228611
2017-10-10T12:50:27.793255: step 892, loss 0.442777, acc 0.875, learning_rate 0.000228086
2017-10-10T12:50:28.309012: step 893, loss 0.349251, acc 0.921875, learning_rate 0.000227563
2017-10-10T12:50:28.834127: step 894, loss 0.275002, acc 0.875, learning_rate 0.000227043
2017-10-10T12:50:29.365104: step 895, loss 0.406035, acc 0.859375, learning_rate 0.000226524
2017-10-10T12:50:29.828841: step 896, loss 0.224877, acc 0.90625, learning_rate 0.000226008
2017-10-10T12:50:30.299042: step 897, loss 0.232033, acc 0.90625, learning_rate 0.000225493
2017-10-10T12:50:30.771461: step 898, loss 0.235099, acc 0.875, learning_rate 0.000224981
2017-10-10T12:50:31.301189: step 899, loss 0.396551, acc 0.890625, learning_rate 0.000224471
2017-10-10T12:50:31.912859: step 900, loss 0.36092, acc 0.859375, learning_rate 0.000223963
2017-10-10T12:50:32.468834: step 901, loss 0.183614, acc 0.921875, learning_rate 0.000223457
2017-10-10T12:50:32.968839: step 902, loss 0.192318, acc 0.921875, learning_rate 0.000222953
2017-10-10T12:50:33.605081: step 903, loss 0.27708, acc 0.875, learning_rate 0.000222451
2017-10-10T12:50:34.208731: step 904, loss 0.313261, acc 0.859375, learning_rate 0.000221951
2017-10-10T12:50:34.660828: step 905, loss 0.304015, acc 0.90625, learning_rate 0.000221453
2017-10-10T12:50:35.117409: step 906, loss 0.319307, acc 0.84375, learning_rate 0.000220958
2017-10-10T12:50:35.593857: step 907, loss 0.348452, acc 0.890625, learning_rate 0.000220464
2017-10-10T12:50:36.121498: step 908, loss 0.358671, acc 0.859375, learning_rate 0.000219972
2017-10-10T12:50:36.634322: step 909, loss 0.238246, acc 0.921875, learning_rate 0.000219483
2017-10-10T12:50:37.118190: step 910, loss 0.36636, acc 0.90625, learning_rate 0.000218995
2017-10-10T12:50:37.648307: step 911, loss 0.218686, acc 0.9375, learning_rate 0.000218509
2017-10-10T12:50:38.097185: step 912, loss 0.248971, acc 0.890625, learning_rate 0.000218025
2017-10-10T12:50:38.585008: step 913, loss 0.329714, acc 0.859375, learning_rate 0.000217544
2017-10-10T12:50:39.116934: step 914, loss 0.245809, acc 0.953125, learning_rate 0.000217064
2017-10-10T12:50:39.649953: step 915, loss 0.282605, acc 0.921875, learning_rate 0.000216586
2017-10-10T12:50:40.137137: step 916, loss 0.19413, acc 0.96875, learning_rate 0.00021611
2017-10-10T12:50:40.659299: step 917, loss 0.291101, acc 0.875, learning_rate 0.000215636
2017-10-10T12:50:41.182561: step 918, loss 0.106412, acc 0.96875, learning_rate 0.000215164
2017-10-10T12:50:41.709033: step 919, loss 0.231587, acc 0.921875, learning_rate 0.000214694
2017-10-10T12:50:42.208839: step 920, loss 0.291988, acc 0.875, learning_rate 0.000214226

Evaluation:
2017-10-10T12:50:43.373178: step 920, loss 0.256166, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-920

2017-10-10T12:50:45.096965: step 921, loss 0.363511, acc 0.921875, learning_rate 0.00021376
2017-10-10T12:50:45.684932: step 922, loss 0.463893, acc 0.84375, learning_rate 0.000213295
2017-10-10T12:50:46.201783: step 923, loss 0.308352, acc 0.890625, learning_rate 0.000212833
2017-10-10T12:50:46.604832: step 924, loss 0.289359, acc 0.890625, learning_rate 0.000212372
2017-10-10T12:50:47.045756: step 925, loss 0.210606, acc 0.9375, learning_rate 0.000211914
2017-10-10T12:50:47.579098: step 926, loss 0.151442, acc 0.921875, learning_rate 0.000211457
2017-10-10T12:50:48.095699: step 927, loss 0.297234, acc 0.921875, learning_rate 0.000211002
2017-10-10T12:50:48.612907: step 928, loss 0.199548, acc 0.921875, learning_rate 0.000210549
2017-10-10T12:50:49.146083: step 929, loss 0.320581, acc 0.875, learning_rate 0.000210098
2017-10-10T12:50:49.695974: step 930, loss 0.252127, acc 0.90625, learning_rate 0.000209648
2017-10-10T12:50:50.242683: step 931, loss 0.432386, acc 0.84375, learning_rate 0.000209201
2017-10-10T12:50:50.792827: step 932, loss 0.120453, acc 0.984375, learning_rate 0.000208755
2017-10-10T12:50:51.274354: step 933, loss 0.262464, acc 0.9375, learning_rate 0.000208311
2017-10-10T12:50:51.835640: step 934, loss 0.312297, acc 0.859375, learning_rate 0.000207869
2017-10-10T12:50:52.517990: step 935, loss 0.323461, acc 0.921875, learning_rate 0.000207429
2017-10-10T12:50:52.963657: step 936, loss 0.392097, acc 0.828125, learning_rate 0.00020699
2017-10-10T12:50:53.423948: step 937, loss 0.109921, acc 0.953125, learning_rate 0.000206554
2017-10-10T12:50:53.944950: step 938, loss 0.227873, acc 0.90625, learning_rate 0.000206119
2017-10-10T12:50:54.524296: step 939, loss 0.292318, acc 0.90625, learning_rate 0.000205685
2017-10-10T12:50:55.085947: step 940, loss 0.362202, acc 0.859375, learning_rate 0.000205254
2017-10-10T12:50:55.616860: step 941, loss 0.23701, acc 0.9375, learning_rate 0.000204824
2017-10-10T12:50:56.147830: step 942, loss 0.442313, acc 0.828125, learning_rate 0.000204397
2017-10-10T12:50:56.656925: step 943, loss 0.358384, acc 0.859375, learning_rate 0.00020397
2017-10-10T12:50:57.256152: step 944, loss 0.419751, acc 0.875, learning_rate 0.000203546
2017-10-10T12:50:57.657524: step 945, loss 0.179226, acc 0.90625, learning_rate 0.000203123
2017-10-10T12:50:58.126337: step 946, loss 0.523357, acc 0.84375, learning_rate 0.000202702
2017-10-10T12:50:58.641320: step 947, loss 0.264561, acc 0.890625, learning_rate 0.000202283
2017-10-10T12:50:59.087627: step 948, loss 0.194068, acc 0.9375, learning_rate 0.000201866
2017-10-10T12:50:59.592920: step 949, loss 0.319702, acc 0.890625, learning_rate 0.00020145
2017-10-10T12:51:00.103629: step 950, loss 0.276843, acc 0.921875, learning_rate 0.000201036
2017-10-10T12:51:00.584885: step 951, loss 0.35861, acc 0.875, learning_rate 0.000200623
2017-10-10T12:51:01.162046: step 952, loss 0.188652, acc 0.921875, learning_rate 0.000200213
2017-10-10T12:51:01.659085: step 953, loss 0.354658, acc 0.890625, learning_rate 0.000199804
2017-10-10T12:51:02.183347: step 954, loss 0.305844, acc 0.90625, learning_rate 0.000199396
2017-10-10T12:51:02.708879: step 955, loss 0.493563, acc 0.828125, learning_rate 0.000198991
2017-10-10T12:51:03.208911: step 956, loss 0.261987, acc 0.90625, learning_rate 0.000198587
2017-10-10T12:51:03.697152: step 957, loss 0.34357, acc 0.875, learning_rate 0.000198184
2017-10-10T12:51:04.213530: step 958, loss 0.182457, acc 0.9375, learning_rate 0.000197783
2017-10-10T12:51:04.677051: step 959, loss 0.214013, acc 0.921875, learning_rate 0.000197384
2017-10-10T12:51:05.132845: step 960, loss 0.466799, acc 0.875, learning_rate 0.000196987

Evaluation:
2017-10-10T12:51:06.255467: step 960, loss 0.256251, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-960

2017-10-10T12:51:07.796631: step 961, loss 0.335689, acc 0.921875, learning_rate 0.000196591
2017-10-10T12:51:08.360949: step 962, loss 0.2851, acc 0.875, learning_rate 0.000196197
2017-10-10T12:51:08.912908: step 963, loss 0.289104, acc 0.921875, learning_rate 0.000195804
2017-10-10T12:51:09.389087: step 964, loss 0.335997, acc 0.875, learning_rate 0.000195413
2017-10-10T12:51:09.775277: step 965, loss 0.182649, acc 0.921875, learning_rate 0.000195023
2017-10-10T12:51:10.173637: step 966, loss 0.215908, acc 0.9375, learning_rate 0.000194636
2017-10-10T12:51:10.656054: step 967, loss 0.27464, acc 0.90625, learning_rate 0.000194249
2017-10-10T12:51:11.158021: step 968, loss 0.486507, acc 0.828125, learning_rate 0.000193865
2017-10-10T12:51:11.692058: step 969, loss 0.317543, acc 0.875, learning_rate 0.000193482
2017-10-10T12:51:12.237062: step 970, loss 0.271611, acc 0.953125, learning_rate 0.0001931
2017-10-10T12:51:12.689526: step 971, loss 0.346236, acc 0.9375, learning_rate 0.00019272
2017-10-10T12:51:13.212902: step 972, loss 0.207615, acc 0.890625, learning_rate 0.000192341
2017-10-10T12:51:13.747233: step 973, loss 0.249726, acc 0.90625, learning_rate 0.000191965
2017-10-10T12:51:14.216431: step 974, loss 0.526359, acc 0.859375, learning_rate 0.000191589
2017-10-10T12:51:14.772666: step 975, loss 0.256909, acc 0.875, learning_rate 0.000191215
2017-10-10T12:51:15.271454: step 976, loss 0.17374, acc 0.921875, learning_rate 0.000190843
2017-10-10T12:51:15.728249: step 977, loss 0.255645, acc 0.890625, learning_rate 0.000190472
2017-10-10T12:51:16.156821: step 978, loss 0.340651, acc 0.875, learning_rate 0.000190103
2017-10-10T12:51:16.545023: step 979, loss 0.194933, acc 0.953125, learning_rate 0.000189735
2017-10-10T12:51:16.933002: step 980, loss 0.24, acc 0.901961, learning_rate 0.000189369
2017-10-10T12:51:17.451380: step 981, loss 0.377507, acc 0.828125, learning_rate 0.000189004
2017-10-10T12:51:18.021341: step 982, loss 0.323318, acc 0.859375, learning_rate 0.000188641
2017-10-10T12:51:18.469120: step 983, loss 0.170019, acc 0.921875, learning_rate 0.000188279
2017-10-10T12:51:19.072941: step 984, loss 0.252195, acc 0.90625, learning_rate 0.000187919
2017-10-10T12:51:19.591218: step 985, loss 0.266999, acc 0.875, learning_rate 0.00018756
2017-10-10T12:51:20.124963: step 986, loss 0.226917, acc 0.890625, learning_rate 0.000187202
2017-10-10T12:51:20.688879: step 987, loss 0.261876, acc 0.90625, learning_rate 0.000186846
2017-10-10T12:51:21.111668: step 988, loss 0.260268, acc 0.90625, learning_rate 0.000186492
2017-10-10T12:51:21.564766: step 989, loss 0.263875, acc 0.921875, learning_rate 0.000186139
2017-10-10T12:51:22.113064: step 990, loss 0.238687, acc 0.890625, learning_rate 0.000185787
2017-10-10T12:51:22.632956: step 991, loss 0.226255, acc 0.9375, learning_rate 0.000185437
2017-10-10T12:51:23.184862: step 992, loss 0.266175, acc 0.890625, learning_rate 0.000185088
2017-10-10T12:51:23.704952: step 993, loss 0.194089, acc 0.9375, learning_rate 0.000184741
2017-10-10T12:51:24.205233: step 994, loss 0.178613, acc 0.9375, learning_rate 0.000184395
2017-10-10T12:51:24.695617: step 995, loss 0.346409, acc 0.890625, learning_rate 0.000184051
2017-10-10T12:51:25.173096: step 996, loss 0.26226, acc 0.890625, learning_rate 0.000183708
2017-10-10T12:51:25.747719: step 997, loss 0.334508, acc 0.859375, learning_rate 0.000183366
2017-10-10T12:51:26.265054: step 998, loss 0.288213, acc 0.859375, learning_rate 0.000183026
2017-10-10T12:51:26.837314: step 999, loss 0.340495, acc 0.90625, learning_rate 0.000182687
2017-10-10T12:51:27.305022: step 1000, loss 0.225848, acc 0.921875, learning_rate 0.000182349

Evaluation:
2017-10-10T12:51:28.392960: step 1000, loss 0.2569, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1000

2017-10-10T12:51:30.073039: step 1001, loss 0.25742, acc 0.921875, learning_rate 0.000182013
2017-10-10T12:51:30.585009: step 1002, loss 0.176671, acc 0.9375, learning_rate 0.000181678
2017-10-10T12:51:31.164952: step 1003, loss 0.243975, acc 0.890625, learning_rate 0.000181345
2017-10-10T12:51:31.720863: step 1004, loss 0.332719, acc 0.875, learning_rate 0.000181013
2017-10-10T12:51:32.102801: step 1005, loss 0.242425, acc 0.921875, learning_rate 0.000180682
2017-10-10T12:51:32.518577: step 1006, loss 0.352863, acc 0.90625, learning_rate 0.000180353
2017-10-10T12:51:33.020868: step 1007, loss 0.301632, acc 0.890625, learning_rate 0.000180025
2017-10-10T12:51:33.632208: step 1008, loss 0.320473, acc 0.921875, learning_rate 0.000179698
2017-10-10T12:51:34.140990: step 1009, loss 0.305178, acc 0.90625, learning_rate 0.000179373
2017-10-10T12:51:34.649196: step 1010, loss 0.221919, acc 0.9375, learning_rate 0.000179049
2017-10-10T12:51:35.231425: step 1011, loss 0.372817, acc 0.890625, learning_rate 0.000178726
2017-10-10T12:51:35.772954: step 1012, loss 0.157565, acc 0.921875, learning_rate 0.000178405
2017-10-10T12:51:36.300864: step 1013, loss 0.237459, acc 0.90625, learning_rate 0.000178085
2017-10-10T12:51:36.834080: step 1014, loss 0.210845, acc 0.90625, learning_rate 0.000177766
2017-10-10T12:51:37.400352: step 1015, loss 0.361408, acc 0.875, learning_rate 0.000177449
2017-10-10T12:51:38.032860: step 1016, loss 0.251508, acc 0.9375, learning_rate 0.000177133
2017-10-10T12:51:38.440842: step 1017, loss 0.220258, acc 0.9375, learning_rate 0.000176818
2017-10-10T12:51:38.836873: step 1018, loss 0.316162, acc 0.890625, learning_rate 0.000176504
2017-10-10T12:51:39.333256: step 1019, loss 0.276388, acc 0.90625, learning_rate 0.000176192
2017-10-10T12:51:39.833022: step 1020, loss 0.219634, acc 0.921875, learning_rate 0.000175881
2017-10-10T12:51:40.396983: step 1021, loss 0.246865, acc 0.96875, learning_rate 0.000175571
2017-10-10T12:51:40.981254: step 1022, loss 0.357101, acc 0.875, learning_rate 0.000175263
2017-10-10T12:51:41.488917: step 1023, loss 0.257761, acc 0.9375, learning_rate 0.000174956
2017-10-10T12:51:42.057083: step 1024, loss 0.215204, acc 0.90625, learning_rate 0.00017465
2017-10-10T12:51:42.549787: step 1025, loss 0.309059, acc 0.90625, learning_rate 0.000174345
2017-10-10T12:51:43.152850: step 1026, loss 0.230986, acc 0.890625, learning_rate 0.000174042
2017-10-10T12:51:43.642880: step 1027, loss 0.308182, acc 0.90625, learning_rate 0.000173739
2017-10-10T12:51:44.068857: step 1028, loss 0.175376, acc 0.96875, learning_rate 0.000173438
2017-10-10T12:51:44.526938: step 1029, loss 0.13924, acc 0.96875, learning_rate 0.000173139
2017-10-10T12:51:45.088900: step 1030, loss 0.352462, acc 0.9375, learning_rate 0.00017284
2017-10-10T12:51:45.601649: step 1031, loss 0.316354, acc 0.90625, learning_rate 0.000172543
2017-10-10T12:51:46.132994: step 1032, loss 0.34835, acc 0.890625, learning_rate 0.000172247
2017-10-10T12:51:46.689097: step 1033, loss 0.351058, acc 0.875, learning_rate 0.000171952
2017-10-10T12:51:47.176870: step 1034, loss 0.25352, acc 0.921875, learning_rate 0.000171658
2017-10-10T12:51:47.769982: step 1035, loss 0.333687, acc 0.84375, learning_rate 0.000171366
2017-10-10T12:51:48.335541: step 1036, loss 0.398868, acc 0.859375, learning_rate 0.000171074
2017-10-10T12:51:48.852849: step 1037, loss 0.349952, acc 0.859375, learning_rate 0.000170784
2017-10-10T12:51:49.354740: step 1038, loss 0.267247, acc 0.9375, learning_rate 0.000170495
2017-10-10T12:51:49.844920: step 1039, loss 0.372721, acc 0.875, learning_rate 0.000170208
2017-10-10T12:51:50.396850: step 1040, loss 0.420302, acc 0.875, learning_rate 0.000169921

Evaluation:
2017-10-10T12:51:51.588969: step 1040, loss 0.254371, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1040

2017-10-10T12:51:53.072863: step 1041, loss 0.25758, acc 0.921875, learning_rate 0.000169636
2017-10-10T12:51:53.611490: step 1042, loss 0.132776, acc 0.953125, learning_rate 0.000169351
2017-10-10T12:51:54.236053: step 1043, loss 0.24962, acc 0.9375, learning_rate 0.000169068
2017-10-10T12:51:54.818910: step 1044, loss 0.271342, acc 0.875, learning_rate 0.000168786
2017-10-10T12:51:55.207949: step 1045, loss 0.379437, acc 0.859375, learning_rate 0.000168506
2017-10-10T12:51:55.577045: step 1046, loss 0.345549, acc 0.890625, learning_rate 0.000168226
2017-10-10T12:51:56.069156: step 1047, loss 0.423554, acc 0.875, learning_rate 0.000167947
2017-10-10T12:51:56.594744: step 1048, loss 0.176793, acc 0.921875, learning_rate 0.00016767
2017-10-10T12:51:57.100027: step 1049, loss 0.365268, acc 0.859375, learning_rate 0.000167394
2017-10-10T12:51:57.652876: step 1050, loss 0.279668, acc 0.90625, learning_rate 0.000167119
2017-10-10T12:51:58.156930: step 1051, loss 0.149817, acc 0.921875, learning_rate 0.000166845
2017-10-10T12:51:58.705676: step 1052, loss 0.341563, acc 0.859375, learning_rate 0.000166572
2017-10-10T12:51:59.188300: step 1053, loss 0.222905, acc 0.921875, learning_rate 0.0001663
2017-10-10T12:51:59.760068: step 1054, loss 0.112699, acc 0.984375, learning_rate 0.00016603
2017-10-10T12:52:00.324924: step 1055, loss 0.416134, acc 0.890625, learning_rate 0.00016576
2017-10-10T12:52:00.908979: step 1056, loss 0.290185, acc 0.921875, learning_rate 0.000165492
2017-10-10T12:52:01.495934: step 1057, loss 0.267344, acc 0.90625, learning_rate 0.000165224
2017-10-10T12:52:01.996444: step 1058, loss 0.40719, acc 0.875, learning_rate 0.000164958
2017-10-10T12:52:02.460136: step 1059, loss 0.375303, acc 0.90625, learning_rate 0.000164693
2017-10-10T12:52:02.992575: step 1060, loss 0.325924, acc 0.859375, learning_rate 0.000164429
2017-10-10T12:52:03.548972: step 1061, loss 0.354434, acc 0.9375, learning_rate 0.000164166
2017-10-10T12:52:04.112412: step 1062, loss 0.276585, acc 0.890625, learning_rate 0.000163904
2017-10-10T12:52:04.668942: step 1063, loss 0.299948, acc 0.859375, learning_rate 0.000163643
2017-10-10T12:52:05.224721: step 1064, loss 0.180602, acc 0.953125, learning_rate 0.000163383
2017-10-10T12:52:05.821047: step 1065, loss 0.233915, acc 0.9375, learning_rate 0.000163125
2017-10-10T12:52:06.472876: step 1066, loss 0.246449, acc 0.921875, learning_rate 0.000162867
2017-10-10T12:52:06.928839: step 1067, loss 0.225277, acc 0.9375, learning_rate 0.00016261
2017-10-10T12:52:07.392848: step 1068, loss 0.284717, acc 0.90625, learning_rate 0.000162355
2017-10-10T12:52:07.942318: step 1069, loss 0.294845, acc 0.921875, learning_rate 0.0001621
2017-10-10T12:52:08.485197: step 1070, loss 0.312668, acc 0.890625, learning_rate 0.000161847
2017-10-10T12:52:08.984963: step 1071, loss 0.478201, acc 0.828125, learning_rate 0.000161594
2017-10-10T12:52:09.522638: step 1072, loss 0.218065, acc 0.90625, learning_rate 0.000161343
2017-10-10T12:52:10.012952: step 1073, loss 0.350333, acc 0.890625, learning_rate 0.000161093
2017-10-10T12:52:10.605123: step 1074, loss 0.234424, acc 0.90625, learning_rate 0.000160843
2017-10-10T12:52:11.031914: step 1075, loss 0.285923, acc 0.90625, learning_rate 0.000160595
2017-10-10T12:52:11.559994: step 1076, loss 0.225164, acc 0.90625, learning_rate 0.000160348
2017-10-10T12:52:12.133242: step 1077, loss 0.143648, acc 0.96875, learning_rate 0.000160101
2017-10-10T12:52:12.548852: step 1078, loss 0.200174, acc 0.941176, learning_rate 0.000159856
2017-10-10T12:52:13.131674: step 1079, loss 0.371817, acc 0.859375, learning_rate 0.000159612
2017-10-10T12:52:13.677011: step 1080, loss 0.195, acc 0.890625, learning_rate 0.000159368

Evaluation:
2017-10-10T12:52:14.843001: step 1080, loss 0.253938, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1080

2017-10-10T12:52:16.380939: step 1081, loss 0.333965, acc 0.859375, learning_rate 0.000159126
2017-10-10T12:52:16.964883: step 1082, loss 0.22788, acc 0.890625, learning_rate 0.000158885
2017-10-10T12:52:17.544891: step 1083, loss 0.534748, acc 0.859375, learning_rate 0.000158644
2017-10-10T12:52:17.878638: step 1084, loss 0.213151, acc 0.921875, learning_rate 0.000158405
2017-10-10T12:52:18.260737: step 1085, loss 0.208834, acc 0.953125, learning_rate 0.000158167
2017-10-10T12:52:18.701639: step 1086, loss 0.117966, acc 0.984375, learning_rate 0.000157929
2017-10-10T12:52:19.233356: step 1087, loss 0.150021, acc 0.953125, learning_rate 0.000157693
2017-10-10T12:52:19.812827: step 1088, loss 0.287376, acc 0.875, learning_rate 0.000157457
2017-10-10T12:52:20.328361: step 1089, loss 0.286075, acc 0.890625, learning_rate 0.000157223
2017-10-10T12:52:20.864343: step 1090, loss 0.317354, acc 0.859375, learning_rate 0.000156989
2017-10-10T12:52:21.431231: step 1091, loss 0.364802, acc 0.90625, learning_rate 0.000156757
2017-10-10T12:52:21.988820: step 1092, loss 0.294863, acc 0.90625, learning_rate 0.000156525
2017-10-10T12:52:22.535548: step 1093, loss 0.234857, acc 0.921875, learning_rate 0.000156294
2017-10-10T12:52:23.100974: step 1094, loss 0.409365, acc 0.84375, learning_rate 0.000156064
2017-10-10T12:52:23.665599: step 1095, loss 0.179913, acc 0.9375, learning_rate 0.000155836
2017-10-10T12:52:24.200670: step 1096, loss 0.376793, acc 0.875, learning_rate 0.000155608
2017-10-10T12:52:24.644914: step 1097, loss 0.119875, acc 0.953125, learning_rate 0.000155381
2017-10-10T12:52:25.103057: step 1098, loss 0.234631, acc 0.921875, learning_rate 0.000155155
2017-10-10T12:52:25.523393: step 1099, loss 0.24109, acc 0.90625, learning_rate 0.000154929
2017-10-10T12:52:26.087976: step 1100, loss 0.634391, acc 0.84375, learning_rate 0.000154705
2017-10-10T12:52:26.590282: step 1101, loss 0.423495, acc 0.90625, learning_rate 0.000154482
2017-10-10T12:52:27.108125: step 1102, loss 0.286883, acc 0.90625, learning_rate 0.00015426
2017-10-10T12:52:27.616855: step 1103, loss 0.42521, acc 0.859375, learning_rate 0.000154038
2017-10-10T12:52:28.156880: step 1104, loss 0.294716, acc 0.859375, learning_rate 0.000153818
2017-10-10T12:52:28.780921: step 1105, loss 0.245962, acc 0.921875, learning_rate 0.000153598
2017-10-10T12:52:29.298850: step 1106, loss 0.513422, acc 0.8125, learning_rate 0.000153379
2017-10-10T12:52:29.701461: step 1107, loss 0.455712, acc 0.859375, learning_rate 0.000153161
2017-10-10T12:52:30.117207: step 1108, loss 0.466678, acc 0.90625, learning_rate 0.000152944
2017-10-10T12:52:30.557720: step 1109, loss 0.164005, acc 0.953125, learning_rate 0.000152728
2017-10-10T12:52:31.131169: step 1110, loss 0.192817, acc 0.90625, learning_rate 0.000152513
2017-10-10T12:52:31.668917: step 1111, loss 0.174083, acc 0.953125, learning_rate 0.000152299
2017-10-10T12:52:32.280159: step 1112, loss 0.23617, acc 0.890625, learning_rate 0.000152085
2017-10-10T12:52:32.779181: step 1113, loss 0.176625, acc 0.953125, learning_rate 0.000151872
2017-10-10T12:52:33.236857: step 1114, loss 0.379567, acc 0.84375, learning_rate 0.000151661
2017-10-10T12:52:33.724416: step 1115, loss 0.279227, acc 0.875, learning_rate 0.00015145
2017-10-10T12:52:34.304910: step 1116, loss 0.324889, acc 0.921875, learning_rate 0.00015124
2017-10-10T12:52:34.861424: step 1117, loss 0.267686, acc 0.890625, learning_rate 0.000151031
2017-10-10T12:52:35.378377: step 1118, loss 0.2199, acc 0.90625, learning_rate 0.000150822
2017-10-10T12:52:35.907200: step 1119, loss 0.388597, acc 0.828125, learning_rate 0.000150615
2017-10-10T12:52:36.478869: step 1120, loss 0.305851, acc 0.890625, learning_rate 0.000150408

Evaluation:
2017-10-10T12:52:37.739773: step 1120, loss 0.256434, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1120

2017-10-10T12:52:39.519992: step 1121, loss 0.397016, acc 0.859375, learning_rate 0.000150203
2017-10-10T12:52:40.085163: step 1122, loss 0.157039, acc 0.96875, learning_rate 0.000149998
2017-10-10T12:52:40.569063: step 1123, loss 0.235398, acc 0.90625, learning_rate 0.000149794
2017-10-10T12:52:41.126441: step 1124, loss 0.337974, acc 0.828125, learning_rate 0.00014959
2017-10-10T12:52:41.513374: step 1125, loss 0.27836, acc 0.90625, learning_rate 0.000149388
2017-10-10T12:52:42.049151: step 1126, loss 0.404963, acc 0.875, learning_rate 0.000149186
2017-10-10T12:52:42.545695: step 1127, loss 0.309292, acc 0.890625, learning_rate 0.000148986
2017-10-10T12:52:42.992481: step 1128, loss 0.269367, acc 0.9375, learning_rate 0.000148786
2017-10-10T12:52:43.468607: step 1129, loss 0.373076, acc 0.890625, learning_rate 0.000148587
2017-10-10T12:52:44.001392: step 1130, loss 0.263826, acc 0.90625, learning_rate 0.000148388
2017-10-10T12:52:44.536022: step 1131, loss 0.426758, acc 0.84375, learning_rate 0.000148191
2017-10-10T12:52:45.081108: step 1132, loss 0.384629, acc 0.90625, learning_rate 0.000147994
2017-10-10T12:52:45.632102: step 1133, loss 0.231622, acc 0.9375, learning_rate 0.000147798
2017-10-10T12:52:46.164901: step 1134, loss 0.477574, acc 0.859375, learning_rate 0.000147603
2017-10-10T12:52:46.706332: step 1135, loss 0.203329, acc 0.953125, learning_rate 0.000147409
2017-10-10T12:52:47.275471: step 1136, loss 0.251945, acc 0.890625, learning_rate 0.000147215
2017-10-10T12:52:47.751166: step 1137, loss 0.202642, acc 0.9375, learning_rate 0.000147022
2017-10-10T12:52:48.209030: step 1138, loss 0.191848, acc 0.953125, learning_rate 0.000146831
2017-10-10T12:52:48.748829: step 1139, loss 0.305462, acc 0.875, learning_rate 0.000146639
2017-10-10T12:52:49.299217: step 1140, loss 0.305647, acc 0.859375, learning_rate 0.000146449
2017-10-10T12:52:49.850831: step 1141, loss 0.293884, acc 0.890625, learning_rate 0.000146259
2017-10-10T12:52:50.383725: step 1142, loss 0.233206, acc 0.921875, learning_rate 0.000146071
2017-10-10T12:52:50.918161: step 1143, loss 0.247515, acc 0.90625, learning_rate 0.000145883
2017-10-10T12:52:51.373032: step 1144, loss 0.209668, acc 0.90625, learning_rate 0.000145695
2017-10-10T12:52:51.993893: step 1145, loss 0.574392, acc 0.8125, learning_rate 0.000145509
2017-10-10T12:52:52.552807: step 1146, loss 0.251287, acc 0.921875, learning_rate 0.000145323
2017-10-10T12:52:53.024026: step 1147, loss 0.22127, acc 0.875, learning_rate 0.000145138
2017-10-10T12:52:53.465471: step 1148, loss 0.262661, acc 0.90625, learning_rate 0.000144954
2017-10-10T12:52:53.984840: step 1149, loss 0.419988, acc 0.84375, learning_rate 0.00014477
2017-10-10T12:52:54.564792: step 1150, loss 0.235889, acc 0.90625, learning_rate 0.000144588
2017-10-10T12:52:55.126510: step 1151, loss 0.322179, acc 0.875, learning_rate 0.000144406
2017-10-10T12:52:55.588833: step 1152, loss 0.352038, acc 0.890625, learning_rate 0.000144224
2017-10-10T12:52:56.118017: step 1153, loss 0.232654, acc 0.875, learning_rate 0.000144044
2017-10-10T12:52:56.697062: step 1154, loss 0.189322, acc 0.953125, learning_rate 0.000143864
2017-10-10T12:52:57.212860: step 1155, loss 0.166352, acc 0.96875, learning_rate 0.000143685
2017-10-10T12:52:57.783888: step 1156, loss 0.155132, acc 0.9375, learning_rate 0.000143507
2017-10-10T12:52:58.319909: step 1157, loss 0.340123, acc 0.859375, learning_rate 0.000143329
2017-10-10T12:52:58.840878: step 1158, loss 0.184589, acc 0.9375, learning_rate 0.000143152
2017-10-10T12:52:59.388911: step 1159, loss 0.148579, acc 0.953125, learning_rate 0.000142976
2017-10-10T12:52:59.938278: step 1160, loss 0.166088, acc 0.921875, learning_rate 0.000142801

Evaluation:
2017-10-10T12:53:01.094463: step 1160, loss 0.254399, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1160

2017-10-10T12:53:02.528935: step 1161, loss 0.245289, acc 0.9375, learning_rate 0.000142626
2017-10-10T12:53:03.149122: step 1162, loss 0.218971, acc 0.921875, learning_rate 0.000142452
2017-10-10T12:53:03.816989: step 1163, loss 0.237696, acc 0.890625, learning_rate 0.000142279
2017-10-10T12:53:04.289471: step 1164, loss 0.341048, acc 0.921875, learning_rate 0.000142106
2017-10-10T12:53:04.682381: step 1165, loss 0.225184, acc 0.9375, learning_rate 0.000141934
2017-10-10T12:53:05.153527: step 1166, loss 0.154358, acc 0.96875, learning_rate 0.000141763
2017-10-10T12:53:05.672717: step 1167, loss 0.284974, acc 0.921875, learning_rate 0.000141593
2017-10-10T12:53:06.182514: step 1168, loss 0.317931, acc 0.890625, learning_rate 0.000141423
2017-10-10T12:53:06.701220: step 1169, loss 0.145924, acc 0.953125, learning_rate 0.000141254
2017-10-10T12:53:07.248415: step 1170, loss 0.275277, acc 0.90625, learning_rate 0.000141085
2017-10-10T12:53:07.774693: step 1171, loss 0.12725, acc 0.96875, learning_rate 0.000140918
2017-10-10T12:53:08.270528: step 1172, loss 0.283369, acc 0.9375, learning_rate 0.000140751
2017-10-10T12:53:08.746211: step 1173, loss 0.215079, acc 0.921875, learning_rate 0.000140584
2017-10-10T12:53:09.316603: step 1174, loss 0.245982, acc 0.921875, learning_rate 0.000140419
2017-10-10T12:53:09.853064: step 1175, loss 0.176673, acc 0.921875, learning_rate 0.000140254
2017-10-10T12:53:10.433332: step 1176, loss 0.176547, acc 0.941176, learning_rate 0.000140089
2017-10-10T12:53:10.900799: step 1177, loss 0.141969, acc 0.953125, learning_rate 0.000139926
2017-10-10T12:53:11.299272: step 1178, loss 0.253939, acc 0.921875, learning_rate 0.000139763
2017-10-10T12:53:11.773098: step 1179, loss 0.241088, acc 0.90625, learning_rate 0.0001396
2017-10-10T12:53:12.285444: step 1180, loss 0.232639, acc 0.90625, learning_rate 0.000139439
2017-10-10T12:53:12.764945: step 1181, loss 0.131339, acc 0.953125, learning_rate 0.000139278
2017-10-10T12:53:13.291504: step 1182, loss 0.244607, acc 0.90625, learning_rate 0.000139118
2017-10-10T12:53:13.838978: step 1183, loss 0.142617, acc 0.96875, learning_rate 0.000138958
2017-10-10T12:53:14.342483: step 1184, loss 0.190157, acc 0.953125, learning_rate 0.000138799
2017-10-10T12:53:14.929145: step 1185, loss 0.277414, acc 0.890625, learning_rate 0.00013864
2017-10-10T12:53:15.498541: step 1186, loss 0.107786, acc 0.96875, learning_rate 0.000138483
2017-10-10T12:53:15.945283: step 1187, loss 0.490271, acc 0.796875, learning_rate 0.000138326
2017-10-10T12:53:16.352339: step 1188, loss 0.411662, acc 0.859375, learning_rate 0.000138169
2017-10-10T12:53:16.808955: step 1189, loss 0.299887, acc 0.875, learning_rate 0.000138013
2017-10-10T12:53:17.373182: step 1190, loss 0.224859, acc 0.890625, learning_rate 0.000137858
2017-10-10T12:53:17.904918: step 1191, loss 0.206057, acc 0.90625, learning_rate 0.000137704
2017-10-10T12:53:18.458333: step 1192, loss 0.423267, acc 0.875, learning_rate 0.00013755
2017-10-10T12:53:18.922076: step 1193, loss 0.180274, acc 0.9375, learning_rate 0.000137397
2017-10-10T12:53:19.431723: step 1194, loss 0.293339, acc 0.90625, learning_rate 0.000137244
2017-10-10T12:53:19.956832: step 1195, loss 0.289071, acc 0.875, learning_rate 0.000137092
2017-10-10T12:53:20.464861: step 1196, loss 0.329767, acc 0.90625, learning_rate 0.000136941
2017-10-10T12:53:21.019142: step 1197, loss 0.285028, acc 0.875, learning_rate 0.00013679
2017-10-10T12:53:21.516868: step 1198, loss 0.508185, acc 0.828125, learning_rate 0.00013664
2017-10-10T12:53:22.069385: step 1199, loss 0.275836, acc 0.890625, learning_rate 0.00013649
2017-10-10T12:53:22.615137: step 1200, loss 0.274079, acc 0.90625, learning_rate 0.000136341

Evaluation:
2017-10-10T12:53:23.752832: step 1200, loss 0.253488, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1200

2017-10-10T12:53:25.435279: step 1201, loss 0.273661, acc 0.90625, learning_rate 0.000136193
2017-10-10T12:53:26.016198: step 1202, loss 0.217242, acc 0.9375, learning_rate 0.000136045
2017-10-10T12:53:26.504904: step 1203, loss 0.16329, acc 0.953125, learning_rate 0.000135898
2017-10-10T12:53:26.926415: step 1204, loss 0.26305, acc 0.921875, learning_rate 0.000135751
2017-10-10T12:53:27.329410: step 1205, loss 0.428589, acc 0.828125, learning_rate 0.000135605
2017-10-10T12:53:27.856871: step 1206, loss 0.293272, acc 0.90625, learning_rate 0.00013546
2017-10-10T12:53:28.402247: step 1207, loss 0.223855, acc 0.921875, learning_rate 0.000135315
2017-10-10T12:53:28.902034: step 1208, loss 0.257677, acc 0.9375, learning_rate 0.000135171
2017-10-10T12:53:29.403280: step 1209, loss 0.283557, acc 0.921875, learning_rate 0.000135028
2017-10-10T12:53:29.912957: step 1210, loss 0.258948, acc 0.890625, learning_rate 0.000134885
2017-10-10T12:53:30.474849: step 1211, loss 0.405506, acc 0.859375, learning_rate 0.000134742
2017-10-10T12:53:31.004861: step 1212, loss 0.271694, acc 0.890625, learning_rate 0.0001346
2017-10-10T12:53:31.544995: step 1213, loss 0.237435, acc 0.890625, learning_rate 0.000134459
2017-10-10T12:53:32.084889: step 1214, loss 0.274468, acc 0.921875, learning_rate 0.000134319
2017-10-10T12:53:32.574367: step 1215, loss 0.580456, acc 0.796875, learning_rate 0.000134178
2017-10-10T12:53:33.257610: step 1216, loss 0.189709, acc 0.9375, learning_rate 0.000134039
2017-10-10T12:53:33.743798: step 1217, loss 0.244874, acc 0.890625, learning_rate 0.0001339
2017-10-10T12:53:34.197115: step 1218, loss 0.336963, acc 0.890625, learning_rate 0.000133762
2017-10-10T12:53:34.653055: step 1219, loss 0.218316, acc 0.9375, learning_rate 0.000133624
2017-10-10T12:53:35.223548: step 1220, loss 0.227178, acc 0.9375, learning_rate 0.000133487
2017-10-10T12:53:35.763579: step 1221, loss 0.139525, acc 0.984375, learning_rate 0.00013335
2017-10-10T12:53:36.308841: step 1222, loss 0.179139, acc 0.953125, learning_rate 0.000133214
2017-10-10T12:53:36.866847: step 1223, loss 0.239079, acc 0.90625, learning_rate 0.000133078
2017-10-10T12:53:37.406523: step 1224, loss 0.299984, acc 0.890625, learning_rate 0.000132943
2017-10-10T12:53:38.028727: step 1225, loss 0.228173, acc 0.90625, learning_rate 0.000132809
2017-10-10T12:53:38.612932: step 1226, loss 0.201715, acc 0.921875, learning_rate 0.000132675
2017-10-10T12:53:39.198019: step 1227, loss 0.319273, acc 0.90625, learning_rate 0.000132541
2017-10-10T12:53:39.588750: step 1228, loss 0.249659, acc 0.921875, learning_rate 0.000132409
2017-10-10T12:53:40.091727: step 1229, loss 0.296731, acc 0.890625, learning_rate 0.000132276
2017-10-10T12:53:40.655879: step 1230, loss 0.36014, acc 0.859375, learning_rate 0.000132145
2017-10-10T12:53:41.196845: step 1231, loss 0.219559, acc 0.890625, learning_rate 0.000132013
2017-10-10T12:53:41.734237: step 1232, loss 0.228207, acc 0.9375, learning_rate 0.000131883
2017-10-10T12:53:42.255381: step 1233, loss 0.445355, acc 0.796875, learning_rate 0.000131753
2017-10-10T12:53:42.804589: step 1234, loss 0.10057, acc 0.953125, learning_rate 0.000131623
2017-10-10T12:53:43.375873: step 1235, loss 0.177431, acc 0.953125, learning_rate 0.000131494
2017-10-10T12:53:43.923941: step 1236, loss 0.346495, acc 0.828125, learning_rate 0.000131365
2017-10-10T12:53:44.473137: step 1237, loss 0.248577, acc 0.921875, learning_rate 0.000131237
2017-10-10T12:53:45.033001: step 1238, loss 0.251984, acc 0.921875, learning_rate 0.00013111
2017-10-10T12:53:45.624851: step 1239, loss 0.350798, acc 0.875, learning_rate 0.000130983
2017-10-10T12:53:46.188212: step 1240, loss 0.188063, acc 0.9375, learning_rate 0.000130856

Evaluation:
2017-10-10T12:53:47.357679: step 1240, loss 0.251255, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1240

2017-10-10T12:53:49.209044: step 1241, loss 0.193475, acc 0.9375, learning_rate 0.00013073
2017-10-10T12:53:49.739470: step 1242, loss 0.253821, acc 0.921875, learning_rate 0.000130605
2017-10-10T12:53:50.128372: step 1243, loss 0.204983, acc 0.90625, learning_rate 0.00013048
2017-10-10T12:53:50.497426: step 1244, loss 0.159868, acc 0.921875, learning_rate 0.000130356
2017-10-10T12:53:50.868211: step 1245, loss 0.404944, acc 0.84375, learning_rate 0.000130232
2017-10-10T12:53:51.452837: step 1246, loss 0.217126, acc 0.921875, learning_rate 0.000130108
2017-10-10T12:53:52.076841: step 1247, loss 0.167309, acc 0.9375, learning_rate 0.000129985
2017-10-10T12:53:52.577159: step 1248, loss 0.273489, acc 0.90625, learning_rate 0.000129863
2017-10-10T12:53:53.065781: step 1249, loss 0.265231, acc 0.890625, learning_rate 0.000129741
2017-10-10T12:53:53.588276: step 1250, loss 0.148177, acc 0.953125, learning_rate 0.00012962
2017-10-10T12:53:54.070986: step 1251, loss 0.327874, acc 0.859375, learning_rate 0.000129499
2017-10-10T12:53:54.600896: step 1252, loss 0.288715, acc 0.90625, learning_rate 0.000129378
2017-10-10T12:53:55.115513: step 1253, loss 0.146919, acc 0.953125, learning_rate 0.000129259
2017-10-10T12:53:55.678257: step 1254, loss 0.253372, acc 0.921875, learning_rate 0.000129139
2017-10-10T12:53:56.240693: step 1255, loss 0.0913886, acc 0.96875, learning_rate 0.00012902
2017-10-10T12:53:56.820198: step 1256, loss 0.253625, acc 0.875, learning_rate 0.000128902
2017-10-10T12:53:57.404869: step 1257, loss 0.212766, acc 0.921875, learning_rate 0.000128784
2017-10-10T12:53:57.831494: step 1258, loss 0.23252, acc 0.9375, learning_rate 0.000128666
2017-10-10T12:53:58.268745: step 1259, loss 0.273419, acc 0.90625, learning_rate 0.000128549
2017-10-10T12:53:58.806434: step 1260, loss 0.362232, acc 0.875, learning_rate 0.000128433
2017-10-10T12:53:59.313055: step 1261, loss 0.236185, acc 0.921875, learning_rate 0.000128317
2017-10-10T12:53:59.892876: step 1262, loss 0.178736, acc 0.921875, learning_rate 0.000128201
2017-10-10T12:54:00.432030: step 1263, loss 0.33515, acc 0.875, learning_rate 0.000128086
2017-10-10T12:54:00.888897: step 1264, loss 0.326505, acc 0.890625, learning_rate 0.000127971
2017-10-10T12:54:01.352712: step 1265, loss 0.177621, acc 0.9375, learning_rate 0.000127857
2017-10-10T12:54:01.852970: step 1266, loss 0.537412, acc 0.875, learning_rate 0.000127743
2017-10-10T12:54:02.333076: step 1267, loss 0.204723, acc 0.921875, learning_rate 0.00012763
2017-10-10T12:54:02.797083: step 1268, loss 0.295064, acc 0.921875, learning_rate 0.000127517
2017-10-10T12:54:03.328451: step 1269, loss 0.226764, acc 0.90625, learning_rate 0.000127405
2017-10-10T12:54:03.821190: step 1270, loss 0.263208, acc 0.890625, learning_rate 0.000127293
2017-10-10T12:54:04.392886: step 1271, loss 0.27103, acc 0.921875, learning_rate 0.000127182
2017-10-10T12:54:04.902458: step 1272, loss 0.189179, acc 0.921875, learning_rate 0.000127071
2017-10-10T12:54:05.452874: step 1273, loss 0.299701, acc 0.890625, learning_rate 0.00012696
2017-10-10T12:54:05.856932: step 1274, loss 0.323558, acc 0.862745, learning_rate 0.00012685
2017-10-10T12:54:06.415364: step 1275, loss 0.211241, acc 0.90625, learning_rate 0.000126741
2017-10-10T12:54:07.000752: step 1276, loss 0.164531, acc 0.9375, learning_rate 0.000126632
2017-10-10T12:54:07.584496: step 1277, loss 0.221189, acc 0.90625, learning_rate 0.000126523
2017-10-10T12:54:08.140112: step 1278, loss 0.235395, acc 0.921875, learning_rate 0.000126415
2017-10-10T12:54:08.686695: step 1279, loss 0.465629, acc 0.859375, learning_rate 0.000126307
2017-10-10T12:54:09.243036: step 1280, loss 0.156115, acc 0.953125, learning_rate 0.000126199

Evaluation:
2017-10-10T12:54:10.568932: step 1280, loss 0.253686, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1280

2017-10-10T12:54:11.967289: step 1281, loss 0.218263, acc 0.953125, learning_rate 0.000126093
2017-10-10T12:54:12.639621: step 1282, loss 0.388514, acc 0.875, learning_rate 0.000125986
2017-10-10T12:54:13.121372: step 1283, loss 0.262541, acc 0.90625, learning_rate 0.00012588
2017-10-10T12:54:13.556941: step 1284, loss 0.230381, acc 0.890625, learning_rate 0.000125774
2017-10-10T12:54:13.935128: step 1285, loss 0.168766, acc 0.90625, learning_rate 0.000125669
2017-10-10T12:54:14.394846: step 1286, loss 0.223371, acc 0.9375, learning_rate 0.000125564
2017-10-10T12:54:14.875612: step 1287, loss 0.193255, acc 0.890625, learning_rate 0.00012546
2017-10-10T12:54:15.420856: step 1288, loss 0.390115, acc 0.90625, learning_rate 0.000125356
2017-10-10T12:54:15.964189: step 1289, loss 0.313125, acc 0.875, learning_rate 0.000125253
2017-10-10T12:54:16.525103: step 1290, loss 0.250478, acc 0.9375, learning_rate 0.00012515
2017-10-10T12:54:17.081406: step 1291, loss 0.40255, acc 0.84375, learning_rate 0.000125047
2017-10-10T12:54:17.620814: step 1292, loss 0.297457, acc 0.890625, learning_rate 0.000124945
2017-10-10T12:54:18.153533: step 1293, loss 0.449173, acc 0.796875, learning_rate 0.000124843
2017-10-10T12:54:18.721112: step 1294, loss 0.315635, acc 0.890625, learning_rate 0.000124741
2017-10-10T12:54:19.315514: step 1295, loss 0.220491, acc 0.9375, learning_rate 0.00012464
2017-10-10T12:54:19.923781: step 1296, loss 0.317367, acc 0.875, learning_rate 0.00012454
2017-10-10T12:54:20.505788: step 1297, loss 0.319217, acc 0.921875, learning_rate 0.00012444
2017-10-10T12:54:20.948616: step 1298, loss 0.329545, acc 0.890625, learning_rate 0.00012434
2017-10-10T12:54:21.432353: step 1299, loss 0.162853, acc 0.953125, learning_rate 0.000124241
2017-10-10T12:54:21.944875: step 1300, loss 0.140421, acc 0.953125, learning_rate 0.000124142
2017-10-10T12:54:22.514726: step 1301, loss 0.151483, acc 0.953125, learning_rate 0.000124043
2017-10-10T12:54:22.957014: step 1302, loss 0.265883, acc 0.859375, learning_rate 0.000123945
2017-10-10T12:54:23.415363: step 1303, loss 0.224457, acc 0.90625, learning_rate 0.000123847
2017-10-10T12:54:23.917047: step 1304, loss 0.264686, acc 0.90625, learning_rate 0.00012375
2017-10-10T12:54:24.588846: step 1305, loss 0.336233, acc 0.90625, learning_rate 0.000123653
2017-10-10T12:54:25.132909: step 1306, loss 0.204378, acc 0.953125, learning_rate 0.000123556
2017-10-10T12:54:25.545248: step 1307, loss 0.214827, acc 0.953125, learning_rate 0.00012346
2017-10-10T12:54:26.008896: step 1308, loss 0.140557, acc 0.96875, learning_rate 0.000123364
2017-10-10T12:54:26.570227: step 1309, loss 0.0974027, acc 0.984375, learning_rate 0.000123269
2017-10-10T12:54:27.128908: step 1310, loss 0.236511, acc 0.921875, learning_rate 0.000123174
2017-10-10T12:54:27.630321: step 1311, loss 0.160542, acc 0.953125, learning_rate 0.00012308
2017-10-10T12:54:28.152835: step 1312, loss 0.23295, acc 0.953125, learning_rate 0.000122985
2017-10-10T12:54:28.696841: step 1313, loss 0.344456, acc 0.90625, learning_rate 0.000122892
2017-10-10T12:54:29.293151: step 1314, loss 0.373109, acc 0.875, learning_rate 0.000122798
2017-10-10T12:54:29.799508: step 1315, loss 0.367029, acc 0.890625, learning_rate 0.000122705
2017-10-10T12:54:30.335412: step 1316, loss 0.136521, acc 0.9375, learning_rate 0.000122612
2017-10-10T12:54:30.857048: step 1317, loss 0.121994, acc 0.984375, learning_rate 0.00012252
2017-10-10T12:54:31.414402: step 1318, loss 0.273931, acc 0.921875, learning_rate 0.000122428
2017-10-10T12:54:31.984684: step 1319, loss 0.415011, acc 0.84375, learning_rate 0.000122337
2017-10-10T12:54:32.488892: step 1320, loss 0.244106, acc 0.90625, learning_rate 0.000122245

Evaluation:
2017-10-10T12:54:33.628465: step 1320, loss 0.250797, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1320

2017-10-10T12:54:35.139980: step 1321, loss 0.306786, acc 0.890625, learning_rate 0.000122155
2017-10-10T12:54:35.611805: step 1322, loss 0.515713, acc 0.859375, learning_rate 0.000122064
2017-10-10T12:54:36.110577: step 1323, loss 0.256951, acc 0.90625, learning_rate 0.000121974
2017-10-10T12:54:36.609011: step 1324, loss 0.355452, acc 0.875, learning_rate 0.000121884
2017-10-10T12:54:37.183993: step 1325, loss 0.270789, acc 0.9375, learning_rate 0.000121795
2017-10-10T12:54:37.763601: step 1326, loss 0.200563, acc 0.9375, learning_rate 0.000121706
2017-10-10T12:54:38.328836: step 1327, loss 0.272346, acc 0.90625, learning_rate 0.000121618
2017-10-10T12:54:38.880862: step 1328, loss 0.264131, acc 0.875, learning_rate 0.000121529
2017-10-10T12:54:39.412989: step 1329, loss 0.250048, acc 0.90625, learning_rate 0.000121441
2017-10-10T12:54:39.950614: step 1330, loss 0.273577, acc 0.890625, learning_rate 0.000121354
2017-10-10T12:54:40.471103: step 1331, loss 0.463203, acc 0.859375, learning_rate 0.000121267
2017-10-10T12:54:41.011045: step 1332, loss 0.275615, acc 0.90625, learning_rate 0.00012118
2017-10-10T12:54:41.529797: step 1333, loss 0.175701, acc 0.9375, learning_rate 0.000121093
2017-10-10T12:54:42.027058: step 1334, loss 0.210971, acc 0.890625, learning_rate 0.000121007
2017-10-10T12:54:42.508374: step 1335, loss 0.423159, acc 0.859375, learning_rate 0.000120922
2017-10-10T12:54:43.096917: step 1336, loss 0.206137, acc 0.9375, learning_rate 0.000120836
2017-10-10T12:54:43.725052: step 1337, loss 0.112449, acc 0.984375, learning_rate 0.000120751
2017-10-10T12:54:44.189974: step 1338, loss 0.338418, acc 0.890625, learning_rate 0.000120666
2017-10-10T12:54:44.644949: step 1339, loss 0.226641, acc 0.890625, learning_rate 0.000120582
2017-10-10T12:54:45.150589: step 1340, loss 0.309401, acc 0.875, learning_rate 0.000120498
2017-10-10T12:54:45.704161: step 1341, loss 0.272506, acc 0.9375, learning_rate 0.000120414
2017-10-10T12:54:46.263295: step 1342, loss 0.308773, acc 0.890625, learning_rate 0.000120331
2017-10-10T12:54:46.764844: step 1343, loss 0.194096, acc 0.953125, learning_rate 0.000120248
2017-10-10T12:54:47.316927: step 1344, loss 0.219854, acc 0.921875, learning_rate 0.000120165
2017-10-10T12:54:47.952921: step 1345, loss 0.157839, acc 0.9375, learning_rate 0.000120083
2017-10-10T12:54:48.364885: step 1346, loss 0.20294, acc 0.890625, learning_rate 0.000120001
2017-10-10T12:54:48.816545: step 1347, loss 0.254344, acc 0.921875, learning_rate 0.00011992
2017-10-10T12:54:49.288962: step 1348, loss 0.343701, acc 0.84375, learning_rate 0.000119838
2017-10-10T12:54:49.789107: step 1349, loss 0.230867, acc 0.953125, learning_rate 0.000119757
2017-10-10T12:54:50.301038: step 1350, loss 0.33608, acc 0.875, learning_rate 0.000119677
2017-10-10T12:54:50.776846: step 1351, loss 0.374782, acc 0.875, learning_rate 0.000119596
2017-10-10T12:54:51.341253: step 1352, loss 0.364153, acc 0.890625, learning_rate 0.000119516
2017-10-10T12:54:51.890796: step 1353, loss 0.262533, acc 0.84375, learning_rate 0.000119437
2017-10-10T12:54:52.417367: step 1354, loss 0.376551, acc 0.875, learning_rate 0.000119357
2017-10-10T12:54:52.868734: step 1355, loss 0.323579, acc 0.9375, learning_rate 0.000119278
2017-10-10T12:54:53.351192: step 1356, loss 0.164819, acc 0.9375, learning_rate 0.0001192
2017-10-10T12:54:53.856810: step 1357, loss 0.241654, acc 0.921875, learning_rate 0.000119121
2017-10-10T12:54:54.448904: step 1358, loss 0.153571, acc 0.953125, learning_rate 0.000119043
2017-10-10T12:54:55.005133: step 1359, loss 0.387447, acc 0.84375, learning_rate 0.000118965
2017-10-10T12:54:55.545202: step 1360, loss 0.203354, acc 0.921875, learning_rate 0.000118888

Evaluation:
2017-10-10T12:54:56.868660: step 1360, loss 0.251239, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1360

2017-10-10T12:54:58.400829: step 1361, loss 0.270827, acc 0.921875, learning_rate 0.000118811
2017-10-10T12:54:58.856855: step 1362, loss 0.240379, acc 0.90625, learning_rate 0.000118734
2017-10-10T12:54:59.324171: step 1363, loss 0.329917, acc 0.9375, learning_rate 0.000118658
2017-10-10T12:54:59.881374: step 1364, loss 0.258217, acc 0.90625, learning_rate 0.000118582
2017-10-10T12:55:00.358631: step 1365, loss 0.326613, acc 0.953125, learning_rate 0.000118506
2017-10-10T12:55:00.793133: step 1366, loss 0.249569, acc 0.9375, learning_rate 0.00011843
2017-10-10T12:55:01.312216: step 1367, loss 0.281752, acc 0.90625, learning_rate 0.000118355
2017-10-10T12:55:01.813052: step 1368, loss 0.205028, acc 0.96875, learning_rate 0.00011828
2017-10-10T12:55:02.317082: step 1369, loss 0.283843, acc 0.9375, learning_rate 0.000118205
2017-10-10T12:55:02.857298: step 1370, loss 0.176191, acc 0.921875, learning_rate 0.000118131
2017-10-10T12:55:03.369075: step 1371, loss 0.139712, acc 0.96875, learning_rate 0.000118057
2017-10-10T12:55:03.833115: step 1372, loss 0.170283, acc 0.960784, learning_rate 0.000117983
2017-10-10T12:55:04.373139: step 1373, loss 0.261326, acc 0.84375, learning_rate 0.00011791
2017-10-10T12:55:04.900968: step 1374, loss 0.189515, acc 0.953125, learning_rate 0.000117837
2017-10-10T12:55:05.409821: step 1375, loss 0.197839, acc 0.953125, learning_rate 0.000117764
2017-10-10T12:55:05.863290: step 1376, loss 0.268991, acc 0.90625, learning_rate 0.000117692
2017-10-10T12:55:06.529122: step 1377, loss 0.231935, acc 0.90625, learning_rate 0.000117619
2017-10-10T12:55:07.101119: step 1378, loss 0.148254, acc 0.953125, learning_rate 0.000117547
2017-10-10T12:55:07.573536: step 1379, loss 0.223921, acc 0.953125, learning_rate 0.000117476
2017-10-10T12:55:08.028516: step 1380, loss 0.179146, acc 0.921875, learning_rate 0.000117404
2017-10-10T12:55:08.508380: step 1381, loss 0.160083, acc 0.9375, learning_rate 0.000117333
2017-10-10T12:55:09.024393: step 1382, loss 0.359364, acc 0.875, learning_rate 0.000117263
2017-10-10T12:55:09.497032: step 1383, loss 0.217483, acc 0.90625, learning_rate 0.000117192
2017-10-10T12:55:10.073092: step 1384, loss 0.235012, acc 0.9375, learning_rate 0.000117122
2017-10-10T12:55:10.611601: step 1385, loss 0.201956, acc 0.9375, learning_rate 0.000117052
2017-10-10T12:55:11.036945: step 1386, loss 0.22639, acc 0.9375, learning_rate 0.000116983
2017-10-10T12:55:11.580967: step 1387, loss 0.417083, acc 0.828125, learning_rate 0.000116913
2017-10-10T12:55:12.061039: step 1388, loss 0.178126, acc 0.9375, learning_rate 0.000116844
2017-10-10T12:55:12.568921: step 1389, loss 0.207415, acc 0.90625, learning_rate 0.000116775
2017-10-10T12:55:13.072883: step 1390, loss 0.392349, acc 0.859375, learning_rate 0.000116707
2017-10-10T12:55:13.477061: step 1391, loss 0.281211, acc 0.9375, learning_rate 0.000116639
2017-10-10T12:55:13.997189: step 1392, loss 0.275799, acc 0.890625, learning_rate 0.000116571
2017-10-10T12:55:14.561713: step 1393, loss 0.283002, acc 0.921875, learning_rate 0.000116503
2017-10-10T12:55:15.100846: step 1394, loss 0.335952, acc 0.921875, learning_rate 0.000116436
2017-10-10T12:55:15.645969: step 1395, loss 0.25449, acc 0.890625, learning_rate 0.000116369
2017-10-10T12:55:16.199753: step 1396, loss 0.220891, acc 0.9375, learning_rate 0.000116302
2017-10-10T12:55:16.738352: step 1397, loss 0.243474, acc 0.9375, learning_rate 0.000116235
2017-10-10T12:55:17.286960: step 1398, loss 0.244376, acc 0.9375, learning_rate 0.000116169
2017-10-10T12:55:17.820658: step 1399, loss 0.243141, acc 0.90625, learning_rate 0.000116103
2017-10-10T12:55:18.359489: step 1400, loss 0.204408, acc 0.953125, learning_rate 0.000116037

Evaluation:
2017-10-10T12:55:19.529635: step 1400, loss 0.253192, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1400

2017-10-10T12:55:21.380958: step 1401, loss 0.346501, acc 0.875, learning_rate 0.000115972
2017-10-10T12:55:21.900747: step 1402, loss 0.226572, acc 0.90625, learning_rate 0.000115907
2017-10-10T12:55:22.347367: step 1403, loss 0.236197, acc 0.90625, learning_rate 0.000115842
2017-10-10T12:55:22.908903: step 1404, loss 0.301925, acc 0.9375, learning_rate 0.000115777
2017-10-10T12:55:23.452796: step 1405, loss 0.284224, acc 0.875, learning_rate 0.000115713
2017-10-10T12:55:23.940784: step 1406, loss 0.151746, acc 0.953125, learning_rate 0.000115649
2017-10-10T12:55:24.487379: step 1407, loss 0.17953, acc 0.953125, learning_rate 0.000115585
2017-10-10T12:55:25.040379: step 1408, loss 0.272126, acc 0.921875, learning_rate 0.000115521
2017-10-10T12:55:25.584607: step 1409, loss 0.174986, acc 0.9375, learning_rate 0.000115458
2017-10-10T12:55:26.143430: step 1410, loss 0.220758, acc 0.921875, learning_rate 0.000115395
2017-10-10T12:55:26.652886: step 1411, loss 0.365278, acc 0.875, learning_rate 0.000115332
2017-10-10T12:55:27.204866: step 1412, loss 0.28067, acc 0.859375, learning_rate 0.000115269
2017-10-10T12:55:27.805083: step 1413, loss 0.255556, acc 0.9375, learning_rate 0.000115207
2017-10-10T12:55:28.327174: step 1414, loss 0.258195, acc 0.9375, learning_rate 0.000115145
2017-10-10T12:55:28.849059: step 1415, loss 0.179192, acc 0.9375, learning_rate 0.000115083
2017-10-10T12:55:29.497102: step 1416, loss 0.12673, acc 1, learning_rate 0.000115022
2017-10-10T12:55:30.052871: step 1417, loss 0.304561, acc 0.890625, learning_rate 0.00011496
2017-10-10T12:55:30.481118: step 1418, loss 0.126053, acc 0.96875, learning_rate 0.000114899
2017-10-10T12:55:30.877743: step 1419, loss 0.27589, acc 0.890625, learning_rate 0.000114838
2017-10-10T12:55:31.386920: step 1420, loss 0.252749, acc 0.875, learning_rate 0.000114778
2017-10-10T12:55:31.933815: step 1421, loss 0.310856, acc 0.890625, learning_rate 0.000114717
2017-10-10T12:55:32.506148: step 1422, loss 0.201625, acc 0.953125, learning_rate 0.000114657
2017-10-10T12:55:33.042637: step 1423, loss 0.0999529, acc 0.953125, learning_rate 0.000114598
2017-10-10T12:55:33.632981: step 1424, loss 0.249711, acc 0.921875, learning_rate 0.000114538
2017-10-10T12:55:34.212832: step 1425, loss 0.366571, acc 0.90625, learning_rate 0.000114479
2017-10-10T12:55:34.659855: step 1426, loss 0.184967, acc 0.9375, learning_rate 0.00011442
2017-10-10T12:55:35.113997: step 1427, loss 0.426429, acc 0.859375, learning_rate 0.000114361
2017-10-10T12:55:35.579714: step 1428, loss 0.123996, acc 0.96875, learning_rate 0.000114302
2017-10-10T12:55:36.112294: step 1429, loss 0.254396, acc 0.90625, learning_rate 0.000114244
2017-10-10T12:55:36.644930: step 1430, loss 0.23582, acc 0.90625, learning_rate 0.000114186
2017-10-10T12:55:37.076958: step 1431, loss 0.492129, acc 0.890625, learning_rate 0.000114128
2017-10-10T12:55:37.561318: step 1432, loss 0.196844, acc 0.90625, learning_rate 0.00011407
2017-10-10T12:55:38.128957: step 1433, loss 0.159215, acc 0.96875, learning_rate 0.000114013
2017-10-10T12:55:38.713212: step 1434, loss 0.265586, acc 0.890625, learning_rate 0.000113955
2017-10-10T12:55:39.181636: step 1435, loss 0.30404, acc 0.90625, learning_rate 0.000113898
2017-10-10T12:55:39.738104: step 1436, loss 0.214065, acc 0.90625, learning_rate 0.000113842
2017-10-10T12:55:40.242845: step 1437, loss 0.24959, acc 0.90625, learning_rate 0.000113785
2017-10-10T12:55:40.733447: step 1438, loss 0.290613, acc 0.921875, learning_rate 0.000113729
2017-10-10T12:55:41.192929: step 1439, loss 0.339224, acc 0.859375, learning_rate 0.000113673
2017-10-10T12:55:41.723015: step 1440, loss 0.246419, acc 0.90625, learning_rate 0.000113617

Evaluation:
2017-10-10T12:55:42.949203: step 1440, loss 0.251885, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1440

2017-10-10T12:55:44.389370: step 1441, loss 0.297431, acc 0.9375, learning_rate 0.000113561
2017-10-10T12:55:44.925215: step 1442, loss 0.252329, acc 0.90625, learning_rate 0.000113506
2017-10-10T12:55:45.318268: step 1443, loss 0.26955, acc 0.90625, learning_rate 0.000113451
2017-10-10T12:55:45.709993: step 1444, loss 0.430253, acc 0.921875, learning_rate 0.000113396
2017-10-10T12:55:46.263328: step 1445, loss 0.324447, acc 0.859375, learning_rate 0.000113341
2017-10-10T12:55:46.790403: step 1446, loss 0.245573, acc 0.875, learning_rate 0.000113287
2017-10-10T12:55:47.309031: step 1447, loss 0.270287, acc 0.90625, learning_rate 0.000113233
2017-10-10T12:55:47.824865: step 1448, loss 0.248805, acc 0.90625, learning_rate 0.000113179
2017-10-10T12:55:48.332847: step 1449, loss 0.371618, acc 0.828125, learning_rate 0.000113125
2017-10-10T12:55:48.905279: step 1450, loss 0.222014, acc 0.890625, learning_rate 0.000113071
2017-10-10T12:55:49.419909: step 1451, loss 0.27682, acc 0.875, learning_rate 0.000113018
2017-10-10T12:55:49.997412: step 1452, loss 0.3237, acc 0.890625, learning_rate 0.000112965
2017-10-10T12:55:50.539690: step 1453, loss 0.219972, acc 0.921875, learning_rate 0.000112912
2017-10-10T12:55:51.004327: step 1454, loss 0.300472, acc 0.875, learning_rate 0.000112859
2017-10-10T12:55:51.568850: step 1455, loss 0.206693, acc 0.9375, learning_rate 0.000112807
2017-10-10T12:55:52.084850: step 1456, loss 0.300512, acc 0.90625, learning_rate 0.000112754
2017-10-10T12:55:52.626917: step 1457, loss 0.278753, acc 0.890625, learning_rate 0.000112702
2017-10-10T12:55:53.169126: step 1458, loss 0.225198, acc 0.90625, learning_rate 0.000112651
2017-10-10T12:55:53.635819: step 1459, loss 0.40678, acc 0.90625, learning_rate 0.000112599
2017-10-10T12:55:54.037180: step 1460, loss 0.343702, acc 0.859375, learning_rate 0.000112547
2017-10-10T12:55:54.404347: step 1461, loss 0.281257, acc 0.859375, learning_rate 0.000112496
2017-10-10T12:55:54.937007: step 1462, loss 0.176409, acc 0.9375, learning_rate 0.000112445
2017-10-10T12:55:55.497154: step 1463, loss 0.308949, acc 0.90625, learning_rate 0.000112394
2017-10-10T12:55:56.076238: step 1464, loss 0.323542, acc 0.921875, learning_rate 0.000112344
2017-10-10T12:55:56.596758: step 1465, loss 0.291655, acc 0.921875, learning_rate 0.000112293
2017-10-10T12:55:57.198845: step 1466, loss 0.262307, acc 0.921875, learning_rate 0.000112243
2017-10-10T12:55:57.696855: step 1467, loss 0.295753, acc 0.9375, learning_rate 0.000112193
2017-10-10T12:55:58.110714: step 1468, loss 0.0857921, acc 0.984375, learning_rate 0.000112144
2017-10-10T12:55:58.641557: step 1469, loss 0.224708, acc 0.875, learning_rate 0.000112094
2017-10-10T12:55:59.128852: step 1470, loss 0.127048, acc 0.941176, learning_rate 0.000112045
2017-10-10T12:55:59.686952: step 1471, loss 0.241796, acc 0.921875, learning_rate 0.000111995
2017-10-10T12:56:00.201901: step 1472, loss 0.420396, acc 0.828125, learning_rate 0.000111946
2017-10-10T12:56:00.736867: step 1473, loss 0.189578, acc 0.9375, learning_rate 0.000111898
2017-10-10T12:56:01.261249: step 1474, loss 0.341552, acc 0.875, learning_rate 0.000111849
2017-10-10T12:56:01.773356: step 1475, loss 0.135623, acc 0.96875, learning_rate 0.000111801
2017-10-10T12:56:02.304355: step 1476, loss 0.231069, acc 0.921875, learning_rate 0.000111753
2017-10-10T12:56:02.832916: step 1477, loss 0.180948, acc 0.9375, learning_rate 0.000111705
2017-10-10T12:56:03.344873: step 1478, loss 0.270625, acc 0.875, learning_rate 0.000111657
2017-10-10T12:56:03.853206: step 1479, loss 0.218567, acc 0.953125, learning_rate 0.000111609
2017-10-10T12:56:04.370400: step 1480, loss 0.248626, acc 0.90625, learning_rate 0.000111562

Evaluation:
2017-10-10T12:56:05.547699: step 1480, loss 0.25031, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1480

2017-10-10T12:56:07.220405: step 1481, loss 0.225008, acc 0.9375, learning_rate 0.000111515
2017-10-10T12:56:07.812883: step 1482, loss 0.260052, acc 0.90625, learning_rate 0.000111468
2017-10-10T12:56:08.212916: step 1483, loss 0.380357, acc 0.890625, learning_rate 0.000111421
2017-10-10T12:56:08.697798: step 1484, loss 0.200714, acc 0.953125, learning_rate 0.000111374
2017-10-10T12:56:09.170281: step 1485, loss 0.332648, acc 0.9375, learning_rate 0.000111328
2017-10-10T12:56:09.732902: step 1486, loss 0.232055, acc 0.90625, learning_rate 0.000111282
2017-10-10T12:56:10.232820: step 1487, loss 0.180047, acc 0.9375, learning_rate 0.000111236
2017-10-10T12:56:10.797195: step 1488, loss 0.0859743, acc 0.984375, learning_rate 0.00011119
2017-10-10T12:56:11.368966: step 1489, loss 0.25401, acc 0.921875, learning_rate 0.000111144
2017-10-10T12:56:11.865238: step 1490, loss 0.301137, acc 0.90625, learning_rate 0.000111099
2017-10-10T12:56:12.420987: step 1491, loss 0.246043, acc 0.890625, learning_rate 0.000111053
2017-10-10T12:56:12.905196: step 1492, loss 0.188077, acc 0.9375, learning_rate 0.000111008
2017-10-10T12:56:13.391496: step 1493, loss 0.333191, acc 0.875, learning_rate 0.000110963
2017-10-10T12:56:13.893408: step 1494, loss 0.449821, acc 0.890625, learning_rate 0.000110918
2017-10-10T12:56:14.405930: step 1495, loss 0.265624, acc 0.90625, learning_rate 0.000110874
2017-10-10T12:56:14.937994: step 1496, loss 0.273989, acc 0.90625, learning_rate 0.00011083
2017-10-10T12:56:15.448754: step 1497, loss 0.464002, acc 0.828125, learning_rate 0.000110785
2017-10-10T12:56:16.085218: step 1498, loss 0.280739, acc 0.890625, learning_rate 0.000110741
2017-10-10T12:56:16.537938: step 1499, loss 0.158142, acc 0.953125, learning_rate 0.000110697
2017-10-10T12:56:16.963872: step 1500, loss 0.243969, acc 0.9375, learning_rate 0.000110654
2017-10-10T12:56:17.383564: step 1501, loss 0.221211, acc 0.9375, learning_rate 0.00011061
2017-10-10T12:56:17.947563: step 1502, loss 0.138998, acc 0.953125, learning_rate 0.000110567
2017-10-10T12:56:18.497482: step 1503, loss 0.205191, acc 0.921875, learning_rate 0.000110524
2017-10-10T12:56:19.192888: step 1504, loss 0.223769, acc 0.921875, learning_rate 0.000110481
2017-10-10T12:56:19.759443: step 1505, loss 0.323144, acc 0.90625, learning_rate 0.000110438
2017-10-10T12:56:20.150642: step 1506, loss 0.302567, acc 0.875, learning_rate 0.000110396
2017-10-10T12:56:20.616838: step 1507, loss 0.114565, acc 0.984375, learning_rate 0.000110353
2017-10-10T12:56:21.180853: step 1508, loss 0.156514, acc 0.9375, learning_rate 0.000110311
2017-10-10T12:56:21.748268: step 1509, loss 0.26756, acc 0.890625, learning_rate 0.000110269
2017-10-10T12:56:22.309704: step 1510, loss 0.26148, acc 0.875, learning_rate 0.000110227
2017-10-10T12:56:22.810927: step 1511, loss 0.228142, acc 0.921875, learning_rate 0.000110185
2017-10-10T12:56:23.219125: step 1512, loss 0.261018, acc 0.90625, learning_rate 0.000110144
2017-10-10T12:56:23.626474: step 1513, loss 0.199927, acc 0.953125, learning_rate 0.000110102
2017-10-10T12:56:24.139972: step 1514, loss 0.236178, acc 0.890625, learning_rate 0.000110061
2017-10-10T12:56:24.688529: step 1515, loss 0.385187, acc 0.84375, learning_rate 0.00011002
2017-10-10T12:56:25.237768: step 1516, loss 0.326018, acc 0.875, learning_rate 0.000109979
2017-10-10T12:56:25.791256: step 1517, loss 0.328038, acc 0.875, learning_rate 0.000109938
2017-10-10T12:56:26.288487: step 1518, loss 0.250547, acc 0.875, learning_rate 0.000109898
2017-10-10T12:56:26.796906: step 1519, loss 0.281752, acc 0.875, learning_rate 0.000109857
2017-10-10T12:56:27.305593: step 1520, loss 0.457864, acc 0.859375, learning_rate 0.000109817

Evaluation:
2017-10-10T12:56:28.462420: step 1520, loss 0.248804, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1520

2017-10-10T12:56:30.221849: step 1521, loss 0.300734, acc 0.890625, learning_rate 0.000109777
2017-10-10T12:56:30.868867: step 1522, loss 0.157751, acc 0.9375, learning_rate 0.000109737
2017-10-10T12:56:31.289779: step 1523, loss 0.116361, acc 0.96875, learning_rate 0.000109697
2017-10-10T12:56:31.771620: step 1524, loss 0.268582, acc 0.90625, learning_rate 0.000109658
2017-10-10T12:56:32.332400: step 1525, loss 0.253806, acc 0.90625, learning_rate 0.000109618
2017-10-10T12:56:32.965635: step 1526, loss 0.140093, acc 0.953125, learning_rate 0.000109579
2017-10-10T12:56:33.507274: step 1527, loss 0.353054, acc 0.90625, learning_rate 0.00010954
2017-10-10T12:56:34.038491: step 1528, loss 0.136828, acc 0.96875, learning_rate 0.000109501
2017-10-10T12:56:34.663678: step 1529, loss 0.240112, acc 0.921875, learning_rate 0.000109462
2017-10-10T12:56:35.236335: step 1530, loss 0.27002, acc 0.859375, learning_rate 0.000109424
2017-10-10T12:56:35.792563: step 1531, loss 0.237881, acc 0.875, learning_rate 0.000109385
2017-10-10T12:56:36.360056: step 1532, loss 0.123203, acc 0.96875, learning_rate 0.000109347
2017-10-10T12:56:36.882362: step 1533, loss 0.230538, acc 0.953125, learning_rate 0.000109309
2017-10-10T12:56:37.427793: step 1534, loss 0.477764, acc 0.84375, learning_rate 0.000109271
2017-10-10T12:56:37.982438: step 1535, loss 0.247468, acc 0.921875, learning_rate 0.000109233
2017-10-10T12:56:38.537633: step 1536, loss 0.263668, acc 0.9375, learning_rate 0.000109195
2017-10-10T12:56:39.195367: step 1537, loss 0.306696, acc 0.921875, learning_rate 0.000109158
2017-10-10T12:56:39.743886: step 1538, loss 0.230628, acc 0.9375, learning_rate 0.00010912
2017-10-10T12:56:40.163597: step 1539, loss 0.301635, acc 0.890625, learning_rate 0.000109083
2017-10-10T12:56:40.564847: step 1540, loss 0.110239, acc 0.984375, learning_rate 0.000109046
2017-10-10T12:56:41.076950: step 1541, loss 0.377969, acc 0.828125, learning_rate 0.000109009
2017-10-10T12:56:41.584153: step 1542, loss 0.335196, acc 0.828125, learning_rate 0.000108972
2017-10-10T12:56:42.110309: step 1543, loss 0.328382, acc 0.875, learning_rate 0.000108936
2017-10-10T12:56:42.688032: step 1544, loss 0.195569, acc 0.9375, learning_rate 0.000108899
2017-10-10T12:56:43.191092: step 1545, loss 0.226926, acc 0.953125, learning_rate 0.000108863
2017-10-10T12:56:43.677557: step 1546, loss 0.358546, acc 0.90625, learning_rate 0.000108827
2017-10-10T12:56:44.110537: step 1547, loss 0.460705, acc 0.84375, learning_rate 0.000108791
2017-10-10T12:56:44.616964: step 1548, loss 0.274389, acc 0.859375, learning_rate 0.000108755
2017-10-10T12:56:45.144051: step 1549, loss 0.175588, acc 0.9375, learning_rate 0.000108719
2017-10-10T12:56:45.609013: step 1550, loss 0.17658, acc 0.9375, learning_rate 0.000108683
2017-10-10T12:56:46.149021: step 1551, loss 0.311863, acc 0.921875, learning_rate 0.000108648
2017-10-10T12:56:46.724901: step 1552, loss 0.202734, acc 0.953125, learning_rate 0.000108613
2017-10-10T12:56:47.221079: step 1553, loss 0.254021, acc 0.9375, learning_rate 0.000108577
2017-10-10T12:56:47.736947: step 1554, loss 0.384262, acc 0.828125, learning_rate 0.000108542
2017-10-10T12:56:48.196880: step 1555, loss 0.25648, acc 0.9375, learning_rate 0.000108508
2017-10-10T12:56:48.727671: step 1556, loss 0.240906, acc 0.921875, learning_rate 0.000108473
2017-10-10T12:56:49.194271: step 1557, loss 0.198464, acc 0.90625, learning_rate 0.000108438
2017-10-10T12:56:49.693259: step 1558, loss 0.190425, acc 0.90625, learning_rate 0.000108404
2017-10-10T12:56:50.164358: step 1559, loss 0.348251, acc 0.875, learning_rate 0.00010837
2017-10-10T12:56:50.628637: step 1560, loss 0.14669, acc 0.9375, learning_rate 0.000108335

Evaluation:
2017-10-10T12:56:51.791940: step 1560, loss 0.250384, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1560

2017-10-10T12:56:53.367290: step 1561, loss 0.213227, acc 0.90625, learning_rate 0.000108301
2017-10-10T12:56:53.943187: step 1562, loss 0.21569, acc 0.90625, learning_rate 0.000108267
2017-10-10T12:56:54.348997: step 1563, loss 0.380971, acc 0.8125, learning_rate 0.000108234
2017-10-10T12:56:54.814856: step 1564, loss 0.442235, acc 0.859375, learning_rate 0.0001082
2017-10-10T12:56:55.412391: step 1565, loss 0.211511, acc 0.9375, learning_rate 0.000108167
2017-10-10T12:56:55.952344: step 1566, loss 0.231342, acc 0.921875, learning_rate 0.000108133
2017-10-10T12:56:56.493687: step 1567, loss 0.199844, acc 0.953125, learning_rate 0.0001081
2017-10-10T12:56:56.884963: step 1568, loss 0.140356, acc 0.960784, learning_rate 0.000108067
2017-10-10T12:56:57.402121: step 1569, loss 0.212417, acc 0.9375, learning_rate 0.000108034
2017-10-10T12:56:57.976902: step 1570, loss 0.198021, acc 0.953125, learning_rate 0.000108001
2017-10-10T12:56:58.448889: step 1571, loss 0.396382, acc 0.875, learning_rate 0.000107969
2017-10-10T12:56:58.928885: step 1572, loss 0.208032, acc 0.984375, learning_rate 0.000107936
2017-10-10T12:56:59.444826: step 1573, loss 0.276591, acc 0.875, learning_rate 0.000107904
2017-10-10T12:56:59.972452: step 1574, loss 0.288874, acc 0.828125, learning_rate 0.000107871
2017-10-10T12:57:00.484638: step 1575, loss 0.147549, acc 0.953125, learning_rate 0.000107839
2017-10-10T12:57:01.037116: step 1576, loss 0.335511, acc 0.875, learning_rate 0.000107807
2017-10-10T12:57:01.591281: step 1577, loss 0.399409, acc 0.828125, learning_rate 0.000107775
2017-10-10T12:57:02.165091: step 1578, loss 0.344206, acc 0.859375, learning_rate 0.000107744
2017-10-10T12:57:02.785150: step 1579, loss 0.295404, acc 0.890625, learning_rate 0.000107712
2017-10-10T12:57:03.269016: step 1580, loss 0.227848, acc 0.90625, learning_rate 0.000107681
2017-10-10T12:57:03.688004: step 1581, loss 0.186128, acc 0.9375, learning_rate 0.000107649
2017-10-10T12:57:04.191528: step 1582, loss 0.235782, acc 0.921875, learning_rate 0.000107618
2017-10-10T12:57:04.756905: step 1583, loss 0.444655, acc 0.859375, learning_rate 0.000107587
2017-10-10T12:57:05.384959: step 1584, loss 0.212314, acc 0.9375, learning_rate 0.000107556
2017-10-10T12:57:05.945076: step 1585, loss 0.301736, acc 0.953125, learning_rate 0.000107525
2017-10-10T12:57:06.429015: step 1586, loss 0.254131, acc 0.953125, learning_rate 0.000107494
2017-10-10T12:57:06.875292: step 1587, loss 0.243583, acc 0.90625, learning_rate 0.000107464
2017-10-10T12:57:07.403098: step 1588, loss 0.241476, acc 0.953125, learning_rate 0.000107433
2017-10-10T12:57:07.956508: step 1589, loss 0.289802, acc 0.875, learning_rate 0.000107403
2017-10-10T12:57:08.525030: step 1590, loss 0.158386, acc 0.953125, learning_rate 0.000107373
2017-10-10T12:57:09.075502: step 1591, loss 0.315605, acc 0.921875, learning_rate 0.000107343
2017-10-10T12:57:09.605586: step 1592, loss 0.328774, acc 0.890625, learning_rate 0.000107313
2017-10-10T12:57:10.163530: step 1593, loss 0.27001, acc 0.90625, learning_rate 0.000107283
2017-10-10T12:57:10.712975: step 1594, loss 0.207159, acc 0.90625, learning_rate 0.000107253
2017-10-10T12:57:11.253355: step 1595, loss 0.128015, acc 0.96875, learning_rate 0.000107224
2017-10-10T12:57:11.832509: step 1596, loss 0.261793, acc 0.90625, learning_rate 0.000107194
2017-10-10T12:57:12.229102: step 1597, loss 0.266991, acc 0.921875, learning_rate 0.000107165
2017-10-10T12:57:12.715664: step 1598, loss 0.162379, acc 0.921875, learning_rate 0.000107136
2017-10-10T12:57:13.226330: step 1599, loss 0.266736, acc 0.921875, learning_rate 0.000107106
2017-10-10T12:57:13.701000: step 1600, loss 0.341305, acc 0.921875, learning_rate 0.000107077

Evaluation:
2017-10-10T12:57:14.801294: step 1600, loss 0.248767, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1600

2017-10-10T12:57:16.388966: step 1601, loss 0.420083, acc 0.859375, learning_rate 0.000107048
2017-10-10T12:57:16.941998: step 1602, loss 0.287184, acc 0.9375, learning_rate 0.00010702
2017-10-10T12:57:17.327200: step 1603, loss 0.230685, acc 0.9375, learning_rate 0.000106991
2017-10-10T12:57:17.766748: step 1604, loss 0.296753, acc 0.875, learning_rate 0.000106963
2017-10-10T12:57:18.204899: step 1605, loss 0.207912, acc 0.90625, learning_rate 0.000106934
2017-10-10T12:57:18.728222: step 1606, loss 0.206918, acc 0.9375, learning_rate 0.000106906
2017-10-10T12:57:19.252891: step 1607, loss 0.436494, acc 0.875, learning_rate 0.000106878
2017-10-10T12:57:19.772133: step 1608, loss 0.13698, acc 0.984375, learning_rate 0.00010685
2017-10-10T12:57:20.352567: step 1609, loss 0.176055, acc 0.921875, learning_rate 0.000106822
2017-10-10T12:57:20.898569: step 1610, loss 0.255444, acc 0.890625, learning_rate 0.000106794
2017-10-10T12:57:21.461299: step 1611, loss 0.217502, acc 0.921875, learning_rate 0.000106766
2017-10-10T12:57:21.982696: step 1612, loss 0.153491, acc 0.921875, learning_rate 0.000106738
2017-10-10T12:57:22.494045: step 1613, loss 0.186453, acc 0.9375, learning_rate 0.000106711
2017-10-10T12:57:22.933114: step 1614, loss 0.261604, acc 0.953125, learning_rate 0.000106684
2017-10-10T12:57:23.400917: step 1615, loss 0.185239, acc 0.9375, learning_rate 0.000106656
2017-10-10T12:57:23.878643: step 1616, loss 0.232917, acc 0.890625, learning_rate 0.000106629
2017-10-10T12:57:24.380996: step 1617, loss 0.101844, acc 0.984375, learning_rate 0.000106602
2017-10-10T12:57:24.944964: step 1618, loss 0.337144, acc 0.890625, learning_rate 0.000106575
2017-10-10T12:57:25.503771: step 1619, loss 0.299367, acc 0.875, learning_rate 0.000106548
2017-10-10T12:57:26.097081: step 1620, loss 0.238008, acc 0.90625, learning_rate 0.000106521
2017-10-10T12:57:26.576939: step 1621, loss 0.220382, acc 0.90625, learning_rate 0.000106495
2017-10-10T12:57:27.001196: step 1622, loss 0.326514, acc 0.953125, learning_rate 0.000106468
2017-10-10T12:57:27.390139: step 1623, loss 0.169475, acc 0.9375, learning_rate 0.000106442
2017-10-10T12:57:27.906228: step 1624, loss 0.202667, acc 0.9375, learning_rate 0.000106416
2017-10-10T12:57:28.560874: step 1625, loss 0.24977, acc 0.9375, learning_rate 0.000106389
2017-10-10T12:57:29.146224: step 1626, loss 0.102044, acc 0.96875, learning_rate 0.000106363
2017-10-10T12:57:29.596128: step 1627, loss 0.354589, acc 0.84375, learning_rate 0.000106337
2017-10-10T12:57:30.032705: step 1628, loss 0.196311, acc 0.9375, learning_rate 0.000106312
2017-10-10T12:57:30.507772: step 1629, loss 0.229767, acc 0.9375, learning_rate 0.000106286
2017-10-10T12:57:31.040882: step 1630, loss 0.165096, acc 0.953125, learning_rate 0.00010626
2017-10-10T12:57:31.521086: step 1631, loss 0.255675, acc 0.921875, learning_rate 0.000106235
2017-10-10T12:57:32.160846: step 1632, loss 0.32304, acc 0.875, learning_rate 0.000106209
2017-10-10T12:57:32.676946: step 1633, loss 0.376765, acc 0.890625, learning_rate 0.000106184
2017-10-10T12:57:33.246842: step 1634, loss 0.141417, acc 0.953125, learning_rate 0.000106159
2017-10-10T12:57:33.770729: step 1635, loss 0.178704, acc 0.953125, learning_rate 0.000106133
2017-10-10T12:57:34.264884: step 1636, loss 0.243943, acc 0.9375, learning_rate 0.000106108
2017-10-10T12:57:34.822348: step 1637, loss 0.199633, acc 0.953125, learning_rate 0.000106083
2017-10-10T12:57:35.320897: step 1638, loss 0.438061, acc 0.90625, learning_rate 0.000106059
2017-10-10T12:57:35.864008: step 1639, loss 0.180167, acc 0.921875, learning_rate 0.000106034
2017-10-10T12:57:36.376859: step 1640, loss 0.184579, acc 0.921875, learning_rate 0.000106009

Evaluation:
2017-10-10T12:57:37.520031: step 1640, loss 0.247528, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1640

2017-10-10T12:57:39.299569: step 1641, loss 0.348074, acc 0.84375, learning_rate 0.000105985
2017-10-10T12:57:39.884516: step 1642, loss 0.16322, acc 0.921875, learning_rate 0.00010596
2017-10-10T12:57:40.333911: step 1643, loss 0.362803, acc 0.859375, learning_rate 0.000105936
2017-10-10T12:57:40.800824: step 1644, loss 0.248574, acc 0.90625, learning_rate 0.000105912
2017-10-10T12:57:41.355332: step 1645, loss 0.311452, acc 0.890625, learning_rate 0.000105888
2017-10-10T12:57:41.807170: step 1646, loss 0.230724, acc 0.921875, learning_rate 0.000105864
2017-10-10T12:57:42.337626: step 1647, loss 0.147392, acc 0.953125, learning_rate 0.00010584
2017-10-10T12:57:42.867082: step 1648, loss 0.31395, acc 0.875, learning_rate 0.000105816
2017-10-10T12:57:43.437019: step 1649, loss 0.290786, acc 0.859375, learning_rate 0.000105792
2017-10-10T12:57:43.959531: step 1650, loss 0.249607, acc 0.90625, learning_rate 0.000105768
2017-10-10T12:57:44.469385: step 1651, loss 0.162819, acc 0.9375, learning_rate 0.000105745
2017-10-10T12:57:45.073366: step 1652, loss 0.170891, acc 0.90625, learning_rate 0.000105721
2017-10-10T12:57:45.615554: step 1653, loss 0.161129, acc 0.9375, learning_rate 0.000105698
2017-10-10T12:57:46.115006: step 1654, loss 0.28052, acc 0.90625, learning_rate 0.000105675
2017-10-10T12:57:46.642569: step 1655, loss 0.257491, acc 0.890625, learning_rate 0.000105652
2017-10-10T12:57:47.261031: step 1656, loss 0.234934, acc 0.96875, learning_rate 0.000105629
2017-10-10T12:57:47.809610: step 1657, loss 0.147157, acc 0.96875, learning_rate 0.000105606
2017-10-10T12:57:48.392942: step 1658, loss 0.368009, acc 0.84375, learning_rate 0.000105583
2017-10-10T12:57:48.981007: step 1659, loss 0.245661, acc 0.90625, learning_rate 0.00010556
2017-10-10T12:57:49.444881: step 1660, loss 0.36561, acc 0.875, learning_rate 0.000105537
2017-10-10T12:57:49.902300: step 1661, loss 0.256754, acc 0.921875, learning_rate 0.000105515
2017-10-10T12:57:50.474548: step 1662, loss 0.123545, acc 0.96875, learning_rate 0.000105492
2017-10-10T12:57:51.022228: step 1663, loss 0.181137, acc 0.9375, learning_rate 0.00010547
2017-10-10T12:57:51.773677: step 1664, loss 0.32416, acc 0.921875, learning_rate 0.000105447
2017-10-10T12:57:52.321207: step 1665, loss 0.138156, acc 0.96875, learning_rate 0.000105425
2017-10-10T12:57:52.668851: step 1666, loss 0.212461, acc 0.921569, learning_rate 0.000105403
2017-10-10T12:57:53.083138: step 1667, loss 0.118129, acc 0.953125, learning_rate 0.000105381
2017-10-10T12:57:53.495318: step 1668, loss 0.4271, acc 0.859375, learning_rate 0.000105359
2017-10-10T12:57:53.977609: step 1669, loss 0.208885, acc 0.953125, learning_rate 0.000105337
2017-10-10T12:57:54.549012: step 1670, loss 0.148991, acc 0.953125, learning_rate 0.000105315
2017-10-10T12:57:55.130575: step 1671, loss 0.212057, acc 0.953125, learning_rate 0.000105294
2017-10-10T12:57:55.675703: step 1672, loss 0.156934, acc 0.96875, learning_rate 0.000105272
2017-10-10T12:57:56.272740: step 1673, loss 0.345226, acc 0.859375, learning_rate 0.000105251
2017-10-10T12:57:56.860911: step 1674, loss 0.205937, acc 0.921875, learning_rate 0.000105229
2017-10-10T12:57:57.366806: step 1675, loss 0.223907, acc 0.921875, learning_rate 0.000105208
2017-10-10T12:57:57.861270: step 1676, loss 0.302897, acc 0.90625, learning_rate 0.000105186
2017-10-10T12:57:58.389005: step 1677, loss 0.275847, acc 0.875, learning_rate 0.000105165
2017-10-10T12:57:58.864983: step 1678, loss 0.214785, acc 0.9375, learning_rate 0.000105144
2017-10-10T12:57:59.537260: step 1679, loss 0.106582, acc 0.96875, learning_rate 0.000105123
2017-10-10T12:58:00.090638: step 1680, loss 0.191535, acc 0.890625, learning_rate 0.000105102

Evaluation:
2017-10-10T12:58:01.249531: step 1680, loss 0.249404, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1680

2017-10-10T12:58:02.784822: step 1681, loss 0.299599, acc 0.859375, learning_rate 0.000105081
2017-10-10T12:58:03.156941: step 1682, loss 0.169812, acc 0.953125, learning_rate 0.000105061
2017-10-10T12:58:03.572855: step 1683, loss 0.166815, acc 0.953125, learning_rate 0.00010504
2017-10-10T12:58:04.078260: step 1684, loss 0.305237, acc 0.90625, learning_rate 0.00010502
2017-10-10T12:58:04.521045: step 1685, loss 0.246453, acc 0.890625, learning_rate 0.000104999
2017-10-10T12:58:04.965134: step 1686, loss 0.199172, acc 0.9375, learning_rate 0.000104979
2017-10-10T12:58:05.529588: step 1687, loss 0.271195, acc 0.875, learning_rate 0.000104958
2017-10-10T12:58:06.027095: step 1688, loss 0.283275, acc 0.90625, learning_rate 0.000104938
2017-10-10T12:58:06.526379: step 1689, loss 0.251124, acc 0.9375, learning_rate 0.000104918
2017-10-10T12:58:07.025399: step 1690, loss 0.242199, acc 0.90625, learning_rate 0.000104898
2017-10-10T12:58:07.609323: step 1691, loss 0.173522, acc 0.9375, learning_rate 0.000104878
2017-10-10T12:58:08.137176: step 1692, loss 0.256653, acc 0.90625, learning_rate 0.000104858
2017-10-10T12:58:08.673349: step 1693, loss 0.309975, acc 0.9375, learning_rate 0.000104838
2017-10-10T12:58:09.217085: step 1694, loss 0.276861, acc 0.90625, learning_rate 0.000104818
2017-10-10T12:58:09.646554: step 1695, loss 0.24766, acc 0.921875, learning_rate 0.000104799
2017-10-10T12:58:10.151212: step 1696, loss 0.232488, acc 0.890625, learning_rate 0.000104779
2017-10-10T12:58:10.686074: step 1697, loss 0.307217, acc 0.90625, learning_rate 0.00010476
2017-10-10T12:58:11.372886: step 1698, loss 0.216403, acc 0.90625, learning_rate 0.00010474
2017-10-10T12:58:12.012860: step 1699, loss 0.0857164, acc 0.984375, learning_rate 0.000104721
2017-10-10T12:58:12.428485: step 1700, loss 0.218115, acc 0.953125, learning_rate 0.000104702
2017-10-10T12:58:12.887691: step 1701, loss 0.342927, acc 0.84375, learning_rate 0.000104682
2017-10-10T12:58:13.478416: step 1702, loss 0.265963, acc 0.90625, learning_rate 0.000104663
2017-10-10T12:58:14.008804: step 1703, loss 0.224085, acc 0.921875, learning_rate 0.000104644
2017-10-10T12:58:14.628979: step 1704, loss 0.280194, acc 0.890625, learning_rate 0.000104625
2017-10-10T12:58:15.241148: step 1705, loss 0.291741, acc 0.90625, learning_rate 0.000104606
2017-10-10T12:58:15.589828: step 1706, loss 0.149963, acc 0.953125, learning_rate 0.000104588
2017-10-10T12:58:16.055906: step 1707, loss 0.260515, acc 0.9375, learning_rate 0.000104569
2017-10-10T12:58:16.576919: step 1708, loss 0.17492, acc 0.9375, learning_rate 0.00010455
2017-10-10T12:58:17.116922: step 1709, loss 0.287537, acc 0.875, learning_rate 0.000104532
2017-10-10T12:58:17.632769: step 1710, loss 0.203198, acc 0.9375, learning_rate 0.000104513
2017-10-10T12:58:18.153415: step 1711, loss 0.216098, acc 0.921875, learning_rate 0.000104495
2017-10-10T12:58:18.669212: step 1712, loss 0.294484, acc 0.859375, learning_rate 0.000104476
2017-10-10T12:58:19.224828: step 1713, loss 0.213474, acc 0.890625, learning_rate 0.000104458
2017-10-10T12:58:19.789809: step 1714, loss 0.139434, acc 0.96875, learning_rate 0.00010444
2017-10-10T12:58:20.330202: step 1715, loss 0.263697, acc 0.90625, learning_rate 0.000104422
2017-10-10T12:58:20.876840: step 1716, loss 0.140501, acc 0.9375, learning_rate 0.000104404
2017-10-10T12:58:21.407695: step 1717, loss 0.266949, acc 0.90625, learning_rate 0.000104386
2017-10-10T12:58:21.945857: step 1718, loss 0.319945, acc 0.875, learning_rate 0.000104368
2017-10-10T12:58:22.480898: step 1719, loss 0.277175, acc 0.90625, learning_rate 0.00010435
2017-10-10T12:58:22.951806: step 1720, loss 0.331296, acc 0.921875, learning_rate 0.000104332

Evaluation:
2017-10-10T12:58:24.121182: step 1720, loss 0.249718, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1720

2017-10-10T12:58:25.615460: step 1721, loss 0.379688, acc 0.84375, learning_rate 0.000104315
2017-10-10T12:58:26.057567: step 1722, loss 0.311655, acc 0.90625, learning_rate 0.000104297
2017-10-10T12:58:26.593825: step 1723, loss 0.361809, acc 0.859375, learning_rate 0.000104279
2017-10-10T12:58:27.125061: step 1724, loss 0.154494, acc 0.9375, learning_rate 0.000104262
2017-10-10T12:58:27.661051: step 1725, loss 0.339456, acc 0.875, learning_rate 0.000104245
2017-10-10T12:58:28.181335: step 1726, loss 0.123164, acc 0.96875, learning_rate 0.000104227
2017-10-10T12:58:28.648948: step 1727, loss 0.195229, acc 0.9375, learning_rate 0.00010421
2017-10-10T12:58:29.224943: step 1728, loss 0.249948, acc 0.890625, learning_rate 0.000104193
2017-10-10T12:58:29.734171: step 1729, loss 0.25421, acc 0.90625, learning_rate 0.000104176
2017-10-10T12:58:30.260875: step 1730, loss 0.218716, acc 0.921875, learning_rate 0.000104159
2017-10-10T12:58:30.764951: step 1731, loss 0.165245, acc 0.9375, learning_rate 0.000104142
2017-10-10T12:58:31.249039: step 1732, loss 0.304177, acc 0.875, learning_rate 0.000104125
2017-10-10T12:58:31.752127: step 1733, loss 0.103251, acc 0.96875, learning_rate 0.000104108
2017-10-10T12:58:32.209016: step 1734, loss 0.295319, acc 0.890625, learning_rate 0.000104091
2017-10-10T12:58:32.769643: step 1735, loss 0.475295, acc 0.828125, learning_rate 0.000104074
2017-10-10T12:58:33.240888: step 1736, loss 0.242368, acc 0.953125, learning_rate 0.000104058
2017-10-10T12:58:33.810054: step 1737, loss 0.122618, acc 0.984375, learning_rate 0.000104041
2017-10-10T12:58:34.376931: step 1738, loss 0.286999, acc 0.890625, learning_rate 0.000104025
2017-10-10T12:58:35.021040: step 1739, loss 0.281186, acc 0.953125, learning_rate 0.000104008
2017-10-10T12:58:35.564210: step 1740, loss 0.324294, acc 0.90625, learning_rate 0.000103992
2017-10-10T12:58:36.032418: step 1741, loss 0.126465, acc 0.96875, learning_rate 0.000103976
2017-10-10T12:58:36.520930: step 1742, loss 0.333845, acc 0.90625, learning_rate 0.000103959
2017-10-10T12:58:37.014363: step 1743, loss 0.260095, acc 0.9375, learning_rate 0.000103943
2017-10-10T12:58:37.516585: step 1744, loss 0.549294, acc 0.84375, learning_rate 0.000103927
2017-10-10T12:58:38.160899: step 1745, loss 0.319597, acc 0.90625, learning_rate 0.000103911
2017-10-10T12:58:38.613032: step 1746, loss 0.249457, acc 0.90625, learning_rate 0.000103895
2017-10-10T12:58:39.036908: step 1747, loss 0.221481, acc 0.90625, learning_rate 0.000103879
2017-10-10T12:58:39.432454: step 1748, loss 0.274161, acc 0.9375, learning_rate 0.000103863
2017-10-10T12:58:39.961660: step 1749, loss 0.0961755, acc 0.96875, learning_rate 0.000103848
2017-10-10T12:58:40.461761: step 1750, loss 0.31014, acc 0.890625, learning_rate 0.000103832
2017-10-10T12:58:40.988856: step 1751, loss 0.185885, acc 0.90625, learning_rate 0.000103816
2017-10-10T12:58:41.532610: step 1752, loss 0.202254, acc 0.921875, learning_rate 0.000103801
2017-10-10T12:58:42.016961: step 1753, loss 0.195789, acc 0.9375, learning_rate 0.000103785
2017-10-10T12:58:42.501071: step 1754, loss 0.142864, acc 0.953125, learning_rate 0.00010377
2017-10-10T12:58:42.995900: step 1755, loss 0.270148, acc 0.921875, learning_rate 0.000103754
2017-10-10T12:58:43.566486: step 1756, loss 0.189641, acc 0.9375, learning_rate 0.000103739
2017-10-10T12:58:44.081083: step 1757, loss 0.264755, acc 0.90625, learning_rate 0.000103724
2017-10-10T12:58:44.506093: step 1758, loss 0.287114, acc 0.875, learning_rate 0.000103709
2017-10-10T12:58:45.039763: step 1759, loss 0.362439, acc 0.875, learning_rate 0.000103694
2017-10-10T12:58:45.646489: step 1760, loss 0.27077, acc 0.875, learning_rate 0.000103678

Evaluation:
2017-10-10T12:58:46.766509: step 1760, loss 0.246277, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1760

2017-10-10T12:58:48.876840: step 1761, loss 0.249561, acc 0.921875, learning_rate 0.000103663
2017-10-10T12:58:49.350290: step 1762, loss 0.166894, acc 0.9375, learning_rate 0.000103648
2017-10-10T12:58:49.877726: step 1763, loss 0.349789, acc 0.859375, learning_rate 0.000103634
2017-10-10T12:58:50.261221: step 1764, loss 0.282992, acc 0.921569, learning_rate 0.000103619
2017-10-10T12:58:50.741004: step 1765, loss 0.254144, acc 0.890625, learning_rate 0.000103604
2017-10-10T12:58:51.313094: step 1766, loss 0.202444, acc 0.9375, learning_rate 0.000103589
2017-10-10T12:58:51.867927: step 1767, loss 0.362153, acc 0.84375, learning_rate 0.000103575
2017-10-10T12:58:52.390675: step 1768, loss 0.139996, acc 0.96875, learning_rate 0.00010356
2017-10-10T12:58:52.904895: step 1769, loss 0.209182, acc 0.953125, learning_rate 0.000103545
2017-10-10T12:58:53.393249: step 1770, loss 0.266423, acc 0.90625, learning_rate 0.000103531
2017-10-10T12:58:53.982187: step 1771, loss 0.429605, acc 0.84375, learning_rate 0.000103517
2017-10-10T12:58:54.499630: step 1772, loss 0.272864, acc 0.90625, learning_rate 0.000103502
2017-10-10T12:58:55.035366: step 1773, loss 0.117275, acc 0.953125, learning_rate 0.000103488
2017-10-10T12:58:55.541073: step 1774, loss 0.224233, acc 0.9375, learning_rate 0.000103474
2017-10-10T12:58:56.092498: step 1775, loss 0.161145, acc 0.9375, learning_rate 0.00010346
2017-10-10T12:58:56.608914: step 1776, loss 0.219908, acc 0.9375, learning_rate 0.000103445
2017-10-10T12:58:57.180859: step 1777, loss 0.0947642, acc 0.984375, learning_rate 0.000103431
2017-10-10T12:58:57.766831: step 1778, loss 0.343272, acc 0.875, learning_rate 0.000103417
2017-10-10T12:58:58.200828: step 1779, loss 0.270594, acc 0.875, learning_rate 0.000103403
2017-10-10T12:58:58.618138: step 1780, loss 0.247471, acc 0.90625, learning_rate 0.00010339
2017-10-10T12:58:59.009077: step 1781, loss 0.176635, acc 0.921875, learning_rate 0.000103376
2017-10-10T12:58:59.480099: step 1782, loss 0.193259, acc 0.953125, learning_rate 0.000103362
2017-10-10T12:58:59.961054: step 1783, loss 0.214533, acc 0.9375, learning_rate 0.000103348
2017-10-10T12:59:00.662759: step 1784, loss 0.322353, acc 0.890625, learning_rate 0.000103335
2017-10-10T12:59:01.121031: step 1785, loss 0.221825, acc 0.90625, learning_rate 0.000103321
2017-10-10T12:59:01.585944: step 1786, loss 0.418789, acc 0.84375, learning_rate 0.000103307
2017-10-10T12:59:02.004959: step 1787, loss 0.234462, acc 0.9375, learning_rate 0.000103294
2017-10-10T12:59:02.485533: step 1788, loss 0.215725, acc 0.90625, learning_rate 0.00010328
2017-10-10T12:59:02.976890: step 1789, loss 0.135317, acc 0.984375, learning_rate 0.000103267
2017-10-10T12:59:03.581103: step 1790, loss 0.207766, acc 0.921875, learning_rate 0.000103254
2017-10-10T12:59:04.100854: step 1791, loss 0.292716, acc 0.859375, learning_rate 0.00010324
2017-10-10T12:59:04.529050: step 1792, loss 0.249323, acc 0.9375, learning_rate 0.000103227
2017-10-10T12:59:05.118549: step 1793, loss 0.251092, acc 0.890625, learning_rate 0.000103214
2017-10-10T12:59:05.701498: step 1794, loss 0.366921, acc 0.875, learning_rate 0.000103201
2017-10-10T12:59:06.258129: step 1795, loss 0.185528, acc 0.9375, learning_rate 0.000103188
2017-10-10T12:59:06.785709: step 1796, loss 0.182877, acc 0.96875, learning_rate 0.000103175
2017-10-10T12:59:07.300937: step 1797, loss 0.187635, acc 0.9375, learning_rate 0.000103162
2017-10-10T12:59:07.861078: step 1798, loss 0.394002, acc 0.828125, learning_rate 0.000103149
2017-10-10T12:59:08.408861: step 1799, loss 0.354669, acc 0.90625, learning_rate 0.000103136
2017-10-10T12:59:08.967808: step 1800, loss 0.128859, acc 0.953125, learning_rate 0.000103123

Evaluation:
2017-10-10T12:59:10.176997: step 1800, loss 0.246618, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1800

2017-10-10T12:59:11.647680: step 1801, loss 0.263623, acc 0.90625, learning_rate 0.000103111
2017-10-10T12:59:12.096908: step 1802, loss 0.383214, acc 0.953125, learning_rate 0.000103098
2017-10-10T12:59:12.581319: step 1803, loss 0.195972, acc 0.921875, learning_rate 0.000103085
2017-10-10T12:59:13.127495: step 1804, loss 0.200206, acc 0.921875, learning_rate 0.000103073
2017-10-10T12:59:13.632890: step 1805, loss 0.196205, acc 0.921875, learning_rate 0.00010306
2017-10-10T12:59:14.170240: step 1806, loss 0.362074, acc 0.890625, learning_rate 0.000103048
2017-10-10T12:59:14.721278: step 1807, loss 0.272935, acc 0.9375, learning_rate 0.000103035
2017-10-10T12:59:15.265409: step 1808, loss 0.193093, acc 0.9375, learning_rate 0.000103023
2017-10-10T12:59:15.709508: step 1809, loss 0.220653, acc 0.9375, learning_rate 0.00010301
2017-10-10T12:59:16.217262: step 1810, loss 0.268722, acc 0.859375, learning_rate 0.000102998
2017-10-10T12:59:16.758732: step 1811, loss 0.270829, acc 0.890625, learning_rate 0.000102986
2017-10-10T12:59:17.273393: step 1812, loss 0.372961, acc 0.84375, learning_rate 0.000102974
2017-10-10T12:59:17.737908: step 1813, loss 0.201871, acc 0.9375, learning_rate 0.000102962
2017-10-10T12:59:18.282399: step 1814, loss 0.191481, acc 0.890625, learning_rate 0.000102949
2017-10-10T12:59:18.751273: step 1815, loss 0.184535, acc 0.953125, learning_rate 0.000102937
2017-10-10T12:59:19.256875: step 1816, loss 0.233115, acc 0.921875, learning_rate 0.000102925
2017-10-10T12:59:19.760893: step 1817, loss 0.23068, acc 0.921875, learning_rate 0.000102913
2017-10-10T12:59:20.249002: step 1818, loss 0.261127, acc 0.921875, learning_rate 0.000102902
2017-10-10T12:59:20.837004: step 1819, loss 0.416448, acc 0.90625, learning_rate 0.00010289
2017-10-10T12:59:21.453012: step 1820, loss 0.327448, acc 0.875, learning_rate 0.000102878
2017-10-10T12:59:21.816886: step 1821, loss 0.261793, acc 0.90625, learning_rate 0.000102866
2017-10-10T12:59:22.227134: step 1822, loss 0.228589, acc 0.90625, learning_rate 0.000102855
2017-10-10T12:59:22.852888: step 1823, loss 0.211976, acc 0.953125, learning_rate 0.000102843
2017-10-10T12:59:23.480876: step 1824, loss 0.355728, acc 0.875, learning_rate 0.000102831
2017-10-10T12:59:23.977045: step 1825, loss 0.231019, acc 0.90625, learning_rate 0.00010282
2017-10-10T12:59:24.377043: step 1826, loss 0.269565, acc 0.90625, learning_rate 0.000102808
2017-10-10T12:59:24.897143: step 1827, loss 0.248908, acc 0.953125, learning_rate 0.000102797
2017-10-10T12:59:25.414074: step 1828, loss 0.204094, acc 0.9375, learning_rate 0.000102785
2017-10-10T12:59:25.916929: step 1829, loss 0.158648, acc 0.96875, learning_rate 0.000102774
2017-10-10T12:59:26.401046: step 1830, loss 0.176953, acc 0.953125, learning_rate 0.000102763
2017-10-10T12:59:26.889979: step 1831, loss 0.184532, acc 0.9375, learning_rate 0.000102751
2017-10-10T12:59:27.439914: step 1832, loss 0.181871, acc 0.9375, learning_rate 0.00010274
2017-10-10T12:59:27.964724: step 1833, loss 0.180236, acc 0.921875, learning_rate 0.000102729
2017-10-10T12:59:28.481409: step 1834, loss 0.149945, acc 0.96875, learning_rate 0.000102718
2017-10-10T12:59:28.993200: step 1835, loss 0.188084, acc 0.921875, learning_rate 0.000102707
2017-10-10T12:59:29.506126: step 1836, loss 0.363411, acc 0.875, learning_rate 0.000102696
2017-10-10T12:59:30.050855: step 1837, loss 0.278074, acc 0.90625, learning_rate 0.000102685
2017-10-10T12:59:30.582029: step 1838, loss 0.285431, acc 0.921875, learning_rate 0.000102674
2017-10-10T12:59:31.101445: step 1839, loss 0.429251, acc 0.859375, learning_rate 0.000102663
2017-10-10T12:59:31.625902: step 1840, loss 0.239155, acc 0.90625, learning_rate 0.000102652

Evaluation:
2017-10-10T12:59:46.217927: step 1840, loss 0.245488, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1840

2017-10-10T12:59:48.718274: step 1841, loss 0.327828, acc 0.9375, learning_rate 0.000102641
2017-10-10T12:59:49.266186: step 1842, loss 0.278616, acc 0.90625, learning_rate 0.00010263
2017-10-10T12:59:49.783552: step 1843, loss 0.202841, acc 0.9375, learning_rate 0.00010262
2017-10-10T12:59:50.211314: step 1844, loss 0.218134, acc 0.90625, learning_rate 0.000102609
2017-10-10T12:59:50.763916: step 1845, loss 0.23933, acc 0.921875, learning_rate 0.000102598
2017-10-10T12:59:51.260858: step 1846, loss 0.390951, acc 0.921875, learning_rate 0.000102588
2017-10-10T12:59:51.808845: step 1847, loss 0.259909, acc 0.890625, learning_rate 0.000102577
2017-10-10T12:59:52.386232: step 1848, loss 0.200053, acc 0.90625, learning_rate 0.000102567
2017-10-10T12:59:52.924858: step 1849, loss 0.238594, acc 0.921875, learning_rate 0.000102556
2017-10-10T12:59:53.480972: step 1850, loss 0.217063, acc 0.921875, learning_rate 0.000102546
2017-10-10T12:59:54.010263: step 1851, loss 0.192726, acc 0.953125, learning_rate 0.000102535
2017-10-10T12:59:54.527819: step 1852, loss 0.303032, acc 0.90625, learning_rate 0.000102525
2017-10-10T12:59:55.016883: step 1853, loss 0.388495, acc 0.90625, learning_rate 0.000102515
2017-10-10T12:59:55.536066: step 1854, loss 0.236248, acc 0.921875, learning_rate 0.000102504
2017-10-10T12:59:55.993965: step 1855, loss 0.223743, acc 0.90625, learning_rate 0.000102494
2017-10-10T12:59:56.591368: step 1856, loss 0.199916, acc 0.90625, learning_rate 0.000102484
2017-10-10T12:59:57.047764: step 1857, loss 0.172119, acc 0.921875, learning_rate 0.000102474
2017-10-10T12:59:57.504324: step 1858, loss 0.167594, acc 0.953125, learning_rate 0.000102464
2017-10-10T12:59:58.039365: step 1859, loss 0.238277, acc 0.90625, learning_rate 0.000102454
2017-10-10T12:59:58.559841: step 1860, loss 0.206394, acc 0.953125, learning_rate 0.000102444
2017-10-10T12:59:58.993488: step 1861, loss 0.168578, acc 0.953125, learning_rate 0.000102434
2017-10-10T12:59:59.434435: step 1862, loss 0.288596, acc 0.901961, learning_rate 0.000102424
2017-10-10T13:00:00.024835: step 1863, loss 0.142912, acc 0.96875, learning_rate 0.000102414
2017-10-10T13:00:00.558805: step 1864, loss 0.345308, acc 0.890625, learning_rate 0.000102404
2017-10-10T13:00:01.080831: step 1865, loss 0.090892, acc 0.953125, learning_rate 0.000102394
2017-10-10T13:00:01.649118: step 1866, loss 0.255872, acc 0.90625, learning_rate 0.000102384
2017-10-10T13:00:02.168856: step 1867, loss 0.329836, acc 0.90625, learning_rate 0.000102375
2017-10-10T13:00:02.688855: step 1868, loss 0.172782, acc 0.9375, learning_rate 0.000102365
2017-10-10T13:00:03.216849: step 1869, loss 0.285365, acc 0.921875, learning_rate 0.000102355
2017-10-10T13:00:03.763955: step 1870, loss 0.189639, acc 0.90625, learning_rate 0.000102346
2017-10-10T13:00:04.293084: step 1871, loss 0.25484, acc 0.921875, learning_rate 0.000102336
2017-10-10T13:00:04.844853: step 1872, loss 0.23254, acc 0.9375, learning_rate 0.000102327
2017-10-10T13:00:05.358187: step 1873, loss 0.159087, acc 0.953125, learning_rate 0.000102317
2017-10-10T13:00:06.319993: step 1874, loss 0.31314, acc 0.890625, learning_rate 0.000102308
2017-10-10T13:00:06.859114: step 1875, loss 0.181375, acc 0.921875, learning_rate 0.000102298
2017-10-10T13:00:07.418460: step 1876, loss 0.320419, acc 0.859375, learning_rate 0.000102289
2017-10-10T13:00:07.950426: step 1877, loss 0.244404, acc 0.90625, learning_rate 0.000102279
2017-10-10T13:00:08.507512: step 1878, loss 0.210144, acc 0.875, learning_rate 0.00010227
2017-10-10T13:00:09.044867: step 1879, loss 0.1875, acc 0.953125, learning_rate 0.000102261
2017-10-10T13:00:09.596187: step 1880, loss 0.123725, acc 0.9375, learning_rate 0.000102252

Evaluation:
2017-10-10T13:00:11.057047: step 1880, loss 0.247845, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1880

2017-10-10T13:00:12.872966: step 1881, loss 0.228638, acc 0.921875, learning_rate 0.000102242
2017-10-10T13:00:13.387445: step 1882, loss 0.43844, acc 0.859375, learning_rate 0.000102233
2017-10-10T13:00:13.916300: step 1883, loss 0.267266, acc 0.921875, learning_rate 0.000102224
2017-10-10T13:00:14.488874: step 1884, loss 0.374936, acc 0.90625, learning_rate 0.000102215
2017-10-10T13:00:15.057242: step 1885, loss 0.107356, acc 0.953125, learning_rate 0.000102206
2017-10-10T13:00:15.581014: step 1886, loss 0.125511, acc 0.96875, learning_rate 0.000102197
2017-10-10T13:00:16.043993: step 1887, loss 0.212154, acc 0.9375, learning_rate 0.000102188
2017-10-10T13:00:16.588578: step 1888, loss 0.419255, acc 0.875, learning_rate 0.000102179
2017-10-10T13:00:17.069070: step 1889, loss 0.22469, acc 0.9375, learning_rate 0.00010217
2017-10-10T13:00:17.620607: step 1890, loss 0.303841, acc 0.890625, learning_rate 0.000102161
2017-10-10T13:00:18.189115: step 1891, loss 0.255397, acc 0.9375, learning_rate 0.000102153
2017-10-10T13:00:18.708148: step 1892, loss 0.222159, acc 0.953125, learning_rate 0.000102144
2017-10-10T13:00:19.253705: step 1893, loss 0.216977, acc 0.953125, learning_rate 0.000102135
2017-10-10T13:00:19.872987: step 1894, loss 0.150745, acc 0.96875, learning_rate 0.000102126
2017-10-10T13:00:20.365056: step 1895, loss 0.174124, acc 0.96875, learning_rate 0.000102118
2017-10-10T13:00:20.828848: step 1896, loss 0.28333, acc 0.90625, learning_rate 0.000102109
2017-10-10T13:00:21.271614: step 1897, loss 0.199256, acc 0.9375, learning_rate 0.0001021
2017-10-10T13:00:21.704844: step 1898, loss 0.251363, acc 0.90625, learning_rate 0.000102092
2017-10-10T13:00:22.142902: step 1899, loss 0.280452, acc 0.90625, learning_rate 0.000102083
2017-10-10T13:00:22.702701: step 1900, loss 0.206295, acc 0.9375, learning_rate 0.000102075
2017-10-10T13:00:23.208829: step 1901, loss 0.302798, acc 0.890625, learning_rate 0.000102066
2017-10-10T13:00:23.700885: step 1902, loss 0.220859, acc 0.921875, learning_rate 0.000102058
2017-10-10T13:00:24.269243: step 1903, loss 0.214195, acc 0.921875, learning_rate 0.00010205
2017-10-10T13:00:24.788883: step 1904, loss 0.183724, acc 0.921875, learning_rate 0.000102041
2017-10-10T13:00:25.234110: step 1905, loss 0.16558, acc 0.953125, learning_rate 0.000102033
2017-10-10T13:00:25.755906: step 1906, loss 0.227637, acc 0.921875, learning_rate 0.000102025
2017-10-10T13:00:26.221468: step 1907, loss 0.141772, acc 0.9375, learning_rate 0.000102016
2017-10-10T13:00:26.767385: step 1908, loss 0.112794, acc 0.984375, learning_rate 0.000102008
2017-10-10T13:00:27.237629: step 1909, loss 0.187038, acc 0.953125, learning_rate 0.000102
2017-10-10T13:00:27.732914: step 1910, loss 0.14492, acc 0.96875, learning_rate 0.000101992
2017-10-10T13:00:28.264912: step 1911, loss 0.300562, acc 0.875, learning_rate 0.000101984
2017-10-10T13:00:28.909126: step 1912, loss 0.223963, acc 0.9375, learning_rate 0.000101975
2017-10-10T13:00:29.420936: step 1913, loss 0.208866, acc 0.921875, learning_rate 0.000101967
2017-10-10T13:00:30.024513: step 1914, loss 0.209878, acc 0.90625, learning_rate 0.000101959
2017-10-10T13:00:30.567088: step 1915, loss 0.24343, acc 0.90625, learning_rate 0.000101951
2017-10-10T13:00:31.104833: step 1916, loss 0.234727, acc 0.921875, learning_rate 0.000101943
2017-10-10T13:00:31.527776: step 1917, loss 0.206386, acc 0.921875, learning_rate 0.000101935
2017-10-10T13:00:32.027611: step 1918, loss 0.26819, acc 0.921875, learning_rate 0.000101928
2017-10-10T13:00:32.568963: step 1919, loss 0.209695, acc 0.90625, learning_rate 0.00010192
2017-10-10T13:00:33.096430: step 1920, loss 0.196819, acc 0.9375, learning_rate 0.000101912

Evaluation:
2017-10-10T13:00:34.497121: step 1920, loss 0.246436, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1920

2017-10-10T13:00:35.944026: step 1921, loss 0.175258, acc 0.96875, learning_rate 0.000101904
2017-10-10T13:00:36.469165: step 1922, loss 0.306618, acc 0.875, learning_rate 0.000101896
2017-10-10T13:00:37.089092: step 1923, loss 0.229334, acc 0.9375, learning_rate 0.000101889
2017-10-10T13:00:37.607608: step 1924, loss 0.23978, acc 0.9375, learning_rate 0.000101881
2017-10-10T13:00:38.156894: step 1925, loss 0.290212, acc 0.9375, learning_rate 0.000101873
2017-10-10T13:00:38.706497: step 1926, loss 0.220319, acc 0.890625, learning_rate 0.000101865
2017-10-10T13:00:39.271425: step 1927, loss 0.29725, acc 0.9375, learning_rate 0.000101858
2017-10-10T13:00:39.796934: step 1928, loss 0.275362, acc 0.890625, learning_rate 0.00010185
2017-10-10T13:00:40.392840: step 1929, loss 0.145479, acc 0.96875, learning_rate 0.000101843
2017-10-10T13:00:40.912889: step 1930, loss 0.124718, acc 0.953125, learning_rate 0.000101835
2017-10-10T13:00:41.456841: step 1931, loss 0.0786948, acc 1, learning_rate 0.000101828
2017-10-10T13:00:42.000051: step 1932, loss 0.162461, acc 0.9375, learning_rate 0.00010182
2017-10-10T13:00:42.551423: step 1933, loss 0.352235, acc 0.828125, learning_rate 0.000101813
2017-10-10T13:00:43.133099: step 1934, loss 0.289603, acc 0.921875, learning_rate 0.000101805
2017-10-10T13:00:43.781252: step 1935, loss 0.333234, acc 0.875, learning_rate 0.000101798
2017-10-10T13:00:43.973073: step 1936, loss 0.252364, acc 0.921875, learning_rate 0.000101791
2017-10-10T13:00:44.329477: step 1937, loss 0.236682, acc 0.921875, learning_rate 0.000101783
2017-10-10T13:00:44.788381: step 1938, loss 0.25431, acc 0.90625, learning_rate 0.000101776
2017-10-10T13:00:45.221394: step 1939, loss 0.209154, acc 0.921875, learning_rate 0.000101769
2017-10-10T13:00:45.767908: step 1940, loss 0.324193, acc 0.875, learning_rate 0.000101762
2017-10-10T13:00:46.352968: step 1941, loss 0.21291, acc 0.96875, learning_rate 0.000101754
2017-10-10T13:00:46.856593: step 1942, loss 0.284096, acc 0.890625, learning_rate 0.000101747
2017-10-10T13:00:47.473210: step 1943, loss 0.170742, acc 0.9375, learning_rate 0.00010174
2017-10-10T13:00:47.940958: step 1944, loss 0.265334, acc 0.90625, learning_rate 0.000101733
2017-10-10T13:00:48.404692: step 1945, loss 0.239637, acc 0.9375, learning_rate 0.000101726
2017-10-10T13:00:48.945606: step 1946, loss 0.42533, acc 0.859375, learning_rate 0.000101719
2017-10-10T13:00:49.920380: step 1947, loss 0.371668, acc 0.890625, learning_rate 0.000101712
2017-10-10T13:00:50.524409: step 1948, loss 0.245391, acc 0.890625, learning_rate 0.000101705
2017-10-10T13:00:51.052848: step 1949, loss 0.197784, acc 0.9375, learning_rate 0.000101698
2017-10-10T13:00:51.594635: step 1950, loss 0.188625, acc 0.921875, learning_rate 0.000101691
2017-10-10T13:00:52.185035: step 1951, loss 0.220304, acc 0.890625, learning_rate 0.000101684
2017-10-10T13:00:52.706030: step 1952, loss 0.157982, acc 0.9375, learning_rate 0.000101677
2017-10-10T13:00:53.221405: step 1953, loss 0.26331, acc 0.921875, learning_rate 0.00010167
2017-10-10T13:00:53.775411: step 1954, loss 0.337313, acc 0.890625, learning_rate 0.000101664
2017-10-10T13:00:54.307513: step 1955, loss 0.186807, acc 0.953125, learning_rate 0.000101657
2017-10-10T13:00:54.799017: step 1956, loss 0.175025, acc 0.953125, learning_rate 0.00010165
2017-10-10T13:00:55.305352: step 1957, loss 0.169106, acc 0.921875, learning_rate 0.000101643
2017-10-10T13:00:55.822827: step 1958, loss 0.203599, acc 0.921875, learning_rate 0.000101637
2017-10-10T13:00:56.316009: step 1959, loss 0.312538, acc 0.84375, learning_rate 0.00010163
2017-10-10T13:00:56.749977: step 1960, loss 0.150809, acc 0.960784, learning_rate 0.000101623

Evaluation:
2017-10-10T13:00:58.147235: step 1960, loss 0.245731, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-1960

2017-10-10T13:00:59.666886: step 1961, loss 0.109759, acc 0.953125, learning_rate 0.000101617
2017-10-10T13:01:00.155195: step 1962, loss 0.231569, acc 0.921875, learning_rate 0.00010161
2017-10-10T13:01:00.712942: step 1963, loss 0.285217, acc 0.90625, learning_rate 0.000101604
2017-10-10T13:01:01.256615: step 1964, loss 0.302738, acc 0.875, learning_rate 0.000101597
2017-10-10T13:01:01.846358: step 1965, loss 0.281321, acc 0.90625, learning_rate 0.00010159
2017-10-10T13:01:02.404830: step 1966, loss 0.179358, acc 0.921875, learning_rate 0.000101584
2017-10-10T13:01:02.941314: step 1967, loss 0.244796, acc 0.9375, learning_rate 0.000101577
2017-10-10T13:01:03.484848: step 1968, loss 0.33939, acc 0.875, learning_rate 0.000101571
2017-10-10T13:01:04.005358: step 1969, loss 0.410316, acc 0.890625, learning_rate 0.000101565
2017-10-10T13:01:04.535520: step 1970, loss 0.156902, acc 0.953125, learning_rate 0.000101558
2017-10-10T13:01:05.083599: step 1971, loss 0.186585, acc 0.921875, learning_rate 0.000101552
2017-10-10T13:01:05.597262: step 1972, loss 0.235244, acc 0.921875, learning_rate 0.000101546
2017-10-10T13:01:06.155615: step 1973, loss 0.297047, acc 0.890625, learning_rate 0.000101539
2017-10-10T13:01:06.708751: step 1974, loss 0.123663, acc 0.9375, learning_rate 0.000101533
2017-10-10T13:01:07.242855: step 1975, loss 0.406899, acc 0.84375, learning_rate 0.000101527
2017-10-10T13:01:07.577485: step 1976, loss 0.348059, acc 0.921875, learning_rate 0.00010152
2017-10-10T13:01:07.927052: step 1977, loss 0.192481, acc 0.96875, learning_rate 0.000101514
2017-10-10T13:01:08.392861: step 1978, loss 0.124795, acc 0.96875, learning_rate 0.000101508
2017-10-10T13:01:08.949093: step 1979, loss 0.298342, acc 0.875, learning_rate 0.000101502
2017-10-10T13:01:09.505386: step 1980, loss 0.162983, acc 0.953125, learning_rate 0.000101496
2017-10-10T13:01:09.983516: step 1981, loss 0.273252, acc 0.90625, learning_rate 0.00010149
2017-10-10T13:01:10.476989: step 1982, loss 0.186158, acc 0.921875, learning_rate 0.000101484
2017-10-10T13:01:10.968456: step 1983, loss 0.258695, acc 0.890625, learning_rate 0.000101478
2017-10-10T13:01:11.460862: step 1984, loss 0.281083, acc 0.859375, learning_rate 0.000101472
2017-10-10T13:01:11.984880: step 1985, loss 0.29453, acc 0.890625, learning_rate 0.000101466
2017-10-10T13:01:12.430373: step 1986, loss 0.184922, acc 0.9375, learning_rate 0.00010146
2017-10-10T13:01:12.969049: step 1987, loss 0.174859, acc 0.9375, learning_rate 0.000101454
2017-10-10T13:01:13.371774: step 1988, loss 0.327887, acc 0.875, learning_rate 0.000101448
2017-10-10T13:01:13.889170: step 1989, loss 0.242123, acc 0.890625, learning_rate 0.000101442
2017-10-10T13:01:14.441062: step 1990, loss 0.200662, acc 0.953125, learning_rate 0.000101436
2017-10-10T13:01:15.100391: step 1991, loss 0.138742, acc 0.953125, learning_rate 0.00010143
2017-10-10T13:01:15.688342: step 1992, loss 0.191277, acc 0.9375, learning_rate 0.000101424
2017-10-10T13:01:16.180918: step 1993, loss 0.330512, acc 0.890625, learning_rate 0.000101418
2017-10-10T13:01:16.691955: step 1994, loss 0.313096, acc 0.84375, learning_rate 0.000101413
2017-10-10T13:01:17.205081: step 1995, loss 0.0914942, acc 0.96875, learning_rate 0.000101407
2017-10-10T13:01:17.712886: step 1996, loss 0.211943, acc 0.90625, learning_rate 0.000101401
2017-10-10T13:01:18.161101: step 1997, loss 0.225133, acc 0.890625, learning_rate 0.000101395
2017-10-10T13:01:18.593203: step 1998, loss 0.238614, acc 0.90625, learning_rate 0.00010139
2017-10-10T13:01:19.121122: step 1999, loss 0.230343, acc 0.921875, learning_rate 0.000101384
2017-10-10T13:01:19.645503: step 2000, loss 0.305341, acc 0.90625, learning_rate 0.000101378

Evaluation:
2017-10-10T13:01:20.909006: step 2000, loss 0.246715, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2000

2017-10-10T13:01:22.537122: step 2001, loss 0.177316, acc 0.9375, learning_rate 0.000101373
2017-10-10T13:01:23.057962: step 2002, loss 0.271918, acc 0.875, learning_rate 0.000101367
2017-10-10T13:01:23.555048: step 2003, loss 0.482358, acc 0.875, learning_rate 0.000101362
2017-10-10T13:01:24.060667: step 2004, loss 0.37558, acc 0.875, learning_rate 0.000101356
2017-10-10T13:01:24.646788: step 2005, loss 0.341598, acc 0.90625, learning_rate 0.00010135
2017-10-10T13:01:25.184998: step 2006, loss 0.270131, acc 0.90625, learning_rate 0.000101345
2017-10-10T13:01:25.725140: step 2007, loss 0.201902, acc 0.9375, learning_rate 0.000101339
2017-10-10T13:01:26.256930: step 2008, loss 0.467776, acc 0.90625, learning_rate 0.000101334
2017-10-10T13:01:26.762277: step 2009, loss 0.299097, acc 0.9375, learning_rate 0.000101328
2017-10-10T13:01:27.338671: step 2010, loss 0.385286, acc 0.875, learning_rate 0.000101323
2017-10-10T13:01:27.763261: step 2011, loss 0.271378, acc 0.90625, learning_rate 0.000101318
2017-10-10T13:01:28.236957: step 2012, loss 0.258281, acc 0.875, learning_rate 0.000101312
2017-10-10T13:01:28.757004: step 2013, loss 0.249441, acc 0.890625, learning_rate 0.000101307
2017-10-10T13:01:29.521017: step 2014, loss 0.0970325, acc 0.984375, learning_rate 0.000101302
2017-10-10T13:01:30.026430: step 2015, loss 0.201642, acc 0.90625, learning_rate 0.000101296
2017-10-10T13:01:30.321361: step 2016, loss 0.177438, acc 0.921875, learning_rate 0.000101291
2017-10-10T13:01:30.676929: step 2017, loss 0.399955, acc 0.828125, learning_rate 0.000101286
2017-10-10T13:01:31.003361: step 2018, loss 0.168671, acc 0.921875, learning_rate 0.00010128
2017-10-10T13:01:31.556183: step 2019, loss 0.200121, acc 0.921875, learning_rate 0.000101275
2017-10-10T13:01:32.129026: step 2020, loss 0.114125, acc 0.984375, learning_rate 0.00010127
2017-10-10T13:01:32.624882: step 2021, loss 0.389595, acc 0.859375, learning_rate 0.000101265
2017-10-10T13:01:33.078893: step 2022, loss 0.279603, acc 0.921875, learning_rate 0.00010126
2017-10-10T13:01:33.588223: step 2023, loss 0.114651, acc 0.984375, learning_rate 0.000101255
2017-10-10T13:01:34.049052: step 2024, loss 0.196183, acc 0.921875, learning_rate 0.000101249
2017-10-10T13:01:34.621025: step 2025, loss 0.341063, acc 0.90625, learning_rate 0.000101244
2017-10-10T13:01:35.130076: step 2026, loss 0.162706, acc 0.9375, learning_rate 0.000101239
2017-10-10T13:01:35.625734: step 2027, loss 0.353791, acc 0.875, learning_rate 0.000101234
2017-10-10T13:01:36.093765: step 2028, loss 0.288254, acc 0.90625, learning_rate 0.000101229
2017-10-10T13:01:36.599784: step 2029, loss 0.296545, acc 0.90625, learning_rate 0.000101224
2017-10-10T13:01:37.130347: step 2030, loss 0.283803, acc 0.875, learning_rate 0.000101219
2017-10-10T13:01:37.629075: step 2031, loss 0.311529, acc 0.859375, learning_rate 0.000101214
2017-10-10T13:01:38.155608: step 2032, loss 0.209474, acc 0.921875, learning_rate 0.000101209
2017-10-10T13:01:38.675860: step 2033, loss 0.223258, acc 0.859375, learning_rate 0.000101204
2017-10-10T13:01:39.195135: step 2034, loss 0.249445, acc 0.90625, learning_rate 0.000101199
2017-10-10T13:01:39.722705: step 2035, loss 0.184586, acc 0.9375, learning_rate 0.000101194
2017-10-10T13:01:40.177050: step 2036, loss 0.10738, acc 0.984375, learning_rate 0.00010119
2017-10-10T13:01:40.644414: step 2037, loss 0.233787, acc 0.859375, learning_rate 0.000101185
2017-10-10T13:01:41.122590: step 2038, loss 0.325398, acc 0.921875, learning_rate 0.00010118
2017-10-10T13:01:41.720911: step 2039, loss 0.175318, acc 0.953125, learning_rate 0.000101175
2017-10-10T13:01:42.220968: step 2040, loss 0.342383, acc 0.84375, learning_rate 0.00010117

Evaluation:
2017-10-10T13:01:43.400875: step 2040, loss 0.245423, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2040

2017-10-10T13:01:45.745122: step 2041, loss 0.247614, acc 0.90625, learning_rate 0.000101166
2017-10-10T13:01:46.222695: step 2042, loss 0.350082, acc 0.90625, learning_rate 0.000101161
2017-10-10T13:01:46.743555: step 2043, loss 0.290778, acc 0.890625, learning_rate 0.000101156
2017-10-10T13:01:47.332887: step 2044, loss 0.212535, acc 0.921875, learning_rate 0.000101151
2017-10-10T13:01:47.874773: step 2045, loss 0.294315, acc 0.890625, learning_rate 0.000101147
2017-10-10T13:01:48.408961: step 2046, loss 0.151205, acc 0.9375, learning_rate 0.000101142
2017-10-10T13:01:48.909667: step 2047, loss 0.357248, acc 0.875, learning_rate 0.000101137
2017-10-10T13:01:49.445941: step 2048, loss 0.106809, acc 0.984375, learning_rate 0.000101133
2017-10-10T13:01:50.012078: step 2049, loss 0.209285, acc 0.953125, learning_rate 0.000101128
2017-10-10T13:01:50.512980: step 2050, loss 0.176397, acc 0.953125, learning_rate 0.000101123
2017-10-10T13:01:51.056056: step 2051, loss 0.200963, acc 0.921875, learning_rate 0.000101119
2017-10-10T13:01:51.591185: step 2052, loss 0.183742, acc 0.9375, learning_rate 0.000101114
2017-10-10T13:01:52.174001: step 2053, loss 0.142328, acc 0.953125, learning_rate 0.00010111
2017-10-10T13:01:52.700860: step 2054, loss 0.129682, acc 0.96875, learning_rate 0.000101105
2017-10-10T13:01:53.260560: step 2055, loss 0.352883, acc 0.859375, learning_rate 0.000101101
2017-10-10T13:01:53.524331: step 2056, loss 0.196384, acc 0.9375, learning_rate 0.000101096
2017-10-10T13:01:54.032473: step 2057, loss 0.234718, acc 0.9375, learning_rate 0.000101092
2017-10-10T13:01:54.483059: step 2058, loss 0.224887, acc 0.901961, learning_rate 0.000101087
2017-10-10T13:01:54.993148: step 2059, loss 0.236702, acc 0.90625, learning_rate 0.000101083
2017-10-10T13:01:55.464846: step 2060, loss 0.0973637, acc 0.984375, learning_rate 0.000101078
2017-10-10T13:01:55.954758: step 2061, loss 0.41164, acc 0.890625, learning_rate 0.000101074
2017-10-10T13:01:56.416946: step 2062, loss 0.190161, acc 0.9375, learning_rate 0.00010107
2017-10-10T13:01:56.965195: step 2063, loss 0.173391, acc 0.953125, learning_rate 0.000101065
2017-10-10T13:01:57.468968: step 2064, loss 0.139197, acc 0.96875, learning_rate 0.000101061
2017-10-10T13:01:58.005011: step 2065, loss 0.204765, acc 0.9375, learning_rate 0.000101057
2017-10-10T13:01:58.514433: step 2066, loss 0.369067, acc 0.921875, learning_rate 0.000101052
2017-10-10T13:01:59.021629: step 2067, loss 0.320896, acc 0.890625, learning_rate 0.000101048
2017-10-10T13:01:59.554342: step 2068, loss 0.155982, acc 0.953125, learning_rate 0.000101044
2017-10-10T13:02:00.046991: step 2069, loss 0.101403, acc 0.96875, learning_rate 0.000101039
2017-10-10T13:02:00.556621: step 2070, loss 0.258274, acc 0.890625, learning_rate 0.000101035
2017-10-10T13:02:01.092952: step 2071, loss 0.326826, acc 0.9375, learning_rate 0.000101031
2017-10-10T13:02:01.673089: step 2072, loss 0.311785, acc 0.875, learning_rate 0.000101027
2017-10-10T13:02:02.298187: step 2073, loss 0.225383, acc 0.9375, learning_rate 0.000101023
2017-10-10T13:02:02.822808: step 2074, loss 0.234692, acc 0.890625, learning_rate 0.000101018
2017-10-10T13:02:03.381960: step 2075, loss 0.166519, acc 0.953125, learning_rate 0.000101014
2017-10-10T13:02:03.917764: step 2076, loss 0.332198, acc 0.859375, learning_rate 0.00010101
2017-10-10T13:02:04.463338: step 2077, loss 0.159707, acc 0.9375, learning_rate 0.000101006
2017-10-10T13:02:04.944869: step 2078, loss 0.355749, acc 0.890625, learning_rate 0.000101002
2017-10-10T13:02:05.440653: step 2079, loss 0.251858, acc 0.9375, learning_rate 0.000100998
2017-10-10T13:02:05.980444: step 2080, loss 0.273331, acc 0.921875, learning_rate 0.000100994

Evaluation:
2017-10-10T13:02:07.163540: step 2080, loss 0.243834, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2080

2017-10-10T13:02:08.604877: step 2081, loss 0.240076, acc 0.90625, learning_rate 0.00010099
2017-10-10T13:02:09.174201: step 2082, loss 0.205458, acc 0.890625, learning_rate 0.000100986
2017-10-10T13:02:09.690075: step 2083, loss 0.394046, acc 0.875, learning_rate 0.000100982
2017-10-10T13:02:10.176924: step 2084, loss 0.260395, acc 0.9375, learning_rate 0.000100978
2017-10-10T13:02:10.649197: step 2085, loss 0.208577, acc 0.9375, learning_rate 0.000100974
2017-10-10T13:02:11.125132: step 2086, loss 0.237823, acc 0.9375, learning_rate 0.00010097
2017-10-10T13:02:11.673300: step 2087, loss 0.343798, acc 0.875, learning_rate 0.000100966
2017-10-10T13:02:12.168393: step 2088, loss 0.194364, acc 0.9375, learning_rate 0.000100962
2017-10-10T13:02:12.701638: step 2089, loss 0.248264, acc 0.921875, learning_rate 0.000100958
2017-10-10T13:02:13.193122: step 2090, loss 0.250356, acc 0.953125, learning_rate 0.000100954
2017-10-10T13:02:13.723134: step 2091, loss 0.358437, acc 0.84375, learning_rate 0.00010095
2017-10-10T13:02:14.277138: step 2092, loss 0.228333, acc 0.953125, learning_rate 0.000100946
2017-10-10T13:02:14.829008: step 2093, loss 0.234392, acc 0.9375, learning_rate 0.000100942
2017-10-10T13:02:15.409007: step 2094, loss 0.28004, acc 0.875, learning_rate 0.000100938
2017-10-10T13:02:15.893274: step 2095, loss 0.245601, acc 0.875, learning_rate 0.000100935
2017-10-10T13:02:16.525712: step 2096, loss 0.275587, acc 0.90625, learning_rate 0.000100931
2017-10-10T13:02:16.980827: step 2097, loss 0.233053, acc 0.890625, learning_rate 0.000100927
2017-10-10T13:02:17.404814: step 2098, loss 0.22885, acc 0.9375, learning_rate 0.000100923
2017-10-10T13:02:17.867364: step 2099, loss 0.333474, acc 0.890625, learning_rate 0.000100919
2017-10-10T13:02:18.385036: step 2100, loss 0.353458, acc 0.90625, learning_rate 0.000100916
2017-10-10T13:02:18.944930: step 2101, loss 0.0936843, acc 0.953125, learning_rate 0.000100912
2017-10-10T13:02:19.437552: step 2102, loss 0.0808959, acc 0.96875, learning_rate 0.000100908
2017-10-10T13:02:19.881179: step 2103, loss 0.110045, acc 0.96875, learning_rate 0.000100904
2017-10-10T13:02:20.393286: step 2104, loss 0.293743, acc 0.90625, learning_rate 0.000100901
2017-10-10T13:02:20.945905: step 2105, loss 0.213565, acc 0.921875, learning_rate 0.000100897
2017-10-10T13:02:21.402257: step 2106, loss 0.153186, acc 0.96875, learning_rate 0.000100893
2017-10-10T13:02:21.936994: step 2107, loss 0.299207, acc 0.890625, learning_rate 0.00010089
2017-10-10T13:02:22.496945: step 2108, loss 0.258424, acc 0.890625, learning_rate 0.000100886
2017-10-10T13:02:23.021666: step 2109, loss 0.186436, acc 0.953125, learning_rate 0.000100883
2017-10-10T13:02:23.565343: step 2110, loss 0.214037, acc 0.921875, learning_rate 0.000100879
2017-10-10T13:02:24.144833: step 2111, loss 0.295135, acc 0.90625, learning_rate 0.000100875
2017-10-10T13:02:24.724851: step 2112, loss 0.301849, acc 0.875, learning_rate 0.000100872
2017-10-10T13:02:25.243426: step 2113, loss 0.342182, acc 0.890625, learning_rate 0.000100868
2017-10-10T13:02:25.748852: step 2114, loss 0.406426, acc 0.84375, learning_rate 0.000100865
2017-10-10T13:02:26.240906: step 2115, loss 0.204416, acc 0.90625, learning_rate 0.000100861
2017-10-10T13:02:26.784935: step 2116, loss 0.185705, acc 0.921875, learning_rate 0.000100858
2017-10-10T13:02:27.295558: step 2117, loss 0.164358, acc 0.9375, learning_rate 0.000100854
2017-10-10T13:02:27.829072: step 2118, loss 0.328613, acc 0.90625, learning_rate 0.000100851
2017-10-10T13:02:28.404950: step 2119, loss 0.221401, acc 0.921875, learning_rate 0.000100847
2017-10-10T13:02:28.950398: step 2120, loss 0.27238, acc 0.921875, learning_rate 0.000100844

Evaluation:
2017-10-10T13:02:30.165945: step 2120, loss 0.244001, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2120

2017-10-10T13:02:31.589059: step 2121, loss 0.106651, acc 0.96875, learning_rate 0.00010084
2017-10-10T13:02:32.190751: step 2122, loss 0.285476, acc 0.890625, learning_rate 0.000100837
2017-10-10T13:02:32.694976: step 2123, loss 0.186868, acc 0.90625, learning_rate 0.000100833
2017-10-10T13:02:33.163298: step 2124, loss 0.0951621, acc 0.984375, learning_rate 0.00010083
2017-10-10T13:02:33.740972: step 2125, loss 0.128349, acc 0.96875, learning_rate 0.000100827
2017-10-10T13:02:34.272024: step 2126, loss 0.194621, acc 0.9375, learning_rate 0.000100823
2017-10-10T13:02:34.801258: step 2127, loss 0.213306, acc 0.90625, learning_rate 0.00010082
2017-10-10T13:02:35.421006: step 2128, loss 0.128924, acc 0.96875, learning_rate 0.000100817
2017-10-10T13:02:35.888875: step 2129, loss 0.181531, acc 0.921875, learning_rate 0.000100813
2017-10-10T13:02:36.416915: step 2130, loss 0.294619, acc 0.90625, learning_rate 0.00010081
2017-10-10T13:02:36.997047: step 2131, loss 0.260578, acc 0.890625, learning_rate 0.000100807
2017-10-10T13:02:37.605472: step 2132, loss 0.171035, acc 0.921875, learning_rate 0.000100803
2017-10-10T13:02:38.124864: step 2133, loss 0.203323, acc 0.921875, learning_rate 0.0001008
2017-10-10T13:02:38.533143: step 2134, loss 0.195023, acc 0.90625, learning_rate 0.000100797
2017-10-10T13:02:39.109085: step 2135, loss 0.194005, acc 0.921875, learning_rate 0.000100793
2017-10-10T13:02:39.626566: step 2136, loss 0.237729, acc 0.921875, learning_rate 0.00010079
2017-10-10T13:02:40.179478: step 2137, loss 0.288906, acc 0.890625, learning_rate 0.000100787
2017-10-10T13:02:40.621533: step 2138, loss 0.23784, acc 0.890625, learning_rate 0.000100784
2017-10-10T13:02:41.081014: step 2139, loss 0.187084, acc 0.9375, learning_rate 0.000100781
2017-10-10T13:02:41.567690: step 2140, loss 0.270728, acc 0.90625, learning_rate 0.000100777
2017-10-10T13:02:42.105042: step 2141, loss 0.281725, acc 0.890625, learning_rate 0.000100774
2017-10-10T13:02:42.588574: step 2142, loss 0.120969, acc 0.96875, learning_rate 0.000100771
2017-10-10T13:02:43.119430: step 2143, loss 0.208659, acc 0.9375, learning_rate 0.000100768
2017-10-10T13:02:43.652659: step 2144, loss 0.139107, acc 0.9375, learning_rate 0.000100765
2017-10-10T13:02:44.187422: step 2145, loss 0.232078, acc 0.9375, learning_rate 0.000100762
2017-10-10T13:02:44.684877: step 2146, loss 0.174351, acc 0.953125, learning_rate 0.000100759
2017-10-10T13:02:45.109084: step 2147, loss 0.158375, acc 0.9375, learning_rate 0.000100755
2017-10-10T13:02:45.565110: step 2148, loss 0.142574, acc 0.953125, learning_rate 0.000100752
2017-10-10T13:02:46.094561: step 2149, loss 0.356837, acc 0.921875, learning_rate 0.000100749
2017-10-10T13:02:46.567242: step 2150, loss 0.263111, acc 0.890625, learning_rate 0.000100746
2017-10-10T13:02:47.044924: step 2151, loss 0.250282, acc 0.921875, learning_rate 0.000100743
2017-10-10T13:02:47.540944: step 2152, loss 0.438899, acc 0.875, learning_rate 0.00010074
2017-10-10T13:02:48.117046: step 2153, loss 0.277602, acc 0.9375, learning_rate 0.000100737
2017-10-10T13:02:48.634365: step 2154, loss 0.182072, acc 0.953125, learning_rate 0.000100734
2017-10-10T13:02:49.144974: step 2155, loss 0.238555, acc 0.875, learning_rate 0.000100731
2017-10-10T13:02:49.590177: step 2156, loss 0.396458, acc 0.862745, learning_rate 0.000100728
2017-10-10T13:02:50.111621: step 2157, loss 0.143514, acc 0.96875, learning_rate 0.000100725
2017-10-10T13:02:50.616898: step 2158, loss 0.173696, acc 0.9375, learning_rate 0.000100722
2017-10-10T13:02:51.101245: step 2159, loss 0.22176, acc 0.9375, learning_rate 0.000100719
2017-10-10T13:02:51.676956: step 2160, loss 0.190872, acc 0.9375, learning_rate 0.000100716

Evaluation:
2017-10-10T13:02:53.072995: step 2160, loss 0.24161, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2160

2017-10-10T13:02:54.778681: step 2161, loss 0.239988, acc 0.90625, learning_rate 0.000100713
2017-10-10T13:02:55.296944: step 2162, loss 0.250203, acc 0.90625, learning_rate 0.000100711
2017-10-10T13:02:55.793060: step 2163, loss 0.172094, acc 0.9375, learning_rate 0.000100708
2017-10-10T13:02:56.288033: step 2164, loss 0.265205, acc 0.921875, learning_rate 0.000100705
2017-10-10T13:02:56.820070: step 2165, loss 0.225438, acc 0.921875, learning_rate 0.000100702
2017-10-10T13:02:57.347162: step 2166, loss 0.228639, acc 0.90625, learning_rate 0.000100699
2017-10-10T13:02:57.861080: step 2167, loss 0.135426, acc 0.96875, learning_rate 0.000100696
2017-10-10T13:02:58.384287: step 2168, loss 0.325991, acc 0.875, learning_rate 0.000100693
2017-10-10T13:02:58.933475: step 2169, loss 0.290603, acc 0.890625, learning_rate 0.00010069
2017-10-10T13:02:59.527430: step 2170, loss 0.230466, acc 0.921875, learning_rate 0.000100688
2017-10-10T13:03:00.070133: step 2171, loss 0.261015, acc 0.875, learning_rate 0.000100685
2017-10-10T13:03:00.488864: step 2172, loss 0.202346, acc 0.90625, learning_rate 0.000100682
2017-10-10T13:03:00.904746: step 2173, loss 0.351082, acc 0.84375, learning_rate 0.000100679
2017-10-10T13:03:01.449028: step 2174, loss 0.320454, acc 0.859375, learning_rate 0.000100677
2017-10-10T13:03:01.962295: step 2175, loss 0.301904, acc 0.875, learning_rate 0.000100674
2017-10-10T13:03:02.496890: step 2176, loss 0.189099, acc 0.921875, learning_rate 0.000100671
2017-10-10T13:03:03.123769: step 2177, loss 0.105932, acc 0.96875, learning_rate 0.000100668
2017-10-10T13:03:03.576807: step 2178, loss 0.333632, acc 0.90625, learning_rate 0.000100666
2017-10-10T13:03:04.033788: step 2179, loss 0.349149, acc 0.890625, learning_rate 0.000100663
2017-10-10T13:03:04.578198: step 2180, loss 0.292129, acc 0.90625, learning_rate 0.00010066
2017-10-10T13:03:05.123518: step 2181, loss 0.63359, acc 0.8125, learning_rate 0.000100657
2017-10-10T13:03:05.665968: step 2182, loss 0.221835, acc 0.9375, learning_rate 0.000100655
2017-10-10T13:03:06.199360: step 2183, loss 0.193717, acc 0.921875, learning_rate 0.000100652
2017-10-10T13:03:06.736882: step 2184, loss 0.321108, acc 0.890625, learning_rate 0.000100649
2017-10-10T13:03:07.279869: step 2185, loss 0.113454, acc 0.953125, learning_rate 0.000100647
2017-10-10T13:03:07.848881: step 2186, loss 0.239877, acc 0.921875, learning_rate 0.000100644
2017-10-10T13:03:08.365082: step 2187, loss 0.291241, acc 0.890625, learning_rate 0.000100641
2017-10-10T13:03:08.840014: step 2188, loss 0.498991, acc 0.84375, learning_rate 0.000100639
2017-10-10T13:03:09.346536: step 2189, loss 0.152591, acc 0.9375, learning_rate 0.000100636
2017-10-10T13:03:09.873096: step 2190, loss 0.133836, acc 0.96875, learning_rate 0.000100634
2017-10-10T13:03:10.380909: step 2191, loss 0.257602, acc 0.921875, learning_rate 0.000100631
2017-10-10T13:03:10.868048: step 2192, loss 0.260305, acc 0.890625, learning_rate 0.000100628
2017-10-10T13:03:11.327407: step 2193, loss 0.365789, acc 0.875, learning_rate 0.000100626
2017-10-10T13:03:11.836946: step 2194, loss 0.122472, acc 0.96875, learning_rate 0.000100623
2017-10-10T13:03:12.332539: step 2195, loss 0.20162, acc 0.90625, learning_rate 0.000100621
2017-10-10T13:03:12.829002: step 2196, loss 0.198356, acc 0.921875, learning_rate 0.000100618
2017-10-10T13:03:13.407396: step 2197, loss 0.170621, acc 0.9375, learning_rate 0.000100616
2017-10-10T13:03:13.892897: step 2198, loss 0.15321, acc 0.921875, learning_rate 0.000100613
2017-10-10T13:03:14.413155: step 2199, loss 0.16962, acc 0.921875, learning_rate 0.000100611
2017-10-10T13:03:14.928807: step 2200, loss 0.204629, acc 0.9375, learning_rate 0.000100608

Evaluation:
2017-10-10T13:03:16.325869: step 2200, loss 0.242264, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2200

2017-10-10T13:03:17.637025: step 2201, loss 0.183024, acc 0.921875, learning_rate 0.000100606
2017-10-10T13:03:18.195179: step 2202, loss 0.351272, acc 0.890625, learning_rate 0.000100603
2017-10-10T13:03:18.766263: step 2203, loss 0.116595, acc 0.96875, learning_rate 0.000100601
2017-10-10T13:03:19.298625: step 2204, loss 0.114046, acc 0.96875, learning_rate 0.000100598
2017-10-10T13:03:19.768991: step 2205, loss 0.262909, acc 0.90625, learning_rate 0.000100596
2017-10-10T13:03:20.288684: step 2206, loss 0.313707, acc 0.890625, learning_rate 0.000100594
2017-10-10T13:03:20.888855: step 2207, loss 0.155326, acc 0.953125, learning_rate 0.000100591
2017-10-10T13:03:21.440869: step 2208, loss 0.229599, acc 0.9375, learning_rate 0.000100589
2017-10-10T13:03:21.975786: step 2209, loss 0.229825, acc 0.921875, learning_rate 0.000100586
2017-10-10T13:03:22.561422: step 2210, loss 0.215958, acc 0.890625, learning_rate 0.000100584
2017-10-10T13:03:23.142578: step 2211, loss 0.12825, acc 0.96875, learning_rate 0.000100581
2017-10-10T13:03:23.741026: step 2212, loss 0.170837, acc 0.921875, learning_rate 0.000100579
2017-10-10T13:03:24.201472: step 2213, loss 0.238771, acc 0.921875, learning_rate 0.000100577
2017-10-10T13:03:24.661276: step 2214, loss 0.215131, acc 0.921875, learning_rate 0.000100574
2017-10-10T13:03:25.240957: step 2215, loss 0.304527, acc 0.875, learning_rate 0.000100572
2017-10-10T13:03:25.808185: step 2216, loss 0.187528, acc 0.921875, learning_rate 0.00010057
2017-10-10T13:03:26.282271: step 2217, loss 0.240325, acc 0.90625, learning_rate 0.000100567
2017-10-10T13:03:26.700868: step 2218, loss 0.245469, acc 0.9375, learning_rate 0.000100565
2017-10-10T13:03:27.116837: step 2219, loss 0.132926, acc 0.953125, learning_rate 0.000100563
2017-10-10T13:03:27.661323: step 2220, loss 0.164533, acc 0.921875, learning_rate 0.00010056
2017-10-10T13:03:28.217271: step 2221, loss 0.221838, acc 0.921875, learning_rate 0.000100558
2017-10-10T13:03:28.784005: step 2222, loss 0.365926, acc 0.921875, learning_rate 0.000100556
2017-10-10T13:03:29.306652: step 2223, loss 0.390352, acc 0.84375, learning_rate 0.000100554
2017-10-10T13:03:29.812084: step 2224, loss 0.15781, acc 0.96875, learning_rate 0.000100551
2017-10-10T13:03:30.381375: step 2225, loss 0.191381, acc 0.90625, learning_rate 0.000100549
2017-10-10T13:03:30.893101: step 2226, loss 0.196107, acc 0.921875, learning_rate 0.000100547
2017-10-10T13:03:31.397530: step 2227, loss 0.214797, acc 0.9375, learning_rate 0.000100545
2017-10-10T13:03:31.985900: step 2228, loss 0.319204, acc 0.875, learning_rate 0.000100542
2017-10-10T13:03:32.537038: step 2229, loss 0.187918, acc 0.9375, learning_rate 0.00010054
2017-10-10T13:03:33.041189: step 2230, loss 0.165029, acc 0.953125, learning_rate 0.000100538
2017-10-10T13:03:33.461742: step 2231, loss 0.156598, acc 0.9375, learning_rate 0.000100536
2017-10-10T13:03:33.889467: step 2232, loss 0.263816, acc 0.9375, learning_rate 0.000100534
2017-10-10T13:03:34.337503: step 2233, loss 0.230317, acc 0.921875, learning_rate 0.000100531
2017-10-10T13:03:34.836862: step 2234, loss 0.349615, acc 0.875, learning_rate 0.000100529
2017-10-10T13:03:35.397739: step 2235, loss 0.2494, acc 0.890625, learning_rate 0.000100527
2017-10-10T13:03:35.899092: step 2236, loss 0.398251, acc 0.859375, learning_rate 0.000100525
2017-10-10T13:03:36.415806: step 2237, loss 0.137177, acc 0.953125, learning_rate 0.000100523
2017-10-10T13:03:36.937010: step 2238, loss 0.298835, acc 0.921875, learning_rate 0.000100521
2017-10-10T13:03:37.464941: step 2239, loss 0.202038, acc 0.9375, learning_rate 0.000100519
2017-10-10T13:03:38.025704: step 2240, loss 0.237407, acc 0.90625, learning_rate 0.000100516

Evaluation:
2017-10-10T13:03:39.314118: step 2240, loss 0.24334, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2240

2017-10-10T13:03:40.839787: step 2241, loss 0.197469, acc 0.921875, learning_rate 0.000100514
2017-10-10T13:03:41.321099: step 2242, loss 0.214331, acc 0.9375, learning_rate 0.000100512
2017-10-10T13:03:41.944191: step 2243, loss 0.238029, acc 0.890625, learning_rate 0.00010051
2017-10-10T13:03:42.436566: step 2244, loss 0.163055, acc 0.9375, learning_rate 0.000100508
2017-10-10T13:03:42.969113: step 2245, loss 0.233817, acc 0.90625, learning_rate 0.000100506
2017-10-10T13:03:43.548466: step 2246, loss 0.211823, acc 0.921875, learning_rate 0.000100504
2017-10-10T13:03:44.119742: step 2247, loss 0.285876, acc 0.921875, learning_rate 0.000100502
2017-10-10T13:03:44.667422: step 2248, loss 0.266588, acc 0.921875, learning_rate 0.0001005
2017-10-10T13:03:45.216695: step 2249, loss 0.268715, acc 0.875, learning_rate 0.000100498
2017-10-10T13:03:45.780837: step 2250, loss 0.195745, acc 0.921875, learning_rate 0.000100496
2017-10-10T13:03:46.408858: step 2251, loss 0.282689, acc 0.90625, learning_rate 0.000100494
2017-10-10T13:03:46.828607: step 2252, loss 0.479343, acc 0.8125, learning_rate 0.000100492
2017-10-10T13:03:47.250458: step 2253, loss 0.126815, acc 0.96875, learning_rate 0.00010049
2017-10-10T13:03:47.731386: step 2254, loss 0.225793, acc 0.941176, learning_rate 0.000100488
2017-10-10T13:03:48.234950: step 2255, loss 0.328064, acc 0.90625, learning_rate 0.000100486
2017-10-10T13:03:48.756921: step 2256, loss 0.303098, acc 0.890625, learning_rate 0.000100484
2017-10-10T13:03:49.288849: step 2257, loss 0.232041, acc 0.921875, learning_rate 0.000100482
2017-10-10T13:03:49.695131: step 2258, loss 0.220004, acc 0.953125, learning_rate 0.00010048
2017-10-10T13:03:50.108871: step 2259, loss 0.213735, acc 0.9375, learning_rate 0.000100478
2017-10-10T13:03:50.545390: step 2260, loss 0.166987, acc 0.96875, learning_rate 0.000100476
2017-10-10T13:03:51.083427: step 2261, loss 0.099208, acc 0.96875, learning_rate 0.000100474
2017-10-10T13:03:51.562629: step 2262, loss 0.241558, acc 0.90625, learning_rate 0.000100472
2017-10-10T13:03:52.185871: step 2263, loss 0.322356, acc 0.890625, learning_rate 0.00010047
2017-10-10T13:03:52.765721: step 2264, loss 0.212382, acc 0.90625, learning_rate 0.000100468
2017-10-10T13:03:53.290045: step 2265, loss 0.193745, acc 0.90625, learning_rate 0.000100466
2017-10-10T13:03:53.808857: step 2266, loss 0.270913, acc 0.875, learning_rate 0.000100464
2017-10-10T13:03:54.312215: step 2267, loss 0.277865, acc 0.9375, learning_rate 0.000100462
2017-10-10T13:03:54.876983: step 2268, loss 0.14297, acc 0.953125, learning_rate 0.000100461
2017-10-10T13:03:55.401043: step 2269, loss 0.22664, acc 0.921875, learning_rate 0.000100459
2017-10-10T13:03:55.876872: step 2270, loss 0.388098, acc 0.90625, learning_rate 0.000100457
2017-10-10T13:03:56.328924: step 2271, loss 0.299355, acc 0.875, learning_rate 0.000100455
2017-10-10T13:03:56.820935: step 2272, loss 0.197148, acc 0.953125, learning_rate 0.000100453
2017-10-10T13:03:57.428998: step 2273, loss 0.330914, acc 0.890625, learning_rate 0.000100451
2017-10-10T13:03:57.880928: step 2274, loss 0.205782, acc 0.9375, learning_rate 0.000100449
2017-10-10T13:03:58.413085: step 2275, loss 0.216687, acc 0.9375, learning_rate 0.000100448
2017-10-10T13:03:58.878814: step 2276, loss 0.131914, acc 0.953125, learning_rate 0.000100446
2017-10-10T13:03:59.412878: step 2277, loss 0.257437, acc 0.90625, learning_rate 0.000100444
2017-10-10T13:03:59.992011: step 2278, loss 0.248682, acc 0.90625, learning_rate 0.000100442
2017-10-10T13:04:00.511853: step 2279, loss 0.245354, acc 0.90625, learning_rate 0.00010044
2017-10-10T13:04:00.996613: step 2280, loss 0.377891, acc 0.890625, learning_rate 0.000100439

Evaluation:
2017-10-10T13:04:02.141192: step 2280, loss 0.241189, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2280

2017-10-10T13:04:04.516943: step 2281, loss 0.281864, acc 0.921875, learning_rate 0.000100437
2017-10-10T13:04:05.037405: step 2282, loss 0.267007, acc 0.890625, learning_rate 0.000100435
2017-10-10T13:04:05.562109: step 2283, loss 0.300618, acc 0.9375, learning_rate 0.000100433
2017-10-10T13:04:06.078720: step 2284, loss 0.345536, acc 0.9375, learning_rate 0.000100431
2017-10-10T13:04:06.613092: step 2285, loss 0.36734, acc 0.859375, learning_rate 0.00010043
2017-10-10T13:04:07.146205: step 2286, loss 0.298694, acc 0.859375, learning_rate 0.000100428
2017-10-10T13:04:07.645121: step 2287, loss 0.298233, acc 0.90625, learning_rate 0.000100426
2017-10-10T13:04:08.165620: step 2288, loss 0.268069, acc 0.875, learning_rate 0.000100424
2017-10-10T13:04:08.568696: step 2289, loss 0.278447, acc 0.890625, learning_rate 0.000100423
2017-10-10T13:04:09.036970: step 2290, loss 0.178084, acc 0.953125, learning_rate 0.000100421
2017-10-10T13:04:09.516863: step 2291, loss 0.210909, acc 0.953125, learning_rate 0.000100419
2017-10-10T13:04:10.057362: step 2292, loss 0.128621, acc 0.96875, learning_rate 0.000100418
2017-10-10T13:04:10.558791: step 2293, loss 0.300146, acc 0.875, learning_rate 0.000100416
2017-10-10T13:04:11.104882: step 2294, loss 0.311074, acc 0.890625, learning_rate 0.000100414
2017-10-10T13:04:11.650553: step 2295, loss 0.1321, acc 0.96875, learning_rate 0.000100412
2017-10-10T13:04:12.196903: step 2296, loss 0.269227, acc 0.890625, learning_rate 0.000100411
2017-10-10T13:04:12.644780: step 2297, loss 0.179687, acc 0.9375, learning_rate 0.000100409
2017-10-10T13:04:13.137074: step 2298, loss 0.312285, acc 0.875, learning_rate 0.000100407
2017-10-10T13:04:13.761685: step 2299, loss 0.319949, acc 0.890625, learning_rate 0.000100406
2017-10-10T13:04:14.305010: step 2300, loss 0.136464, acc 0.953125, learning_rate 0.000100404
2017-10-10T13:04:14.829071: step 2301, loss 0.353123, acc 0.90625, learning_rate 0.000100402
2017-10-10T13:04:15.320831: step 2302, loss 0.327091, acc 0.875, learning_rate 0.000100401
2017-10-10T13:04:15.824870: step 2303, loss 0.188155, acc 0.953125, learning_rate 0.000100399
2017-10-10T13:04:16.368824: step 2304, loss 0.176107, acc 0.9375, learning_rate 0.000100398
2017-10-10T13:04:16.929949: step 2305, loss 0.194306, acc 0.9375, learning_rate 0.000100396
2017-10-10T13:04:17.450554: step 2306, loss 0.308263, acc 0.890625, learning_rate 0.000100394
2017-10-10T13:04:17.971321: step 2307, loss 0.230472, acc 0.90625, learning_rate 0.000100393
2017-10-10T13:04:18.478601: step 2308, loss 0.288062, acc 0.90625, learning_rate 0.000100391
2017-10-10T13:04:18.997084: step 2309, loss 0.227428, acc 0.921875, learning_rate 0.000100389
2017-10-10T13:04:19.514690: step 2310, loss 0.273725, acc 0.890625, learning_rate 0.000100388
2017-10-10T13:04:20.029246: step 2311, loss 0.222306, acc 0.890625, learning_rate 0.000100386
2017-10-10T13:04:20.501667: step 2312, loss 0.171296, acc 0.9375, learning_rate 0.000100385
2017-10-10T13:04:21.012017: step 2313, loss 0.221574, acc 0.9375, learning_rate 0.000100383
2017-10-10T13:04:21.544094: step 2314, loss 0.177109, acc 0.90625, learning_rate 0.000100382
2017-10-10T13:04:22.090587: step 2315, loss 0.319004, acc 0.90625, learning_rate 0.00010038
2017-10-10T13:04:22.630072: step 2316, loss 0.370815, acc 0.828125, learning_rate 0.000100378
2017-10-10T13:04:23.208429: step 2317, loss 0.110752, acc 0.984375, learning_rate 0.000100377
2017-10-10T13:04:23.756858: step 2318, loss 0.169236, acc 0.9375, learning_rate 0.000100375
2017-10-10T13:04:24.376975: step 2319, loss 0.219956, acc 0.921875, learning_rate 0.000100374
2017-10-10T13:04:24.912925: step 2320, loss 0.199715, acc 0.921875, learning_rate 0.000100372

Evaluation:
2017-10-10T13:04:26.223645: step 2320, loss 0.241588, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2320

2017-10-10T13:04:27.581058: step 2321, loss 0.245791, acc 0.9375, learning_rate 0.000100371
2017-10-10T13:04:28.107400: step 2322, loss 0.170327, acc 0.9375, learning_rate 0.000100369
2017-10-10T13:04:28.665351: step 2323, loss 0.221278, acc 0.90625, learning_rate 0.000100368
2017-10-10T13:04:29.164617: step 2324, loss 0.208871, acc 0.9375, learning_rate 0.000100366
2017-10-10T13:04:29.754907: step 2325, loss 0.342062, acc 0.875, learning_rate 0.000100365
2017-10-10T13:04:30.282486: step 2326, loss 0.240629, acc 0.9375, learning_rate 0.000100363
2017-10-10T13:04:30.848084: step 2327, loss 0.270814, acc 0.90625, learning_rate 0.000100362
2017-10-10T13:04:31.416158: step 2328, loss 0.14334, acc 0.921875, learning_rate 0.00010036
2017-10-10T13:04:31.876942: step 2329, loss 0.277192, acc 0.890625, learning_rate 0.000100359
2017-10-10T13:04:32.356650: step 2330, loss 0.227738, acc 0.9375, learning_rate 0.000100357
2017-10-10T13:04:32.793038: step 2331, loss 0.276123, acc 0.875, learning_rate 0.000100356
2017-10-10T13:04:33.305629: step 2332, loss 0.118646, acc 0.96875, learning_rate 0.000100354
2017-10-10T13:04:33.921322: step 2333, loss 0.237588, acc 0.921875, learning_rate 0.000100353
2017-10-10T13:04:34.385853: step 2334, loss 0.434429, acc 0.875, learning_rate 0.000100352
2017-10-10T13:04:34.800817: step 2335, loss 0.160914, acc 0.953125, learning_rate 0.00010035
2017-10-10T13:04:35.228627: step 2336, loss 0.341286, acc 0.890625, learning_rate 0.000100349
2017-10-10T13:04:35.765082: step 2337, loss 0.226537, acc 0.9375, learning_rate 0.000100347
2017-10-10T13:04:36.288767: step 2338, loss 0.240511, acc 0.953125, learning_rate 0.000100346
2017-10-10T13:04:36.828971: step 2339, loss 0.346005, acc 0.921875, learning_rate 0.000100344
2017-10-10T13:04:37.341000: step 2340, loss 0.22468, acc 0.90625, learning_rate 0.000100343
2017-10-10T13:04:37.905024: step 2341, loss 0.158453, acc 0.984375, learning_rate 0.000100342
2017-10-10T13:04:38.480881: step 2342, loss 0.131447, acc 0.96875, learning_rate 0.00010034
2017-10-10T13:04:39.007445: step 2343, loss 0.246755, acc 0.90625, learning_rate 0.000100339
2017-10-10T13:04:39.565236: step 2344, loss 0.350535, acc 0.890625, learning_rate 0.000100338
2017-10-10T13:04:40.092921: step 2345, loss 0.207409, acc 0.9375, learning_rate 0.000100336
2017-10-10T13:04:40.526524: step 2346, loss 0.361211, acc 0.875, learning_rate 0.000100335
2017-10-10T13:04:41.012966: step 2347, loss 0.174463, acc 0.953125, learning_rate 0.000100333
2017-10-10T13:04:41.561273: step 2348, loss 0.194924, acc 0.9375, learning_rate 0.000100332
2017-10-10T13:04:42.129917: step 2349, loss 0.29331, acc 0.90625, learning_rate 0.000100331
2017-10-10T13:04:42.580883: step 2350, loss 0.173722, acc 0.96875, learning_rate 0.000100329
2017-10-10T13:04:43.042639: step 2351, loss 0.30911, acc 0.90625, learning_rate 0.000100328
2017-10-10T13:04:43.489132: step 2352, loss 0.143017, acc 0.941176, learning_rate 0.000100327
2017-10-10T13:04:43.983185: step 2353, loss 0.190682, acc 0.9375, learning_rate 0.000100325
2017-10-10T13:04:44.569074: step 2354, loss 0.251382, acc 0.90625, learning_rate 0.000100324
2017-10-10T13:04:45.151329: step 2355, loss 0.198784, acc 0.921875, learning_rate 0.000100323
2017-10-10T13:04:45.645211: step 2356, loss 0.172096, acc 0.921875, learning_rate 0.000100321
2017-10-10T13:04:46.138312: step 2357, loss 0.177309, acc 0.953125, learning_rate 0.00010032
2017-10-10T13:04:46.710403: step 2358, loss 0.322896, acc 0.921875, learning_rate 0.000100319
2017-10-10T13:04:47.334200: step 2359, loss 0.366345, acc 0.890625, learning_rate 0.000100317
2017-10-10T13:04:47.850546: step 2360, loss 0.161289, acc 0.9375, learning_rate 0.000100316

Evaluation:
2017-10-10T13:04:49.073753: step 2360, loss 0.240408, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2360

2017-10-10T13:04:50.624900: step 2361, loss 0.256622, acc 0.890625, learning_rate 0.000100315
2017-10-10T13:04:51.159557: step 2362, loss 0.218078, acc 0.9375, learning_rate 0.000100314
2017-10-10T13:04:51.673571: step 2363, loss 0.239161, acc 0.90625, learning_rate 0.000100312
2017-10-10T13:04:52.249487: step 2364, loss 0.456231, acc 0.859375, learning_rate 0.000100311
2017-10-10T13:04:52.740325: step 2365, loss 0.136803, acc 0.984375, learning_rate 0.00010031
2017-10-10T13:04:53.271306: step 2366, loss 0.242764, acc 0.90625, learning_rate 0.000100308
2017-10-10T13:04:53.774013: step 2367, loss 0.19174, acc 0.90625, learning_rate 0.000100307
2017-10-10T13:04:54.331865: step 2368, loss 0.173256, acc 0.96875, learning_rate 0.000100306
2017-10-10T13:04:54.832617: step 2369, loss 0.135924, acc 0.9375, learning_rate 0.000100305
2017-10-10T13:04:55.272883: step 2370, loss 0.182694, acc 0.96875, learning_rate 0.000100303
2017-10-10T13:04:55.808279: step 2371, loss 0.203481, acc 0.921875, learning_rate 0.000100302
2017-10-10T13:04:56.380946: step 2372, loss 0.0752287, acc 0.96875, learning_rate 0.000100301
2017-10-10T13:04:56.970911: step 2373, loss 0.21082, acc 0.90625, learning_rate 0.0001003
2017-10-10T13:04:57.435279: step 2374, loss 0.165, acc 0.90625, learning_rate 0.000100299
2017-10-10T13:04:57.896869: step 2375, loss 0.314064, acc 0.890625, learning_rate 0.000100297
2017-10-10T13:04:58.445635: step 2376, loss 0.226318, acc 0.921875, learning_rate 0.000100296
2017-10-10T13:04:58.993551: step 2377, loss 0.330986, acc 0.875, learning_rate 0.000100295
2017-10-10T13:04:59.532402: step 2378, loss 0.241672, acc 0.90625, learning_rate 0.000100294
2017-10-10T13:05:00.080866: step 2379, loss 0.212704, acc 0.953125, learning_rate 0.000100292
2017-10-10T13:05:00.644857: step 2380, loss 0.156493, acc 0.96875, learning_rate 0.000100291
2017-10-10T13:05:01.209593: step 2381, loss 0.189935, acc 0.90625, learning_rate 0.00010029
2017-10-10T13:05:01.694372: step 2382, loss 0.149711, acc 0.984375, learning_rate 0.000100289
2017-10-10T13:05:02.237763: step 2383, loss 0.391241, acc 0.875, learning_rate 0.000100288
2017-10-10T13:05:02.749019: step 2384, loss 0.288152, acc 0.875, learning_rate 0.000100287
2017-10-10T13:05:03.270322: step 2385, loss 0.196249, acc 0.9375, learning_rate 0.000100285
2017-10-10T13:05:03.784615: step 2386, loss 0.220037, acc 0.953125, learning_rate 0.000100284
2017-10-10T13:05:04.353039: step 2387, loss 0.303229, acc 0.921875, learning_rate 0.000100283
2017-10-10T13:05:04.941679: step 2388, loss 0.265953, acc 0.90625, learning_rate 0.000100282
2017-10-10T13:05:05.478957: step 2389, loss 0.240242, acc 0.921875, learning_rate 0.000100281
2017-10-10T13:05:05.936444: step 2390, loss 0.273806, acc 0.90625, learning_rate 0.00010028
2017-10-10T13:05:06.446967: step 2391, loss 0.176478, acc 0.96875, learning_rate 0.000100278
2017-10-10T13:05:06.952863: step 2392, loss 0.162278, acc 0.96875, learning_rate 0.000100277
2017-10-10T13:05:07.470482: step 2393, loss 0.225741, acc 0.90625, learning_rate 0.000100276
2017-10-10T13:05:08.061815: step 2394, loss 0.231353, acc 0.953125, learning_rate 0.000100275
2017-10-10T13:05:08.556943: step 2395, loss 0.21827, acc 0.921875, learning_rate 0.000100274
2017-10-10T13:05:09.106087: step 2396, loss 0.183398, acc 0.921875, learning_rate 0.000100273
2017-10-10T13:05:09.604045: step 2397, loss 0.198906, acc 0.9375, learning_rate 0.000100272
2017-10-10T13:05:10.108972: step 2398, loss 0.257744, acc 0.890625, learning_rate 0.000100271
2017-10-10T13:05:10.623543: step 2399, loss 0.240489, acc 0.90625, learning_rate 0.00010027
2017-10-10T13:05:11.174722: step 2400, loss 0.181687, acc 0.9375, learning_rate 0.000100268

Evaluation:
2017-10-10T13:05:12.273881: step 2400, loss 0.23959, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2400

2017-10-10T13:05:13.757333: step 2401, loss 0.203227, acc 0.9375, learning_rate 0.000100267
2017-10-10T13:05:14.297262: step 2402, loss 0.268456, acc 0.90625, learning_rate 0.000100266
2017-10-10T13:05:14.842146: step 2403, loss 0.178844, acc 0.953125, learning_rate 0.000100265
2017-10-10T13:05:15.413014: step 2404, loss 0.272237, acc 0.921875, learning_rate 0.000100264
2017-10-10T13:05:15.909268: step 2405, loss 0.207069, acc 0.953125, learning_rate 0.000100263
2017-10-10T13:05:16.490473: step 2406, loss 0.323999, acc 0.875, learning_rate 0.000100262
2017-10-10T13:05:17.104816: step 2407, loss 0.180333, acc 0.953125, learning_rate 0.000100261
2017-10-10T13:05:17.568861: step 2408, loss 0.303405, acc 0.84375, learning_rate 0.00010026
2017-10-10T13:05:18.055933: step 2409, loss 0.344696, acc 0.890625, learning_rate 0.000100259
2017-10-10T13:05:18.668903: step 2410, loss 0.246975, acc 0.90625, learning_rate 0.000100258
2017-10-10T13:05:19.175356: step 2411, loss 0.291326, acc 0.921875, learning_rate 0.000100257
2017-10-10T13:05:19.727031: step 2412, loss 0.199767, acc 0.921875, learning_rate 0.000100256
2017-10-10T13:05:20.256795: step 2413, loss 0.414105, acc 0.90625, learning_rate 0.000100255
2017-10-10T13:05:20.704951: step 2414, loss 0.232776, acc 0.921875, learning_rate 0.000100253
2017-10-10T13:05:21.168843: step 2415, loss 0.11162, acc 0.9375, learning_rate 0.000100252
2017-10-10T13:05:21.747572: step 2416, loss 0.310942, acc 0.890625, learning_rate 0.000100251
2017-10-10T13:05:22.284091: step 2417, loss 0.171214, acc 0.921875, learning_rate 0.00010025
2017-10-10T13:05:22.848944: step 2418, loss 0.105362, acc 0.96875, learning_rate 0.000100249
2017-10-10T13:05:23.320474: step 2419, loss 0.231833, acc 0.90625, learning_rate 0.000100248
2017-10-10T13:05:23.821554: step 2420, loss 0.187155, acc 0.921875, learning_rate 0.000100247
2017-10-10T13:05:24.272911: step 2421, loss 0.0926247, acc 0.96875, learning_rate 0.000100246
2017-10-10T13:05:24.766464: step 2422, loss 0.225969, acc 0.9375, learning_rate 0.000100245
2017-10-10T13:05:25.284463: step 2423, loss 0.232756, acc 0.9375, learning_rate 0.000100244
2017-10-10T13:05:25.872861: step 2424, loss 0.236038, acc 0.90625, learning_rate 0.000100243
2017-10-10T13:05:26.425884: step 2425, loss 0.266339, acc 0.90625, learning_rate 0.000100242
2017-10-10T13:05:26.956856: step 2426, loss 0.230205, acc 0.9375, learning_rate 0.000100241
2017-10-10T13:05:27.566821: step 2427, loss 0.221945, acc 0.921875, learning_rate 0.00010024
2017-10-10T13:05:28.173784: step 2428, loss 0.145965, acc 0.9375, learning_rate 0.000100239
2017-10-10T13:05:28.703005: step 2429, loss 0.261758, acc 0.96875, learning_rate 0.000100238
2017-10-10T13:05:29.222631: step 2430, loss 0.177159, acc 0.9375, learning_rate 0.000100237
2017-10-10T13:05:29.726132: step 2431, loss 0.108415, acc 0.984375, learning_rate 0.000100236
2017-10-10T13:05:30.240862: step 2432, loss 0.150715, acc 0.953125, learning_rate 0.000100235
2017-10-10T13:05:30.805397: step 2433, loss 0.0804068, acc 1, learning_rate 0.000100235
2017-10-10T13:05:31.279547: step 2434, loss 0.233884, acc 0.921875, learning_rate 0.000100234
2017-10-10T13:05:31.709066: step 2435, loss 0.338913, acc 0.90625, learning_rate 0.000100233
2017-10-10T13:05:32.249056: step 2436, loss 0.150582, acc 0.9375, learning_rate 0.000100232
2017-10-10T13:05:32.733013: step 2437, loss 0.209211, acc 0.953125, learning_rate 0.000100231
2017-10-10T13:05:33.309071: step 2438, loss 0.234513, acc 0.890625, learning_rate 0.00010023
2017-10-10T13:05:33.801749: step 2439, loss 0.194407, acc 0.9375, learning_rate 0.000100229
2017-10-10T13:05:34.380983: step 2440, loss 0.361214, acc 0.921875, learning_rate 0.000100228

Evaluation:
2017-10-10T13:05:35.300517: step 2440, loss 0.241777, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2440

2017-10-10T13:05:37.062151: step 2441, loss 0.250828, acc 0.9375, learning_rate 0.000100227
2017-10-10T13:05:37.664950: step 2442, loss 0.452158, acc 0.84375, learning_rate 0.000100226
2017-10-10T13:05:38.216800: step 2443, loss 0.123941, acc 0.953125, learning_rate 0.000100225
2017-10-10T13:05:38.738719: step 2444, loss 0.267678, acc 0.921875, learning_rate 0.000100224
2017-10-10T13:05:39.280874: step 2445, loss 0.320269, acc 0.875, learning_rate 0.000100223
2017-10-10T13:05:39.930061: step 2446, loss 0.225148, acc 0.90625, learning_rate 0.000100222
2017-10-10T13:05:40.368406: step 2447, loss 0.225102, acc 0.90625, learning_rate 0.000100221
2017-10-10T13:05:40.842563: step 2448, loss 0.217467, acc 0.890625, learning_rate 0.000100221
2017-10-10T13:05:41.254831: step 2449, loss 0.254872, acc 0.90625, learning_rate 0.00010022
2017-10-10T13:05:41.729377: step 2450, loss 0.375892, acc 0.843137, learning_rate 0.000100219
2017-10-10T13:05:42.157032: step 2451, loss 0.243344, acc 0.90625, learning_rate 0.000100218
2017-10-10T13:05:42.629145: step 2452, loss 0.231354, acc 0.921875, learning_rate 0.000100217
2017-10-10T13:05:43.127222: step 2453, loss 0.403296, acc 0.921875, learning_rate 0.000100216
2017-10-10T13:05:43.604000: step 2454, loss 0.123605, acc 0.984375, learning_rate 0.000100215
2017-10-10T13:05:44.064841: step 2455, loss 0.186584, acc 0.9375, learning_rate 0.000100214
2017-10-10T13:05:44.550356: step 2456, loss 0.17153, acc 0.921875, learning_rate 0.000100213
2017-10-10T13:05:45.133052: step 2457, loss 0.419734, acc 0.859375, learning_rate 0.000100213
2017-10-10T13:05:45.641024: step 2458, loss 0.221079, acc 0.90625, learning_rate 0.000100212
2017-10-10T13:05:46.169024: step 2459, loss 0.201005, acc 0.921875, learning_rate 0.000100211
2017-10-10T13:05:46.655849: step 2460, loss 0.189042, acc 0.9375, learning_rate 0.00010021
2017-10-10T13:05:47.225176: step 2461, loss 0.249413, acc 0.90625, learning_rate 0.000100209
2017-10-10T13:05:47.732899: step 2462, loss 0.221445, acc 0.921875, learning_rate 0.000100208
2017-10-10T13:05:48.239403: step 2463, loss 0.223867, acc 0.90625, learning_rate 0.000100207
2017-10-10T13:05:48.833356: step 2464, loss 0.241213, acc 0.9375, learning_rate 0.000100207
2017-10-10T13:05:49.317674: step 2465, loss 0.368707, acc 0.890625, learning_rate 0.000100206
2017-10-10T13:05:49.830463: step 2466, loss 0.314825, acc 0.890625, learning_rate 0.000100205
2017-10-10T13:05:50.367946: step 2467, loss 0.250785, acc 0.9375, learning_rate 0.000100204
2017-10-10T13:05:50.919959: step 2468, loss 0.163729, acc 0.96875, learning_rate 0.000100203
2017-10-10T13:05:51.490569: step 2469, loss 0.134197, acc 0.984375, learning_rate 0.000100202
2017-10-10T13:05:51.973167: step 2470, loss 0.353644, acc 0.890625, learning_rate 0.000100202
2017-10-10T13:05:52.446590: step 2471, loss 0.150519, acc 0.96875, learning_rate 0.000100201
2017-10-10T13:05:52.940744: step 2472, loss 0.159677, acc 0.9375, learning_rate 0.0001002
2017-10-10T13:05:53.424086: step 2473, loss 0.239662, acc 0.890625, learning_rate 0.000100199
2017-10-10T13:05:53.982440: step 2474, loss 0.252077, acc 0.90625, learning_rate 0.000100198
2017-10-10T13:05:54.512819: step 2475, loss 0.32555, acc 0.890625, learning_rate 0.000100198
2017-10-10T13:05:55.092844: step 2476, loss 0.195424, acc 0.9375, learning_rate 0.000100197
2017-10-10T13:05:55.616853: step 2477, loss 0.388136, acc 0.890625, learning_rate 0.000100196
2017-10-10T13:05:56.145760: step 2478, loss 0.229262, acc 0.921875, learning_rate 0.000100195
2017-10-10T13:05:56.668986: step 2479, loss 0.208739, acc 0.921875, learning_rate 0.000100194
2017-10-10T13:05:57.366997: step 2480, loss 0.130636, acc 0.953125, learning_rate 0.000100194

Evaluation:
2017-10-10T13:05:58.362893: step 2480, loss 0.240154, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2480

2017-10-10T13:05:59.832891: step 2481, loss 0.263817, acc 0.921875, learning_rate 0.000100193
2017-10-10T13:06:00.333069: step 2482, loss 0.442064, acc 0.84375, learning_rate 0.000100192
2017-10-10T13:06:00.916834: step 2483, loss 0.341434, acc 0.84375, learning_rate 0.000100191
2017-10-10T13:06:01.469819: step 2484, loss 0.0810831, acc 0.96875, learning_rate 0.00010019
2017-10-10T13:06:01.961640: step 2485, loss 0.262304, acc 0.90625, learning_rate 0.00010019
2017-10-10T13:06:02.532887: step 2486, loss 0.270987, acc 0.890625, learning_rate 0.000100189
2017-10-10T13:06:03.120852: step 2487, loss 0.225372, acc 0.921875, learning_rate 0.000100188
2017-10-10T13:06:03.603126: step 2488, loss 0.174908, acc 0.953125, learning_rate 0.000100187
2017-10-10T13:06:04.028912: step 2489, loss 0.162152, acc 0.921875, learning_rate 0.000100187
2017-10-10T13:06:04.548174: step 2490, loss 0.237838, acc 0.921875, learning_rate 0.000100186
2017-10-10T13:06:05.047408: step 2491, loss 0.207714, acc 0.921875, learning_rate 0.000100185
2017-10-10T13:06:05.561919: step 2492, loss 0.221033, acc 0.9375, learning_rate 0.000100184
2017-10-10T13:06:06.220558: step 2493, loss 0.125563, acc 0.953125, learning_rate 0.000100183
2017-10-10T13:06:06.634087: step 2494, loss 0.178542, acc 0.953125, learning_rate 0.000100183
2017-10-10T13:06:07.104830: step 2495, loss 0.250442, acc 0.9375, learning_rate 0.000100182
2017-10-10T13:06:07.622030: step 2496, loss 0.139578, acc 0.9375, learning_rate 0.000100181
2017-10-10T13:06:08.127628: step 2497, loss 0.158853, acc 0.96875, learning_rate 0.000100181
2017-10-10T13:06:08.648127: step 2498, loss 0.228448, acc 0.9375, learning_rate 0.00010018
2017-10-10T13:06:09.220889: step 2499, loss 0.335849, acc 0.875, learning_rate 0.000100179
2017-10-10T13:06:09.741058: step 2500, loss 0.212855, acc 0.921875, learning_rate 0.000100178
2017-10-10T13:06:10.289044: step 2501, loss 0.320439, acc 0.921875, learning_rate 0.000100178
2017-10-10T13:06:10.830530: step 2502, loss 0.361296, acc 0.84375, learning_rate 0.000100177
2017-10-10T13:06:11.412976: step 2503, loss 0.22967, acc 0.921875, learning_rate 0.000100176
2017-10-10T13:06:12.021293: step 2504, loss 0.293271, acc 0.90625, learning_rate 0.000100175
2017-10-10T13:06:12.481161: step 2505, loss 0.163143, acc 0.9375, learning_rate 0.000100175
2017-10-10T13:06:13.032905: step 2506, loss 0.105804, acc 0.96875, learning_rate 0.000100174
2017-10-10T13:06:13.536517: step 2507, loss 0.390499, acc 0.890625, learning_rate 0.000100173
2017-10-10T13:06:14.100834: step 2508, loss 0.239061, acc 0.921875, learning_rate 0.000100173
2017-10-10T13:06:14.628853: step 2509, loss 0.275132, acc 0.90625, learning_rate 0.000100172
2017-10-10T13:06:15.158102: step 2510, loss 0.146476, acc 0.953125, learning_rate 0.000100171
2017-10-10T13:06:15.688859: step 2511, loss 0.260294, acc 0.90625, learning_rate 0.00010017
2017-10-10T13:06:16.241543: step 2512, loss 0.226151, acc 0.90625, learning_rate 0.00010017
2017-10-10T13:06:16.744737: step 2513, loss 0.235908, acc 0.921875, learning_rate 0.000100169
2017-10-10T13:06:17.252853: step 2514, loss 0.162575, acc 0.9375, learning_rate 0.000100168
2017-10-10T13:06:17.793109: step 2515, loss 0.320491, acc 0.921875, learning_rate 0.000100168
2017-10-10T13:06:18.290882: step 2516, loss 0.257675, acc 0.90625, learning_rate 0.000100167
2017-10-10T13:06:18.836839: step 2517, loss 0.211834, acc 0.9375, learning_rate 0.000100166
2017-10-10T13:06:19.395993: step 2518, loss 0.194822, acc 0.875, learning_rate 0.000100166
2017-10-10T13:06:19.932834: step 2519, loss 0.272908, acc 0.890625, learning_rate 0.000100165
2017-10-10T13:06:20.493779: step 2520, loss 0.238555, acc 0.953125, learning_rate 0.000100164

Evaluation:
2017-10-10T13:06:21.444975: step 2520, loss 0.240735, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2520

2017-10-10T13:06:22.997871: step 2521, loss 0.262521, acc 0.9375, learning_rate 0.000100164
2017-10-10T13:06:23.509242: step 2522, loss 0.319897, acc 0.90625, learning_rate 0.000100163
2017-10-10T13:06:24.025587: step 2523, loss 0.167592, acc 0.9375, learning_rate 0.000100162
2017-10-10T13:06:24.548661: step 2524, loss 0.148325, acc 0.9375, learning_rate 0.000100162
2017-10-10T13:06:25.068971: step 2525, loss 0.254993, acc 0.953125, learning_rate 0.000100161
2017-10-10T13:06:25.738994: step 2526, loss 0.130185, acc 0.96875, learning_rate 0.00010016
2017-10-10T13:06:26.200872: step 2527, loss 0.254585, acc 0.9375, learning_rate 0.00010016
2017-10-10T13:06:26.622416: step 2528, loss 0.143785, acc 0.953125, learning_rate 0.000100159
2017-10-10T13:06:27.073048: step 2529, loss 0.255306, acc 0.921875, learning_rate 0.000100158
2017-10-10T13:06:27.708345: step 2530, loss 0.158213, acc 0.96875, learning_rate 0.000100158
2017-10-10T13:06:28.196340: step 2531, loss 0.264533, acc 0.921875, learning_rate 0.000100157
2017-10-10T13:06:28.844861: step 2532, loss 0.177679, acc 0.953125, learning_rate 0.000100156
2017-10-10T13:06:29.241194: step 2533, loss 0.276141, acc 0.921875, learning_rate 0.000100156
2017-10-10T13:06:29.748833: step 2534, loss 0.180614, acc 0.96875, learning_rate 0.000100155
2017-10-10T13:06:30.191488: step 2535, loss 0.338409, acc 0.90625, learning_rate 0.000100155
2017-10-10T13:06:30.753073: step 2536, loss 0.254793, acc 0.921875, learning_rate 0.000100154
2017-10-10T13:06:31.281746: step 2537, loss 0.208041, acc 0.921875, learning_rate 0.000100153
2017-10-10T13:06:31.840926: step 2538, loss 0.127813, acc 0.953125, learning_rate 0.000100153
2017-10-10T13:06:32.420232: step 2539, loss 0.106397, acc 0.953125, learning_rate 0.000100152
2017-10-10T13:06:32.924660: step 2540, loss 0.219268, acc 0.953125, learning_rate 0.000100151
2017-10-10T13:06:33.408825: step 2541, loss 0.352205, acc 0.859375, learning_rate 0.000100151
2017-10-10T13:06:33.960238: step 2542, loss 0.127736, acc 0.9375, learning_rate 0.00010015
2017-10-10T13:06:34.458342: step 2543, loss 0.408044, acc 0.875, learning_rate 0.00010015
2017-10-10T13:06:35.023343: step 2544, loss 0.153291, acc 0.96875, learning_rate 0.000100149
2017-10-10T13:06:35.620871: step 2545, loss 0.340441, acc 0.90625, learning_rate 0.000100148
2017-10-10T13:06:36.155692: step 2546, loss 0.291737, acc 0.890625, learning_rate 0.000100148
2017-10-10T13:06:36.648850: step 2547, loss 0.181227, acc 0.9375, learning_rate 0.000100147
2017-10-10T13:06:37.082556: step 2548, loss 0.102634, acc 0.960784, learning_rate 0.000100147
2017-10-10T13:06:37.584884: step 2549, loss 0.255927, acc 0.921875, learning_rate 0.000100146
2017-10-10T13:06:38.005618: step 2550, loss 0.199722, acc 0.953125, learning_rate 0.000100145
2017-10-10T13:06:38.412217: step 2551, loss 0.290157, acc 0.90625, learning_rate 0.000100145
2017-10-10T13:06:38.929067: step 2552, loss 0.215844, acc 0.9375, learning_rate 0.000100144
2017-10-10T13:06:39.506372: step 2553, loss 0.316269, acc 0.921875, learning_rate 0.000100144
2017-10-10T13:06:40.044987: step 2554, loss 0.296213, acc 0.890625, learning_rate 0.000100143
2017-10-10T13:06:40.551679: step 2555, loss 0.225734, acc 0.9375, learning_rate 0.000100142
2017-10-10T13:06:41.089086: step 2556, loss 0.205348, acc 0.921875, learning_rate 0.000100142
2017-10-10T13:06:41.623392: step 2557, loss 0.18386, acc 0.9375, learning_rate 0.000100141
2017-10-10T13:06:42.148406: step 2558, loss 0.172423, acc 0.953125, learning_rate 0.000100141
2017-10-10T13:06:42.747153: step 2559, loss 0.193957, acc 0.953125, learning_rate 0.00010014
2017-10-10T13:06:43.352885: step 2560, loss 0.261917, acc 0.921875, learning_rate 0.00010014

Evaluation:
2017-10-10T13:06:44.406781: step 2560, loss 0.239505, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2560

2017-10-10T13:06:46.467265: step 2561, loss 0.306038, acc 0.90625, learning_rate 0.000100139
2017-10-10T13:06:46.996300: step 2562, loss 0.0945649, acc 0.953125, learning_rate 0.000100138
2017-10-10T13:06:47.545104: step 2563, loss 0.133803, acc 0.953125, learning_rate 0.000100138
2017-10-10T13:06:48.137147: step 2564, loss 0.270492, acc 0.921875, learning_rate 0.000100137
2017-10-10T13:06:48.645113: step 2565, loss 0.101845, acc 0.96875, learning_rate 0.000100137
2017-10-10T13:06:49.104841: step 2566, loss 0.350395, acc 0.875, learning_rate 0.000100136
2017-10-10T13:06:49.564013: step 2567, loss 0.181168, acc 0.9375, learning_rate 0.000100136
2017-10-10T13:06:50.003244: step 2568, loss 0.266284, acc 0.890625, learning_rate 0.000100135
2017-10-10T13:06:50.556871: step 2569, loss 0.248692, acc 0.875, learning_rate 0.000100134
2017-10-10T13:06:51.080921: step 2570, loss 0.295939, acc 0.890625, learning_rate 0.000100134
2017-10-10T13:06:51.701091: step 2571, loss 0.109415, acc 0.96875, learning_rate 0.000100133
2017-10-10T13:06:52.181424: step 2572, loss 0.1879, acc 0.96875, learning_rate 0.000100133
2017-10-10T13:06:52.621552: step 2573, loss 0.266624, acc 0.890625, learning_rate 0.000100132
2017-10-10T13:06:53.203193: step 2574, loss 0.298574, acc 0.90625, learning_rate 0.000100132
2017-10-10T13:06:53.712600: step 2575, loss 0.23491, acc 0.9375, learning_rate 0.000100131
2017-10-10T13:06:54.186455: step 2576, loss 0.345896, acc 0.84375, learning_rate 0.000100131
2017-10-10T13:06:54.561100: step 2577, loss 0.212763, acc 0.9375, learning_rate 0.00010013
2017-10-10T13:06:55.109171: step 2578, loss 0.264931, acc 0.890625, learning_rate 0.00010013
2017-10-10T13:06:55.607885: step 2579, loss 0.291805, acc 0.9375, learning_rate 0.000100129
2017-10-10T13:06:56.051815: step 2580, loss 0.161353, acc 0.953125, learning_rate 0.000100129
2017-10-10T13:06:56.611498: step 2581, loss 0.32901, acc 0.890625, learning_rate 0.000100128
2017-10-10T13:06:57.161855: step 2582, loss 0.278249, acc 0.84375, learning_rate 0.000100128
2017-10-10T13:06:57.672897: step 2583, loss 0.470476, acc 0.84375, learning_rate 0.000100127
2017-10-10T13:06:58.157015: step 2584, loss 0.295494, acc 0.875, learning_rate 0.000100126
2017-10-10T13:06:58.644854: step 2585, loss 0.176586, acc 0.9375, learning_rate 0.000100126
2017-10-10T13:06:59.188311: step 2586, loss 0.11514, acc 0.96875, learning_rate 0.000100125
2017-10-10T13:06:59.625108: step 2587, loss 0.229769, acc 0.953125, learning_rate 0.000100125
2017-10-10T13:07:00.108810: step 2588, loss 0.165212, acc 0.921875, learning_rate 0.000100124
2017-10-10T13:07:00.650472: step 2589, loss 0.219141, acc 0.9375, learning_rate 0.000100124
2017-10-10T13:07:01.209157: step 2590, loss 0.192847, acc 0.921875, learning_rate 0.000100123
2017-10-10T13:07:01.772105: step 2591, loss 0.235923, acc 0.9375, learning_rate 0.000100123
2017-10-10T13:07:02.300934: step 2592, loss 0.165255, acc 0.953125, learning_rate 0.000100122
2017-10-10T13:07:02.804930: step 2593, loss 0.265096, acc 0.890625, learning_rate 0.000100122
2017-10-10T13:07:03.337946: step 2594, loss 0.20106, acc 0.921875, learning_rate 0.000100121
2017-10-10T13:07:03.960928: step 2595, loss 0.253857, acc 0.90625, learning_rate 0.000100121
2017-10-10T13:07:04.509099: step 2596, loss 0.206262, acc 0.9375, learning_rate 0.00010012
2017-10-10T13:07:05.065145: step 2597, loss 0.175917, acc 0.9375, learning_rate 0.00010012
2017-10-10T13:07:05.672896: step 2598, loss 0.269103, acc 0.90625, learning_rate 0.000100119
2017-10-10T13:07:06.207490: step 2599, loss 0.158756, acc 0.921875, learning_rate 0.000100119
2017-10-10T13:07:06.651859: step 2600, loss 0.122487, acc 0.953125, learning_rate 0.000100118

Evaluation:
2017-10-10T13:07:07.810444: step 2600, loss 0.240327, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2600

2017-10-10T13:07:09.276271: step 2601, loss 0.200332, acc 0.9375, learning_rate 0.000100118
2017-10-10T13:07:09.764531: step 2602, loss 0.158646, acc 0.953125, learning_rate 0.000100117
2017-10-10T13:07:10.332865: step 2603, loss 0.131037, acc 0.96875, learning_rate 0.000100117
2017-10-10T13:07:10.836205: step 2604, loss 0.223785, acc 0.921875, learning_rate 0.000100117
2017-10-10T13:07:11.368923: step 2605, loss 0.19213, acc 0.96875, learning_rate 0.000100116
2017-10-10T13:07:11.841793: step 2606, loss 0.104905, acc 0.96875, learning_rate 0.000100116
2017-10-10T13:07:12.274648: step 2607, loss 0.417175, acc 0.84375, learning_rate 0.000100115
2017-10-10T13:07:12.793218: step 2608, loss 0.148369, acc 0.96875, learning_rate 0.000100115
2017-10-10T13:07:13.344594: step 2609, loss 0.276152, acc 0.890625, learning_rate 0.000100114
2017-10-10T13:07:13.841743: step 2610, loss 0.206627, acc 0.921875, learning_rate 0.000100114
2017-10-10T13:07:14.316935: step 2611, loss 0.25055, acc 0.921875, learning_rate 0.000100113
2017-10-10T13:07:14.968929: step 2612, loss 0.165408, acc 0.953125, learning_rate 0.000100113
2017-10-10T13:07:15.460868: step 2613, loss 0.374196, acc 0.875, learning_rate 0.000100112
2017-10-10T13:07:15.825059: step 2614, loss 0.314694, acc 0.875, learning_rate 0.000100112
2017-10-10T13:07:16.237074: step 2615, loss 0.210632, acc 0.90625, learning_rate 0.000100111
2017-10-10T13:07:16.785030: step 2616, loss 0.260323, acc 0.90625, learning_rate 0.000100111
2017-10-10T13:07:17.376882: step 2617, loss 0.170812, acc 0.921875, learning_rate 0.000100111
2017-10-10T13:07:17.927771: step 2618, loss 0.117039, acc 0.953125, learning_rate 0.00010011
2017-10-10T13:07:18.467173: step 2619, loss 0.288654, acc 0.921875, learning_rate 0.00010011
2017-10-10T13:07:18.984855: step 2620, loss 0.229138, acc 0.9375, learning_rate 0.000100109
2017-10-10T13:07:19.494642: step 2621, loss 0.278005, acc 0.890625, learning_rate 0.000100109
2017-10-10T13:07:19.964973: step 2622, loss 0.368897, acc 0.90625, learning_rate 0.000100108
2017-10-10T13:07:20.965908: step 2623, loss 0.29249, acc 0.90625, learning_rate 0.000100108
2017-10-10T13:07:21.529331: step 2624, loss 0.222274, acc 0.90625, learning_rate 0.000100107
2017-10-10T13:07:22.099191: step 2625, loss 0.198305, acc 0.9375, learning_rate 0.000100107
2017-10-10T13:07:22.609071: step 2626, loss 0.190845, acc 0.953125, learning_rate 0.000100107
2017-10-10T13:07:23.204471: step 2627, loss 0.262155, acc 0.90625, learning_rate 0.000100106
2017-10-10T13:07:23.726146: step 2628, loss 0.220032, acc 0.96875, learning_rate 0.000100106
2017-10-10T13:07:24.216490: step 2629, loss 0.141387, acc 0.953125, learning_rate 0.000100105
2017-10-10T13:07:24.706954: step 2630, loss 0.450571, acc 0.796875, learning_rate 0.000100105
2017-10-10T13:07:25.236871: step 2631, loss 0.263065, acc 0.921875, learning_rate 0.000100104
2017-10-10T13:07:25.718304: step 2632, loss 0.205558, acc 0.90625, learning_rate 0.000100104
2017-10-10T13:07:26.256918: step 2633, loss 0.125998, acc 0.96875, learning_rate 0.000100104
2017-10-10T13:07:26.757207: step 2634, loss 0.140507, acc 0.9375, learning_rate 0.000100103
2017-10-10T13:07:27.303450: step 2635, loss 0.198019, acc 0.96875, learning_rate 0.000100103
2017-10-10T13:07:27.877304: step 2636, loss 0.285409, acc 0.90625, learning_rate 0.000100102
2017-10-10T13:07:28.423375: step 2637, loss 0.133518, acc 0.953125, learning_rate 0.000100102
2017-10-10T13:07:29.014122: step 2638, loss 0.140007, acc 0.953125, learning_rate 0.000100101
2017-10-10T13:07:29.380840: step 2639, loss 0.222421, acc 0.9375, learning_rate 0.000100101
2017-10-10T13:07:29.843254: step 2640, loss 0.206289, acc 0.921875, learning_rate 0.000100101

Evaluation:
2017-10-10T13:07:30.988894: step 2640, loss 0.237715, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2640

2017-10-10T13:07:32.579732: step 2641, loss 0.182088, acc 0.96875, learning_rate 0.0001001
2017-10-10T13:07:33.088970: step 2642, loss 0.274398, acc 0.921875, learning_rate 0.0001001
2017-10-10T13:07:33.604914: step 2643, loss 0.211352, acc 0.921875, learning_rate 0.000100099
2017-10-10T13:07:34.267395: step 2644, loss 0.0989974, acc 0.96875, learning_rate 0.000100099
2017-10-10T13:07:34.694215: step 2645, loss 0.291425, acc 0.875, learning_rate 0.000100099
2017-10-10T13:07:35.000966: step 2646, loss 0.158356, acc 0.960784, learning_rate 0.000100098
2017-10-10T13:07:35.427927: step 2647, loss 0.208748, acc 0.9375, learning_rate 0.000100098
2017-10-10T13:07:35.955417: step 2648, loss 0.0953509, acc 0.96875, learning_rate 0.000100097
2017-10-10T13:07:36.452833: step 2649, loss 0.327045, acc 0.890625, learning_rate 0.000100097
2017-10-10T13:07:36.980829: step 2650, loss 0.232841, acc 0.9375, learning_rate 0.000100097
2017-10-10T13:07:37.484926: step 2651, loss 0.203103, acc 0.9375, learning_rate 0.000100096
2017-10-10T13:07:38.097547: step 2652, loss 0.222212, acc 0.9375, learning_rate 0.000100096
2017-10-10T13:07:38.684629: step 2653, loss 0.165436, acc 0.96875, learning_rate 0.000100095
2017-10-10T13:07:39.147173: step 2654, loss 0.188164, acc 0.96875, learning_rate 0.000100095
2017-10-10T13:07:39.586230: step 2655, loss 0.114465, acc 0.96875, learning_rate 0.000100095
2017-10-10T13:07:40.037242: step 2656, loss 0.176242, acc 0.9375, learning_rate 0.000100094
2017-10-10T13:07:40.549442: step 2657, loss 0.2478, acc 0.953125, learning_rate 0.000100094
2017-10-10T13:07:41.112093: step 2658, loss 0.408217, acc 0.921875, learning_rate 0.000100093
2017-10-10T13:07:41.677042: step 2659, loss 0.156275, acc 0.9375, learning_rate 0.000100093
2017-10-10T13:07:42.211452: step 2660, loss 0.162515, acc 0.9375, learning_rate 0.000100093
2017-10-10T13:07:42.780857: step 2661, loss 0.396851, acc 0.890625, learning_rate 0.000100092
2017-10-10T13:07:43.330066: step 2662, loss 0.231516, acc 0.9375, learning_rate 0.000100092
2017-10-10T13:07:43.816880: step 2663, loss 0.165363, acc 0.921875, learning_rate 0.000100092
2017-10-10T13:07:44.292857: step 2664, loss 0.33705, acc 0.875, learning_rate 0.000100091
2017-10-10T13:07:44.880805: step 2665, loss 0.186021, acc 0.921875, learning_rate 0.000100091
2017-10-10T13:07:45.396073: step 2666, loss 0.293467, acc 0.921875, learning_rate 0.00010009
2017-10-10T13:07:46.013185: step 2667, loss 0.361061, acc 0.875, learning_rate 0.00010009
2017-10-10T13:07:46.516042: step 2668, loss 0.233829, acc 0.921875, learning_rate 0.00010009
2017-10-10T13:07:47.045737: step 2669, loss 0.242841, acc 0.921875, learning_rate 0.000100089
2017-10-10T13:07:47.596188: step 2670, loss 0.272444, acc 0.890625, learning_rate 0.000100089
2017-10-10T13:07:48.104856: step 2671, loss 0.330533, acc 0.90625, learning_rate 0.000100089
2017-10-10T13:07:48.684823: step 2672, loss 0.281327, acc 0.890625, learning_rate 0.000100088
2017-10-10T13:07:49.273269: step 2673, loss 0.277777, acc 0.875, learning_rate 0.000100088
2017-10-10T13:07:49.812994: step 2674, loss 0.127979, acc 0.96875, learning_rate 0.000100088
2017-10-10T13:07:50.466719: step 2675, loss 0.181068, acc 0.921875, learning_rate 0.000100087
2017-10-10T13:07:51.112835: step 2676, loss 0.393, acc 0.828125, learning_rate 0.000100087
2017-10-10T13:07:51.720820: step 2677, loss 0.199802, acc 0.921875, learning_rate 0.000100086
2017-10-10T13:07:52.197517: step 2678, loss 0.267726, acc 0.859375, learning_rate 0.000100086
2017-10-10T13:07:52.646731: step 2679, loss 0.198057, acc 0.921875, learning_rate 0.000100086
2017-10-10T13:07:53.187826: step 2680, loss 0.242335, acc 0.90625, learning_rate 0.000100085

Evaluation:
2017-10-10T13:07:54.406830: step 2680, loss 0.236894, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2680

2017-10-10T13:07:56.123435: step 2681, loss 0.199976, acc 0.921875, learning_rate 0.000100085
2017-10-10T13:07:56.600962: step 2682, loss 0.200713, acc 0.96875, learning_rate 0.000100085
2017-10-10T13:07:57.129308: step 2683, loss 0.13456, acc 0.953125, learning_rate 0.000100084
2017-10-10T13:07:57.523343: step 2684, loss 0.253049, acc 0.921875, learning_rate 0.000100084
2017-10-10T13:07:58.000847: step 2685, loss 0.401415, acc 0.859375, learning_rate 0.000100084
2017-10-10T13:07:58.427738: step 2686, loss 0.26199, acc 0.875, learning_rate 0.000100083
2017-10-10T13:07:58.919486: step 2687, loss 0.364976, acc 0.828125, learning_rate 0.000100083
2017-10-10T13:07:59.413028: step 2688, loss 0.168816, acc 0.9375, learning_rate 0.000100083
2017-10-10T13:07:59.957963: step 2689, loss 0.240034, acc 0.9375, learning_rate 0.000100082
2017-10-10T13:08:00.537081: step 2690, loss 0.3214, acc 0.90625, learning_rate 0.000100082
2017-10-10T13:08:01.076974: step 2691, loss 0.205633, acc 0.90625, learning_rate 0.000100082
2017-10-10T13:08:01.572898: step 2692, loss 0.29157, acc 0.9375, learning_rate 0.000100081
2017-10-10T13:08:02.248349: step 2693, loss 0.246046, acc 0.90625, learning_rate 0.000100081
2017-10-10T13:08:02.656857: step 2694, loss 0.179367, acc 0.953125, learning_rate 0.000100081
2017-10-10T13:08:03.001062: step 2695, loss 0.260355, acc 0.953125, learning_rate 0.00010008
2017-10-10T13:08:03.519199: step 2696, loss 0.226156, acc 0.90625, learning_rate 0.00010008
2017-10-10T13:08:04.002970: step 2697, loss 0.312629, acc 0.890625, learning_rate 0.00010008
2017-10-10T13:08:04.457276: step 2698, loss 0.345902, acc 0.859375, learning_rate 0.000100079
2017-10-10T13:08:04.954821: step 2699, loss 0.223142, acc 0.921875, learning_rate 0.000100079
2017-10-10T13:08:05.453873: step 2700, loss 0.150312, acc 0.953125, learning_rate 0.000100079
2017-10-10T13:08:05.988958: step 2701, loss 0.284051, acc 0.890625, learning_rate 0.000100078
2017-10-10T13:08:06.574828: step 2702, loss 0.256881, acc 0.921875, learning_rate 0.000100078
2017-10-10T13:08:07.109039: step 2703, loss 0.16989, acc 0.9375, learning_rate 0.000100078
2017-10-10T13:08:07.610625: step 2704, loss 0.201223, acc 0.953125, learning_rate 0.000100077
2017-10-10T13:08:08.162345: step 2705, loss 0.240241, acc 0.90625, learning_rate 0.000100077
2017-10-10T13:08:08.717665: step 2706, loss 0.128522, acc 0.984375, learning_rate 0.000100077
2017-10-10T13:08:09.296853: step 2707, loss 0.177921, acc 0.921875, learning_rate 0.000100076
2017-10-10T13:08:09.823324: step 2708, loss 0.275063, acc 0.90625, learning_rate 0.000100076
2017-10-10T13:08:10.395139: step 2709, loss 0.153357, acc 0.96875, learning_rate 0.000100076
2017-10-10T13:08:10.936826: step 2710, loss 0.181725, acc 0.96875, learning_rate 0.000100076
2017-10-10T13:08:11.469131: step 2711, loss 0.105358, acc 0.984375, learning_rate 0.000100075
2017-10-10T13:08:12.008467: step 2712, loss 0.156842, acc 0.984375, learning_rate 0.000100075
2017-10-10T13:08:12.584846: step 2713, loss 0.243008, acc 0.90625, learning_rate 0.000100075
2017-10-10T13:08:13.126845: step 2714, loss 0.156541, acc 0.953125, learning_rate 0.000100074
2017-10-10T13:08:13.652963: step 2715, loss 0.107854, acc 0.96875, learning_rate 0.000100074
2017-10-10T13:08:14.285088: step 2716, loss 0.257612, acc 0.875, learning_rate 0.000100074
2017-10-10T13:08:14.829626: step 2717, loss 0.127471, acc 0.96875, learning_rate 0.000100073
2017-10-10T13:08:15.228838: step 2718, loss 0.286936, acc 0.921875, learning_rate 0.000100073
2017-10-10T13:08:15.638476: step 2719, loss 0.294625, acc 0.875, learning_rate 0.000100073
2017-10-10T13:08:16.169570: step 2720, loss 0.157966, acc 0.953125, learning_rate 0.000100073

Evaluation:
2017-10-10T13:08:17.306649: step 2720, loss 0.239412, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2720

2017-10-10T13:08:18.760148: step 2721, loss 0.299443, acc 0.921875, learning_rate 0.000100072
2017-10-10T13:08:19.324662: step 2722, loss 0.268188, acc 0.90625, learning_rate 0.000100072
2017-10-10T13:08:19.856861: step 2723, loss 0.102443, acc 0.96875, learning_rate 0.000100072
2017-10-10T13:08:20.480117: step 2724, loss 0.213691, acc 0.90625, learning_rate 0.000100071
2017-10-10T13:08:20.950714: step 2725, loss 0.172963, acc 0.9375, learning_rate 0.000100071
2017-10-10T13:08:21.408393: step 2726, loss 0.191162, acc 0.921875, learning_rate 0.000100071
2017-10-10T13:08:21.984403: step 2727, loss 0.143008, acc 0.953125, learning_rate 0.00010007
2017-10-10T13:08:22.549632: step 2728, loss 0.127919, acc 0.953125, learning_rate 0.00010007
2017-10-10T13:08:23.068367: step 2729, loss 0.295736, acc 0.9375, learning_rate 0.00010007
2017-10-10T13:08:23.598440: step 2730, loss 0.411065, acc 0.859375, learning_rate 0.00010007
2017-10-10T13:08:24.085010: step 2731, loss 0.163418, acc 0.953125, learning_rate 0.000100069
2017-10-10T13:08:24.636895: step 2732, loss 0.170897, acc 0.9375, learning_rate 0.000100069
2017-10-10T13:08:25.212917: step 2733, loss 0.215239, acc 0.9375, learning_rate 0.000100069
2017-10-10T13:08:25.856965: step 2734, loss 0.348892, acc 0.875, learning_rate 0.000100068
2017-10-10T13:08:26.308331: step 2735, loss 0.189878, acc 0.953125, learning_rate 0.000100068
2017-10-10T13:08:26.760773: step 2736, loss 0.20127, acc 0.96875, learning_rate 0.000100068
2017-10-10T13:08:27.283520: step 2737, loss 0.257877, acc 0.9375, learning_rate 0.000100068
2017-10-10T13:08:27.804688: step 2738, loss 0.274325, acc 0.921875, learning_rate 0.000100067
2017-10-10T13:08:28.349116: step 2739, loss 0.324364, acc 0.875, learning_rate 0.000100067
2017-10-10T13:08:28.970658: step 2740, loss 0.315298, acc 0.90625, learning_rate 0.000100067
2017-10-10T13:08:29.479966: step 2741, loss 0.223401, acc 0.921875, learning_rate 0.000100067
2017-10-10T13:08:29.989107: step 2742, loss 0.210169, acc 0.9375, learning_rate 0.000100066
2017-10-10T13:08:30.463358: step 2743, loss 0.199491, acc 0.921875, learning_rate 0.000100066
2017-10-10T13:08:30.920610: step 2744, loss 0.224879, acc 0.941176, learning_rate 0.000100066
2017-10-10T13:08:31.475308: step 2745, loss 0.265383, acc 0.953125, learning_rate 0.000100065
2017-10-10T13:08:32.024806: step 2746, loss 0.209914, acc 0.921875, learning_rate 0.000100065
2017-10-10T13:08:32.608760: step 2747, loss 0.274068, acc 0.875, learning_rate 0.000100065
2017-10-10T13:08:33.092705: step 2748, loss 0.156933, acc 0.96875, learning_rate 0.000100065
2017-10-10T13:08:34.012843: step 2749, loss 0.21407, acc 0.90625, learning_rate 0.000100064
2017-10-10T13:08:34.561132: step 2750, loss 0.143109, acc 0.953125, learning_rate 0.000100064
2017-10-10T13:08:35.129235: step 2751, loss 0.210984, acc 0.921875, learning_rate 0.000100064
2017-10-10T13:08:35.650641: step 2752, loss 0.26077, acc 0.921875, learning_rate 0.000100064
2017-10-10T13:08:36.264520: step 2753, loss 0.1662, acc 0.96875, learning_rate 0.000100063
2017-10-10T13:08:36.807677: step 2754, loss 0.268699, acc 0.875, learning_rate 0.000100063
2017-10-10T13:08:37.417663: step 2755, loss 0.169481, acc 0.9375, learning_rate 0.000100063
2017-10-10T13:08:38.025685: step 2756, loss 0.291067, acc 0.921875, learning_rate 0.000100063
2017-10-10T13:08:38.465053: step 2757, loss 0.100605, acc 0.96875, learning_rate 0.000100062
2017-10-10T13:08:38.904858: step 2758, loss 0.128405, acc 0.96875, learning_rate 0.000100062
2017-10-10T13:08:39.467548: step 2759, loss 0.333238, acc 0.875, learning_rate 0.000100062
2017-10-10T13:08:40.014280: step 2760, loss 0.136692, acc 0.96875, learning_rate 0.000100062

Evaluation:
2017-10-10T13:08:41.185586: step 2760, loss 0.237158, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2760

2017-10-10T13:08:42.789413: step 2761, loss 0.100725, acc 0.953125, learning_rate 0.000100061
2017-10-10T13:08:43.353896: step 2762, loss 0.243714, acc 0.921875, learning_rate 0.000100061
2017-10-10T13:08:43.902808: step 2763, loss 0.280238, acc 0.921875, learning_rate 0.000100061
2017-10-10T13:08:44.378073: step 2764, loss 0.141258, acc 0.9375, learning_rate 0.000100061
2017-10-10T13:08:44.855994: step 2765, loss 0.112394, acc 0.96875, learning_rate 0.00010006
2017-10-10T13:08:45.399922: step 2766, loss 0.297696, acc 0.921875, learning_rate 0.00010006
2017-10-10T13:08:45.904974: step 2767, loss 0.318114, acc 0.921875, learning_rate 0.00010006
2017-10-10T13:08:46.467350: step 2768, loss 0.252458, acc 0.921875, learning_rate 0.00010006
2017-10-10T13:08:46.940992: step 2769, loss 0.310726, acc 0.921875, learning_rate 0.000100059
2017-10-10T13:08:47.524997: step 2770, loss 0.261841, acc 0.890625, learning_rate 0.000100059
2017-10-10T13:08:47.989011: step 2771, loss 0.210018, acc 0.921875, learning_rate 0.000100059
2017-10-10T13:08:48.497935: step 2772, loss 0.124453, acc 0.96875, learning_rate 0.000100059
2017-10-10T13:08:49.116914: step 2773, loss 0.175703, acc 0.953125, learning_rate 0.000100058
2017-10-10T13:08:49.700884: step 2774, loss 0.233339, acc 0.921875, learning_rate 0.000100058
2017-10-10T13:08:50.167231: step 2775, loss 0.209597, acc 0.9375, learning_rate 0.000100058
2017-10-10T13:08:50.606607: step 2776, loss 0.203856, acc 0.9375, learning_rate 0.000100058
2017-10-10T13:08:51.124855: step 2777, loss 0.084736, acc 0.984375, learning_rate 0.000100057
2017-10-10T13:08:51.616869: step 2778, loss 0.306055, acc 0.90625, learning_rate 0.000100057
2017-10-10T13:08:52.140828: step 2779, loss 0.147938, acc 0.953125, learning_rate 0.000100057
2017-10-10T13:08:52.680810: step 2780, loss 0.197215, acc 0.890625, learning_rate 0.000100057
2017-10-10T13:08:53.247996: step 2781, loss 0.468207, acc 0.90625, learning_rate 0.000100056
2017-10-10T13:08:53.764844: step 2782, loss 0.275985, acc 0.90625, learning_rate 0.000100056
2017-10-10T13:08:54.308916: step 2783, loss 0.211386, acc 0.9375, learning_rate 0.000100056
2017-10-10T13:08:54.852467: step 2784, loss 0.144456, acc 0.96875, learning_rate 0.000100056
2017-10-10T13:08:55.432843: step 2785, loss 0.24978, acc 0.921875, learning_rate 0.000100056
2017-10-10T13:08:55.906447: step 2786, loss 0.298834, acc 0.875, learning_rate 0.000100055
2017-10-10T13:08:56.472848: step 2787, loss 0.240887, acc 0.9375, learning_rate 0.000100055
2017-10-10T13:08:56.949572: step 2788, loss 0.271867, acc 0.90625, learning_rate 0.000100055
2017-10-10T13:08:57.472905: step 2789, loss 0.310579, acc 0.890625, learning_rate 0.000100055
2017-10-10T13:08:57.968345: step 2790, loss 0.273985, acc 0.875, learning_rate 0.000100054
2017-10-10T13:08:58.417231: step 2791, loss 0.196958, acc 0.90625, learning_rate 0.000100054
2017-10-10T13:08:58.937121: step 2792, loss 0.081977, acc 1, learning_rate 0.000100054
2017-10-10T13:08:59.593290: step 2793, loss 0.443586, acc 0.796875, learning_rate 0.000100054
2017-10-10T13:09:00.126572: step 2794, loss 0.148362, acc 0.921875, learning_rate 0.000100054
2017-10-10T13:09:00.640832: step 2795, loss 0.190352, acc 0.921875, learning_rate 0.000100053
2017-10-10T13:09:01.091675: step 2796, loss 0.160752, acc 0.96875, learning_rate 0.000100053
2017-10-10T13:09:01.560885: step 2797, loss 0.0477044, acc 1, learning_rate 0.000100053
2017-10-10T13:09:02.073062: step 2798, loss 0.236829, acc 0.9375, learning_rate 0.000100053
2017-10-10T13:09:02.633649: step 2799, loss 0.253444, acc 0.890625, learning_rate 0.000100052
2017-10-10T13:09:03.197552: step 2800, loss 0.178999, acc 0.921875, learning_rate 0.000100052

Evaluation:
2017-10-10T13:09:04.350560: step 2800, loss 0.236152, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2800

2017-10-10T13:09:06.088937: step 2801, loss 0.123023, acc 0.984375, learning_rate 0.000100052
2017-10-10T13:09:06.528892: step 2802, loss 0.209369, acc 0.921875, learning_rate 0.000100052
2017-10-10T13:09:06.944623: step 2803, loss 0.200234, acc 0.921875, learning_rate 0.000100052
2017-10-10T13:09:07.466901: step 2804, loss 0.20313, acc 0.90625, learning_rate 0.000100051
2017-10-10T13:09:07.952989: step 2805, loss 0.178447, acc 0.953125, learning_rate 0.000100051
2017-10-10T13:09:08.484636: step 2806, loss 0.344142, acc 0.875, learning_rate 0.000100051
2017-10-10T13:09:08.969172: step 2807, loss 0.266504, acc 0.953125, learning_rate 0.000100051
2017-10-10T13:09:09.441032: step 2808, loss 0.109075, acc 0.96875, learning_rate 0.000100051
2017-10-10T13:09:09.976928: step 2809, loss 0.194283, acc 0.9375, learning_rate 0.00010005
2017-10-10T13:09:10.511909: step 2810, loss 0.262886, acc 0.921875, learning_rate 0.00010005
2017-10-10T13:09:11.044878: step 2811, loss 0.279022, acc 0.921875, learning_rate 0.00010005
2017-10-10T13:09:11.589058: step 2812, loss 0.228441, acc 0.921875, learning_rate 0.00010005
2017-10-10T13:09:12.105742: step 2813, loss 0.264008, acc 0.875, learning_rate 0.00010005
2017-10-10T13:09:12.708977: step 2814, loss 0.376739, acc 0.859375, learning_rate 0.000100049
2017-10-10T13:09:13.264861: step 2815, loss 0.18626, acc 0.9375, learning_rate 0.000100049
2017-10-10T13:09:13.707599: step 2816, loss 0.146181, acc 0.953125, learning_rate 0.000100049
2017-10-10T13:09:14.132486: step 2817, loss 0.278473, acc 0.890625, learning_rate 0.000100049
2017-10-10T13:09:14.634750: step 2818, loss 0.243273, acc 0.890625, learning_rate 0.000100049
2017-10-10T13:09:15.160937: step 2819, loss 0.107273, acc 0.953125, learning_rate 0.000100048
2017-10-10T13:09:15.680910: step 2820, loss 0.101276, acc 0.96875, learning_rate 0.000100048
2017-10-10T13:09:16.189731: step 2821, loss 0.246058, acc 0.921875, learning_rate 0.000100048
2017-10-10T13:09:16.713461: step 2822, loss 0.220041, acc 0.9375, learning_rate 0.000100048
2017-10-10T13:09:17.284136: step 2823, loss 0.12562, acc 0.96875, learning_rate 0.000100048
2017-10-10T13:09:17.824381: step 2824, loss 0.37118, acc 0.953125, learning_rate 0.000100047
2017-10-10T13:09:18.397300: step 2825, loss 0.351616, acc 0.890625, learning_rate 0.000100047
2017-10-10T13:09:18.948839: step 2826, loss 0.217603, acc 0.90625, learning_rate 0.000100047
2017-10-10T13:09:19.504871: step 2827, loss 0.240455, acc 0.890625, learning_rate 0.000100047
2017-10-10T13:09:20.045270: step 2828, loss 0.198385, acc 0.9375, learning_rate 0.000100047
2017-10-10T13:09:20.537071: step 2829, loss 0.178382, acc 0.9375, learning_rate 0.000100046
2017-10-10T13:09:21.036929: step 2830, loss 0.144605, acc 0.953125, learning_rate 0.000100046
2017-10-10T13:09:21.505137: step 2831, loss 0.156115, acc 0.921875, learning_rate 0.000100046
2017-10-10T13:09:22.020971: step 2832, loss 0.177789, acc 0.96875, learning_rate 0.000100046
2017-10-10T13:09:22.493053: step 2833, loss 0.221883, acc 0.890625, learning_rate 0.000100046
2017-10-10T13:09:23.117762: step 2834, loss 0.29829, acc 0.921875, learning_rate 0.000100045
2017-10-10T13:09:23.681026: step 2835, loss 0.324459, acc 0.859375, learning_rate 0.000100045
2017-10-10T13:09:24.084928: step 2836, loss 0.305635, acc 0.890625, learning_rate 0.000100045
2017-10-10T13:09:24.503314: step 2837, loss 0.236009, acc 0.9375, learning_rate 0.000100045
2017-10-10T13:09:25.032862: step 2838, loss 0.250999, acc 0.90625, learning_rate 0.000100045
2017-10-10T13:09:25.507192: step 2839, loss 0.234423, acc 0.90625, learning_rate 0.000100045
2017-10-10T13:09:26.010482: step 2840, loss 0.209274, acc 0.890625, learning_rate 0.000100044

Evaluation:
2017-10-10T13:09:27.066329: step 2840, loss 0.2359, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2840

2017-10-10T13:09:28.568778: step 2841, loss 0.208896, acc 0.890625, learning_rate 0.000100044
2017-10-10T13:09:29.052585: step 2842, loss 0.414448, acc 0.901961, learning_rate 0.000100044
2017-10-10T13:09:29.526838: step 2843, loss 0.222337, acc 0.9375, learning_rate 0.000100044
2017-10-10T13:09:29.999376: step 2844, loss 0.274024, acc 0.859375, learning_rate 0.000100044
2017-10-10T13:09:30.554771: step 2845, loss 0.303931, acc 0.90625, learning_rate 0.000100043
2017-10-10T13:09:31.104127: step 2846, loss 0.189485, acc 0.953125, learning_rate 0.000100043
2017-10-10T13:09:31.638334: step 2847, loss 0.191601, acc 0.921875, learning_rate 0.000100043
2017-10-10T13:09:32.225825: step 2848, loss 0.208127, acc 0.921875, learning_rate 0.000100043
2017-10-10T13:09:32.807178: step 2849, loss 0.293141, acc 0.890625, learning_rate 0.000100043
2017-10-10T13:09:33.384887: step 2850, loss 0.226698, acc 0.921875, learning_rate 0.000100043
2017-10-10T13:09:33.903742: step 2851, loss 0.213999, acc 0.921875, learning_rate 0.000100042
2017-10-10T13:09:34.402898: step 2852, loss 0.168931, acc 0.90625, learning_rate 0.000100042
2017-10-10T13:09:34.945161: step 2853, loss 0.175931, acc 0.9375, learning_rate 0.000100042
2017-10-10T13:09:35.519188: step 2854, loss 0.198143, acc 0.921875, learning_rate 0.000100042
2017-10-10T13:09:36.075073: step 2855, loss 0.308308, acc 0.90625, learning_rate 0.000100042
2017-10-10T13:09:36.507453: step 2856, loss 0.366317, acc 0.875, learning_rate 0.000100042
2017-10-10T13:09:36.968026: step 2857, loss 0.127543, acc 0.96875, learning_rate 0.000100041
2017-10-10T13:09:37.444898: step 2858, loss 0.237048, acc 0.9375, learning_rate 0.000100041
2017-10-10T13:09:37.999899: step 2859, loss 0.16475, acc 0.921875, learning_rate 0.000100041
2017-10-10T13:09:38.445902: step 2860, loss 0.219656, acc 0.921875, learning_rate 0.000100041
2017-10-10T13:09:38.944981: step 2861, loss 0.167801, acc 0.9375, learning_rate 0.000100041
2017-10-10T13:09:39.530141: step 2862, loss 0.251893, acc 0.90625, learning_rate 0.000100041
2017-10-10T13:09:40.081017: step 2863, loss 0.168939, acc 0.953125, learning_rate 0.00010004
2017-10-10T13:09:40.697314: step 2864, loss 0.200318, acc 0.921875, learning_rate 0.00010004
2017-10-10T13:09:41.260979: step 2865, loss 0.160956, acc 0.953125, learning_rate 0.00010004
2017-10-10T13:09:41.681305: step 2866, loss 0.152026, acc 0.96875, learning_rate 0.00010004
2017-10-10T13:09:42.253259: step 2867, loss 0.152699, acc 0.96875, learning_rate 0.00010004
2017-10-10T13:09:42.692547: step 2868, loss 0.205859, acc 0.953125, learning_rate 0.00010004
2017-10-10T13:09:43.244950: step 2869, loss 0.269755, acc 0.890625, learning_rate 0.000100039
2017-10-10T13:09:43.737316: step 2870, loss 0.167663, acc 0.9375, learning_rate 0.000100039
2017-10-10T13:09:44.246157: step 2871, loss 0.15757, acc 0.96875, learning_rate 0.000100039
2017-10-10T13:09:44.746612: step 2872, loss 0.140055, acc 0.984375, learning_rate 0.000100039
2017-10-10T13:09:45.285097: step 2873, loss 0.236526, acc 0.90625, learning_rate 0.000100039
2017-10-10T13:09:45.920982: step 2874, loss 0.169172, acc 0.9375, learning_rate 0.000100039
2017-10-10T13:09:46.349120: step 2875, loss 0.192489, acc 0.953125, learning_rate 0.000100038
2017-10-10T13:09:46.786173: step 2876, loss 0.123509, acc 0.984375, learning_rate 0.000100038
2017-10-10T13:09:47.266719: step 2877, loss 0.171259, acc 0.9375, learning_rate 0.000100038
2017-10-10T13:09:47.765247: step 2878, loss 0.0956383, acc 0.96875, learning_rate 0.000100038
2017-10-10T13:09:48.267229: step 2879, loss 0.195843, acc 0.921875, learning_rate 0.000100038
2017-10-10T13:09:48.745839: step 2880, loss 0.171879, acc 0.9375, learning_rate 0.000100038

Evaluation:
2017-10-10T13:09:49.928397: step 2880, loss 0.237161, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2880

2017-10-10T13:09:51.649166: step 2881, loss 0.210132, acc 0.953125, learning_rate 0.000100038
2017-10-10T13:09:52.254804: step 2882, loss 0.212054, acc 0.953125, learning_rate 0.000100037
2017-10-10T13:09:52.675314: step 2883, loss 0.395252, acc 0.84375, learning_rate 0.000100037
2017-10-10T13:09:53.106061: step 2884, loss 0.188706, acc 0.953125, learning_rate 0.000100037
2017-10-10T13:09:53.680091: step 2885, loss 0.177523, acc 0.953125, learning_rate 0.000100037
2017-10-10T13:09:54.222810: step 2886, loss 0.258344, acc 0.890625, learning_rate 0.000100037
2017-10-10T13:09:54.773254: step 2887, loss 0.245628, acc 0.921875, learning_rate 0.000100037
2017-10-10T13:09:55.344407: step 2888, loss 0.329195, acc 0.890625, learning_rate 0.000100036
2017-10-10T13:09:55.884857: step 2889, loss 0.149982, acc 0.953125, learning_rate 0.000100036
2017-10-10T13:09:56.419427: step 2890, loss 0.247846, acc 0.90625, learning_rate 0.000100036
2017-10-10T13:09:57.009843: step 2891, loss 0.242342, acc 0.890625, learning_rate 0.000100036
2017-10-10T13:09:57.564884: step 2892, loss 0.142128, acc 0.9375, learning_rate 0.000100036
2017-10-10T13:09:58.104709: step 2893, loss 0.305369, acc 0.921875, learning_rate 0.000100036
2017-10-10T13:09:58.700842: step 2894, loss 0.248161, acc 0.875, learning_rate 0.000100036
2017-10-10T13:09:59.320466: step 2895, loss 0.130457, acc 0.953125, learning_rate 0.000100035
2017-10-10T13:09:59.753451: step 2896, loss 0.200831, acc 0.921875, learning_rate 0.000100035
2017-10-10T13:10:00.200787: step 2897, loss 0.17659, acc 0.921875, learning_rate 0.000100035
2017-10-10T13:10:00.697108: step 2898, loss 0.271555, acc 0.90625, learning_rate 0.000100035
2017-10-10T13:10:01.249063: step 2899, loss 0.123854, acc 0.953125, learning_rate 0.000100035
2017-10-10T13:10:01.816858: step 2900, loss 0.23825, acc 0.890625, learning_rate 0.000100035
2017-10-10T13:10:02.324871: step 2901, loss 0.137352, acc 0.921875, learning_rate 0.000100035
2017-10-10T13:10:02.740833: step 2902, loss 0.150384, acc 0.9375, learning_rate 0.000100034
2017-10-10T13:10:03.292939: step 2903, loss 0.285666, acc 0.875, learning_rate 0.000100034
2017-10-10T13:10:03.872988: step 2904, loss 0.0982294, acc 0.984375, learning_rate 0.000100034
2017-10-10T13:10:04.409026: step 2905, loss 0.338942, acc 0.875, learning_rate 0.000100034
2017-10-10T13:10:04.892142: step 2906, loss 0.245704, acc 0.890625, learning_rate 0.000100034
2017-10-10T13:10:05.398174: step 2907, loss 0.28801, acc 0.890625, learning_rate 0.000100034
2017-10-10T13:10:05.945052: step 2908, loss 0.183452, acc 0.890625, learning_rate 0.000100034
2017-10-10T13:10:06.444615: step 2909, loss 0.297937, acc 0.875, learning_rate 0.000100033
2017-10-10T13:10:06.945003: step 2910, loss 0.171573, acc 0.9375, learning_rate 0.000100033
2017-10-10T13:10:07.538270: step 2911, loss 0.161179, acc 0.9375, learning_rate 0.000100033
2017-10-10T13:10:08.040843: step 2912, loss 0.182488, acc 0.953125, learning_rate 0.000100033
2017-10-10T13:10:08.620929: step 2913, loss 0.340375, acc 0.921875, learning_rate 0.000100033
2017-10-10T13:10:09.228153: step 2914, loss 0.201758, acc 0.921875, learning_rate 0.000100033
2017-10-10T13:10:09.654749: step 2915, loss 0.214005, acc 0.921875, learning_rate 0.000100033
2017-10-10T13:10:10.112528: step 2916, loss 0.173586, acc 0.9375, learning_rate 0.000100033
2017-10-10T13:10:10.550526: step 2917, loss 0.169786, acc 0.9375, learning_rate 0.000100032
2017-10-10T13:10:11.121971: step 2918, loss 0.191003, acc 0.9375, learning_rate 0.000100032
2017-10-10T13:10:11.682958: step 2919, loss 0.154456, acc 0.96875, learning_rate 0.000100032
2017-10-10T13:10:12.313322: step 2920, loss 0.214115, acc 0.890625, learning_rate 0.000100032

Evaluation:
2017-10-10T13:10:13.498460: step 2920, loss 0.236858, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2920

2017-10-10T13:10:15.362643: step 2921, loss 0.131388, acc 0.953125, learning_rate 0.000100032
2017-10-10T13:10:15.836531: step 2922, loss 0.270248, acc 0.84375, learning_rate 0.000100032
2017-10-10T13:10:16.288270: step 2923, loss 0.197153, acc 0.9375, learning_rate 0.000100032
2017-10-10T13:10:16.780937: step 2924, loss 0.0697885, acc 1, learning_rate 0.000100031
2017-10-10T13:10:17.342723: step 2925, loss 0.136205, acc 0.9375, learning_rate 0.000100031
2017-10-10T13:10:17.879952: step 2926, loss 0.410857, acc 0.859375, learning_rate 0.000100031
2017-10-10T13:10:18.438049: step 2927, loss 0.214503, acc 0.90625, learning_rate 0.000100031
2017-10-10T13:10:19.000669: step 2928, loss 0.293208, acc 0.890625, learning_rate 0.000100031
2017-10-10T13:10:19.526526: step 2929, loss 0.178561, acc 0.953125, learning_rate 0.000100031
2017-10-10T13:10:20.080360: step 2930, loss 0.25488, acc 0.96875, learning_rate 0.000100031
2017-10-10T13:10:20.619113: step 2931, loss 0.190334, acc 0.953125, learning_rate 0.000100031
2017-10-10T13:10:21.167143: step 2932, loss 0.27314, acc 0.890625, learning_rate 0.00010003
2017-10-10T13:10:21.720158: step 2933, loss 0.223114, acc 0.90625, learning_rate 0.00010003
2017-10-10T13:10:22.332916: step 2934, loss 0.194226, acc 0.921875, learning_rate 0.00010003
2017-10-10T13:10:22.976834: step 2935, loss 0.232875, acc 0.890625, learning_rate 0.00010003
2017-10-10T13:10:23.425653: step 2936, loss 0.25188, acc 0.90625, learning_rate 0.00010003
2017-10-10T13:10:23.877213: step 2937, loss 0.325717, acc 0.890625, learning_rate 0.00010003
2017-10-10T13:10:24.277925: step 2938, loss 0.127688, acc 0.953125, learning_rate 0.00010003
2017-10-10T13:10:24.784887: step 2939, loss 0.320946, acc 0.921875, learning_rate 0.00010003
2017-10-10T13:10:25.197133: step 2940, loss 0.206405, acc 0.980392, learning_rate 0.000100029
2017-10-10T13:10:25.724911: step 2941, loss 0.198751, acc 0.953125, learning_rate 0.000100029
2017-10-10T13:10:26.184875: step 2942, loss 0.429177, acc 0.859375, learning_rate 0.000100029
2017-10-10T13:10:26.709203: step 2943, loss 0.26383, acc 0.90625, learning_rate 0.000100029
2017-10-10T13:10:27.256851: step 2944, loss 0.242957, acc 0.953125, learning_rate 0.000100029
2017-10-10T13:10:27.788830: step 2945, loss 0.205177, acc 0.921875, learning_rate 0.000100029
2017-10-10T13:10:28.327674: step 2946, loss 0.203295, acc 0.9375, learning_rate 0.000100029
2017-10-10T13:10:28.848861: step 2947, loss 0.205534, acc 0.9375, learning_rate 0.000100029
2017-10-10T13:10:29.432928: step 2948, loss 0.181473, acc 0.921875, learning_rate 0.000100029
2017-10-10T13:10:30.040857: step 2949, loss 0.379952, acc 0.84375, learning_rate 0.000100028
2017-10-10T13:10:30.580989: step 2950, loss 0.383421, acc 0.875, learning_rate 0.000100028
2017-10-10T13:10:31.100493: step 2951, loss 0.319317, acc 0.890625, learning_rate 0.000100028
2017-10-10T13:10:31.635868: step 2952, loss 0.28683, acc 0.859375, learning_rate 0.000100028
2017-10-10T13:10:32.176985: step 2953, loss 0.157657, acc 0.9375, learning_rate 0.000100028
2017-10-10T13:10:32.759557: step 2954, loss 0.170014, acc 0.9375, learning_rate 0.000100028
2017-10-10T13:10:33.203383: step 2955, loss 0.172838, acc 0.9375, learning_rate 0.000100028
2017-10-10T13:10:33.695333: step 2956, loss 0.269372, acc 0.90625, learning_rate 0.000100028
2017-10-10T13:10:34.195805: step 2957, loss 0.175401, acc 0.9375, learning_rate 0.000100028
2017-10-10T13:10:34.635101: step 2958, loss 0.121682, acc 0.953125, learning_rate 0.000100027
2017-10-10T13:10:35.177833: step 2959, loss 0.234763, acc 0.921875, learning_rate 0.000100027
2017-10-10T13:10:35.692873: step 2960, loss 0.158589, acc 0.921875, learning_rate 0.000100027

Evaluation:
2017-10-10T13:10:36.857624: step 2960, loss 0.237554, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-2960

2017-10-10T13:10:38.298517: step 2961, loss 0.248783, acc 0.921875, learning_rate 0.000100027
2017-10-10T13:10:38.720937: step 2962, loss 0.352353, acc 0.921875, learning_rate 0.000100027
2017-10-10T13:10:39.212830: step 2963, loss 0.170502, acc 0.953125, learning_rate 0.000100027
2017-10-10T13:10:39.664965: step 2964, loss 0.0982624, acc 0.984375, learning_rate 0.000100027
2017-10-10T13:10:40.209040: step 2965, loss 0.240712, acc 0.90625, learning_rate 0.000100027
2017-10-10T13:10:40.800521: step 2966, loss 0.191735, acc 0.90625, learning_rate 0.000100027
2017-10-10T13:10:41.309117: step 2967, loss 0.361559, acc 0.90625, learning_rate 0.000100026
2017-10-10T13:10:41.803116: step 2968, loss 0.139328, acc 0.96875, learning_rate 0.000100026
2017-10-10T13:10:42.349373: step 2969, loss 0.230536, acc 0.90625, learning_rate 0.000100026
2017-10-10T13:10:42.885152: step 2970, loss 0.286136, acc 0.875, learning_rate 0.000100026
2017-10-10T13:10:43.429952: step 2971, loss 0.173003, acc 0.9375, learning_rate 0.000100026
2017-10-10T13:10:43.900940: step 2972, loss 0.330316, acc 0.890625, learning_rate 0.000100026
2017-10-10T13:10:44.445337: step 2973, loss 0.11664, acc 0.96875, learning_rate 0.000100026
2017-10-10T13:10:44.920988: step 2974, loss 0.171008, acc 0.9375, learning_rate 0.000100026
2017-10-10T13:10:45.468978: step 2975, loss 0.345275, acc 0.875, learning_rate 0.000100026
2017-10-10T13:10:46.056905: step 2976, loss 0.132049, acc 0.9375, learning_rate 0.000100025
2017-10-10T13:10:46.681425: step 2977, loss 0.269446, acc 0.921875, learning_rate 0.000100025
2017-10-10T13:10:47.121353: step 2978, loss 0.310772, acc 0.890625, learning_rate 0.000100025
2017-10-10T13:10:47.559046: step 2979, loss 0.219133, acc 0.921875, learning_rate 0.000100025
2017-10-10T13:10:48.114339: step 2980, loss 0.197799, acc 0.953125, learning_rate 0.000100025
2017-10-10T13:10:48.678833: step 2981, loss 0.196547, acc 0.921875, learning_rate 0.000100025
2017-10-10T13:10:49.204884: step 2982, loss 0.207083, acc 0.90625, learning_rate 0.000100025
2017-10-10T13:10:49.687288: step 2983, loss 0.366891, acc 0.859375, learning_rate 0.000100025
2017-10-10T13:10:50.220129: step 2984, loss 0.200284, acc 0.921875, learning_rate 0.000100025
2017-10-10T13:10:50.724858: step 2985, loss 0.189002, acc 0.921875, learning_rate 0.000100025
2017-10-10T13:10:51.236954: step 2986, loss 0.157297, acc 0.953125, learning_rate 0.000100024
2017-10-10T13:10:51.782219: step 2987, loss 0.274799, acc 0.890625, learning_rate 0.000100024
2017-10-10T13:10:52.352391: step 2988, loss 0.2239, acc 0.90625, learning_rate 0.000100024
2017-10-10T13:10:52.873194: step 2989, loss 0.390874, acc 0.875, learning_rate 0.000100024
2017-10-10T13:10:53.416764: step 2990, loss 0.317234, acc 0.890625, learning_rate 0.000100024
2017-10-10T13:10:53.948861: step 2991, loss 0.143125, acc 0.9375, learning_rate 0.000100024
2017-10-10T13:10:54.498151: step 2992, loss 0.228832, acc 0.890625, learning_rate 0.000100024
2017-10-10T13:10:54.997495: step 2993, loss 0.160997, acc 0.953125, learning_rate 0.000100024
2017-10-10T13:10:55.615563: step 2994, loss 0.218257, acc 0.921875, learning_rate 0.000100024
2017-10-10T13:10:56.076862: step 2995, loss 0.0911721, acc 0.984375, learning_rate 0.000100024
2017-10-10T13:10:56.488980: step 2996, loss 0.213606, acc 0.9375, learning_rate 0.000100023
2017-10-10T13:10:56.905219: step 2997, loss 0.0664861, acc 0.984375, learning_rate 0.000100023
2017-10-10T13:10:57.404572: step 2998, loss 0.198477, acc 0.90625, learning_rate 0.000100023
2017-10-10T13:10:57.885762: step 2999, loss 0.206644, acc 0.953125, learning_rate 0.000100023
2017-10-10T13:10:58.401609: step 3000, loss 0.208845, acc 0.921875, learning_rate 0.000100023

Evaluation:
2017-10-10T13:10:59.644917: step 3000, loss 0.236579, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3000

2017-10-10T13:11:01.204466: step 3001, loss 0.229845, acc 0.890625, learning_rate 0.000100023
2017-10-10T13:11:01.718466: step 3002, loss 0.192392, acc 0.921875, learning_rate 0.000100023
2017-10-10T13:11:02.121129: step 3003, loss 0.316715, acc 0.875, learning_rate 0.000100023
2017-10-10T13:11:02.574935: step 3004, loss 0.252362, acc 0.90625, learning_rate 0.000100023
2017-10-10T13:11:03.033637: step 3005, loss 0.223965, acc 0.90625, learning_rate 0.000100023
2017-10-10T13:11:03.516938: step 3006, loss 0.221173, acc 0.890625, learning_rate 0.000100023
2017-10-10T13:11:04.003249: step 3007, loss 0.319319, acc 0.890625, learning_rate 0.000100022
2017-10-10T13:11:04.632698: step 3008, loss 0.223115, acc 0.9375, learning_rate 0.000100022
2017-10-10T13:11:05.153054: step 3009, loss 0.24317, acc 0.9375, learning_rate 0.000100022
2017-10-10T13:11:05.698931: step 3010, loss 0.292426, acc 0.875, learning_rate 0.000100022
2017-10-10T13:11:06.251154: step 3011, loss 0.0974261, acc 0.984375, learning_rate 0.000100022
2017-10-10T13:11:06.818636: step 3012, loss 0.173562, acc 0.9375, learning_rate 0.000100022
2017-10-10T13:11:07.328773: step 3013, loss 0.161049, acc 0.953125, learning_rate 0.000100022
2017-10-10T13:11:07.813027: step 3014, loss 0.272157, acc 0.890625, learning_rate 0.000100022
2017-10-10T13:11:08.267194: step 3015, loss 0.241256, acc 0.9375, learning_rate 0.000100022
2017-10-10T13:11:08.745350: step 3016, loss 0.174256, acc 0.921875, learning_rate 0.000100022
2017-10-10T13:11:09.373125: step 3017, loss 0.32383, acc 0.90625, learning_rate 0.000100022
2017-10-10T13:11:09.857081: step 3018, loss 0.161371, acc 0.9375, learning_rate 0.000100021
2017-10-10T13:11:10.313138: step 3019, loss 0.189331, acc 0.921875, learning_rate 0.000100021
2017-10-10T13:11:10.728865: step 3020, loss 0.165378, acc 0.953125, learning_rate 0.000100021
2017-10-10T13:11:11.296963: step 3021, loss 0.18974, acc 0.953125, learning_rate 0.000100021
2017-10-10T13:11:11.831353: step 3022, loss 0.17764, acc 0.9375, learning_rate 0.000100021
2017-10-10T13:11:12.359479: step 3023, loss 0.353719, acc 0.890625, learning_rate 0.000100021
2017-10-10T13:11:12.877045: step 3024, loss 0.18034, acc 0.90625, learning_rate 0.000100021
2017-10-10T13:11:13.404602: step 3025, loss 0.353515, acc 0.890625, learning_rate 0.000100021
2017-10-10T13:11:13.912921: step 3026, loss 0.317796, acc 0.875, learning_rate 0.000100021
2017-10-10T13:11:14.409060: step 3027, loss 0.164539, acc 0.90625, learning_rate 0.000100021
2017-10-10T13:11:14.956970: step 3028, loss 0.285164, acc 0.9375, learning_rate 0.000100021
2017-10-10T13:11:15.536870: step 3029, loss 0.141991, acc 0.9375, learning_rate 0.00010002
2017-10-10T13:11:15.993019: step 3030, loss 0.199579, acc 0.90625, learning_rate 0.00010002
2017-10-10T13:11:16.420982: step 3031, loss 0.254853, acc 0.90625, learning_rate 0.00010002
2017-10-10T13:11:16.961128: step 3032, loss 0.219927, acc 0.953125, learning_rate 0.00010002
2017-10-10T13:11:17.500571: step 3033, loss 0.174266, acc 0.9375, learning_rate 0.00010002
2017-10-10T13:11:17.903624: step 3034, loss 0.354502, acc 0.890625, learning_rate 0.00010002
2017-10-10T13:11:18.564850: step 3035, loss 0.172435, acc 0.921875, learning_rate 0.00010002
2017-10-10T13:11:19.120913: step 3036, loss 0.29497, acc 0.890625, learning_rate 0.00010002
2017-10-10T13:11:19.476956: step 3037, loss 0.0593437, acc 0.984375, learning_rate 0.00010002
2017-10-10T13:11:19.799967: step 3038, loss 0.19887, acc 0.921569, learning_rate 0.00010002
2017-10-10T13:11:20.297011: step 3039, loss 0.161036, acc 0.9375, learning_rate 0.00010002
2017-10-10T13:11:20.897003: step 3040, loss 0.225419, acc 0.953125, learning_rate 0.00010002

Evaluation:
2017-10-10T13:11:21.983967: step 3040, loss 0.234351, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3040

2017-10-10T13:11:23.721046: step 3041, loss 0.256592, acc 0.9375, learning_rate 0.00010002
2017-10-10T13:11:24.254052: step 3042, loss 0.140977, acc 0.96875, learning_rate 0.000100019
2017-10-10T13:11:24.726507: step 3043, loss 0.164058, acc 0.9375, learning_rate 0.000100019
2017-10-10T13:11:25.281189: step 3044, loss 0.292932, acc 0.9375, learning_rate 0.000100019
2017-10-10T13:11:25.828995: step 3045, loss 0.123928, acc 0.953125, learning_rate 0.000100019
2017-10-10T13:11:26.312936: step 3046, loss 0.114417, acc 0.96875, learning_rate 0.000100019
2017-10-10T13:11:26.816996: step 3047, loss 0.172097, acc 0.953125, learning_rate 0.000100019
2017-10-10T13:11:27.348835: step 3048, loss 0.221048, acc 0.9375, learning_rate 0.000100019
2017-10-10T13:11:27.919717: step 3049, loss 0.495058, acc 0.90625, learning_rate 0.000100019
2017-10-10T13:11:28.380828: step 3050, loss 0.13336, acc 0.9375, learning_rate 0.000100019
2017-10-10T13:11:28.964311: step 3051, loss 0.182209, acc 0.9375, learning_rate 0.000100019
2017-10-10T13:11:29.492837: step 3052, loss 0.206829, acc 0.9375, learning_rate 0.000100019
2017-10-10T13:11:30.009277: step 3053, loss 0.0991187, acc 0.984375, learning_rate 0.000100019
2017-10-10T13:11:30.573766: step 3054, loss 0.238967, acc 0.921875, learning_rate 0.000100018
2017-10-10T13:11:31.096663: step 3055, loss 0.193268, acc 0.9375, learning_rate 0.000100018
2017-10-10T13:11:31.626195: step 3056, loss 0.133506, acc 0.9375, learning_rate 0.000100018
2017-10-10T13:11:32.142586: step 3057, loss 0.118343, acc 0.96875, learning_rate 0.000100018
2017-10-10T13:11:32.718927: step 3058, loss 0.187774, acc 0.90625, learning_rate 0.000100018
2017-10-10T13:11:33.175033: step 3059, loss 0.251407, acc 0.875, learning_rate 0.000100018
2017-10-10T13:11:33.612877: step 3060, loss 0.301067, acc 0.890625, learning_rate 0.000100018
2017-10-10T13:11:34.020578: step 3061, loss 0.170891, acc 0.953125, learning_rate 0.000100018
2017-10-10T13:11:34.596867: step 3062, loss 0.359788, acc 0.90625, learning_rate 0.000100018
2017-10-10T13:11:35.094464: step 3063, loss 0.201926, acc 0.921875, learning_rate 0.000100018
2017-10-10T13:11:35.636844: step 3064, loss 0.204631, acc 0.90625, learning_rate 0.000100018
2017-10-10T13:11:36.124017: step 3065, loss 0.200518, acc 0.953125, learning_rate 0.000100018
2017-10-10T13:11:36.684818: step 3066, loss 0.260703, acc 0.9375, learning_rate 0.000100018
2017-10-10T13:11:37.237076: step 3067, loss 0.271244, acc 0.90625, learning_rate 0.000100018
2017-10-10T13:11:37.787548: step 3068, loss 0.158441, acc 0.921875, learning_rate 0.000100017
2017-10-10T13:11:38.299398: step 3069, loss 0.288445, acc 0.859375, learning_rate 0.000100017
2017-10-10T13:11:38.808939: step 3070, loss 0.111075, acc 0.96875, learning_rate 0.000100017
2017-10-10T13:11:39.346384: step 3071, loss 0.217671, acc 0.921875, learning_rate 0.000100017
2017-10-10T13:11:39.879965: step 3072, loss 0.222037, acc 0.921875, learning_rate 0.000100017
2017-10-10T13:11:40.444899: step 3073, loss 0.176306, acc 0.9375, learning_rate 0.000100017
2017-10-10T13:11:40.954392: step 3074, loss 0.245786, acc 0.890625, learning_rate 0.000100017
2017-10-10T13:11:41.482434: step 3075, loss 0.181604, acc 0.9375, learning_rate 0.000100017
2017-10-10T13:11:42.030897: step 3076, loss 0.264612, acc 0.90625, learning_rate 0.000100017
2017-10-10T13:11:42.432891: step 3077, loss 0.233494, acc 0.90625, learning_rate 0.000100017
2017-10-10T13:11:42.861047: step 3078, loss 0.222099, acc 0.9375, learning_rate 0.000100017
2017-10-10T13:11:43.303965: step 3079, loss 0.234406, acc 0.953125, learning_rate 0.000100017
2017-10-10T13:11:43.857192: step 3080, loss 0.134584, acc 0.9375, learning_rate 0.000100017

Evaluation:
2017-10-10T13:11:45.064990: step 3080, loss 0.232571, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3080

2017-10-10T13:11:46.822852: step 3081, loss 0.251524, acc 0.90625, learning_rate 0.000100017
2017-10-10T13:11:47.264497: step 3082, loss 0.206377, acc 0.953125, learning_rate 0.000100016
2017-10-10T13:11:47.693013: step 3083, loss 0.194001, acc 0.9375, learning_rate 0.000100016
2017-10-10T13:11:48.161790: step 3084, loss 0.398527, acc 0.890625, learning_rate 0.000100016
2017-10-10T13:11:48.708941: step 3085, loss 0.292425, acc 0.890625, learning_rate 0.000100016
2017-10-10T13:11:49.252804: step 3086, loss 0.299875, acc 0.890625, learning_rate 0.000100016
2017-10-10T13:11:49.768400: step 3087, loss 0.168765, acc 0.921875, learning_rate 0.000100016
2017-10-10T13:11:50.317778: step 3088, loss 0.178613, acc 0.96875, learning_rate 0.000100016
2017-10-10T13:11:50.832513: step 3089, loss 0.315588, acc 0.875, learning_rate 0.000100016
2017-10-10T13:11:51.265009: step 3090, loss 0.139016, acc 0.953125, learning_rate 0.000100016
2017-10-10T13:11:51.872690: step 3091, loss 0.3297, acc 0.890625, learning_rate 0.000100016
2017-10-10T13:11:52.324958: step 3092, loss 0.29478, acc 0.921875, learning_rate 0.000100016
2017-10-10T13:11:52.777170: step 3093, loss 0.126912, acc 0.96875, learning_rate 0.000100016
2017-10-10T13:11:53.190203: step 3094, loss 0.217108, acc 0.921875, learning_rate 0.000100016
2017-10-10T13:11:53.739150: step 3095, loss 0.370196, acc 0.921875, learning_rate 0.000100016
2017-10-10T13:11:54.234074: step 3096, loss 0.204436, acc 0.921875, learning_rate 0.000100016
2017-10-10T13:11:54.742428: step 3097, loss 0.125122, acc 0.984375, learning_rate 0.000100016
2017-10-10T13:11:55.274468: step 3098, loss 0.158599, acc 0.9375, learning_rate 0.000100015
2017-10-10T13:11:55.859167: step 3099, loss 0.221232, acc 0.9375, learning_rate 0.000100015
2017-10-10T13:11:56.275652: step 3100, loss 0.354357, acc 0.84375, learning_rate 0.000100015
2017-10-10T13:11:56.751282: step 3101, loss 0.276123, acc 0.921875, learning_rate 0.000100015
2017-10-10T13:11:57.285921: step 3102, loss 0.151071, acc 0.953125, learning_rate 0.000100015
2017-10-10T13:11:57.884877: step 3103, loss 0.243362, acc 0.90625, learning_rate 0.000100015
2017-10-10T13:11:58.429518: step 3104, loss 0.133928, acc 0.953125, learning_rate 0.000100015
2017-10-10T13:11:59.019072: step 3105, loss 0.222683, acc 0.9375, learning_rate 0.000100015
2017-10-10T13:11:59.541064: step 3106, loss 0.229938, acc 0.921875, learning_rate 0.000100015
2017-10-10T13:12:00.140925: step 3107, loss 0.337507, acc 0.875, learning_rate 0.000100015
2017-10-10T13:12:00.713416: step 3108, loss 0.128161, acc 0.96875, learning_rate 0.000100015
2017-10-10T13:12:01.224867: step 3109, loss 0.139985, acc 0.984375, learning_rate 0.000100015
2017-10-10T13:12:01.713044: step 3110, loss 0.180447, acc 0.9375, learning_rate 0.000100015
2017-10-10T13:12:02.277032: step 3111, loss 0.268084, acc 0.890625, learning_rate 0.000100015
2017-10-10T13:12:02.797133: step 3112, loss 0.249224, acc 0.875, learning_rate 0.000100015
2017-10-10T13:12:03.348854: step 3113, loss 0.13822, acc 0.9375, learning_rate 0.000100015
2017-10-10T13:12:03.868586: step 3114, loss 0.160624, acc 0.9375, learning_rate 0.000100014
2017-10-10T13:12:04.360385: step 3115, loss 0.233102, acc 0.90625, learning_rate 0.000100014
2017-10-10T13:12:05.040885: step 3116, loss 0.263081, acc 0.921875, learning_rate 0.000100014
2017-10-10T13:12:05.517679: step 3117, loss 0.263798, acc 0.953125, learning_rate 0.000100014
2017-10-10T13:12:05.943499: step 3118, loss 0.163221, acc 0.90625, learning_rate 0.000100014
2017-10-10T13:12:06.423931: step 3119, loss 0.147276, acc 0.921875, learning_rate 0.000100014
2017-10-10T13:12:06.900940: step 3120, loss 0.261802, acc 0.875, learning_rate 0.000100014

Evaluation:
2017-10-10T13:12:08.121559: step 3120, loss 0.233437, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3120

2017-10-10T13:12:09.580048: step 3121, loss 0.12079, acc 0.953125, learning_rate 0.000100014
2017-10-10T13:12:10.124089: step 3122, loss 0.163004, acc 0.9375, learning_rate 0.000100014
2017-10-10T13:12:10.623460: step 3123, loss 0.251668, acc 0.90625, learning_rate 0.000100014
2017-10-10T13:12:11.083118: step 3124, loss 0.167842, acc 0.921875, learning_rate 0.000100014
2017-10-10T13:12:11.637544: step 3125, loss 0.250438, acc 0.953125, learning_rate 0.000100014
2017-10-10T13:12:12.151644: step 3126, loss 0.245509, acc 0.875, learning_rate 0.000100014
2017-10-10T13:12:12.584919: step 3127, loss 0.123039, acc 0.9375, learning_rate 0.000100014
2017-10-10T13:12:13.088965: step 3128, loss 0.156334, acc 0.953125, learning_rate 0.000100014
2017-10-10T13:12:13.661098: step 3129, loss 0.206613, acc 0.953125, learning_rate 0.000100014
2017-10-10T13:12:14.248940: step 3130, loss 0.158237, acc 0.921875, learning_rate 0.000100014
2017-10-10T13:12:14.846236: step 3131, loss 0.147537, acc 0.96875, learning_rate 0.000100014
2017-10-10T13:12:15.415978: step 3132, loss 0.201295, acc 0.9375, learning_rate 0.000100013
2017-10-10T13:12:15.965269: step 3133, loss 0.227034, acc 0.921875, learning_rate 0.000100013
2017-10-10T13:12:16.528868: step 3134, loss 0.173562, acc 0.953125, learning_rate 0.000100013
2017-10-10T13:12:17.044822: step 3135, loss 0.153691, acc 0.96875, learning_rate 0.000100013
2017-10-10T13:12:17.564237: step 3136, loss 0.215954, acc 0.921569, learning_rate 0.000100013
2017-10-10T13:12:18.094165: step 3137, loss 0.222313, acc 0.921875, learning_rate 0.000100013
2017-10-10T13:12:18.724100: step 3138, loss 0.127264, acc 0.96875, learning_rate 0.000100013
2017-10-10T13:12:19.285101: step 3139, loss 0.188979, acc 0.9375, learning_rate 0.000100013
2017-10-10T13:12:19.708932: step 3140, loss 0.227861, acc 0.921875, learning_rate 0.000100013
2017-10-10T13:12:20.130326: step 3141, loss 0.155758, acc 0.953125, learning_rate 0.000100013
2017-10-10T13:12:20.694313: step 3142, loss 0.217739, acc 0.921875, learning_rate 0.000100013
2017-10-10T13:12:21.204697: step 3143, loss 0.112021, acc 0.96875, learning_rate 0.000100013
2017-10-10T13:12:21.737269: step 3144, loss 0.192584, acc 0.90625, learning_rate 0.000100013
2017-10-10T13:12:22.226807: step 3145, loss 0.148638, acc 0.953125, learning_rate 0.000100013
2017-10-10T13:12:22.788919: step 3146, loss 0.123285, acc 0.9375, learning_rate 0.000100013
2017-10-10T13:12:23.374225: step 3147, loss 0.237039, acc 0.921875, learning_rate 0.000100013
2017-10-10T13:12:23.893729: step 3148, loss 0.156235, acc 0.953125, learning_rate 0.000100013
2017-10-10T13:12:24.457152: step 3149, loss 0.198759, acc 0.953125, learning_rate 0.000100013
2017-10-10T13:12:25.048900: step 3150, loss 0.272361, acc 0.921875, learning_rate 0.000100012
2017-10-10T13:12:25.629107: step 3151, loss 0.129014, acc 0.984375, learning_rate 0.000100012
2017-10-10T13:12:26.128993: step 3152, loss 0.219019, acc 0.921875, learning_rate 0.000100012
2017-10-10T13:12:26.617050: step 3153, loss 0.173096, acc 0.9375, learning_rate 0.000100012
2017-10-10T13:12:27.168012: step 3154, loss 0.331497, acc 0.890625, learning_rate 0.000100012
2017-10-10T13:12:27.776900: step 3155, loss 0.169232, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:12:28.341128: step 3156, loss 0.17254, acc 0.921875, learning_rate 0.000100012
2017-10-10T13:12:28.766550: step 3157, loss 0.143161, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:12:29.218452: step 3158, loss 0.187383, acc 0.9375, learning_rate 0.000100012
2017-10-10T13:12:29.710135: step 3159, loss 0.277388, acc 0.875, learning_rate 0.000100012
2017-10-10T13:12:30.241036: step 3160, loss 0.220998, acc 0.921875, learning_rate 0.000100012

Evaluation:
2017-10-10T13:12:31.493546: step 3160, loss 0.233266, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3160

2017-10-10T13:12:32.999476: step 3161, loss 0.280312, acc 0.875, learning_rate 0.000100012
2017-10-10T13:12:33.464811: step 3162, loss 0.36345, acc 0.890625, learning_rate 0.000100012
2017-10-10T13:12:34.052285: step 3163, loss 0.272993, acc 0.890625, learning_rate 0.000100012
2017-10-10T13:12:34.549204: step 3164, loss 0.152157, acc 0.9375, learning_rate 0.000100012
2017-10-10T13:12:35.082238: step 3165, loss 0.168835, acc 0.9375, learning_rate 0.000100012
2017-10-10T13:12:35.615418: step 3166, loss 0.226604, acc 0.890625, learning_rate 0.000100012
2017-10-10T13:12:36.165469: step 3167, loss 0.408196, acc 0.90625, learning_rate 0.000100012
2017-10-10T13:12:36.713249: step 3168, loss 0.233861, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:12:37.335456: step 3169, loss 0.273308, acc 0.921875, learning_rate 0.000100012
2017-10-10T13:12:37.817575: step 3170, loss 0.0819812, acc 0.984375, learning_rate 0.000100012
2017-10-10T13:12:38.324028: step 3171, loss 0.178454, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:12:38.873074: step 3172, loss 0.115059, acc 0.953125, learning_rate 0.000100011
2017-10-10T13:12:39.376844: step 3173, loss 0.178891, acc 0.921875, learning_rate 0.000100011
2017-10-10T13:12:39.902175: step 3174, loss 0.247637, acc 0.859375, learning_rate 0.000100011
2017-10-10T13:12:40.388905: step 3175, loss 0.121059, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:12:40.937035: step 3176, loss 0.278321, acc 0.890625, learning_rate 0.000100011
2017-10-10T13:12:41.428939: step 3177, loss 0.119461, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:12:42.010206: step 3178, loss 0.189614, acc 0.9375, learning_rate 0.000100011
2017-10-10T13:12:42.592905: step 3179, loss 0.226006, acc 0.9375, learning_rate 0.000100011
2017-10-10T13:12:43.153656: step 3180, loss 0.257247, acc 0.875, learning_rate 0.000100011
2017-10-10T13:12:43.612248: step 3181, loss 0.150958, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:12:44.064438: step 3182, loss 0.278848, acc 0.90625, learning_rate 0.000100011
2017-10-10T13:12:44.622381: step 3183, loss 0.0980964, acc 0.96875, learning_rate 0.000100011
2017-10-10T13:12:45.151674: step 3184, loss 0.283565, acc 0.90625, learning_rate 0.000100011
2017-10-10T13:12:45.714088: step 3185, loss 0.194778, acc 0.921875, learning_rate 0.000100011
2017-10-10T13:12:46.278794: step 3186, loss 0.142623, acc 0.9375, learning_rate 0.000100011
2017-10-10T13:12:46.811976: step 3187, loss 0.268241, acc 0.921875, learning_rate 0.000100011
2017-10-10T13:12:47.353179: step 3188, loss 0.195597, acc 0.921875, learning_rate 0.000100011
2017-10-10T13:12:47.892020: step 3189, loss 0.147384, acc 0.953125, learning_rate 0.000100011
2017-10-10T13:12:48.447198: step 3190, loss 0.329381, acc 0.875, learning_rate 0.000100011
2017-10-10T13:12:48.985947: step 3191, loss 0.273797, acc 0.921875, learning_rate 0.000100011
2017-10-10T13:12:49.534025: step 3192, loss 0.28364, acc 0.890625, learning_rate 0.000100011
2017-10-10T13:12:50.060916: step 3193, loss 0.156935, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:12:50.601169: step 3194, loss 0.250711, acc 0.921875, learning_rate 0.00010001
2017-10-10T13:12:51.140929: step 3195, loss 0.288346, acc 0.890625, learning_rate 0.00010001
2017-10-10T13:12:51.664855: step 3196, loss 0.272026, acc 0.890625, learning_rate 0.00010001
2017-10-10T13:12:52.101854: step 3197, loss 0.193289, acc 0.921875, learning_rate 0.00010001
2017-10-10T13:12:52.516866: step 3198, loss 0.239057, acc 0.859375, learning_rate 0.00010001
2017-10-10T13:12:52.981015: step 3199, loss 0.11687, acc 1, learning_rate 0.00010001
2017-10-10T13:12:53.466787: step 3200, loss 0.0909548, acc 0.953125, learning_rate 0.00010001

Evaluation:
2017-10-10T13:12:54.704996: step 3200, loss 0.234274, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3200

2017-10-10T13:12:56.335588: step 3201, loss 0.517302, acc 0.859375, learning_rate 0.00010001
2017-10-10T13:12:56.760826: step 3202, loss 0.176817, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:12:57.237285: step 3203, loss 0.153914, acc 0.96875, learning_rate 0.00010001
2017-10-10T13:12:57.818818: step 3204, loss 0.26924, acc 0.90625, learning_rate 0.00010001
2017-10-10T13:12:58.367620: step 3205, loss 0.168884, acc 0.9375, learning_rate 0.00010001
2017-10-10T13:12:58.913595: step 3206, loss 0.268742, acc 0.890625, learning_rate 0.00010001
2017-10-10T13:12:59.475930: step 3207, loss 0.286364, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:12:59.997459: step 3208, loss 0.171326, acc 0.9375, learning_rate 0.00010001
2017-10-10T13:13:00.482838: step 3209, loss 0.292146, acc 0.875, learning_rate 0.00010001
2017-10-10T13:13:00.964126: step 3210, loss 0.155965, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:13:01.480220: step 3211, loss 0.372813, acc 0.921875, learning_rate 0.00010001
2017-10-10T13:13:02.067873: step 3212, loss 0.242508, acc 0.890625, learning_rate 0.00010001
2017-10-10T13:13:02.631067: step 3213, loss 0.165686, acc 0.953125, learning_rate 0.00010001
2017-10-10T13:13:03.186280: step 3214, loss 0.274224, acc 0.875, learning_rate 0.00010001
2017-10-10T13:13:03.743371: step 3215, loss 0.130563, acc 0.9375, learning_rate 0.00010001
2017-10-10T13:13:04.295858: step 3216, loss 0.132935, acc 0.96875, learning_rate 0.00010001
2017-10-10T13:13:04.856863: step 3217, loss 0.32777, acc 0.84375, learning_rate 0.000100009
2017-10-10T13:13:05.395842: step 3218, loss 0.29115, acc 0.921875, learning_rate 0.000100009
2017-10-10T13:13:05.953331: step 3219, loss 0.180422, acc 0.9375, learning_rate 0.000100009
2017-10-10T13:13:06.577753: step 3220, loss 0.1716, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:13:07.018595: step 3221, loss 0.324196, acc 0.90625, learning_rate 0.000100009
2017-10-10T13:13:07.438759: step 3222, loss 0.18421, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:13:07.964922: step 3223, loss 0.154797, acc 0.9375, learning_rate 0.000100009
2017-10-10T13:13:08.525092: step 3224, loss 0.13716, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:13:09.117070: step 3225, loss 0.259479, acc 0.921875, learning_rate 0.000100009
2017-10-10T13:13:09.617063: step 3226, loss 0.0641597, acc 0.984375, learning_rate 0.000100009
2017-10-10T13:13:10.196961: step 3227, loss 0.258857, acc 0.90625, learning_rate 0.000100009
2017-10-10T13:13:10.720481: step 3228, loss 0.159752, acc 0.9375, learning_rate 0.000100009
2017-10-10T13:13:11.264848: step 3229, loss 0.185613, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:13:11.796024: step 3230, loss 0.403753, acc 0.875, learning_rate 0.000100009
2017-10-10T13:13:12.315814: step 3231, loss 0.142957, acc 0.9375, learning_rate 0.000100009
2017-10-10T13:13:12.839830: step 3232, loss 0.297256, acc 0.9375, learning_rate 0.000100009
2017-10-10T13:13:13.372104: step 3233, loss 0.135556, acc 0.96875, learning_rate 0.000100009
2017-10-10T13:13:13.788898: step 3234, loss 0.261927, acc 0.901961, learning_rate 0.000100009
2017-10-10T13:13:14.432038: step 3235, loss 0.168472, acc 0.921875, learning_rate 0.000100009
2017-10-10T13:13:14.878450: step 3236, loss 0.233036, acc 0.90625, learning_rate 0.000100009
2017-10-10T13:13:15.360824: step 3237, loss 0.327577, acc 0.90625, learning_rate 0.000100009
2017-10-10T13:13:15.887890: step 3238, loss 0.225753, acc 0.921875, learning_rate 0.000100009
2017-10-10T13:13:16.437611: step 3239, loss 0.211239, acc 0.9375, learning_rate 0.000100009
2017-10-10T13:13:16.969884: step 3240, loss 0.216639, acc 0.9375, learning_rate 0.000100009

Evaluation:
2017-10-10T13:13:18.496446: step 3240, loss 0.233294, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3240

2017-10-10T13:13:19.828462: step 3241, loss 0.260511, acc 0.921875, learning_rate 0.000100009
2017-10-10T13:13:20.289067: step 3242, loss 0.161497, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:13:20.856915: step 3243, loss 0.175342, acc 0.90625, learning_rate 0.000100009
2017-10-10T13:13:21.445091: step 3244, loss 0.164621, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:13:21.921346: step 3245, loss 0.314576, acc 0.890625, learning_rate 0.000100008
2017-10-10T13:13:22.348856: step 3246, loss 0.150919, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:13:22.888873: step 3247, loss 0.182687, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:13:23.406538: step 3248, loss 0.269785, acc 0.890625, learning_rate 0.000100008
2017-10-10T13:13:23.920926: step 3249, loss 0.261413, acc 0.9375, learning_rate 0.000100008
2017-10-10T13:13:24.388919: step 3250, loss 0.0969623, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:13:24.861142: step 3251, loss 0.208841, acc 0.921875, learning_rate 0.000100008
2017-10-10T13:13:25.447492: step 3252, loss 0.235736, acc 0.890625, learning_rate 0.000100008
2017-10-10T13:13:25.980879: step 3253, loss 0.227978, acc 0.90625, learning_rate 0.000100008
2017-10-10T13:13:26.449757: step 3254, loss 0.426133, acc 0.84375, learning_rate 0.000100008
2017-10-10T13:13:26.948803: step 3255, loss 0.194579, acc 0.9375, learning_rate 0.000100008
2017-10-10T13:13:27.464896: step 3256, loss 0.134538, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:13:28.012855: step 3257, loss 0.209167, acc 0.9375, learning_rate 0.000100008
2017-10-10T13:13:28.668868: step 3258, loss 0.231524, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:13:29.211662: step 3259, loss 0.425608, acc 0.859375, learning_rate 0.000100008
2017-10-10T13:13:29.648064: step 3260, loss 0.194341, acc 0.921875, learning_rate 0.000100008
2017-10-10T13:13:30.058713: step 3261, loss 0.222328, acc 0.90625, learning_rate 0.000100008
2017-10-10T13:13:30.524900: step 3262, loss 0.355368, acc 0.875, learning_rate 0.000100008
2017-10-10T13:13:31.080930: step 3263, loss 0.156259, acc 0.984375, learning_rate 0.000100008
2017-10-10T13:13:31.601026: step 3264, loss 0.209994, acc 0.9375, learning_rate 0.000100008
2017-10-10T13:13:32.017119: step 3265, loss 0.30956, acc 0.90625, learning_rate 0.000100008
2017-10-10T13:13:32.566651: step 3266, loss 0.0998744, acc 0.96875, learning_rate 0.000100008
2017-10-10T13:13:33.076931: step 3267, loss 0.257897, acc 0.921875, learning_rate 0.000100008
2017-10-10T13:13:33.608852: step 3268, loss 0.119897, acc 0.9375, learning_rate 0.000100008
2017-10-10T13:13:34.174782: step 3269, loss 0.245486, acc 0.921875, learning_rate 0.000100008
2017-10-10T13:13:34.640714: step 3270, loss 0.273243, acc 0.921875, learning_rate 0.000100008
2017-10-10T13:13:35.149463: step 3271, loss 0.153417, acc 0.9375, learning_rate 0.000100008
2017-10-10T13:13:35.684863: step 3272, loss 0.162802, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:13:36.201010: step 3273, loss 0.157997, acc 0.921875, learning_rate 0.000100008
2017-10-10T13:13:36.797082: step 3274, loss 0.181651, acc 0.90625, learning_rate 0.000100008
2017-10-10T13:13:37.321024: step 3275, loss 0.212401, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:13:37.752839: step 3276, loss 0.182661, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:13:38.172889: step 3277, loss 0.215251, acc 0.890625, learning_rate 0.000100007
2017-10-10T13:13:38.755331: step 3278, loss 0.188967, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:13:39.293228: step 3279, loss 0.294184, acc 0.875, learning_rate 0.000100007
2017-10-10T13:13:39.804197: step 3280, loss 0.131877, acc 0.96875, learning_rate 0.000100007

Evaluation:
2017-10-10T13:13:41.048493: step 3280, loss 0.23372, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3280

2017-10-10T13:13:42.489205: step 3281, loss 0.140619, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:13:42.914542: step 3282, loss 0.171821, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:13:43.404923: step 3283, loss 0.211474, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:13:43.916985: step 3284, loss 0.137729, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:13:44.503084: step 3285, loss 0.153561, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:13:44.984877: step 3286, loss 0.125347, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:13:45.528938: step 3287, loss 0.0997905, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:13:46.072835: step 3288, loss 0.163055, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:13:46.528852: step 3289, loss 0.230118, acc 0.90625, learning_rate 0.000100007
2017-10-10T13:13:47.029022: step 3290, loss 0.235701, acc 0.890625, learning_rate 0.000100007
2017-10-10T13:13:47.616971: step 3291, loss 0.141019, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:13:48.156834: step 3292, loss 0.355204, acc 0.859375, learning_rate 0.000100007
2017-10-10T13:13:48.609863: step 3293, loss 0.214464, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:13:49.153013: step 3294, loss 0.203865, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:13:49.656278: step 3295, loss 0.217317, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:13:50.180259: step 3296, loss 0.0940135, acc 0.96875, learning_rate 0.000100007
2017-10-10T13:13:50.693090: step 3297, loss 0.320583, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:13:51.265007: step 3298, loss 0.24766, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:13:51.880921: step 3299, loss 0.228407, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:13:52.421874: step 3300, loss 0.203436, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:13:52.908856: step 3301, loss 0.140358, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:13:53.358096: step 3302, loss 0.390617, acc 0.859375, learning_rate 0.000100007
2017-10-10T13:13:53.866599: step 3303, loss 0.174386, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:13:54.423419: step 3304, loss 0.176066, acc 0.953125, learning_rate 0.000100007
2017-10-10T13:13:54.924859: step 3305, loss 0.246179, acc 0.875, learning_rate 0.000100007
2017-10-10T13:13:55.458692: step 3306, loss 0.29675, acc 0.90625, learning_rate 0.000100007
2017-10-10T13:13:55.952273: step 3307, loss 0.066177, acc 1, learning_rate 0.000100007
2017-10-10T13:13:56.504838: step 3308, loss 0.233629, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:13:57.036850: step 3309, loss 0.336528, acc 0.90625, learning_rate 0.000100007
2017-10-10T13:13:57.560917: step 3310, loss 0.208476, acc 0.90625, learning_rate 0.000100006
2017-10-10T13:13:58.044992: step 3311, loss 0.204541, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:13:58.513029: step 3312, loss 0.139846, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:13:59.037240: step 3313, loss 0.294092, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:13:59.656955: step 3314, loss 0.275935, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:14:00.196840: step 3315, loss 0.33918, acc 0.90625, learning_rate 0.000100006
2017-10-10T13:14:00.628817: step 3316, loss 0.252995, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:14:01.081850: step 3317, loss 0.116189, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:14:01.474499: step 3318, loss 0.28451, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:14:02.051950: step 3319, loss 0.121316, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:14:02.492890: step 3320, loss 0.286772, acc 0.890625, learning_rate 0.000100006

Evaluation:
2017-10-10T13:14:03.766984: step 3320, loss 0.232089, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3320

2017-10-10T13:14:05.457007: step 3321, loss 0.281009, acc 0.90625, learning_rate 0.000100006
2017-10-10T13:14:06.020924: step 3322, loss 0.139732, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:14:06.588830: step 3323, loss 0.13163, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:14:07.156972: step 3324, loss 0.308872, acc 0.90625, learning_rate 0.000100006
2017-10-10T13:14:07.724880: step 3325, loss 0.222234, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:14:08.300693: step 3326, loss 0.129619, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:14:08.800911: step 3327, loss 0.204443, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:14:09.343746: step 3328, loss 0.12078, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:14:09.840403: step 3329, loss 0.25035, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:14:10.329039: step 3330, loss 0.355337, acc 0.875, learning_rate 0.000100006
2017-10-10T13:14:10.796900: step 3331, loss 0.294109, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:14:11.231215: step 3332, loss 0.272362, acc 0.882353, learning_rate 0.000100006
2017-10-10T13:14:11.786317: step 3333, loss 0.253079, acc 0.875, learning_rate 0.000100006
2017-10-10T13:14:12.241297: step 3334, loss 0.23942, acc 0.90625, learning_rate 0.000100006
2017-10-10T13:14:12.832277: step 3335, loss 0.268142, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:14:13.355495: step 3336, loss 0.182263, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:14:13.886038: step 3337, loss 0.174634, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:14:14.452896: step 3338, loss 0.269412, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:14:15.028859: step 3339, loss 0.15207, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:14:15.533301: step 3340, loss 0.215797, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:14:15.990403: step 3341, loss 0.113102, acc 0.984375, learning_rate 0.000100006
2017-10-10T13:14:16.443394: step 3342, loss 0.16696, acc 0.953125, learning_rate 0.000100006
2017-10-10T13:14:16.913354: step 3343, loss 0.196228, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:14:17.402149: step 3344, loss 0.199185, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:14:17.928814: step 3345, loss 0.170862, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:14:18.487792: step 3346, loss 0.34499, acc 0.859375, learning_rate 0.000100006
2017-10-10T13:14:19.036846: step 3347, loss 0.313456, acc 0.875, learning_rate 0.000100006
2017-10-10T13:14:19.572198: step 3348, loss 0.200303, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:14:20.073075: step 3349, loss 0.18024, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:14:20.603028: step 3350, loss 0.105821, acc 1, learning_rate 0.000100006
2017-10-10T13:14:21.135066: step 3351, loss 0.255228, acc 0.90625, learning_rate 0.000100005
2017-10-10T13:14:21.676920: step 3352, loss 0.135517, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:14:22.268901: step 3353, loss 0.172777, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:14:22.912740: step 3354, loss 0.149852, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:14:23.292938: step 3355, loss 0.143918, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:14:23.704899: step 3356, loss 0.245833, acc 0.875, learning_rate 0.000100005
2017-10-10T13:14:24.257899: step 3357, loss 0.124799, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:14:24.679036: step 3358, loss 0.289063, acc 0.90625, learning_rate 0.000100005
2017-10-10T13:14:25.168853: step 3359, loss 0.125036, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:14:25.656441: step 3360, loss 0.217489, acc 0.890625, learning_rate 0.000100005

Evaluation:
2017-10-10T13:14:26.996984: step 3360, loss 0.231534, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3360

2017-10-10T13:14:28.288891: step 3361, loss 0.299847, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:14:28.777512: step 3362, loss 0.143147, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:14:29.301640: step 3363, loss 0.189809, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:14:29.795580: step 3364, loss 0.280305, acc 0.890625, learning_rate 0.000100005
2017-10-10T13:14:30.305050: step 3365, loss 0.269851, acc 0.875, learning_rate 0.000100005
2017-10-10T13:14:30.800988: step 3366, loss 0.20328, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:14:31.262508: step 3367, loss 0.171998, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:14:31.836866: step 3368, loss 0.230893, acc 0.90625, learning_rate 0.000100005
2017-10-10T13:14:32.368568: step 3369, loss 0.121103, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:14:32.989277: step 3370, loss 0.0717934, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:14:33.468928: step 3371, loss 0.196938, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:14:33.976886: step 3372, loss 0.16615, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:14:34.431964: step 3373, loss 0.269621, acc 0.90625, learning_rate 0.000100005
2017-10-10T13:14:34.944749: step 3374, loss 0.188901, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:14:35.392712: step 3375, loss 0.211, acc 0.890625, learning_rate 0.000100005
2017-10-10T13:14:35.916983: step 3376, loss 0.293237, acc 0.90625, learning_rate 0.000100005
2017-10-10T13:14:36.437062: step 3377, loss 0.0958599, acc 0.984375, learning_rate 0.000100005
2017-10-10T13:14:36.933760: step 3378, loss 0.305142, acc 0.859375, learning_rate 0.000100005
2017-10-10T13:14:37.584864: step 3379, loss 0.183139, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:14:38.164978: step 3380, loss 0.211425, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:14:38.561170: step 3381, loss 0.314625, acc 0.890625, learning_rate 0.000100005
2017-10-10T13:14:38.937021: step 3382, loss 0.218447, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:14:39.352883: step 3383, loss 0.232757, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:14:39.893187: step 3384, loss 0.231745, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:14:40.436481: step 3385, loss 0.139707, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:14:40.949042: step 3386, loss 0.233749, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:14:41.446284: step 3387, loss 0.0967613, acc 0.96875, learning_rate 0.000100005
2017-10-10T13:14:41.953169: step 3388, loss 0.156784, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:14:42.509123: step 3389, loss 0.335187, acc 0.875, learning_rate 0.000100005
2017-10-10T13:14:43.053610: step 3390, loss 0.162417, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:14:43.616824: step 3391, loss 0.254448, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:14:44.154564: step 3392, loss 0.267473, acc 0.90625, learning_rate 0.000100005
2017-10-10T13:14:44.682695: step 3393, loss 0.324306, acc 0.90625, learning_rate 0.000100005
2017-10-10T13:14:45.252982: step 3394, loss 0.239675, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:14:45.864974: step 3395, loss 0.154537, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:14:46.319241: step 3396, loss 0.16821, acc 0.953125, learning_rate 0.000100005
2017-10-10T13:14:46.804877: step 3397, loss 0.300856, acc 0.90625, learning_rate 0.000100005
2017-10-10T13:14:47.375926: step 3398, loss 0.171994, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:14:47.876028: step 3399, loss 0.188239, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:14:48.337556: step 3400, loss 0.222877, acc 0.890625, learning_rate 0.000100004

Evaluation:
2017-10-10T13:14:49.521071: step 3400, loss 0.233615, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3400

2017-10-10T13:14:50.985121: step 3401, loss 0.215204, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:14:51.392936: step 3402, loss 0.360735, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:14:51.812927: step 3403, loss 0.209795, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:14:52.324840: step 3404, loss 0.146638, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:14:52.859480: step 3405, loss 0.239516, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:14:53.406500: step 3406, loss 0.278126, acc 0.875, learning_rate 0.000100004
2017-10-10T13:14:53.956559: step 3407, loss 0.196594, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:14:54.474248: step 3408, loss 0.147488, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:14:54.980900: step 3409, loss 0.232547, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:14:55.517088: step 3410, loss 0.359961, acc 0.84375, learning_rate 0.000100004
2017-10-10T13:14:55.977243: step 3411, loss 0.166972, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:14:56.433766: step 3412, loss 0.201824, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:14:56.965045: step 3413, loss 0.109224, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:14:57.463612: step 3414, loss 0.136277, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:14:57.922063: step 3415, loss 0.104222, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:14:58.501186: step 3416, loss 0.270482, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:14:58.948676: step 3417, loss 0.228643, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:14:59.416949: step 3418, loss 0.292164, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:14:59.907984: step 3419, loss 0.252629, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:15:00.404421: step 3420, loss 0.224119, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:15:00.980859: step 3421, loss 0.195931, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:15:01.565025: step 3422, loss 0.177194, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:15:02.026715: step 3423, loss 0.412984, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:15:02.463331: step 3424, loss 0.142247, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:15:02.977748: step 3425, loss 0.253189, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:15:03.557071: step 3426, loss 0.270131, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:15:04.045081: step 3427, loss 0.264646, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:15:04.541138: step 3428, loss 0.229699, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:15:05.041122: step 3429, loss 0.173245, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:15:05.504961: step 3430, loss 0.137816, acc 0.941176, learning_rate 0.000100004
2017-10-10T13:15:05.996826: step 3431, loss 0.157551, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:15:06.560349: step 3432, loss 0.136839, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:15:07.108939: step 3433, loss 0.11078, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:15:07.604948: step 3434, loss 0.167025, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:15:08.149344: step 3435, loss 0.241935, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:15:08.659769: step 3436, loss 0.191755, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:15:09.200677: step 3437, loss 0.191608, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:15:09.702850: step 3438, loss 0.357651, acc 0.859375, learning_rate 0.000100004
2017-10-10T13:15:10.184658: step 3439, loss 0.149935, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:15:10.602458: step 3440, loss 0.186691, acc 0.953125, learning_rate 0.000100004

Evaluation:
2017-10-10T13:15:11.985114: step 3440, loss 0.232624, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3440

2017-10-10T13:15:13.608092: step 3441, loss 0.298472, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:15:14.085202: step 3442, loss 0.283654, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:15:14.525040: step 3443, loss 0.333427, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:15:15.109276: step 3444, loss 0.198967, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:15:15.668801: step 3445, loss 0.138425, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:15:16.182842: step 3446, loss 0.162411, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:15:16.692923: step 3447, loss 0.188636, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:15:17.200876: step 3448, loss 0.28161, acc 0.875, learning_rate 0.000100004
2017-10-10T13:15:17.647070: step 3449, loss 0.0893845, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:15:18.052909: step 3450, loss 0.28009, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:15:18.610051: step 3451, loss 0.0498945, acc 1, learning_rate 0.000100004
2017-10-10T13:15:19.096980: step 3452, loss 0.279847, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:15:19.623206: step 3453, loss 0.168476, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:15:20.136886: step 3454, loss 0.196279, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:15:20.677005: step 3455, loss 0.17358, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:15:21.151793: step 3456, loss 0.173728, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:15:21.681237: step 3457, loss 0.165532, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:15:22.303361: step 3458, loss 0.278025, acc 0.875, learning_rate 0.000100004
2017-10-10T13:15:22.774274: step 3459, loss 0.0875997, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:15:23.338587: step 3460, loss 0.235854, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:15:23.856886: step 3461, loss 0.209532, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:15:24.337047: step 3462, loss 0.222591, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:15:24.836896: step 3463, loss 0.240685, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:15:25.435684: step 3464, loss 0.283387, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:15:25.886403: step 3465, loss 0.322027, acc 0.84375, learning_rate 0.000100003
2017-10-10T13:15:26.308943: step 3466, loss 0.240541, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:15:26.736790: step 3467, loss 0.399962, acc 0.875, learning_rate 0.000100003
2017-10-10T13:15:27.176896: step 3468, loss 0.194413, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:15:27.735043: step 3469, loss 0.137327, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:15:28.228992: step 3470, loss 0.206521, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:15:28.801273: step 3471, loss 0.159808, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:15:29.374457: step 3472, loss 0.125318, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:15:29.847981: step 3473, loss 0.214941, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:15:30.375421: step 3474, loss 0.277121, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:15:31.029306: step 3475, loss 0.12673, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:15:31.584853: step 3476, loss 0.148144, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:15:32.153108: step 3477, loss 0.195753, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:15:32.724868: step 3478, loss 0.145563, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:15:33.217380: step 3479, loss 0.271602, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:15:33.681126: step 3480, loss 0.3229, acc 0.875, learning_rate 0.000100003

Evaluation:
2017-10-10T13:15:34.998484: step 3480, loss 0.230509, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3480

2017-10-10T13:15:36.655346: step 3481, loss 0.27755, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:15:37.433492: step 3482, loss 0.265048, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:15:37.876759: step 3483, loss 0.225628, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:15:38.364969: step 3484, loss 0.194448, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:15:38.876844: step 3485, loss 0.154608, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:15:39.453439: step 3486, loss 0.119069, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:15:40.002969: step 3487, loss 0.11904, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:15:40.548896: step 3488, loss 0.158622, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:15:41.096893: step 3489, loss 0.0649928, acc 1, learning_rate 0.000100003
2017-10-10T13:15:41.537866: step 3490, loss 0.163186, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:15:42.058794: step 3491, loss 0.172176, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:15:42.584892: step 3492, loss 0.132357, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:15:43.065096: step 3493, loss 0.192063, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:15:43.621206: step 3494, loss 0.21753, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:15:44.168834: step 3495, loss 0.341193, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:15:44.716629: step 3496, loss 0.13899, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:15:45.245201: step 3497, loss 0.18712, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:15:45.799340: step 3498, loss 0.299905, acc 0.875, learning_rate 0.000100003
2017-10-10T13:15:46.364819: step 3499, loss 0.257272, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:15:46.938053: step 3500, loss 0.282972, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:15:47.501246: step 3501, loss 0.32706, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:15:48.137919: step 3502, loss 0.302523, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:15:48.620073: step 3503, loss 0.211519, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:15:49.056847: step 3504, loss 0.16412, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:15:49.539594: step 3505, loss 0.126805, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:15:50.116865: step 3506, loss 0.228324, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:15:50.712854: step 3507, loss 0.143958, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:15:51.227439: step 3508, loss 0.145611, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:15:51.793241: step 3509, loss 0.307022, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:15:52.316874: step 3510, loss 0.211924, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:15:52.844864: step 3511, loss 0.336235, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:15:53.408505: step 3512, loss 0.271406, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:15:53.961739: step 3513, loss 0.262717, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:15:54.555840: step 3514, loss 0.37494, acc 0.875, learning_rate 0.000100003
2017-10-10T13:15:55.137107: step 3515, loss 0.265608, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:15:55.522857: step 3516, loss 0.126812, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:15:55.896977: step 3517, loss 0.119123, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:15:56.388881: step 3518, loss 0.10062, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:15:56.981322: step 3519, loss 0.267429, acc 0.859375, learning_rate 0.000100003
2017-10-10T13:15:57.507480: step 3520, loss 0.292166, acc 0.90625, learning_rate 0.000100003

Evaluation:
2017-10-10T13:15:58.687408: step 3520, loss 0.231046, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3520

2017-10-10T13:16:00.084865: step 3521, loss 0.270046, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:16:00.718325: step 3522, loss 0.318031, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:16:01.273032: step 3523, loss 0.18833, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:16:01.727522: step 3524, loss 0.19926, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:16:02.296879: step 3525, loss 0.23899, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:16:02.857082: step 3526, loss 0.121222, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:16:03.410604: step 3527, loss 0.121691, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:16:03.858689: step 3528, loss 0.0646604, acc 0.980392, learning_rate 0.000100003
2017-10-10T13:16:04.416878: step 3529, loss 0.280059, acc 0.875, learning_rate 0.000100003
2017-10-10T13:16:04.951394: step 3530, loss 0.150461, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:16:05.485572: step 3531, loss 0.34581, acc 0.875, learning_rate 0.000100003
2017-10-10T13:16:06.039344: step 3532, loss 0.102388, acc 0.984375, learning_rate 0.000100003
2017-10-10T13:16:06.600466: step 3533, loss 0.225639, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:16:07.154716: step 3534, loss 0.212713, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:16:07.660287: step 3535, loss 0.165963, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:16:08.196113: step 3536, loss 0.134634, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:16:08.740648: step 3537, loss 0.179771, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:16:09.315468: step 3538, loss 0.310188, acc 0.859375, learning_rate 0.000100003
2017-10-10T13:16:09.850196: step 3539, loss 0.283591, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:16:10.417027: step 3540, loss 0.258505, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:16:11.044967: step 3541, loss 0.287166, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:16:11.645009: step 3542, loss 0.273071, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:16:12.092797: step 3543, loss 0.224477, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:16:12.506518: step 3544, loss 0.286664, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:12.984184: step 3545, loss 0.222716, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:13.569171: step 3546, loss 0.420033, acc 0.84375, learning_rate 0.000100002
2017-10-10T13:16:14.133857: step 3547, loss 0.181308, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:16:14.549014: step 3548, loss 0.123133, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:15.085045: step 3549, loss 0.142891, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:15.717102: step 3550, loss 0.152317, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:16.241137: step 3551, loss 0.165795, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:16.815136: step 3552, loss 0.133901, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:17.353196: step 3553, loss 0.235281, acc 0.875, learning_rate 0.000100002
2017-10-10T13:16:17.929043: step 3554, loss 0.0914886, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:16:18.428809: step 3555, loss 0.127991, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:16:18.895050: step 3556, loss 0.137909, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:19.350426: step 3557, loss 0.32592, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:16:19.895134: step 3558, loss 0.145816, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:20.452436: step 3559, loss 0.165316, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:20.956253: step 3560, loss 0.176758, acc 0.9375, learning_rate 0.000100002

Evaluation:
2017-10-10T13:16:22.156814: step 3560, loss 0.231091, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3560

2017-10-10T13:16:23.802824: step 3561, loss 0.1191, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:16:24.268950: step 3562, loss 0.229766, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:24.701697: step 3563, loss 0.217974, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:16:25.275804: step 3564, loss 0.106885, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:25.846430: step 3565, loss 0.175563, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:26.436856: step 3566, loss 0.105707, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:16:26.944888: step 3567, loss 0.180765, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:16:27.517432: step 3568, loss 0.264615, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:28.028856: step 3569, loss 0.323033, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:28.594216: step 3570, loss 0.235044, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:29.112939: step 3571, loss 0.260947, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:29.588901: step 3572, loss 0.161779, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:30.148853: step 3573, loss 0.236078, acc 0.875, learning_rate 0.000100002
2017-10-10T13:16:30.672942: step 3574, loss 0.238179, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:16:31.197400: step 3575, loss 0.101618, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:16:31.696836: step 3576, loss 0.176053, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:32.260011: step 3577, loss 0.151762, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:32.788371: step 3578, loss 0.229241, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:16:33.341158: step 3579, loss 0.200724, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:33.971829: step 3580, loss 0.116391, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:16:34.548180: step 3581, loss 0.263219, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:35.080020: step 3582, loss 0.12542, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:16:35.559454: step 3583, loss 0.197341, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:36.022214: step 3584, loss 0.231696, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:16:36.560847: step 3585, loss 0.138032, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:37.103485: step 3586, loss 0.253848, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:37.693044: step 3587, loss 0.109578, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:16:38.232859: step 3588, loss 0.23971, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:16:38.781225: step 3589, loss 0.163358, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:39.321808: step 3590, loss 0.250644, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:16:39.864877: step 3591, loss 0.169751, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:40.424548: step 3592, loss 0.133389, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:16:40.936843: step 3593, loss 0.227563, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:41.484872: step 3594, loss 0.146728, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:42.080824: step 3595, loss 0.207764, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:42.591696: step 3596, loss 0.161148, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:16:43.059217: step 3597, loss 0.266095, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:16:43.527837: step 3598, loss 0.083591, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:16:44.053115: step 3599, loss 0.196519, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:44.564880: step 3600, loss 0.195544, acc 0.953125, learning_rate 0.000100002

Evaluation:
2017-10-10T13:16:45.779375: step 3600, loss 0.229912, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3600

2017-10-10T13:16:47.548528: step 3601, loss 0.216278, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:48.080686: step 3602, loss 0.153572, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:48.518423: step 3603, loss 0.19696, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:16:49.053007: step 3604, loss 0.131155, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:16:49.504849: step 3605, loss 0.197259, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:49.997139: step 3606, loss 0.151807, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:16:50.452402: step 3607, loss 0.349676, acc 0.875, learning_rate 0.000100002
2017-10-10T13:16:50.984978: step 3608, loss 0.213695, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:51.499353: step 3609, loss 0.251849, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:16:52.010170: step 3610, loss 0.15082, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:52.519987: step 3611, loss 0.402578, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:16:53.001956: step 3612, loss 0.158185, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:53.585242: step 3613, loss 0.196603, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:54.109497: step 3614, loss 0.17319, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:54.630438: step 3615, loss 0.283899, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:16:55.096861: step 3616, loss 0.302692, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:55.600898: step 3617, loss 0.196297, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:56.194234: step 3618, loss 0.178268, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:56.719338: step 3619, loss 0.199693, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:57.256849: step 3620, loss 0.231103, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:57.803107: step 3621, loss 0.181132, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:58.414211: step 3622, loss 0.191126, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:16:58.879398: step 3623, loss 0.143335, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:16:59.369642: step 3624, loss 0.173638, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:16:59.945078: step 3625, loss 0.183055, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:17:00.421115: step 3626, loss 0.223241, acc 0.960784, learning_rate 0.000100002
2017-10-10T13:17:00.989732: step 3627, loss 0.274245, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:17:01.484981: step 3628, loss 0.122715, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:17:02.016944: step 3629, loss 0.136508, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:17:02.572686: step 3630, loss 0.238773, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:17:03.092033: step 3631, loss 0.173374, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:17:03.569750: step 3632, loss 0.253798, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:17:04.176900: step 3633, loss 0.166046, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:17:04.772961: step 3634, loss 0.33502, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:17:05.252985: step 3635, loss 0.232812, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:17:05.663215: step 3636, loss 0.258192, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:17:06.106537: step 3637, loss 0.132183, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:17:06.653920: step 3638, loss 0.165811, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:17:07.179905: step 3639, loss 0.0849293, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:17:07.705135: step 3640, loss 0.0855836, acc 1, learning_rate 0.000100002

Evaluation:
2017-10-10T13:17:08.918564: step 3640, loss 0.229668, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3640

2017-10-10T13:17:10.312925: step 3641, loss 0.306147, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:17:10.784878: step 3642, loss 0.41996, acc 0.875, learning_rate 0.000100002
2017-10-10T13:17:11.316877: step 3643, loss 0.286798, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:17:11.860861: step 3644, loss 0.176948, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:17:12.465141: step 3645, loss 0.216894, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:17:13.024971: step 3646, loss 0.152134, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:17:13.597009: step 3647, loss 0.147433, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:17:14.084947: step 3648, loss 0.119215, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:17:14.649038: step 3649, loss 0.245238, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:17:15.157300: step 3650, loss 0.167988, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:17:15.715199: step 3651, loss 0.194702, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:17:16.228344: step 3652, loss 0.223973, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:17:16.792843: step 3653, loss 0.0935428, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:17:17.314161: step 3654, loss 0.165761, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:17:17.863054: step 3655, loss 0.369106, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:17:18.396878: step 3656, loss 0.159395, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:17:18.952836: step 3657, loss 0.157554, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:17:19.491850: step 3658, loss 0.276037, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:17:20.032267: step 3659, loss 0.11696, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:17:20.589111: step 3660, loss 0.264246, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:17:21.184877: step 3661, loss 0.316107, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:17:21.722236: step 3662, loss 0.30006, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:17:22.216909: step 3663, loss 0.107316, acc 0.96875, learning_rate 0.000100002
2017-10-10T13:17:22.620856: step 3664, loss 0.174292, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:17:23.075138: step 3665, loss 0.215821, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:17:23.589030: step 3666, loss 0.31162, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:17:24.093254: step 3667, loss 0.164947, acc 0.984375, learning_rate 0.000100002
2017-10-10T13:17:24.602370: step 3668, loss 0.308852, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:17:25.096609: step 3669, loss 0.141583, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:17:25.665270: step 3670, loss 0.240196, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:17:26.292924: step 3671, loss 0.172509, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:17:26.744872: step 3672, loss 0.228759, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:17:27.293089: step 3673, loss 0.317826, acc 0.875, learning_rate 0.000100001
2017-10-10T13:17:27.874447: step 3674, loss 0.221434, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:17:28.397746: step 3675, loss 0.10231, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:17:28.815888: step 3676, loss 0.177258, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:17:29.254075: step 3677, loss 0.218903, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:17:29.812860: step 3678, loss 0.308618, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:17:30.341655: step 3679, loss 0.0995866, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:17:30.941163: step 3680, loss 0.180127, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-10-10T13:17:32.301816: step 3680, loss 0.228398, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3680

2017-10-10T13:17:33.810261: step 3681, loss 0.137217, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:17:34.280104: step 3682, loss 0.120822, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:17:34.856855: step 3683, loss 0.167455, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:17:35.424809: step 3684, loss 0.112674, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:17:36.025842: step 3685, loss 0.363397, acc 0.875, learning_rate 0.000100001
2017-10-10T13:17:36.577722: step 3686, loss 0.23304, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:17:37.080354: step 3687, loss 0.450229, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:17:37.633029: step 3688, loss 0.238593, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:17:38.205317: step 3689, loss 0.192486, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:17:38.712854: step 3690, loss 0.223429, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:17:39.252886: step 3691, loss 0.262316, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:17:39.795673: step 3692, loss 0.113113, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:17:40.336827: step 3693, loss 0.129017, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:17:40.856888: step 3694, loss 0.244829, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:17:41.420834: step 3695, loss 0.150682, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:17:41.971462: step 3696, loss 0.257476, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:17:42.508220: step 3697, loss 0.363573, acc 0.875, learning_rate 0.000100001
2017-10-10T13:17:43.028269: step 3698, loss 0.204766, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:17:43.568913: step 3699, loss 0.253752, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:17:44.068915: step 3700, loss 0.121494, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:17:44.600887: step 3701, loss 0.20349, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:17:45.111762: step 3702, loss 0.513437, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:17:45.496793: step 3703, loss 0.168521, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:17:45.920605: step 3704, loss 0.250308, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:17:46.347381: step 3705, loss 0.152108, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:17:46.863501: step 3706, loss 0.162603, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:17:47.468984: step 3707, loss 0.200571, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:17:48.007473: step 3708, loss 0.197135, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:17:48.512156: step 3709, loss 0.135439, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:17:48.986829: step 3710, loss 0.146703, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:17:49.584928: step 3711, loss 0.167074, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:17:50.168881: step 3712, loss 0.181989, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:17:50.788900: step 3713, loss 0.113067, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:17:51.411136: step 3714, loss 0.131875, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:17:51.896405: step 3715, loss 0.259807, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:17:52.329698: step 3716, loss 0.142278, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:17:52.734233: step 3717, loss 0.177611, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:17:53.300872: step 3718, loss 0.0712721, acc 1, learning_rate 0.000100001
2017-10-10T13:17:53.844551: step 3719, loss 0.203723, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:17:54.436984: step 3720, loss 0.217204, acc 0.90625, learning_rate 0.000100001

Evaluation:
2017-10-10T13:17:55.748850: step 3720, loss 0.228212, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3720

2017-10-10T13:17:57.329111: step 3721, loss 0.121643, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:17:57.930292: step 3722, loss 0.246683, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:17:58.399105: step 3723, loss 0.105831, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:17:58.790331: step 3724, loss 0.205184, acc 0.941176, learning_rate 0.000100001
2017-10-10T13:17:59.268881: step 3725, loss 0.115458, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:17:59.872998: step 3726, loss 0.197173, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:00.449125: step 3727, loss 0.289596, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:18:00.989090: step 3728, loss 0.263164, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:18:01.560979: step 3729, loss 0.161551, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:02.162275: step 3730, loss 0.257396, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:18:02.689143: step 3731, loss 0.212515, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:03.229174: step 3732, loss 0.348508, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:18:03.748318: step 3733, loss 0.255151, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:04.285731: step 3734, loss 0.136724, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:04.813022: step 3735, loss 0.21993, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:18:05.388957: step 3736, loss 0.172189, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:05.917217: step 3737, loss 0.157277, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:06.507057: step 3738, loss 0.12352, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:07.209042: step 3739, loss 0.234512, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:07.794773: step 3740, loss 0.117087, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:08.215323: step 3741, loss 0.227692, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:08.630071: step 3742, loss 0.330665, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:18:09.125062: step 3743, loss 0.164016, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:09.645248: step 3744, loss 0.24741, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:10.203983: step 3745, loss 0.25041, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:18:10.749896: step 3746, loss 0.145799, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:11.264861: step 3747, loss 0.13695, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:11.834565: step 3748, loss 0.124134, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:12.399060: step 3749, loss 0.131915, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:12.980548: step 3750, loss 0.151274, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:13.525303: step 3751, loss 0.257464, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:14.098212: step 3752, loss 0.184943, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:14.630616: step 3753, loss 0.137124, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:15.065151: step 3754, loss 0.270474, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:15.404998: step 3755, loss 0.0786015, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:15.868519: step 3756, loss 0.125026, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:16.441133: step 3757, loss 0.194398, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:17.068912: step 3758, loss 0.213821, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:17.660427: step 3759, loss 0.127583, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:18.233486: step 3760, loss 0.279948, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-10-10T13:18:19.308885: step 3760, loss 0.230944, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3760

2017-10-10T13:18:20.761019: step 3761, loss 0.124519, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:21.311111: step 3762, loss 0.0985923, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:21.848804: step 3763, loss 0.194499, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:22.401309: step 3764, loss 0.389553, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:18:22.951818: step 3765, loss 0.134805, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:23.432840: step 3766, loss 0.280404, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:24.016409: step 3767, loss 0.154218, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:24.532339: step 3768, loss 0.146006, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:25.020917: step 3769, loss 0.167626, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:25.489731: step 3770, loss 0.265, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:18:25.935010: step 3771, loss 0.130367, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:26.544979: step 3772, loss 0.449528, acc 0.8125, learning_rate 0.000100001
2017-10-10T13:18:27.144858: step 3773, loss 0.10266, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:18:27.671029: step 3774, loss 0.168807, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:28.167899: step 3775, loss 0.199037, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:28.654333: step 3776, loss 0.18076, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:29.248730: step 3777, loss 0.0918981, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:18:29.760356: step 3778, loss 0.184527, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:30.209147: step 3779, loss 0.133219, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:30.852903: step 3780, loss 0.170485, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:18:31.376867: step 3781, loss 0.137141, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:31.875080: step 3782, loss 0.232957, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:32.280425: step 3783, loss 0.290851, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:18:32.852410: step 3784, loss 0.0977855, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:18:33.417225: step 3785, loss 0.25724, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:33.966721: step 3786, loss 0.141457, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:34.544829: step 3787, loss 0.225908, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:35.026794: step 3788, loss 0.111552, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:35.557618: step 3789, loss 0.28504, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:18:36.089250: step 3790, loss 0.242244, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:36.667882: step 3791, loss 0.159332, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:37.210272: step 3792, loss 0.194788, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:18:37.792862: step 3793, loss 0.222078, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:18:38.228959: step 3794, loss 0.210912, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:38.679952: step 3795, loss 0.140033, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:39.071481: step 3796, loss 0.241754, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:18:39.635557: step 3797, loss 0.288455, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:40.251132: step 3798, loss 0.0730284, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:40.809205: step 3799, loss 0.174278, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:41.263372: step 3800, loss 0.152755, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T13:18:42.417871: step 3800, loss 0.227925, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3800

2017-10-10T13:18:44.040837: step 3801, loss 0.117082, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:44.605045: step 3802, loss 0.177773, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:45.145373: step 3803, loss 0.154875, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:45.697008: step 3804, loss 0.322665, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:18:46.296977: step 3805, loss 0.0905919, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:46.860890: step 3806, loss 0.313723, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:18:47.444812: step 3807, loss 0.303302, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:18:48.020885: step 3808, loss 0.186782, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:48.608855: step 3809, loss 0.233811, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:18:49.180935: step 3810, loss 0.274268, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:18:49.735286: step 3811, loss 0.192867, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:50.236906: step 3812, loss 0.227928, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:18:50.786944: step 3813, loss 0.170758, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:51.266567: step 3814, loss 0.135954, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:18:51.745178: step 3815, loss 0.139296, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:52.224615: step 3816, loss 0.302476, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:18:52.732826: step 3817, loss 0.132446, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:53.297174: step 3818, loss 0.224962, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:18:53.845045: step 3819, loss 0.218665, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:54.469257: step 3820, loss 0.171208, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:55.064887: step 3821, loss 0.206467, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:55.415091: step 3822, loss 0.236109, acc 0.901961, learning_rate 0.000100001
2017-10-10T13:18:55.848866: step 3823, loss 0.302134, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:56.373254: step 3824, loss 0.288644, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:18:56.804308: step 3825, loss 0.139413, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:57.272594: step 3826, loss 0.159792, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:18:57.790428: step 3827, loss 0.145067, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:58.309456: step 3828, loss 0.128342, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:58.878801: step 3829, loss 0.15245, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:18:59.444471: step 3830, loss 0.134658, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:19:00.028953: step 3831, loss 0.120009, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:19:00.673042: step 3832, loss 0.134277, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:01.140093: step 3833, loss 0.100164, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:01.527034: step 3834, loss 0.175253, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:01.933934: step 3835, loss 0.220916, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:19:02.452967: step 3836, loss 0.366859, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:19:03.056154: step 3837, loss 0.280553, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:19:03.516380: step 3838, loss 0.148973, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:03.968846: step 3839, loss 0.181356, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:19:04.392306: step 3840, loss 0.098725, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T13:19:05.688079: step 3840, loss 0.227238, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3840

2017-10-10T13:19:07.539907: step 3841, loss 0.116854, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:19:08.104894: step 3842, loss 0.261705, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:19:08.667652: step 3843, loss 0.148349, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:19:09.168914: step 3844, loss 0.175698, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:09.721615: step 3845, loss 0.288324, acc 0.875, learning_rate 0.000100001
2017-10-10T13:19:10.265287: step 3846, loss 0.163011, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:10.804878: step 3847, loss 0.317241, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:19:11.353057: step 3848, loss 0.134258, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:11.881848: step 3849, loss 0.166263, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:19:12.417155: step 3850, loss 0.254461, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:12.915193: step 3851, loss 0.187279, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:13.416755: step 3852, loss 0.199221, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:13.880552: step 3853, loss 0.250528, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:19:14.449359: step 3854, loss 0.192445, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:19:14.945026: step 3855, loss 0.166257, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:15.478600: step 3856, loss 0.167797, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:16.033291: step 3857, loss 0.192131, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:19:16.488990: step 3858, loss 0.220179, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:19:17.014139: step 3859, loss 0.139811, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:17.599837: step 3860, loss 0.102216, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:19:18.158707: step 3861, loss 0.214964, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:18.710192: step 3862, loss 0.299188, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:19:19.133622: step 3863, loss 0.129204, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:19.605702: step 3864, loss 0.367018, acc 0.875, learning_rate 0.000100001
2017-10-10T13:19:20.164101: step 3865, loss 0.165142, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:20.688882: step 3866, loss 0.120384, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:21.204981: step 3867, loss 0.175057, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:19:21.780857: step 3868, loss 0.276455, acc 0.875, learning_rate 0.000100001
2017-10-10T13:19:22.265998: step 3869, loss 0.201508, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:19:22.825201: step 3870, loss 0.125469, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:23.348837: step 3871, loss 0.172358, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:23.892842: step 3872, loss 0.128651, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:24.353083: step 3873, loss 0.177968, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:24.764820: step 3874, loss 0.330496, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:19:25.193998: step 3875, loss 0.184287, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:25.829297: step 3876, loss 0.204238, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:26.321890: step 3877, loss 0.217534, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:19:26.691842: step 3878, loss 0.188851, acc 0.875, learning_rate 0.000100001
2017-10-10T13:19:27.124825: step 3879, loss 0.263647, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:27.697462: step 3880, loss 0.217101, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T13:19:29.217041: step 3880, loss 0.229307, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3880

2017-10-10T13:19:30.641058: step 3881, loss 0.133208, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:31.138094: step 3882, loss 0.279944, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:19:31.677676: step 3883, loss 0.159887, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:32.199769: step 3884, loss 0.160781, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:19:32.752630: step 3885, loss 0.263144, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:19:33.317236: step 3886, loss 0.262985, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:19:33.836973: step 3887, loss 0.10806, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:19:34.340852: step 3888, loss 0.157441, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:34.860533: step 3889, loss 0.160881, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:35.333000: step 3890, loss 0.160437, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:35.876837: step 3891, loss 0.291197, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:19:36.404811: step 3892, loss 0.121719, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:36.926855: step 3893, loss 0.157076, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:37.521372: step 3894, loss 0.21308, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:38.074658: step 3895, loss 0.447627, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:19:38.592861: step 3896, loss 0.201664, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:39.099852: step 3897, loss 0.386421, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:19:39.596862: step 3898, loss 0.192497, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:40.104069: step 3899, loss 0.196591, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:40.555482: step 3900, loss 0.232751, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:19:41.149633: step 3901, loss 0.133289, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:41.665059: step 3902, loss 0.269164, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:19:42.146541: step 3903, loss 0.186433, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:19:42.589011: step 3904, loss 0.250099, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:19:43.013084: step 3905, loss 0.172063, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:43.515226: step 3906, loss 0.204307, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:19:44.051935: step 3907, loss 0.351873, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:19:44.606770: step 3908, loss 0.218451, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:45.061045: step 3909, loss 0.0922282, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:19:45.555623: step 3910, loss 0.195706, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:46.117845: step 3911, loss 0.342602, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:19:46.702833: step 3912, loss 0.153464, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:47.252683: step 3913, loss 0.158863, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:47.732899: step 3914, loss 0.330945, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:19:48.175724: step 3915, loss 0.341736, acc 0.875, learning_rate 0.000100001
2017-10-10T13:19:48.752925: step 3916, loss 0.105289, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:19:49.352161: step 3917, loss 0.335102, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:19:49.748851: step 3918, loss 0.115203, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:19:50.304888: step 3919, loss 0.215696, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:50.772937: step 3920, loss 0.155164, acc 0.960784, learning_rate 0.000100001

Evaluation:
2017-10-10T13:19:52.061442: step 3920, loss 0.227217, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3920

2017-10-10T13:19:53.726602: step 3921, loss 0.221759, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:54.256866: step 3922, loss 0.200928, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:54.841555: step 3923, loss 0.131589, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:19:55.368860: step 3924, loss 0.266691, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:55.876826: step 3925, loss 0.220962, acc 0.875, learning_rate 0.000100001
2017-10-10T13:19:56.352609: step 3926, loss 0.217517, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:19:56.805175: step 3927, loss 0.177466, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:19:57.319202: step 3928, loss 0.243986, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:19:57.888878: step 3929, loss 0.249948, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:19:58.441116: step 3930, loss 0.149593, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:58.969131: step 3931, loss 0.139157, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:19:59.601102: step 3932, loss 0.160464, acc 0.984375, learning_rate 0.000100001
2017-10-10T13:20:00.125236: step 3933, loss 0.123046, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:20:00.657126: step 3934, loss 0.186578, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:20:01.208932: step 3935, loss 0.235286, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:20:01.788712: step 3936, loss 0.175652, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:20:02.304728: step 3937, loss 0.14066, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:02.876826: step 3938, loss 0.257967, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:03.428821: step 3939, loss 0.155298, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:03.987043: step 3940, loss 0.173356, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:04.464755: step 3941, loss 0.0993187, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:05.016203: step 3942, loss 0.178917, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:05.586597: step 3943, loss 0.248339, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:06.017972: step 3944, loss 0.126625, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:06.452820: step 3945, loss 0.105795, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:06.996358: step 3946, loss 0.149496, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:07.547104: step 3947, loss 0.179059, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:08.132874: step 3948, loss 0.216542, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:08.675923: step 3949, loss 0.216893, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:09.273969: step 3950, loss 0.246637, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:09.892995: step 3951, loss 0.213765, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:10.291514: step 3952, loss 0.21103, acc 0.890625, learning_rate 0.0001
2017-10-10T13:20:10.736824: step 3953, loss 0.129406, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:11.232894: step 3954, loss 0.150282, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:11.728904: step 3955, loss 0.248372, acc 0.890625, learning_rate 0.0001
2017-10-10T13:20:12.272842: step 3956, loss 0.0741342, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:12.712347: step 3957, loss 0.218472, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:13.159396: step 3958, loss 0.136823, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:13.708435: step 3959, loss 0.231378, acc 0.890625, learning_rate 0.0001
2017-10-10T13:20:14.184376: step 3960, loss 0.208619, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:20:15.474376: step 3960, loss 0.228376, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-3960

2017-10-10T13:20:17.308928: step 3961, loss 0.2626, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:17.800964: step 3962, loss 0.172076, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:18.332970: step 3963, loss 0.117407, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:18.788967: step 3964, loss 0.259144, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:19.361176: step 3965, loss 0.225687, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:19.988888: step 3966, loss 0.0925208, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:20.512876: step 3967, loss 0.216385, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:21.008912: step 3968, loss 0.183282, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:21.476344: step 3969, loss 0.117916, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:21.971060: step 3970, loss 0.222719, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:22.485922: step 3971, loss 0.155004, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:22.899633: step 3972, loss 0.191454, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:23.475238: step 3973, loss 0.174931, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:23.933679: step 3974, loss 0.266561, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:24.401710: step 3975, loss 0.116711, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:24.944859: step 3976, loss 0.329276, acc 0.890625, learning_rate 0.0001
2017-10-10T13:20:25.482473: step 3977, loss 0.129227, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:26.012841: step 3978, loss 0.119829, acc 1, learning_rate 0.0001
2017-10-10T13:20:26.523805: step 3979, loss 0.171357, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:27.088909: step 3980, loss 0.25096, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:27.692844: step 3981, loss 0.123388, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:28.268866: step 3982, loss 0.0435215, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:28.790353: step 3983, loss 0.223662, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:29.296903: step 3984, loss 0.175462, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:29.788875: step 3985, loss 0.0864065, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:30.389670: step 3986, loss 0.193979, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:30.933033: step 3987, loss 0.106592, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:31.473990: step 3988, loss 0.217081, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:32.096905: step 3989, loss 0.323193, acc 0.890625, learning_rate 0.0001
2017-10-10T13:20:32.749450: step 3990, loss 0.191157, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:33.259343: step 3991, loss 0.192253, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:33.697072: step 3992, loss 0.145334, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:34.199275: step 3993, loss 0.170658, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:34.772879: step 3994, loss 0.24198, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:35.348150: step 3995, loss 0.340416, acc 0.890625, learning_rate 0.0001
2017-10-10T13:20:35.848853: step 3996, loss 0.263581, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:36.291366: step 3997, loss 0.259856, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:36.737317: step 3998, loss 0.21795, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:37.268885: step 3999, loss 0.200943, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:37.768833: step 4000, loss 0.06764, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:20:39.096931: step 4000, loss 0.228064, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4000

2017-10-10T13:20:40.564886: step 4001, loss 0.234688, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:41.097381: step 4002, loss 0.226019, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:41.627866: step 4003, loss 0.180928, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:42.177513: step 4004, loss 0.199513, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:42.793132: step 4005, loss 0.262603, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:43.280474: step 4006, loss 0.144956, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:43.852500: step 4007, loss 0.165089, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:44.375865: step 4008, loss 0.20045, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:44.997501: step 4009, loss 0.217642, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:45.584875: step 4010, loss 0.185862, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:46.104238: step 4011, loss 0.205387, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:46.656913: step 4012, loss 0.160053, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:47.199755: step 4013, loss 0.20741, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:47.772591: step 4014, loss 0.256064, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:48.319003: step 4015, loss 0.287659, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:48.920834: step 4016, loss 0.139401, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:49.452831: step 4017, loss 0.152751, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:49.912942: step 4018, loss 0.142828, acc 0.941176, learning_rate 0.0001
2017-10-10T13:20:50.392919: step 4019, loss 0.156533, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:50.826632: step 4020, loss 0.121955, acc 0.984375, learning_rate 0.0001
2017-10-10T13:20:51.428812: step 4021, loss 0.153115, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:52.059379: step 4022, loss 0.373791, acc 0.84375, learning_rate 0.0001
2017-10-10T13:20:52.472915: step 4023, loss 0.0601296, acc 1, learning_rate 0.0001
2017-10-10T13:20:52.936877: step 4024, loss 0.164992, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:53.441246: step 4025, loss 0.364112, acc 0.859375, learning_rate 0.0001
2017-10-10T13:20:53.964933: step 4026, loss 0.314913, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:54.507414: step 4027, loss 0.229864, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:55.071163: step 4028, loss 0.280727, acc 0.90625, learning_rate 0.0001
2017-10-10T13:20:56.085384: step 4029, loss 0.148586, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:56.533118: step 4030, loss 0.161143, acc 0.9375, learning_rate 0.0001
2017-10-10T13:20:57.053596: step 4031, loss 0.183275, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:57.584833: step 4032, loss 0.189601, acc 0.953125, learning_rate 0.0001
2017-10-10T13:20:58.088837: step 4033, loss 0.0919577, acc 0.96875, learning_rate 0.0001
2017-10-10T13:20:58.608828: step 4034, loss 0.186255, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:59.233022: step 4035, loss 0.21202, acc 0.921875, learning_rate 0.0001
2017-10-10T13:20:59.701093: step 4036, loss 0.111707, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:00.184112: step 4037, loss 0.158801, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:00.680924: step 4038, loss 0.126536, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:01.280829: step 4039, loss 0.110498, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:01.853215: step 4040, loss 0.283116, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:21:03.162610: step 4040, loss 0.227496, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4040

2017-10-10T13:21:04.797026: step 4041, loss 0.154686, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:05.361100: step 4042, loss 0.15068, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:05.892868: step 4043, loss 0.128268, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:06.333719: step 4044, loss 0.217332, acc 0.90625, learning_rate 0.0001
2017-10-10T13:21:06.833020: step 4045, loss 0.135532, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:07.368043: step 4046, loss 0.242947, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:07.928874: step 4047, loss 0.269382, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:08.380857: step 4048, loss 0.148605, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:08.861066: step 4049, loss 0.157677, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:09.359846: step 4050, loss 0.243716, acc 0.90625, learning_rate 0.0001
2017-10-10T13:21:09.877217: step 4051, loss 0.0831258, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:10.536989: step 4052, loss 0.419567, acc 0.90625, learning_rate 0.0001
2017-10-10T13:21:11.024970: step 4053, loss 0.189595, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:11.567215: step 4054, loss 0.129292, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:12.134631: step 4055, loss 0.303768, acc 0.890625, learning_rate 0.0001
2017-10-10T13:21:12.675473: step 4056, loss 0.095244, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:13.264924: step 4057, loss 0.170615, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:13.744901: step 4058, loss 0.130271, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:14.256903: step 4059, loss 0.158238, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:14.937274: step 4060, loss 0.253296, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:15.468026: step 4061, loss 0.336129, acc 0.890625, learning_rate 0.0001
2017-10-10T13:21:15.920385: step 4062, loss 0.228151, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:16.438498: step 4063, loss 0.10845, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:17.020959: step 4064, loss 0.149743, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:17.488961: step 4065, loss 0.194781, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:18.037069: step 4066, loss 0.219926, acc 0.90625, learning_rate 0.0001
2017-10-10T13:21:18.681320: step 4067, loss 0.228688, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:19.148942: step 4068, loss 0.146491, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:19.601605: step 4069, loss 0.189809, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:20.069592: step 4070, loss 0.290623, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:20.613000: step 4071, loss 0.248772, acc 0.890625, learning_rate 0.0001
2017-10-10T13:21:21.156931: step 4072, loss 0.239365, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:21.716508: step 4073, loss 0.222017, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:22.241358: step 4074, loss 0.165425, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:22.647289: step 4075, loss 0.390701, acc 0.84375, learning_rate 0.0001
2017-10-10T13:21:23.085917: step 4076, loss 0.258862, acc 0.90625, learning_rate 0.0001
2017-10-10T13:21:23.619344: step 4077, loss 0.108779, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:24.209062: step 4078, loss 0.110949, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:24.770698: step 4079, loss 0.306527, acc 0.875, learning_rate 0.0001
2017-10-10T13:21:25.378311: step 4080, loss 0.231097, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:21:26.776909: step 4080, loss 0.22756, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4080

2017-10-10T13:21:28.445653: step 4081, loss 0.247245, acc 0.875, learning_rate 0.0001
2017-10-10T13:21:28.979773: step 4082, loss 0.265314, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:29.510658: step 4083, loss 0.12128, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:30.063988: step 4084, loss 0.154724, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:30.640979: step 4085, loss 0.203271, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:31.257193: step 4086, loss 0.111154, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:31.711923: step 4087, loss 0.0862226, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:32.239208: step 4088, loss 0.123837, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:32.779539: step 4089, loss 0.0851424, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:33.380829: step 4090, loss 0.178916, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:33.914821: step 4091, loss 0.132312, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:34.453522: step 4092, loss 0.12683, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:34.998846: step 4093, loss 0.254073, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:35.557203: step 4094, loss 0.0624591, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:36.119201: step 4095, loss 0.187996, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:36.628838: step 4096, loss 0.0982861, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:37.192879: step 4097, loss 0.181018, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:37.760867: step 4098, loss 0.348119, acc 0.859375, learning_rate 0.0001
2017-10-10T13:21:38.357017: step 4099, loss 0.156282, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:38.817571: step 4100, loss 0.138192, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:39.277947: step 4101, loss 0.246949, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:39.913098: step 4102, loss 0.179723, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:40.548550: step 4103, loss 0.153408, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:41.198214: step 4104, loss 0.149326, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:41.697043: step 4105, loss 0.330453, acc 0.90625, learning_rate 0.0001
2017-10-10T13:21:42.172906: step 4106, loss 0.175654, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:42.619507: step 4107, loss 0.149321, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:43.095248: step 4108, loss 0.140254, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:43.580963: step 4109, loss 0.175098, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:44.124979: step 4110, loss 0.099902, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:44.649196: step 4111, loss 0.244784, acc 0.890625, learning_rate 0.0001
2017-10-10T13:21:45.309049: step 4112, loss 0.119376, acc 0.984375, learning_rate 0.0001
2017-10-10T13:21:45.916834: step 4113, loss 0.199439, acc 0.90625, learning_rate 0.0001
2017-10-10T13:21:46.372835: step 4114, loss 0.142743, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:46.859229: step 4115, loss 0.336426, acc 0.890625, learning_rate 0.0001
2017-10-10T13:21:47.262600: step 4116, loss 0.166047, acc 0.960784, learning_rate 0.0001
2017-10-10T13:21:47.740541: step 4117, loss 0.211743, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:48.281373: step 4118, loss 0.222891, acc 0.90625, learning_rate 0.0001
2017-10-10T13:21:48.776895: step 4119, loss 0.171997, acc 0.9375, learning_rate 0.0001
2017-10-10T13:21:49.294463: step 4120, loss 0.242444, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:21:50.637119: step 4120, loss 0.226025, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4120

2017-10-10T13:21:52.411877: step 4121, loss 0.155013, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:53.004166: step 4122, loss 0.137993, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:53.484851: step 4123, loss 0.27824, acc 0.90625, learning_rate 0.0001
2017-10-10T13:21:53.993090: step 4124, loss 0.23382, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:54.492223: step 4125, loss 0.15056, acc 0.96875, learning_rate 0.0001
2017-10-10T13:21:54.957079: step 4126, loss 0.151818, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:55.496837: step 4127, loss 0.194305, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:56.034037: step 4128, loss 0.283426, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:56.534675: step 4129, loss 0.189008, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:57.021687: step 4130, loss 0.143345, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:57.487578: step 4131, loss 0.161793, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:57.996913: step 4132, loss 0.316537, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:58.581120: step 4133, loss 0.333109, acc 0.921875, learning_rate 0.0001
2017-10-10T13:21:59.124908: step 4134, loss 0.135943, acc 0.953125, learning_rate 0.0001
2017-10-10T13:21:59.644904: step 4135, loss 0.114505, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:00.152938: step 4136, loss 0.0966126, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:00.704971: step 4137, loss 0.233014, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:01.311675: step 4138, loss 0.22425, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:01.701006: step 4139, loss 0.137518, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:02.164872: step 4140, loss 0.174865, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:02.698267: step 4141, loss 0.162531, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:03.216169: step 4142, loss 0.212957, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:03.758024: step 4143, loss 0.0962724, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:04.348850: step 4144, loss 0.0795431, acc 1, learning_rate 0.0001
2017-10-10T13:22:04.926653: step 4145, loss 0.200221, acc 0.875, learning_rate 0.0001
2017-10-10T13:22:05.373280: step 4146, loss 0.0992449, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:05.841730: step 4147, loss 0.135543, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:06.375413: step 4148, loss 0.224296, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:06.883764: step 4149, loss 0.243457, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:07.411186: step 4150, loss 0.134283, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:07.925315: step 4151, loss 0.109442, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:08.465055: step 4152, loss 0.151315, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:08.950267: step 4153, loss 0.178867, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:09.404868: step 4154, loss 0.171224, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:09.772907: step 4155, loss 0.111139, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:10.340309: step 4156, loss 0.110432, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:10.880979: step 4157, loss 0.107986, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:11.385061: step 4158, loss 0.162864, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:11.958724: step 4159, loss 0.258569, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:12.492874: step 4160, loss 0.248138, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:22:13.747237: step 4160, loss 0.226116, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4160

2017-10-10T13:22:15.164854: step 4161, loss 0.186189, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:15.702660: step 4162, loss 0.203221, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:16.219439: step 4163, loss 0.0927178, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:16.746569: step 4164, loss 0.154549, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:17.309651: step 4165, loss 0.104835, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:17.861078: step 4166, loss 0.17277, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:18.411849: step 4167, loss 0.0665575, acc 1, learning_rate 0.0001
2017-10-10T13:22:18.935546: step 4168, loss 0.152965, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:19.464944: step 4169, loss 0.301123, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:20.013077: step 4170, loss 0.215914, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:20.539723: step 4171, loss 0.199573, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:21.021081: step 4172, loss 0.181864, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:21.617589: step 4173, loss 0.325495, acc 0.890625, learning_rate 0.0001
2017-10-10T13:22:22.125166: step 4174, loss 0.196326, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:22.632968: step 4175, loss 0.176278, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:23.117222: step 4176, loss 0.328678, acc 0.875, learning_rate 0.0001
2017-10-10T13:22:23.705090: step 4177, loss 0.186368, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:24.288948: step 4178, loss 0.108042, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:24.893094: step 4179, loss 0.367956, acc 0.875, learning_rate 0.0001
2017-10-10T13:22:25.377796: step 4180, loss 0.230092, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:25.828955: step 4181, loss 0.248208, acc 0.890625, learning_rate 0.0001
2017-10-10T13:22:26.361497: step 4182, loss 0.370711, acc 0.859375, learning_rate 0.0001
2017-10-10T13:22:26.937542: step 4183, loss 0.106268, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:27.484084: step 4184, loss 0.209296, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:28.028952: step 4185, loss 0.177522, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:29.228980: step 4186, loss 0.114873, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:29.695572: step 4187, loss 0.170495, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:30.224882: step 4188, loss 0.157895, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:30.810354: step 4189, loss 0.0967485, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:31.448846: step 4190, loss 0.282128, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:31.937525: step 4191, loss 0.126627, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:32.485103: step 4192, loss 0.309884, acc 0.875, learning_rate 0.0001
2017-10-10T13:22:32.900985: step 4193, loss 0.248378, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:33.345368: step 4194, loss 0.286889, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:33.880989: step 4195, loss 0.231949, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:34.508976: step 4196, loss 0.11973, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:34.963954: step 4197, loss 0.175126, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:35.549214: step 4198, loss 0.169028, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:36.122589: step 4199, loss 0.236959, acc 0.875, learning_rate 0.0001
2017-10-10T13:22:36.721116: step 4200, loss 0.17351, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:22:38.048867: step 4200, loss 0.225256, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4200

2017-10-10T13:22:39.570147: step 4201, loss 0.101215, acc 0.984375, learning_rate 0.0001
2017-10-10T13:22:40.120736: step 4202, loss 0.228687, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:40.666178: step 4203, loss 0.161967, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:41.227117: step 4204, loss 0.208904, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:41.766604: step 4205, loss 0.413895, acc 0.875, learning_rate 0.0001
2017-10-10T13:22:42.267916: step 4206, loss 0.124532, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:42.705004: step 4207, loss 0.172214, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:43.228931: step 4208, loss 0.175125, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:43.741035: step 4209, loss 0.206943, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:44.281102: step 4210, loss 0.226411, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:44.780852: step 4211, loss 0.166633, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:45.161735: step 4212, loss 0.0783715, acc 1, learning_rate 0.0001
2017-10-10T13:22:45.682467: step 4213, loss 0.091912, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:46.162377: step 4214, loss 0.127616, acc 0.941176, learning_rate 0.0001
2017-10-10T13:22:46.733220: step 4215, loss 0.16115, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:47.347213: step 4216, loss 0.262722, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:47.875844: step 4217, loss 0.195189, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:48.491750: step 4218, loss 0.247766, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:49.073911: step 4219, loss 0.260494, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:49.545123: step 4220, loss 0.113793, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:49.988030: step 4221, loss 0.177632, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:50.439886: step 4222, loss 0.166301, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:51.010799: step 4223, loss 0.151798, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:51.555453: step 4224, loss 0.315125, acc 0.875, learning_rate 0.0001
2017-10-10T13:22:52.041190: step 4225, loss 0.231053, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:52.549773: step 4226, loss 0.228386, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:53.001179: step 4227, loss 0.215061, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:53.471098: step 4228, loss 0.206651, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:53.980880: step 4229, loss 0.0805007, acc 1, learning_rate 0.0001
2017-10-10T13:22:54.572866: step 4230, loss 0.166231, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:55.126364: step 4231, loss 0.117833, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:55.594028: step 4232, loss 0.195879, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:56.060869: step 4233, loss 0.151693, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:56.653622: step 4234, loss 0.150344, acc 0.9375, learning_rate 0.0001
2017-10-10T13:22:57.237450: step 4235, loss 0.146347, acc 0.953125, learning_rate 0.0001
2017-10-10T13:22:57.772894: step 4236, loss 0.195248, acc 0.90625, learning_rate 0.0001
2017-10-10T13:22:58.325468: step 4237, loss 0.170728, acc 0.921875, learning_rate 0.0001
2017-10-10T13:22:58.856189: step 4238, loss 0.150466, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:59.416862: step 4239, loss 0.153933, acc 0.96875, learning_rate 0.0001
2017-10-10T13:22:59.950562: step 4240, loss 0.2598, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:23:01.364047: step 4240, loss 0.224646, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4240

2017-10-10T13:23:03.141043: step 4241, loss 0.130436, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:03.726297: step 4242, loss 0.175232, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:04.298077: step 4243, loss 0.200667, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:04.860443: step 4244, loss 0.14745, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:05.422261: step 4245, loss 0.237751, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:06.008743: step 4246, loss 0.309051, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:06.569152: step 4247, loss 0.25849, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:07.094811: step 4248, loss 0.200904, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:07.652837: step 4249, loss 0.180814, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:08.180903: step 4250, loss 0.22593, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:08.724488: step 4251, loss 0.121503, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:09.252863: step 4252, loss 0.0973374, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:09.766682: step 4253, loss 0.192558, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:10.320885: step 4254, loss 0.329566, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:10.840886: step 4255, loss 0.169562, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:11.374536: step 4256, loss 0.0989202, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:12.012884: step 4257, loss 0.175894, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:12.534264: step 4258, loss 0.244271, acc 0.859375, learning_rate 0.0001
2017-10-10T13:23:12.995665: step 4259, loss 0.109226, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:13.551992: step 4260, loss 0.34645, acc 0.859375, learning_rate 0.0001
2017-10-10T13:23:14.134294: step 4261, loss 0.119808, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:14.737202: step 4262, loss 0.229556, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:15.321688: step 4263, loss 0.163212, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:15.738127: step 4264, loss 0.184769, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:16.192916: step 4265, loss 0.0627695, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:16.756833: step 4266, loss 0.2348, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:17.360875: step 4267, loss 0.234377, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:17.939392: step 4268, loss 0.184249, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:18.378414: step 4269, loss 0.24212, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:18.856991: step 4270, loss 0.192916, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:19.350181: step 4271, loss 0.137419, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:19.876887: step 4272, loss 0.18604, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:20.410240: step 4273, loss 0.257412, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:20.953040: step 4274, loss 0.176907, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:21.465088: step 4275, loss 0.335735, acc 0.875, learning_rate 0.0001
2017-10-10T13:23:22.000969: step 4276, loss 0.126866, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:22.584846: step 4277, loss 0.273688, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:23.112968: step 4278, loss 0.163687, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:23.671181: step 4279, loss 0.197378, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:24.206352: step 4280, loss 0.427439, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:23:25.541006: step 4280, loss 0.224487, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4280

2017-10-10T13:23:27.047951: step 4281, loss 0.248588, acc 0.890625, learning_rate 0.0001
2017-10-10T13:23:27.533073: step 4282, loss 0.136322, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:28.061195: step 4283, loss 0.148694, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:28.584404: step 4284, loss 0.105928, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:29.241042: step 4285, loss 0.135035, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:29.817090: step 4286, loss 0.196146, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:30.371005: step 4287, loss 0.203186, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:30.944211: step 4288, loss 0.230888, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:31.398919: step 4289, loss 0.164746, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:31.944425: step 4290, loss 0.225396, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:32.469558: step 4291, loss 0.225814, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:32.948269: step 4292, loss 0.1633, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:33.517025: step 4293, loss 0.134248, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:34.038569: step 4294, loss 0.141404, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:34.620879: step 4295, loss 0.254571, acc 0.890625, learning_rate 0.0001
2017-10-10T13:23:35.199184: step 4296, loss 0.115177, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:35.568953: step 4297, loss 0.065373, acc 1, learning_rate 0.0001
2017-10-10T13:23:35.996902: step 4298, loss 0.119159, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:36.548854: step 4299, loss 0.314463, acc 0.890625, learning_rate 0.0001
2017-10-10T13:23:37.080878: step 4300, loss 0.181301, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:37.621116: step 4301, loss 0.229027, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:38.184871: step 4302, loss 0.102458, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:38.608869: step 4303, loss 0.058709, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:39.004933: step 4304, loss 0.314688, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:39.542679: step 4305, loss 0.109598, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:40.080937: step 4306, loss 0.164616, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:40.672926: step 4307, loss 0.330042, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:41.168929: step 4308, loss 0.164234, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:41.583356: step 4309, loss 0.190873, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:41.989584: step 4310, loss 0.0868362, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:42.543297: step 4311, loss 0.12301, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:43.007387: step 4312, loss 0.301478, acc 0.862745, learning_rate 0.0001
2017-10-10T13:23:43.485621: step 4313, loss 0.0869341, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:43.970762: step 4314, loss 0.407551, acc 0.875, learning_rate 0.0001
2017-10-10T13:23:44.485431: step 4315, loss 0.393917, acc 0.890625, learning_rate 0.0001
2017-10-10T13:23:44.924865: step 4316, loss 0.0863229, acc 0.984375, learning_rate 0.0001
2017-10-10T13:23:45.464814: step 4317, loss 0.210171, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:46.054037: step 4318, loss 0.295673, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:46.555250: step 4319, loss 0.26341, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:47.081081: step 4320, loss 0.230721, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:23:48.389249: step 4320, loss 0.225042, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4320

2017-10-10T13:23:50.045163: step 4321, loss 0.208362, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:50.577123: step 4322, loss 0.190757, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:51.060886: step 4323, loss 0.141709, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:51.565329: step 4324, loss 0.252888, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:52.024870: step 4325, loss 0.317752, acc 0.875, learning_rate 0.0001
2017-10-10T13:23:52.592939: step 4326, loss 0.223089, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:53.113040: step 4327, loss 0.135237, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:53.664391: step 4328, loss 0.145738, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:54.238113: step 4329, loss 0.170542, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:54.763502: step 4330, loss 0.171597, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:55.278365: step 4331, loss 0.280568, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:55.772840: step 4332, loss 0.322665, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:56.324875: step 4333, loss 0.325405, acc 0.859375, learning_rate 0.0001
2017-10-10T13:23:56.831806: step 4334, loss 0.137444, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:57.340857: step 4335, loss 0.192989, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:57.868088: step 4336, loss 0.208785, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:58.339121: step 4337, loss 0.148041, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:58.940455: step 4338, loss 0.263887, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:59.358124: step 4339, loss 0.0725098, acc 1, learning_rate 0.0001
2017-10-10T13:23:59.822744: step 4340, loss 0.144095, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:00.212835: step 4341, loss 0.277615, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:00.800270: step 4342, loss 0.246227, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:01.334569: step 4343, loss 0.119388, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:01.768768: step 4344, loss 0.349219, acc 0.84375, learning_rate 0.0001
2017-10-10T13:24:02.263835: step 4345, loss 0.114853, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:02.786467: step 4346, loss 0.207794, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:03.256998: step 4347, loss 0.203824, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:03.790440: step 4348, loss 0.191732, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:04.225674: step 4349, loss 0.209461, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:04.656389: step 4350, loss 0.24999, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:05.107959: step 4351, loss 0.222913, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:05.561451: step 4352, loss 0.18288, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:06.080396: step 4353, loss 0.271987, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:06.558731: step 4354, loss 0.128036, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:07.067453: step 4355, loss 0.208742, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:07.598895: step 4356, loss 0.306266, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:08.126233: step 4357, loss 0.211296, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:08.701167: step 4358, loss 0.310733, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:09.204890: step 4359, loss 0.206317, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:09.702872: step 4360, loss 0.162338, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:24:11.195665: step 4360, loss 0.223025, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4360

2017-10-10T13:24:13.017015: step 4361, loss 0.10983, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:13.501923: step 4362, loss 0.175509, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:13.995684: step 4363, loss 0.362118, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:14.532866: step 4364, loss 0.11403, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:15.076978: step 4365, loss 0.180688, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:15.605118: step 4366, loss 0.166205, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:16.130067: step 4367, loss 0.18755, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:16.636755: step 4368, loss 0.152742, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:17.135335: step 4369, loss 0.147537, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:17.621515: step 4370, loss 0.172554, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:18.148718: step 4371, loss 0.174981, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:18.666539: step 4372, loss 0.160985, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:19.174956: step 4373, loss 0.245103, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:19.731667: step 4374, loss 0.0857993, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:20.232848: step 4375, loss 0.190959, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:20.793028: step 4376, loss 0.134206, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:21.408901: step 4377, loss 0.145499, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:22.005021: step 4378, loss 0.10545, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:22.468852: step 4379, loss 0.274764, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:22.864953: step 4380, loss 0.134359, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:23.311950: step 4381, loss 0.203077, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:23.867589: step 4382, loss 0.366325, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:24.512659: step 4383, loss 0.183137, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:24.867295: step 4384, loss 0.115494, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:25.266708: step 4385, loss 0.229483, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:25.792947: step 4386, loss 0.276073, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:26.360971: step 4387, loss 0.181209, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:26.943712: step 4388, loss 0.159975, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:27.420968: step 4389, loss 0.0916481, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:27.825446: step 4390, loss 0.184553, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:28.212895: step 4391, loss 0.134376, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:28.694064: step 4392, loss 0.148696, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:29.271094: step 4393, loss 0.235113, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:29.796498: step 4394, loss 0.175854, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:30.248880: step 4395, loss 0.232824, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:30.781874: step 4396, loss 0.22905, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:31.318203: step 4397, loss 0.181409, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:31.860850: step 4398, loss 0.218708, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:32.401015: step 4399, loss 0.247394, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:32.885505: step 4400, loss 0.31545, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:24:34.241092: step 4400, loss 0.224665, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4400

2017-10-10T13:24:35.654221: step 4401, loss 0.177734, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:36.160956: step 4402, loss 0.115097, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:36.649620: step 4403, loss 0.14221, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:37.261020: step 4404, loss 0.138708, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:37.764915: step 4405, loss 0.111608, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:38.356859: step 4406, loss 0.236964, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:38.881070: step 4407, loss 0.182222, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:39.488687: step 4408, loss 0.184794, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:40.020825: step 4409, loss 0.272163, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:40.541645: step 4410, loss 0.325409, acc 0.901961, learning_rate 0.0001
2017-10-10T13:24:41.091447: step 4411, loss 0.21209, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:41.649119: step 4412, loss 0.13639, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:42.156927: step 4413, loss 0.163808, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:42.642870: step 4414, loss 0.263655, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:43.200506: step 4415, loss 0.210535, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:43.705107: step 4416, loss 0.23703, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:44.223821: step 4417, loss 0.146525, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:44.776542: step 4418, loss 0.242175, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:45.244892: step 4419, loss 0.268685, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:45.629129: step 4420, loss 0.131436, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:46.044967: step 4421, loss 0.252246, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:46.624965: step 4422, loss 0.152505, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:47.244967: step 4423, loss 0.361654, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:47.680034: step 4424, loss 0.165604, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:48.069356: step 4425, loss 0.130957, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:48.500292: step 4426, loss 0.170827, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:49.011327: step 4427, loss 0.169378, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:49.467369: step 4428, loss 0.140286, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:50.056916: step 4429, loss 0.265859, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:50.700825: step 4430, loss 0.237137, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:51.111121: step 4431, loss 0.216737, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:51.563628: step 4432, loss 0.174294, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:52.109137: step 4433, loss 0.174748, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:52.665512: step 4434, loss 0.132489, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:53.191909: step 4435, loss 0.222808, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:53.666195: step 4436, loss 0.199262, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:54.221150: step 4437, loss 0.246529, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:54.788337: step 4438, loss 0.111968, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:55.309581: step 4439, loss 0.284809, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:55.816283: step 4440, loss 0.191113, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:24:57.182416: step 4440, loss 0.225936, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4440

2017-10-10T13:24:58.797145: step 4441, loss 0.116032, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:59.401205: step 4442, loss 0.136414, acc 0.984375, learning_rate 0.0001
2017-10-10T13:24:59.936122: step 4443, loss 0.136153, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:00.485045: step 4444, loss 0.241372, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:01.047487: step 4445, loss 0.207354, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:01.590990: step 4446, loss 0.178712, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:02.101001: step 4447, loss 0.393336, acc 0.859375, learning_rate 0.0001
2017-10-10T13:25:02.632890: step 4448, loss 0.204209, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:03.156856: step 4449, loss 0.176852, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:03.595426: step 4450, loss 0.153599, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:04.184941: step 4451, loss 0.124927, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:04.745223: step 4452, loss 0.151357, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:05.223978: step 4453, loss 0.112143, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:05.784878: step 4454, loss 0.190983, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:06.342925: step 4455, loss 0.251641, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:06.821349: step 4456, loss 0.147637, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:07.310256: step 4457, loss 0.256469, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:07.856817: step 4458, loss 0.156828, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:08.421703: step 4459, loss 0.264756, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:08.842110: step 4460, loss 0.107821, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:09.274531: step 4461, loss 0.312609, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:09.801159: step 4462, loss 0.17495, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:10.424879: step 4463, loss 0.0876641, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:10.880415: step 4464, loss 0.17646, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:11.317843: step 4465, loss 0.0846186, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:11.748966: step 4466, loss 0.18784, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:12.190399: step 4467, loss 0.0953303, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:12.754604: step 4468, loss 0.300808, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:13.252824: step 4469, loss 0.178136, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:13.741667: step 4470, loss 0.199511, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:14.210356: step 4471, loss 0.0997386, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:14.716922: step 4472, loss 0.239012, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:15.222371: step 4473, loss 0.102476, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:15.831345: step 4474, loss 0.183203, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:16.313987: step 4475, loss 0.108401, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:16.816896: step 4476, loss 0.115182, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:17.344917: step 4477, loss 0.188533, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:17.865810: step 4478, loss 0.27294, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:18.330063: step 4479, loss 0.168441, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:18.801035: step 4480, loss 0.179022, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:25:20.191967: step 4480, loss 0.224224, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4480

2017-10-10T13:25:21.893609: step 4481, loss 0.245889, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:22.450331: step 4482, loss 0.243447, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:22.969413: step 4483, loss 0.183131, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:23.552106: step 4484, loss 0.270317, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:24.172909: step 4485, loss 0.166234, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:24.774642: step 4486, loss 0.280025, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:25.319234: step 4487, loss 0.226144, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:25.878558: step 4488, loss 0.185196, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:26.424029: step 4489, loss 0.22154, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:26.965022: step 4490, loss 0.178489, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:27.516862: step 4491, loss 0.237831, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:28.068913: step 4492, loss 0.224358, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:28.617167: step 4493, loss 0.246824, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:29.140777: step 4494, loss 0.152365, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:29.734502: step 4495, loss 0.186867, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:30.256871: step 4496, loss 0.123311, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:30.780921: step 4497, loss 0.175206, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:31.400431: step 4498, loss 0.170454, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:31.960407: step 4499, loss 0.16696, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:32.410101: step 4500, loss 0.143611, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:32.876843: step 4501, loss 0.259977, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:33.481268: step 4502, loss 0.202305, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:33.992858: step 4503, loss 0.243609, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:34.419832: step 4504, loss 0.285224, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:34.871809: step 4505, loss 0.048482, acc 1, learning_rate 0.0001
2017-10-10T13:25:35.389882: step 4506, loss 0.292952, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:35.869980: step 4507, loss 0.286664, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:36.352859: step 4508, loss 0.0698983, acc 0.980392, learning_rate 0.0001
2017-10-10T13:25:36.744851: step 4509, loss 0.149302, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:37.197311: step 4510, loss 0.218937, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:37.673486: step 4511, loss 0.192057, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:38.192907: step 4512, loss 0.0539992, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:38.749547: step 4513, loss 0.222161, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:39.284970: step 4514, loss 0.163126, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:39.826896: step 4515, loss 0.106503, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:40.328948: step 4516, loss 0.197503, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:40.899891: step 4517, loss 0.268331, acc 0.875, learning_rate 0.0001
2017-10-10T13:25:41.467841: step 4518, loss 0.133962, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:42.009893: step 4519, loss 0.139523, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:42.578243: step 4520, loss 0.129636, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:25:44.077699: step 4520, loss 0.225517, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4520

2017-10-10T13:25:45.780867: step 4521, loss 0.168249, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:46.287226: step 4522, loss 0.208602, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:46.780670: step 4523, loss 0.18416, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:47.369717: step 4524, loss 0.0566173, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:47.847744: step 4525, loss 0.280294, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:48.368943: step 4526, loss 0.281723, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:48.917003: step 4527, loss 0.227268, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:49.449060: step 4528, loss 0.143436, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:49.984933: step 4529, loss 0.236295, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:50.486928: step 4530, loss 0.168801, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:50.933173: step 4531, loss 0.169651, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:51.401538: step 4532, loss 0.0933826, acc 1, learning_rate 0.0001
2017-10-10T13:25:51.860964: step 4533, loss 0.242267, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:52.394388: step 4534, loss 0.229191, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:52.948516: step 4535, loss 0.172918, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:53.484266: step 4536, loss 0.27756, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:54.076685: step 4537, loss 0.0943078, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:54.587576: step 4538, loss 0.139985, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:55.132887: step 4539, loss 0.205923, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:55.584881: step 4540, loss 0.191821, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:56.121813: step 4541, loss 0.128103, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:56.627968: step 4542, loss 0.183745, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:57.092916: step 4543, loss 0.116585, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:57.543483: step 4544, loss 0.2508, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:58.036869: step 4545, loss 0.209011, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:58.528845: step 4546, loss 0.0853515, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:59.110431: step 4547, loss 0.276378, acc 0.875, learning_rate 0.0001
2017-10-10T13:25:59.679742: step 4548, loss 0.125127, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:00.156265: step 4549, loss 0.0902828, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:00.608669: step 4550, loss 0.166939, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:01.080471: step 4551, loss 0.103783, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:01.604854: step 4552, loss 0.245955, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:02.213016: step 4553, loss 0.205168, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:02.811947: step 4554, loss 0.183579, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:03.353649: step 4555, loss 0.241982, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:03.859059: step 4556, loss 0.186823, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:04.316477: step 4557, loss 0.257151, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:04.827661: step 4558, loss 0.191081, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:05.369094: step 4559, loss 0.193917, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:05.886783: step 4560, loss 0.296598, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:26:07.302472: step 4560, loss 0.222947, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4560

2017-10-10T13:26:08.737933: step 4561, loss 0.197373, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:09.303509: step 4562, loss 0.230122, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:09.872972: step 4563, loss 0.23873, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:10.436957: step 4564, loss 0.230605, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:11.007396: step 4565, loss 0.116447, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:11.620937: step 4566, loss 0.139895, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:12.122654: step 4567, loss 0.179061, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:12.601688: step 4568, loss 0.158803, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:13.143664: step 4569, loss 0.200714, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:13.666282: step 4570, loss 0.148784, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:14.137069: step 4571, loss 0.237062, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:14.630464: step 4572, loss 0.160255, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:15.184624: step 4573, loss 0.108817, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:15.778921: step 4574, loss 0.219914, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:16.212927: step 4575, loss 0.24836, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:16.701102: step 4576, loss 0.174216, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:17.278887: step 4577, loss 0.208663, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:17.825075: step 4578, loss 0.270511, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:18.305943: step 4579, loss 0.203094, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:18.779458: step 4580, loss 0.14115, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:19.297005: step 4581, loss 0.144267, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:19.804940: step 4582, loss 0.224898, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:20.295224: step 4583, loss 0.212194, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:20.762920: step 4584, loss 0.19864, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:21.194142: step 4585, loss 0.193084, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:21.690747: step 4586, loss 0.110204, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:22.316861: step 4587, loss 0.172756, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:22.788893: step 4588, loss 0.173592, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:23.276941: step 4589, loss 0.206757, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:23.700899: step 4590, loss 0.0786012, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:24.265037: step 4591, loss 0.141725, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:24.728882: step 4592, loss 0.131694, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:25.292861: step 4593, loss 0.191003, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:25.798624: step 4594, loss 0.167286, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:26.344971: step 4595, loss 0.210345, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:26.890817: step 4596, loss 0.340902, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:27.453002: step 4597, loss 0.210376, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:27.954416: step 4598, loss 0.0958717, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:28.510446: step 4599, loss 0.282825, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:29.052999: step 4600, loss 0.192562, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:26:30.559696: step 4600, loss 0.222564, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4600

2017-10-10T13:26:32.215513: step 4601, loss 0.226149, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:32.760883: step 4602, loss 0.182571, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:33.284776: step 4603, loss 0.15475, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:33.777760: step 4604, loss 0.127239, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:34.328855: step 4605, loss 0.142177, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:34.827429: step 4606, loss 0.133144, acc 0.980392, learning_rate 0.0001
2017-10-10T13:26:35.351848: step 4607, loss 0.160659, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:35.868926: step 4608, loss 0.0448219, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:36.397111: step 4609, loss 0.273479, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:36.832838: step 4610, loss 0.22165, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:37.334825: step 4611, loss 0.264469, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:37.840394: step 4612, loss 0.158958, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:38.385031: step 4613, loss 0.0447401, acc 1, learning_rate 0.0001
2017-10-10T13:26:38.968399: step 4614, loss 0.211986, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:39.528873: step 4615, loss 0.185221, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:40.028880: step 4616, loss 0.188927, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:40.572830: step 4617, loss 0.109726, acc 0.984375, learning_rate 0.0001
2017-10-10T13:26:41.146108: step 4618, loss 0.204902, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:41.603554: step 4619, loss 0.156044, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:42.072580: step 4620, loss 0.199883, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:42.600958: step 4621, loss 0.14743, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:43.274923: step 4622, loss 0.294932, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:43.668591: step 4623, loss 0.147616, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:44.104877: step 4624, loss 0.230919, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:44.744835: step 4625, loss 0.143561, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:45.264609: step 4626, loss 0.306156, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:45.704876: step 4627, loss 0.082691, acc 1, learning_rate 0.0001
2017-10-10T13:26:46.168815: step 4628, loss 0.115551, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:46.686362: step 4629, loss 0.157958, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:47.287784: step 4630, loss 0.131268, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:47.906887: step 4631, loss 0.231141, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:48.453082: step 4632, loss 0.154733, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:49.019291: step 4633, loss 0.216478, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:49.553689: step 4634, loss 0.300515, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:50.074898: step 4635, loss 0.160673, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:50.635500: step 4636, loss 0.16123, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:51.140584: step 4637, loss 0.159744, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:51.640793: step 4638, loss 0.31687, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:52.172720: step 4639, loss 0.16793, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:52.677468: step 4640, loss 0.141119, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:26:53.980930: step 4640, loss 0.221243, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4640

2017-10-10T13:26:55.818111: step 4641, loss 0.416733, acc 0.828125, learning_rate 0.0001
2017-10-10T13:26:56.272929: step 4642, loss 0.16583, acc 0.96875, learning_rate 0.0001
2017-10-10T13:26:56.795465: step 4643, loss 0.159312, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:57.305676: step 4644, loss 0.104606, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:57.796305: step 4645, loss 0.27171, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:58.268430: step 4646, loss 0.127206, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:58.780974: step 4647, loss 0.203214, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:59.292931: step 4648, loss 0.150402, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:59.830325: step 4649, loss 0.131445, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:00.408964: step 4650, loss 0.139442, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:00.935127: step 4651, loss 0.260024, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:01.424887: step 4652, loss 0.128611, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:01.992171: step 4653, loss 0.101685, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:02.476849: step 4654, loss 0.102684, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:02.982772: step 4655, loss 0.157769, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:03.428955: step 4656, loss 0.0576895, acc 1, learning_rate 0.0001
2017-10-10T13:27:03.900769: step 4657, loss 0.289813, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:04.382899: step 4658, loss 0.157629, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:04.816884: step 4659, loss 0.278397, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:05.312933: step 4660, loss 0.061385, acc 1, learning_rate 0.0001
2017-10-10T13:27:05.781037: step 4661, loss 0.183906, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:06.277963: step 4662, loss 0.256019, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:06.732180: step 4663, loss 0.272023, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:07.232507: step 4664, loss 0.281642, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:07.856878: step 4665, loss 0.0997829, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:08.389217: step 4666, loss 0.205055, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:08.909016: step 4667, loss 0.283583, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:09.272770: step 4668, loss 0.189508, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:09.685366: step 4669, loss 0.298928, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:10.212844: step 4670, loss 0.133502, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:10.792995: step 4671, loss 0.221483, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:11.371603: step 4672, loss 0.168459, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:11.900800: step 4673, loss 0.320613, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:12.450824: step 4674, loss 0.186498, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:12.948275: step 4675, loss 0.122308, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:13.462858: step 4676, loss 0.154725, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:14.013863: step 4677, loss 0.153771, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:14.556288: step 4678, loss 0.177478, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:15.093262: step 4679, loss 0.16114, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:15.641562: step 4680, loss 0.235705, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:27:17.116705: step 4680, loss 0.22303, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4680

2017-10-10T13:27:18.463079: step 4681, loss 0.12866, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:18.996695: step 4682, loss 0.267902, acc 0.859375, learning_rate 0.0001
2017-10-10T13:27:19.429023: step 4683, loss 0.192115, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:20.063870: step 4684, loss 0.135763, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:20.513019: step 4685, loss 0.161227, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:21.068032: step 4686, loss 0.120034, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:21.604972: step 4687, loss 0.1755, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:22.102169: step 4688, loss 0.208072, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:22.658249: step 4689, loss 0.195896, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:23.269189: step 4690, loss 0.192252, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:23.803166: step 4691, loss 0.12573, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:24.352844: step 4692, loss 0.09305, acc 1, learning_rate 0.0001
2017-10-10T13:27:24.940911: step 4693, loss 0.216099, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:25.474968: step 4694, loss 0.142996, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:26.033086: step 4695, loss 0.168016, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:26.603385: step 4696, loss 0.202529, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:27.087903: step 4697, loss 0.206204, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:27.545497: step 4698, loss 0.229771, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:28.072900: step 4699, loss 0.180872, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:28.681130: step 4700, loss 0.120247, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:29.144972: step 4701, loss 0.120056, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:29.656883: step 4702, loss 0.139847, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:30.149569: step 4703, loss 0.156011, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:30.645109: step 4704, loss 0.317733, acc 0.921569, learning_rate 0.0001
2017-10-10T13:27:31.217134: step 4705, loss 0.388185, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:31.764935: step 4706, loss 0.202219, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:32.202324: step 4707, loss 0.172629, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:32.559701: step 4708, loss 0.200656, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:33.068896: step 4709, loss 0.252422, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:33.560881: step 4710, loss 0.2079, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:34.120839: step 4711, loss 0.184449, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:34.652201: step 4712, loss 0.226185, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:35.144957: step 4713, loss 0.122541, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:35.659029: step 4714, loss 0.17743, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:36.236433: step 4715, loss 0.227052, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:36.809009: step 4716, loss 0.116002, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:37.401080: step 4717, loss 0.163129, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:37.925480: step 4718, loss 0.125597, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:38.553096: step 4719, loss 0.16968, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:39.109120: step 4720, loss 0.208533, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:27:40.414318: step 4720, loss 0.223292, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4720

2017-10-10T13:27:42.055710: step 4721, loss 0.180475, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:42.580852: step 4722, loss 0.227497, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:43.005012: step 4723, loss 0.150889, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:43.531364: step 4724, loss 0.147121, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:44.087752: step 4725, loss 0.131726, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:44.689008: step 4726, loss 0.107369, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:45.252885: step 4727, loss 0.182978, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:45.812867: step 4728, loss 0.121335, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:46.356913: step 4729, loss 0.169753, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:46.916998: step 4730, loss 0.2856, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:47.497156: step 4731, loss 0.128279, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:48.031910: step 4732, loss 0.260151, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:48.561168: step 4733, loss 0.158207, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:49.112917: step 4734, loss 0.184009, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:49.644908: step 4735, loss 0.261561, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:50.156780: step 4736, loss 0.23635, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:50.672876: step 4737, loss 0.249093, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:51.220919: step 4738, loss 0.113457, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:51.778275: step 4739, loss 0.226893, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:52.357671: step 4740, loss 0.136486, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:52.806703: step 4741, loss 0.0613295, acc 1, learning_rate 0.0001
2017-10-10T13:27:53.291283: step 4742, loss 0.178958, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:53.843501: step 4743, loss 0.138303, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:54.390486: step 4744, loss 0.340637, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:54.838956: step 4745, loss 0.196648, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:55.278981: step 4746, loss 0.411425, acc 0.84375, learning_rate 0.0001
2017-10-10T13:27:55.836243: step 4747, loss 0.206113, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:56.346224: step 4748, loss 0.0751707, acc 0.984375, learning_rate 0.0001
2017-10-10T13:27:56.931001: step 4749, loss 0.153477, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:57.444273: step 4750, loss 0.114018, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:57.980910: step 4751, loss 0.210214, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:58.557289: step 4752, loss 0.269738, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:59.178656: step 4753, loss 0.240303, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:59.694731: step 4754, loss 0.255971, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:00.244223: step 4755, loss 0.18839, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:00.732868: step 4756, loss 0.261328, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:01.278223: step 4757, loss 0.113964, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:01.789629: step 4758, loss 0.151109, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:02.325046: step 4759, loss 0.213878, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:02.833140: step 4760, loss 0.185868, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:28:04.201873: step 4760, loss 0.222726, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4760

2017-10-10T13:28:06.073537: step 4761, loss 0.183057, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:06.616922: step 4762, loss 0.0844854, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:07.180622: step 4763, loss 0.202926, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:07.761049: step 4764, loss 0.111509, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:08.284974: step 4765, loss 0.201745, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:08.856928: step 4766, loss 0.171955, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:09.384869: step 4767, loss 0.230268, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:09.883153: step 4768, loss 0.295003, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:10.368477: step 4769, loss 0.358074, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:10.930632: step 4770, loss 0.204472, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:11.533902: step 4771, loss 0.213421, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:12.009105: step 4772, loss 0.17914, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:12.552903: step 4773, loss 0.260093, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:13.136719: step 4774, loss 0.137904, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:13.551333: step 4775, loss 0.0991767, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:13.992599: step 4776, loss 0.201312, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:14.445288: step 4777, loss 0.269864, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:14.941064: step 4778, loss 0.187907, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:15.317936: step 4779, loss 0.145776, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:15.741184: step 4780, loss 0.191906, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:16.261057: step 4781, loss 0.104205, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:16.816859: step 4782, loss 0.276839, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:17.396825: step 4783, loss 0.145415, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:17.812718: step 4784, loss 0.113471, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:18.328870: step 4785, loss 0.23634, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:18.810724: step 4786, loss 0.118033, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:19.361077: step 4787, loss 0.0846265, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:19.897754: step 4788, loss 0.0811662, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:20.457421: step 4789, loss 0.11361, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:21.032824: step 4790, loss 0.187209, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:21.582750: step 4791, loss 0.0859579, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:22.117793: step 4792, loss 0.16942, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:22.619152: step 4793, loss 0.13111, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:23.146509: step 4794, loss 0.194204, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:23.681136: step 4795, loss 0.253797, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:24.213127: step 4796, loss 0.10563, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:24.878818: step 4797, loss 0.179934, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:25.403759: step 4798, loss 0.112385, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:25.948830: step 4799, loss 0.0924473, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:26.484859: step 4800, loss 0.254612, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:28:27.720800: step 4800, loss 0.22038, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4800

2017-10-10T13:28:29.180797: step 4801, loss 0.207113, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:29.636882: step 4802, loss 0.145854, acc 0.960784, learning_rate 0.0001
2017-10-10T13:28:30.190895: step 4803, loss 0.133881, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:30.758658: step 4804, loss 0.310991, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:31.336853: step 4805, loss 0.35574, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:31.861292: step 4806, loss 0.110969, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:32.404980: step 4807, loss 0.146551, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:32.994327: step 4808, loss 0.137848, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:33.537018: step 4809, loss 0.579693, acc 0.84375, learning_rate 0.0001
2017-10-10T13:28:34.061063: step 4810, loss 0.190985, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:34.585323: step 4811, loss 0.141379, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:35.098183: step 4812, loss 0.242062, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:35.656991: step 4813, loss 0.217877, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:36.203219: step 4814, loss 0.161617, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:36.658557: step 4815, loss 0.108724, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:37.140832: step 4816, loss 0.260795, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:37.739833: step 4817, loss 0.19085, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:38.227676: step 4818, loss 0.116855, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:38.600169: step 4819, loss 0.260601, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:39.011763: step 4820, loss 0.146473, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:39.557585: step 4821, loss 0.0921376, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:40.112875: step 4822, loss 0.131702, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:40.699836: step 4823, loss 0.222879, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:41.152013: step 4824, loss 0.176541, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:41.604247: step 4825, loss 0.180592, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:42.160737: step 4826, loss 0.0976743, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:42.716341: step 4827, loss 0.129277, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:43.246376: step 4828, loss 0.100136, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:43.787810: step 4829, loss 0.158981, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:44.345554: step 4830, loss 0.173533, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:44.880974: step 4831, loss 0.20645, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:45.417779: step 4832, loss 0.088513, acc 0.984375, learning_rate 0.0001
2017-10-10T13:28:45.925970: step 4833, loss 0.228129, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:46.477821: step 4834, loss 0.202085, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:47.033832: step 4835, loss 0.18399, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:47.595932: step 4836, loss 0.234224, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:48.160119: step 4837, loss 0.129567, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:48.717793: step 4838, loss 0.123178, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:49.258130: step 4839, loss 0.187138, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:49.788959: step 4840, loss 0.17741, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:28:51.036827: step 4840, loss 0.221046, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4840

2017-10-10T13:28:52.581044: step 4841, loss 0.160635, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:53.133125: step 4842, loss 0.101489, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:53.656911: step 4843, loss 0.139226, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:54.170050: step 4844, loss 0.188636, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:54.736843: step 4845, loss 0.149085, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:55.259204: step 4846, loss 0.152482, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:55.836898: step 4847, loss 0.308012, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:56.396286: step 4848, loss 0.185796, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:56.902499: step 4849, loss 0.230718, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:57.429145: step 4850, loss 0.207147, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:57.993115: step 4851, loss 0.253561, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:58.496812: step 4852, loss 0.125846, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:59.134962: step 4853, loss 0.207632, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:59.580095: step 4854, loss 0.131965, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:00.090304: step 4855, loss 0.143788, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:00.592846: step 4856, loss 0.161261, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:01.161823: step 4857, loss 0.195399, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:01.712866: step 4858, loss 0.202587, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:02.160895: step 4859, loss 0.0936143, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:02.605012: step 4860, loss 0.294232, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:03.198827: step 4861, loss 0.183451, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:03.728961: step 4862, loss 0.12781, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:04.285243: step 4863, loss 0.0987754, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:04.771706: step 4864, loss 0.174356, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:05.228074: step 4865, loss 0.216787, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:05.740507: step 4866, loss 0.195435, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:06.239006: step 4867, loss 0.296525, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:06.804851: step 4868, loss 0.135328, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:07.349902: step 4869, loss 0.0485129, acc 1, learning_rate 0.0001
2017-10-10T13:29:07.876839: step 4870, loss 0.243297, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:08.404419: step 4871, loss 0.128909, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:08.948965: step 4872, loss 0.117193, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:09.544613: step 4873, loss 0.179892, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:10.074204: step 4874, loss 0.286779, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:10.589089: step 4875, loss 0.187449, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:11.136971: step 4876, loss 0.257824, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:11.664871: step 4877, loss 0.0693208, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:12.192305: step 4878, loss 0.0813779, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:12.748826: step 4879, loss 0.0428099, acc 1, learning_rate 0.0001
2017-10-10T13:29:13.296429: step 4880, loss 0.29635, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:29:14.820840: step 4880, loss 0.221446, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4880

2017-10-10T13:29:16.556936: step 4881, loss 0.0663819, acc 1, learning_rate 0.0001
2017-10-10T13:29:17.116856: step 4882, loss 0.309843, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:17.624954: step 4883, loss 0.250223, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:18.109451: step 4884, loss 0.237379, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:18.580393: step 4885, loss 0.167989, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:19.091407: step 4886, loss 0.23693, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:19.609552: step 4887, loss 0.198355, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:20.170917: step 4888, loss 0.170612, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:20.756821: step 4889, loss 0.259659, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:21.249037: step 4890, loss 0.141198, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:21.779547: step 4891, loss 0.133476, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:22.256921: step 4892, loss 0.141318, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:22.878280: step 4893, loss 0.227683, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:23.353099: step 4894, loss 0.227359, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:23.896902: step 4895, loss 0.127362, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:24.372507: step 4896, loss 0.117656, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:24.706818: step 4897, loss 0.233589, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:25.282450: step 4898, loss 0.116725, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:25.764882: step 4899, loss 0.202813, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:26.312841: step 4900, loss 0.242369, acc 0.921569, learning_rate 0.0001
2017-10-10T13:29:26.891908: step 4901, loss 0.0989831, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:27.366110: step 4902, loss 0.103876, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:27.820848: step 4903, loss 0.240658, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:28.293763: step 4904, loss 0.0459708, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:28.823222: step 4905, loss 0.16745, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:29.372479: step 4906, loss 0.112499, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:29.930420: step 4907, loss 0.283398, acc 0.84375, learning_rate 0.0001
2017-10-10T13:29:30.470743: step 4908, loss 0.209648, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:31.006415: step 4909, loss 0.23937, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:31.537020: step 4910, loss 0.227281, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:32.068806: step 4911, loss 0.143959, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:32.621060: step 4912, loss 0.136742, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:33.245327: step 4913, loss 0.210448, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:33.739948: step 4914, loss 0.107744, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:34.283093: step 4915, loss 0.192648, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:34.797882: step 4916, loss 0.283851, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:35.282609: step 4917, loss 0.152496, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:35.802567: step 4918, loss 0.0632712, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:36.375393: step 4919, loss 0.125236, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:36.873021: step 4920, loss 0.196955, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:29:38.308857: step 4920, loss 0.222107, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4920

2017-10-10T13:29:39.730276: step 4921, loss 0.0842814, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:40.212926: step 4922, loss 0.195727, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:40.725896: step 4923, loss 0.240992, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:41.277342: step 4924, loss 0.120521, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:41.827949: step 4925, loss 0.159674, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:42.363825: step 4926, loss 0.10807, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:42.928958: step 4927, loss 0.109621, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:43.488871: step 4928, loss 0.190676, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:44.033562: step 4929, loss 0.268168, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:44.560811: step 4930, loss 0.217419, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:45.102760: step 4931, loss 0.331511, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:45.640839: step 4932, loss 0.296871, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:46.168727: step 4933, loss 0.130727, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:46.649141: step 4934, loss 0.10099, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:47.115858: step 4935, loss 0.179831, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:47.583744: step 4936, loss 0.168912, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:48.080943: step 4937, loss 0.172107, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:48.529059: step 4938, loss 0.163901, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:48.984980: step 4939, loss 0.113401, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:49.536525: step 4940, loss 0.189063, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:50.107004: step 4941, loss 0.227731, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:50.597093: step 4942, loss 0.189447, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:51.012616: step 4943, loss 0.166901, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:51.440615: step 4944, loss 0.265867, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:51.999974: step 4945, loss 0.257067, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:52.550073: step 4946, loss 0.243191, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:53.072485: step 4947, loss 0.155361, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:53.632433: step 4948, loss 0.183364, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:54.199414: step 4949, loss 0.104261, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:54.748826: step 4950, loss 0.207857, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:55.296347: step 4951, loss 0.313472, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:55.816475: step 4952, loss 0.213315, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:56.356395: step 4953, loss 0.21134, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:56.856880: step 4954, loss 0.160239, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:57.445004: step 4955, loss 0.11962, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:57.938415: step 4956, loss 0.200874, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:58.439326: step 4957, loss 0.0756527, acc 0.984375, learning_rate 0.0001
2017-10-10T13:29:58.960936: step 4958, loss 0.156441, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:59.435129: step 4959, loss 0.278063, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:59.936154: step 4960, loss 0.117481, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:30:01.284910: step 4960, loss 0.220302, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-4960

2017-10-10T13:30:02.853864: step 4961, loss 0.157777, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:03.420847: step 4962, loss 0.196833, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:03.992931: step 4963, loss 0.141506, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:04.540920: step 4964, loss 0.0741847, acc 1, learning_rate 0.0001
2017-10-10T13:30:05.089153: step 4965, loss 0.164725, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:05.619359: step 4966, loss 0.112762, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:06.136543: step 4967, loss 0.167724, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:06.639608: step 4968, loss 0.181059, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:07.128935: step 4969, loss 0.180007, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:07.701145: step 4970, loss 0.239379, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:08.148954: step 4971, loss 0.183803, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:08.716949: step 4972, loss 0.181984, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:09.273172: step 4973, loss 0.1348, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:09.797958: step 4974, loss 0.259596, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:10.300829: step 4975, loss 0.288328, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:10.815358: step 4976, loss 0.135875, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:11.362696: step 4977, loss 0.173253, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:11.823900: step 4978, loss 0.153706, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:12.270830: step 4979, loss 0.266546, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:12.698097: step 4980, loss 0.193546, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:13.245667: step 4981, loss 0.182218, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:13.860856: step 4982, loss 0.155891, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:14.302752: step 4983, loss 0.18654, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:14.743993: step 4984, loss 0.144375, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:15.256911: step 4985, loss 0.169531, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:15.781150: step 4986, loss 0.118269, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:16.331239: step 4987, loss 0.124755, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:16.848903: step 4988, loss 0.0849098, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:17.405102: step 4989, loss 0.181033, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:17.932906: step 4990, loss 0.432299, acc 0.84375, learning_rate 0.0001
2017-10-10T13:30:18.440977: step 4991, loss 0.15705, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:18.952802: step 4992, loss 0.251382, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:19.576529: step 4993, loss 0.0982139, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:20.121072: step 4994, loss 0.239942, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:20.675535: step 4995, loss 0.186085, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:21.247861: step 4996, loss 0.207889, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:21.798881: step 4997, loss 0.153176, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:22.287439: step 4998, loss 0.163951, acc 0.960784, learning_rate 0.0001
2017-10-10T13:30:22.788932: step 4999, loss 0.165079, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:23.331360: step 5000, loss 0.190641, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:30:24.700927: step 5000, loss 0.220428, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5000

2017-10-10T13:30:26.445014: step 5001, loss 0.152107, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:27.073033: step 5002, loss 0.178717, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:27.642760: step 5003, loss 0.13759, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:28.155803: step 5004, loss 0.238459, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:28.724857: step 5005, loss 0.12323, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:29.265025: step 5006, loss 0.166217, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:29.800569: step 5007, loss 0.164189, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:30.361639: step 5008, loss 0.260537, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:30.945131: step 5009, loss 0.125015, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:31.512839: step 5010, loss 0.137187, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:32.088862: step 5011, loss 0.0858905, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:32.701060: step 5012, loss 0.134636, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:33.141449: step 5013, loss 0.0945235, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:33.632505: step 5014, loss 0.145191, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:34.161248: step 5015, loss 0.103562, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:34.675835: step 5016, loss 0.185592, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:35.100183: step 5017, loss 0.214195, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:35.519648: step 5018, loss 0.14848, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:36.071181: step 5019, loss 0.244392, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:36.567640: step 5020, loss 0.225137, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:36.964961: step 5021, loss 0.294617, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:37.464913: step 5022, loss 0.210739, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:37.885108: step 5023, loss 0.130829, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:38.379059: step 5024, loss 0.187604, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:38.928399: step 5025, loss 0.177332, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:39.492851: step 5026, loss 0.16355, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:40.037222: step 5027, loss 0.177333, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:40.591796: step 5028, loss 0.118174, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:41.124839: step 5029, loss 0.226043, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:41.620780: step 5030, loss 0.227052, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:42.175384: step 5031, loss 0.240375, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:42.646615: step 5032, loss 0.0771222, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:43.132943: step 5033, loss 0.197331, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:43.573116: step 5034, loss 0.313899, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:43.998403: step 5035, loss 0.112876, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:44.476216: step 5036, loss 0.138019, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:45.004880: step 5037, loss 0.118692, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:45.537568: step 5038, loss 0.232133, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:46.081306: step 5039, loss 0.170324, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:46.623472: step 5040, loss 0.219905, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:30:47.881965: step 5040, loss 0.218127, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5040

2017-10-10T13:30:49.377028: step 5041, loss 0.122359, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:49.984899: step 5042, loss 0.160947, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:50.478385: step 5043, loss 0.113835, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:50.984457: step 5044, loss 0.113547, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:51.535762: step 5045, loss 0.157276, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:52.089549: step 5046, loss 0.166527, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:52.664724: step 5047, loss 0.17154, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:53.216843: step 5048, loss 0.178964, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:53.798272: step 5049, loss 0.136252, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:54.338655: step 5050, loss 0.168426, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:54.914644: step 5051, loss 0.268663, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:55.412469: step 5052, loss 0.163026, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:55.868914: step 5053, loss 0.225194, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:56.396859: step 5054, loss 0.099169, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:56.940893: step 5055, loss 0.137772, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:57.436077: step 5056, loss 0.149855, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:57.797045: step 5057, loss 0.0941985, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:58.275716: step 5058, loss 0.0859403, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:58.696884: step 5059, loss 0.183056, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:59.196848: step 5060, loss 0.212957, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:59.770930: step 5061, loss 0.123985, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:00.309885: step 5062, loss 0.13266, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:00.672949: step 5063, loss 0.148923, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:01.221144: step 5064, loss 0.173631, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:01.810120: step 5065, loss 0.238864, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:02.272842: step 5066, loss 0.226641, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:02.744865: step 5067, loss 0.184651, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:03.443014: step 5068, loss 0.0944281, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:03.984533: step 5069, loss 0.146923, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:04.520853: step 5070, loss 0.265279, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:05.052527: step 5071, loss 0.278567, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:05.551679: step 5072, loss 0.124394, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:06.060888: step 5073, loss 0.176006, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:06.591417: step 5074, loss 0.334417, acc 0.875, learning_rate 0.0001
2017-10-10T13:31:07.126117: step 5075, loss 0.205232, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:07.663234: step 5076, loss 0.265915, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:08.189113: step 5077, loss 0.141487, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:08.628103: step 5078, loss 0.11496, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:09.173441: step 5079, loss 0.133011, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:09.720863: step 5080, loss 0.168593, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:31:10.985180: step 5080, loss 0.219193, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5080

2017-10-10T13:31:12.620339: step 5081, loss 0.161258, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:13.161054: step 5082, loss 0.116777, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:13.672292: step 5083, loss 0.206655, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:14.152567: step 5084, loss 0.161132, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:14.636811: step 5085, loss 0.265782, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:15.202493: step 5086, loss 0.149122, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:15.766767: step 5087, loss 0.191222, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:16.324991: step 5088, loss 0.151327, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:16.880929: step 5089, loss 0.167846, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:17.385117: step 5090, loss 0.275754, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:17.947692: step 5091, loss 0.140545, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:18.481717: step 5092, loss 0.175836, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:18.984531: step 5093, loss 0.243755, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:19.456869: step 5094, loss 0.124382, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:20.054910: step 5095, loss 0.190732, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:20.549710: step 5096, loss 0.100167, acc 0.980392, learning_rate 0.0001
2017-10-10T13:31:20.993001: step 5097, loss 0.117856, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:21.348722: step 5098, loss 0.168205, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:21.652356: step 5099, loss 0.137357, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:22.072071: step 5100, loss 0.180961, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:22.636848: step 5101, loss 0.0763201, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:23.185052: step 5102, loss 0.0939946, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:23.601066: step 5103, loss 0.209993, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:24.114159: step 5104, loss 0.105208, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:24.441035: step 5105, loss 0.282171, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:24.957143: step 5106, loss 0.26104, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:25.529389: step 5107, loss 0.195203, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:26.042327: step 5108, loss 0.200633, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:26.585354: step 5109, loss 0.113916, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:27.102982: step 5110, loss 0.215825, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:27.661795: step 5111, loss 0.231669, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:28.149088: step 5112, loss 0.0901279, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:28.640955: step 5113, loss 0.260621, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:29.169035: step 5114, loss 0.203606, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:29.696915: step 5115, loss 0.111937, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:30.197061: step 5116, loss 0.145139, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:30.721184: step 5117, loss 0.132973, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:31.264945: step 5118, loss 0.168546, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:31.789067: step 5119, loss 0.183624, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:32.298904: step 5120, loss 0.265839, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:31:33.512970: step 5120, loss 0.21896, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5120

2017-10-10T13:31:35.138566: step 5121, loss 0.114604, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:35.664860: step 5122, loss 0.224727, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:36.195319: step 5123, loss 0.0987911, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:36.773888: step 5124, loss 0.209192, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:37.363029: step 5125, loss 0.272203, acc 0.875, learning_rate 0.0001
2017-10-10T13:31:37.896979: step 5126, loss 0.347245, acc 0.859375, learning_rate 0.0001
2017-10-10T13:31:38.405159: step 5127, loss 0.118542, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:38.949119: step 5128, loss 0.157481, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:39.509145: step 5129, loss 0.228434, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:40.037131: step 5130, loss 0.177556, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:40.642499: step 5131, loss 0.121065, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:41.177042: step 5132, loss 0.303345, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:41.777994: step 5133, loss 0.143866, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:42.266765: step 5134, loss 0.11019, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:42.736910: step 5135, loss 0.253792, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:43.311709: step 5136, loss 0.155802, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:43.967545: step 5137, loss 0.191929, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:44.424841: step 5138, loss 0.224974, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:44.791922: step 5139, loss 0.131414, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:45.267157: step 5140, loss 0.10751, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:45.780980: step 5141, loss 0.155776, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:46.256456: step 5142, loss 0.0938942, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:46.688920: step 5143, loss 0.185354, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:47.088814: step 5144, loss 0.0780648, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:47.660881: step 5145, loss 0.133832, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:48.219963: step 5146, loss 0.162701, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:48.757252: step 5147, loss 0.254252, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:49.200917: step 5148, loss 0.189264, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:49.733530: step 5149, loss 0.129021, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:50.294831: step 5150, loss 0.134533, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:50.887138: step 5151, loss 0.135152, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:51.398213: step 5152, loss 0.170051, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:51.962921: step 5153, loss 0.141622, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:52.502489: step 5154, loss 0.126574, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:53.031248: step 5155, loss 0.168725, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:53.539800: step 5156, loss 0.209012, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:54.070309: step 5157, loss 0.172178, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:54.521576: step 5158, loss 0.142644, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:55.036811: step 5159, loss 0.204868, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:55.555363: step 5160, loss 0.127539, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:31:56.811829: step 5160, loss 0.220767, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5160

2017-10-10T13:31:58.692073: step 5161, loss 0.117439, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:59.253285: step 5162, loss 0.118051, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:59.821009: step 5163, loss 0.0859012, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:00.414700: step 5164, loss 0.177994, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:00.960826: step 5165, loss 0.130184, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:01.537838: step 5166, loss 0.0693998, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:02.091704: step 5167, loss 0.0941464, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:02.644433: step 5168, loss 0.136935, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:03.193248: step 5169, loss 0.141115, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:03.735580: step 5170, loss 0.18574, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:04.308883: step 5171, loss 0.274647, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:04.800217: step 5172, loss 0.0913047, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:05.300859: step 5173, loss 0.246444, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:05.836899: step 5174, loss 0.176087, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:06.256992: step 5175, loss 0.152627, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:06.869033: step 5176, loss 0.0817858, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:07.435650: step 5177, loss 0.2832, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:07.819017: step 5178, loss 0.106711, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:08.093846: step 5179, loss 0.160712, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:08.645610: step 5180, loss 0.151563, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:09.172867: step 5181, loss 0.203956, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:09.625541: step 5182, loss 0.0846573, acc 1, learning_rate 0.0001
2017-10-10T13:32:10.044951: step 5183, loss 0.174589, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:10.500992: step 5184, loss 0.160366, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:11.005247: step 5185, loss 0.216366, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:11.484916: step 5186, loss 0.340764, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:12.052092: step 5187, loss 0.190474, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:12.580652: step 5188, loss 0.0606931, acc 1, learning_rate 0.0001
2017-10-10T13:32:13.096957: step 5189, loss 0.121623, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:13.596857: step 5190, loss 0.325077, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:14.122837: step 5191, loss 0.133283, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:14.579297: step 5192, loss 0.17264, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:15.163023: step 5193, loss 0.127535, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:15.548046: step 5194, loss 0.193128, acc 0.960784, learning_rate 0.0001
2017-10-10T13:32:16.069064: step 5195, loss 0.146075, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:16.585461: step 5196, loss 0.216996, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:17.124993: step 5197, loss 0.118636, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:17.690682: step 5198, loss 0.216663, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:18.272948: step 5199, loss 0.333123, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:18.780135: step 5200, loss 0.126293, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:32:19.912342: step 5200, loss 0.219139, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5200

2017-10-10T13:32:21.277123: step 5201, loss 0.117782, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:21.798017: step 5202, loss 0.16257, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:22.365263: step 5203, loss 0.123831, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:22.940997: step 5204, loss 0.29628, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:23.516689: step 5205, loss 0.0984046, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:24.032903: step 5206, loss 0.287191, acc 0.875, learning_rate 0.0001
2017-10-10T13:32:24.509102: step 5207, loss 0.0827882, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:25.077016: step 5208, loss 0.200636, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:25.527092: step 5209, loss 0.21706, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:26.025471: step 5210, loss 0.164964, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:26.613015: step 5211, loss 0.168252, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:27.176956: step 5212, loss 0.125211, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:27.657189: step 5213, loss 0.188567, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:28.251810: step 5214, loss 0.326017, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:28.804930: step 5215, loss 0.175062, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:29.295864: step 5216, loss 0.264438, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:29.900075: step 5217, loss 0.146781, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:30.259115: step 5218, loss 0.231914, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:30.491517: step 5219, loss 0.143898, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:30.924884: step 5220, loss 0.138669, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:31.263712: step 5221, loss 0.155933, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:31.459569: step 5222, loss 0.120473, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:31.634585: step 5223, loss 0.149074, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:31.818318: step 5224, loss 0.0961405, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:32.040770: step 5225, loss 0.104941, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:32.658087: step 5226, loss 0.228894, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:32.981304: step 5227, loss 0.191241, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:33.561876: step 5228, loss 0.146101, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:33.872681: step 5229, loss 0.28142, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:34.328853: step 5230, loss 0.164894, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:34.866329: step 5231, loss 0.124259, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:35.411232: step 5232, loss 0.21526, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:35.993119: step 5233, loss 0.131278, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:36.524532: step 5234, loss 0.275183, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:37.090658: step 5235, loss 0.243429, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:37.649148: step 5236, loss 0.112883, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:38.178886: step 5237, loss 0.161196, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:38.703388: step 5238, loss 0.0982655, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:39.187810: step 5239, loss 0.158283, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:39.675728: step 5240, loss 0.121811, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:32:40.708412: step 5240, loss 0.217726, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5240

2017-10-10T13:32:42.237636: step 5241, loss 0.14346, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:42.725025: step 5242, loss 0.159666, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:43.237015: step 5243, loss 0.129315, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:43.770026: step 5244, loss 0.132088, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:44.269024: step 5245, loss 0.273282, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:44.733226: step 5246, loss 0.145829, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:45.276509: step 5247, loss 0.145149, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:45.781020: step 5248, loss 0.136838, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:46.287551: step 5249, loss 0.0812103, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:46.788964: step 5250, loss 0.121826, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:47.377039: step 5251, loss 0.165814, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:47.874136: step 5252, loss 0.119377, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:48.419830: step 5253, loss 0.198513, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:49.015248: step 5254, loss 0.10938, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:49.581857: step 5255, loss 0.152331, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:50.130507: step 5256, loss 0.266026, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:50.608848: step 5257, loss 0.0985544, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:51.100429: step 5258, loss 0.137925, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:51.565026: step 5259, loss 0.0844209, acc 0.984375, learning_rate 0.0001
2017-10-10T13:32:52.140888: step 5260, loss 0.0975583, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:52.570122: step 5261, loss 0.102797, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:53.101096: step 5262, loss 0.151129, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:53.639353: step 5263, loss 0.184637, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:54.049050: step 5264, loss 0.233686, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:54.515384: step 5265, loss 0.126728, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:55.021036: step 5266, loss 0.187203, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:55.588715: step 5267, loss 0.118114, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:56.050627: step 5268, loss 0.13206, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:56.480895: step 5269, loss 0.14848, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:57.040901: step 5270, loss 0.318283, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:57.614371: step 5271, loss 0.311283, acc 0.875, learning_rate 0.0001
2017-10-10T13:32:57.908514: step 5272, loss 0.284887, acc 0.84375, learning_rate 0.0001
2017-10-10T13:32:58.392996: step 5273, loss 0.182199, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:58.781053: step 5274, loss 0.116154, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:59.291098: step 5275, loss 0.207905, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:59.932244: step 5276, loss 0.284099, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:00.464517: step 5277, loss 0.264159, acc 0.859375, learning_rate 0.0001
2017-10-10T13:33:00.945872: step 5278, loss 0.161829, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:01.469099: step 5279, loss 0.23656, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:01.984957: step 5280, loss 0.191954, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:33:03.122809: step 5280, loss 0.219037, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5280

2017-10-10T13:33:04.879029: step 5281, loss 0.13304, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:05.404891: step 5282, loss 0.165117, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:05.923886: step 5283, loss 0.152688, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:06.479989: step 5284, loss 0.17494, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:07.051171: step 5285, loss 0.175794, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:07.615416: step 5286, loss 0.112381, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:08.128924: step 5287, loss 0.186415, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:08.617004: step 5288, loss 0.204802, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:09.161211: step 5289, loss 0.122075, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:09.760983: step 5290, loss 0.0685873, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:10.255402: step 5291, loss 0.0615459, acc 1, learning_rate 0.0001
2017-10-10T13:33:10.721948: step 5292, loss 0.285044, acc 0.941176, learning_rate 0.0001
2017-10-10T13:33:11.297438: step 5293, loss 0.349411, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:11.742596: step 5294, loss 0.145016, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:12.290309: step 5295, loss 0.0851988, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:12.804084: step 5296, loss 0.0770229, acc 1, learning_rate 0.0001
2017-10-10T13:33:13.331495: step 5297, loss 0.18395, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:13.881010: step 5298, loss 0.139058, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:14.466309: step 5299, loss 0.12513, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:15.032532: step 5300, loss 0.155177, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:15.594408: step 5301, loss 0.118357, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:16.132912: step 5302, loss 0.217007, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:16.706790: step 5303, loss 0.259984, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:17.156621: step 5304, loss 0.145845, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:17.704970: step 5305, loss 0.158126, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:18.380904: step 5306, loss 0.185014, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:18.799165: step 5307, loss 0.162134, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:19.220711: step 5308, loss 0.103444, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:19.672609: step 5309, loss 0.268385, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:20.243765: step 5310, loss 0.125907, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:20.861049: step 5311, loss 0.0959988, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:21.294783: step 5312, loss 0.245086, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:21.813895: step 5313, loss 0.171969, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:22.328936: step 5314, loss 0.21818, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:22.889414: step 5315, loss 0.0831833, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:23.432589: step 5316, loss 0.0967418, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:24.004945: step 5317, loss 0.201241, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:24.505159: step 5318, loss 0.171394, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:25.073137: step 5319, loss 0.175865, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:25.560516: step 5320, loss 0.0747494, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:33:26.751263: step 5320, loss 0.219035, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5320

2017-10-10T13:33:28.308872: step 5321, loss 0.230301, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:28.805098: step 5322, loss 0.333181, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:29.303517: step 5323, loss 0.262882, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:29.791331: step 5324, loss 0.120939, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:30.292874: step 5325, loss 0.132509, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:30.756964: step 5326, loss 0.0950629, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:31.284844: step 5327, loss 0.147424, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:31.820426: step 5328, loss 0.101512, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:32.359327: step 5329, loss 0.202552, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:32.897038: step 5330, loss 0.24249, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:33.453513: step 5331, loss 0.0665056, acc 1, learning_rate 0.0001
2017-10-10T13:33:33.981964: step 5332, loss 0.104263, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:34.504590: step 5333, loss 0.16465, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:34.994253: step 5334, loss 0.113886, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:35.453196: step 5335, loss 0.205665, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:35.841101: step 5336, loss 0.24801, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:36.374120: step 5337, loss 0.273909, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:36.896923: step 5338, loss 0.223264, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:37.494700: step 5339, loss 0.119223, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:38.000872: step 5340, loss 0.219195, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:38.532878: step 5341, loss 0.296942, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:39.235527: step 5342, loss 0.313529, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:39.731454: step 5343, loss 0.40162, acc 0.859375, learning_rate 0.0001
2017-10-10T13:33:40.188804: step 5344, loss 0.141646, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:40.680912: step 5345, loss 0.32909, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:41.272874: step 5346, loss 0.253157, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:41.867653: step 5347, loss 0.159885, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:42.328824: step 5348, loss 0.134729, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:42.828167: step 5349, loss 0.0602219, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:43.446670: step 5350, loss 0.274983, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:44.017015: step 5351, loss 0.256562, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:44.349161: step 5352, loss 0.295301, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:44.768971: step 5353, loss 0.13956, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:45.245314: step 5354, loss 0.132011, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:45.833075: step 5355, loss 0.190454, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:46.337105: step 5356, loss 0.212215, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:46.769187: step 5357, loss 0.177088, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:47.357329: step 5358, loss 0.266373, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:48.014869: step 5359, loss 0.137209, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:48.537102: step 5360, loss 0.27086, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:33:49.605288: step 5360, loss 0.217198, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5360

2017-10-10T13:33:51.285949: step 5361, loss 0.166997, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:51.835593: step 5362, loss 0.165344, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:52.388876: step 5363, loss 0.153769, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:53.040852: step 5364, loss 0.203428, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:53.594450: step 5365, loss 0.219256, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:54.156870: step 5366, loss 0.125088, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:54.633037: step 5367, loss 0.128386, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:55.160938: step 5368, loss 0.28312, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:55.737172: step 5369, loss 0.149822, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:56.286331: step 5370, loss 0.312313, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:56.804985: step 5371, loss 0.0878197, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:57.373205: step 5372, loss 0.184658, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:57.928846: step 5373, loss 0.261313, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:58.488947: step 5374, loss 0.18666, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:58.980989: step 5375, loss 0.0993018, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:59.544851: step 5376, loss 0.110522, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:00.088834: step 5377, loss 0.115232, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:00.618277: step 5378, loss 0.100769, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:01.128918: step 5379, loss 0.229187, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:01.748422: step 5380, loss 0.161082, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:02.316849: step 5381, loss 0.282006, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:02.786102: step 5382, loss 0.0898232, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:03.246083: step 5383, loss 0.223716, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:03.786748: step 5384, loss 0.205612, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:04.388461: step 5385, loss 0.116741, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:04.819330: step 5386, loss 0.262463, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:05.252472: step 5387, loss 0.166283, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:05.763556: step 5388, loss 0.166541, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:06.213059: step 5389, loss 0.0694491, acc 1, learning_rate 0.0001
2017-10-10T13:34:06.636982: step 5390, loss 0.125291, acc 0.960784, learning_rate 0.0001
2017-10-10T13:34:07.249401: step 5391, loss 0.088907, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:07.636864: step 5392, loss 0.204471, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:08.108248: step 5393, loss 0.0894569, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:08.641147: step 5394, loss 0.151215, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:09.156859: step 5395, loss 0.172155, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:09.620668: step 5396, loss 0.154151, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:10.113073: step 5397, loss 0.135941, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:10.621348: step 5398, loss 0.150494, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:11.166791: step 5399, loss 0.177468, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:11.718378: step 5400, loss 0.146526, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:34:12.829964: step 5400, loss 0.219135, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5400

2017-10-10T13:34:14.557709: step 5401, loss 0.173811, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:15.096528: step 5402, loss 0.146072, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:15.617054: step 5403, loss 0.154814, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:16.121023: step 5404, loss 0.164745, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:16.652397: step 5405, loss 0.107514, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:17.189126: step 5406, loss 0.180427, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:17.717026: step 5407, loss 0.119157, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:18.320948: step 5408, loss 0.15435, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:18.869114: step 5409, loss 0.134472, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:19.387212: step 5410, loss 0.176523, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:19.956208: step 5411, loss 0.206856, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:20.496858: step 5412, loss 0.255731, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:21.076948: step 5413, loss 0.266528, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:21.640890: step 5414, loss 0.189536, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:22.145240: step 5415, loss 0.102337, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:22.773185: step 5416, loss 0.0873711, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:23.349066: step 5417, loss 0.161662, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:23.853864: step 5418, loss 0.118883, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:24.500867: step 5419, loss 0.115902, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:25.040854: step 5420, loss 0.125891, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:25.471724: step 5421, loss 0.0572638, acc 1, learning_rate 0.0001
2017-10-10T13:34:25.920893: step 5422, loss 0.188322, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:26.440967: step 5423, loss 0.161264, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:27.050186: step 5424, loss 0.163778, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:27.673475: step 5425, loss 0.227591, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:28.131125: step 5426, loss 0.104489, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:28.548867: step 5427, loss 0.200034, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:29.001024: step 5428, loss 0.218401, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:29.504938: step 5429, loss 0.189153, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:30.057083: step 5430, loss 0.137125, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:30.482599: step 5431, loss 0.0702547, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:30.915100: step 5432, loss 0.178027, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:31.375741: step 5433, loss 0.237602, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:31.912587: step 5434, loss 0.144015, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:32.430962: step 5435, loss 0.144245, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:32.916981: step 5436, loss 0.268637, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:33.453178: step 5437, loss 0.13641, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:33.962646: step 5438, loss 0.114794, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:34.509210: step 5439, loss 0.157289, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:35.004995: step 5440, loss 0.153165, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:34:36.321214: step 5440, loss 0.215661, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5440

2017-10-10T13:34:37.727828: step 5441, loss 0.113065, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:38.212897: step 5442, loss 0.109286, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:38.665525: step 5443, loss 0.161809, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:39.234045: step 5444, loss 0.12313, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:39.867934: step 5445, loss 0.095426, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:40.400859: step 5446, loss 0.170968, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:40.880914: step 5447, loss 0.0921765, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:41.436851: step 5448, loss 0.339947, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:41.920904: step 5449, loss 0.155802, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:42.425107: step 5450, loss 0.128994, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:42.900974: step 5451, loss 0.107307, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:43.433115: step 5452, loss 0.240698, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:43.940966: step 5453, loss 0.0812486, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:44.437070: step 5454, loss 0.188136, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:44.989033: step 5455, loss 0.245602, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:45.578128: step 5456, loss 0.257085, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:46.096281: step 5457, loss 0.324278, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:46.644868: step 5458, loss 0.265674, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:47.220862: step 5459, loss 0.134584, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:47.765358: step 5460, loss 0.186299, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:48.196828: step 5461, loss 0.364778, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:48.608876: step 5462, loss 0.17804, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:49.140827: step 5463, loss 0.221281, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:49.596792: step 5464, loss 0.0802758, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:50.136063: step 5465, loss 0.147185, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:50.800868: step 5466, loss 0.106519, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:51.288412: step 5467, loss 0.265744, acc 0.859375, learning_rate 0.0001
2017-10-10T13:34:51.741989: step 5468, loss 0.1104, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:52.289041: step 5469, loss 0.230202, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:52.884833: step 5470, loss 0.145747, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:53.312367: step 5471, loss 0.145439, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:53.726977: step 5472, loss 0.24224, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:54.181950: step 5473, loss 0.248467, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:54.786823: step 5474, loss 0.158474, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:55.308600: step 5475, loss 0.215389, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:55.910477: step 5476, loss 0.154455, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:56.418374: step 5477, loss 0.0869703, acc 0.984375, learning_rate 0.0001
2017-10-10T13:34:56.975145: step 5478, loss 0.125118, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:57.520944: step 5479, loss 0.206931, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:58.050852: step 5480, loss 0.225308, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:34:59.169695: step 5480, loss 0.216902, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5480

2017-10-10T13:35:00.785770: step 5481, loss 0.133333, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:01.356920: step 5482, loss 0.0932986, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:01.835988: step 5483, loss 0.140805, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:02.360503: step 5484, loss 0.108935, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:02.929146: step 5485, loss 0.136495, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:03.436987: step 5486, loss 0.15614, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:03.925833: step 5487, loss 0.137227, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:04.363668: step 5488, loss 0.240739, acc 0.980392, learning_rate 0.0001
2017-10-10T13:35:04.832067: step 5489, loss 0.205934, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:05.340947: step 5490, loss 0.276938, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:05.915938: step 5491, loss 0.175754, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:06.339701: step 5492, loss 0.094522, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:06.872353: step 5493, loss 0.238616, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:07.365186: step 5494, loss 0.178008, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:07.879803: step 5495, loss 0.180782, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:08.452974: step 5496, loss 0.242411, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:08.997157: step 5497, loss 0.102532, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:09.517095: step 5498, loss 0.205556, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:10.032876: step 5499, loss 0.171997, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:10.538593: step 5500, loss 0.145585, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:10.970833: step 5501, loss 0.325735, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:11.427817: step 5502, loss 0.0580432, acc 1, learning_rate 0.0001
2017-10-10T13:35:11.911634: step 5503, loss 0.237622, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:12.486875: step 5504, loss 0.173834, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:13.002976: step 5505, loss 0.116529, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:13.617619: step 5506, loss 0.298243, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:14.040904: step 5507, loss 0.214189, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:14.484614: step 5508, loss 0.113896, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:14.860867: step 5509, loss 0.120712, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:15.412701: step 5510, loss 0.213385, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:16.036961: step 5511, loss 0.230056, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:16.472864: step 5512, loss 0.271087, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:16.970641: step 5513, loss 0.200733, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:17.332675: step 5514, loss 0.168125, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:17.830673: step 5515, loss 0.220035, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:18.396868: step 5516, loss 0.288078, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:18.891404: step 5517, loss 0.105691, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:19.432201: step 5518, loss 0.0720434, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:20.004836: step 5519, loss 0.152037, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:20.499527: step 5520, loss 0.147932, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:35:21.728914: step 5520, loss 0.21691, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5520

2017-10-10T13:35:23.250601: step 5521, loss 0.148485, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:23.781239: step 5522, loss 0.167818, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:24.313182: step 5523, loss 0.149433, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:24.793921: step 5524, loss 0.0970004, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:25.299679: step 5525, loss 0.102072, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:25.793236: step 5526, loss 0.167853, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:26.361226: step 5527, loss 0.121438, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:26.927506: step 5528, loss 0.251127, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:27.512942: step 5529, loss 0.296912, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:28.069272: step 5530, loss 0.179895, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:28.617001: step 5531, loss 0.125781, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:29.168982: step 5532, loss 0.0850762, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:29.704974: step 5533, loss 0.268081, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:30.270762: step 5534, loss 0.0851704, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:30.792944: step 5535, loss 0.198098, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:31.309738: step 5536, loss 0.126026, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:31.921065: step 5537, loss 0.0949756, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:32.536832: step 5538, loss 0.198435, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:33.152603: step 5539, loss 0.0856246, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:33.562974: step 5540, loss 0.158481, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:34.044966: step 5541, loss 0.127345, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:34.550795: step 5542, loss 0.151394, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:35.112826: step 5543, loss 0.130778, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:35.681800: step 5544, loss 0.217314, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:36.204373: step 5545, loss 0.129038, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:36.804873: step 5546, loss 0.240141, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:37.366445: step 5547, loss 0.136099, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:37.820601: step 5548, loss 0.105134, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:38.299455: step 5549, loss 0.297927, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:38.806218: step 5550, loss 0.193787, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:39.414889: step 5551, loss 0.22771, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:39.831065: step 5552, loss 0.245817, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:40.296930: step 5553, loss 0.143665, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:40.812972: step 5554, loss 0.0722052, acc 1, learning_rate 0.0001
2017-10-10T13:35:41.324552: step 5555, loss 0.100088, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:41.880881: step 5556, loss 0.135534, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:42.359278: step 5557, loss 0.109153, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:42.829666: step 5558, loss 0.128069, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:43.392936: step 5559, loss 0.218553, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:43.960880: step 5560, loss 0.111329, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:35:45.146984: step 5560, loss 0.217481, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5560

2017-10-10T13:35:46.981050: step 5561, loss 0.143998, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:47.520395: step 5562, loss 0.322969, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:47.988087: step 5563, loss 0.150043, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:48.536928: step 5564, loss 0.161045, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:49.181074: step 5565, loss 0.259197, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:49.743555: step 5566, loss 0.13162, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:50.180280: step 5567, loss 0.219242, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:50.722915: step 5568, loss 0.126862, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:51.212985: step 5569, loss 0.213995, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:51.777112: step 5570, loss 0.122563, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:52.338633: step 5571, loss 0.200738, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:52.835064: step 5572, loss 0.115711, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:53.336941: step 5573, loss 0.109591, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:53.905214: step 5574, loss 0.186449, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:54.476862: step 5575, loss 0.10045, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:55.025051: step 5576, loss 0.14496, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:55.553673: step 5577, loss 0.203131, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:56.021676: step 5578, loss 0.189429, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:56.442406: step 5579, loss 0.186936, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:56.905241: step 5580, loss 0.158864, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:57.354775: step 5581, loss 0.19877, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:57.920916: step 5582, loss 0.144964, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:58.438134: step 5583, loss 0.0615511, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:58.943860: step 5584, loss 0.0927591, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:59.390478: step 5585, loss 0.0837102, acc 0.984375, learning_rate 0.0001
2017-10-10T13:35:59.851998: step 5586, loss 0.204533, acc 0.882353, learning_rate 0.0001
2017-10-10T13:36:00.397269: step 5587, loss 0.181157, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:00.840920: step 5588, loss 0.186967, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:01.267473: step 5589, loss 0.091946, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:01.945103: step 5590, loss 0.141384, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:02.323620: step 5591, loss 0.182744, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:02.747705: step 5592, loss 0.138514, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:03.302612: step 5593, loss 0.166577, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:03.841933: step 5594, loss 0.104949, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:04.364843: step 5595, loss 0.157505, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:04.833026: step 5596, loss 0.11363, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:05.380142: step 5597, loss 0.0919513, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:05.933249: step 5598, loss 0.260793, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:06.444237: step 5599, loss 0.11671, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:06.903906: step 5600, loss 0.275406, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:36:07.944233: step 5600, loss 0.21654, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5600

2017-10-10T13:36:09.362773: step 5601, loss 0.135709, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:09.821564: step 5602, loss 0.214863, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:10.415935: step 5603, loss 0.233371, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:10.969052: step 5604, loss 0.072513, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:11.504547: step 5605, loss 0.146442, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:12.055837: step 5606, loss 0.293411, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:12.600841: step 5607, loss 0.12997, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:13.138074: step 5608, loss 0.253008, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:13.668234: step 5609, loss 0.229851, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:14.218947: step 5610, loss 0.221939, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:14.757724: step 5611, loss 0.150162, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:15.327250: step 5612, loss 0.0674137, acc 1, learning_rate 0.0001
2017-10-10T13:36:15.918521: step 5613, loss 0.228576, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:16.444208: step 5614, loss 0.198186, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:16.977259: step 5615, loss 0.0530118, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:17.501516: step 5616, loss 0.120016, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:18.076943: step 5617, loss 0.156017, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:18.656956: step 5618, loss 0.197237, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:19.184851: step 5619, loss 0.202405, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:19.771226: step 5620, loss 0.141863, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:20.218946: step 5621, loss 0.126038, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:20.688980: step 5622, loss 0.135225, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:21.121507: step 5623, loss 0.139846, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:21.678127: step 5624, loss 0.220665, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:22.225785: step 5625, loss 0.136186, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:22.796379: step 5626, loss 0.206123, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:23.399606: step 5627, loss 0.104054, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:23.892964: step 5628, loss 0.162265, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:24.408871: step 5629, loss 0.417672, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:24.891209: step 5630, loss 0.087119, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:25.352891: step 5631, loss 0.22268, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:25.818087: step 5632, loss 0.194887, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:26.376964: step 5633, loss 0.0732213, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:26.897670: step 5634, loss 0.119987, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:27.451298: step 5635, loss 0.130671, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:27.991406: step 5636, loss 0.0631983, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:28.500862: step 5637, loss 0.0922085, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:29.001675: step 5638, loss 0.170434, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:29.492216: step 5639, loss 0.103816, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:30.056903: step 5640, loss 0.260438, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:36:31.151036: step 5640, loss 0.215281, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5640

2017-10-10T13:36:32.701772: step 5641, loss 0.146809, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:33.252565: step 5642, loss 0.0566097, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:33.797175: step 5643, loss 0.0740002, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:34.349758: step 5644, loss 0.224953, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:34.897418: step 5645, loss 0.176476, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:35.458342: step 5646, loss 0.188006, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:36.005632: step 5647, loss 0.180429, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:36.564973: step 5648, loss 0.120243, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:37.123922: step 5649, loss 0.115519, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:37.693685: step 5650, loss 0.126705, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:38.244430: step 5651, loss 0.0744385, acc 1, learning_rate 0.0001
2017-10-10T13:36:38.794326: step 5652, loss 0.157686, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:39.350986: step 5653, loss 0.133346, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:39.926177: step 5654, loss 0.183978, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:40.544899: step 5655, loss 0.162839, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:41.060415: step 5656, loss 0.22832, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:41.596989: step 5657, loss 0.121211, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:42.136831: step 5658, loss 0.240343, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:42.670753: step 5659, loss 0.136789, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:43.308608: step 5660, loss 0.273313, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:43.809015: step 5661, loss 0.100165, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:44.206356: step 5662, loss 0.137868, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:44.675150: step 5663, loss 0.137347, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:45.216928: step 5664, loss 0.194902, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:45.766271: step 5665, loss 0.163595, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:46.476201: step 5666, loss 0.23675, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:46.890544: step 5667, loss 0.173113, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:47.261067: step 5668, loss 0.0992179, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:47.800972: step 5669, loss 0.145485, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:48.417567: step 5670, loss 0.189637, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:48.890342: step 5671, loss 0.0963847, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:49.314993: step 5672, loss 0.255923, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:49.849390: step 5673, loss 0.0991674, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:50.408963: step 5674, loss 0.140682, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:50.960864: step 5675, loss 0.11527, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:51.525086: step 5676, loss 0.143681, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:52.012953: step 5677, loss 0.243049, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:52.531293: step 5678, loss 0.141487, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:53.148970: step 5679, loss 0.112981, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:53.709075: step 5680, loss 0.187273, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:36:54.857051: step 5680, loss 0.215195, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5680

2017-10-10T13:36:56.640843: step 5681, loss 0.168925, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:57.132946: step 5682, loss 0.0847332, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:57.683903: step 5683, loss 0.137445, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:58.156696: step 5684, loss 0.116334, acc 0.960784, learning_rate 0.0001
2017-10-10T13:36:58.709035: step 5685, loss 0.160718, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:59.256942: step 5686, loss 0.213384, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:59.724942: step 5687, loss 0.193064, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:00.209209: step 5688, loss 0.183315, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:00.742929: step 5689, loss 0.279888, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:01.228818: step 5690, loss 0.178821, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:01.737182: step 5691, loss 0.0861838, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:02.284846: step 5692, loss 0.120684, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:02.824897: step 5693, loss 0.139131, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:03.401311: step 5694, loss 0.0822768, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:03.948226: step 5695, loss 0.183435, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:04.513024: step 5696, loss 0.13175, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:04.946845: step 5697, loss 0.163942, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:05.532866: step 5698, loss 0.166422, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:06.173867: step 5699, loss 0.171289, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:06.666231: step 5700, loss 0.20911, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:07.152952: step 5701, loss 0.271153, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:07.570530: step 5702, loss 0.184406, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:08.129030: step 5703, loss 0.221834, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:08.668859: step 5704, loss 0.0839264, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:09.300991: step 5705, loss 0.127728, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:09.844865: step 5706, loss 0.170643, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:10.345121: step 5707, loss 0.31312, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:10.963062: step 5708, loss 0.0912637, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:11.443315: step 5709, loss 0.222834, acc 0.859375, learning_rate 0.0001
2017-10-10T13:37:11.876960: step 5710, loss 0.222682, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:12.460878: step 5711, loss 0.172479, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:12.993294: step 5712, loss 0.150132, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:13.530206: step 5713, loss 0.195679, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:14.104995: step 5714, loss 0.22662, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:14.594137: step 5715, loss 0.12102, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:15.025210: step 5716, loss 0.164334, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:15.558320: step 5717, loss 0.152382, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:16.001009: step 5718, loss 0.120606, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:16.508920: step 5719, loss 0.206407, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:17.124932: step 5720, loss 0.247836, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:37:18.222292: step 5720, loss 0.21716, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5720

2017-10-10T13:37:19.589144: step 5721, loss 0.164191, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:20.199575: step 5722, loss 0.198613, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:20.690798: step 5723, loss 0.193892, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:21.212882: step 5724, loss 0.31599, acc 0.875, learning_rate 0.0001
2017-10-10T13:37:21.737539: step 5725, loss 0.177321, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:22.256446: step 5726, loss 0.113252, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:22.738132: step 5727, loss 0.0525013, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:23.246021: step 5728, loss 0.113663, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:23.800654: step 5729, loss 0.138464, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:24.268853: step 5730, loss 0.154711, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:24.838105: step 5731, loss 0.243983, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:25.392854: step 5732, loss 0.0673687, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:25.940861: step 5733, loss 0.215962, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:26.436829: step 5734, loss 0.100248, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:26.950762: step 5735, loss 0.0898963, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:27.456945: step 5736, loss 0.200796, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:28.007756: step 5737, loss 0.0618707, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:28.532902: step 5738, loss 0.172934, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:29.129009: step 5739, loss 0.15312, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:29.624927: step 5740, loss 0.149563, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:30.066342: step 5741, loss 0.113128, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:30.556711: step 5742, loss 0.205039, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:31.062710: step 5743, loss 0.136641, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:31.635577: step 5744, loss 0.325336, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:32.188523: step 5745, loss 0.173014, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:32.617987: step 5746, loss 0.0487805, acc 1, learning_rate 0.0001
2017-10-10T13:37:33.112878: step 5747, loss 0.24418, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:33.672964: step 5748, loss 0.283554, acc 0.875, learning_rate 0.0001
2017-10-10T13:37:34.139599: step 5749, loss 0.153766, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:34.583631: step 5750, loss 0.101369, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:35.075698: step 5751, loss 0.22469, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:35.617063: step 5752, loss 0.255491, acc 0.875, learning_rate 0.0001
2017-10-10T13:37:36.131602: step 5753, loss 0.124635, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:36.692833: step 5754, loss 0.101402, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:37.223714: step 5755, loss 0.183791, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:37.845004: step 5756, loss 0.217642, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:38.408980: step 5757, loss 0.160484, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:38.915111: step 5758, loss 0.0738487, acc 1, learning_rate 0.0001
2017-10-10T13:37:39.460993: step 5759, loss 0.266437, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:39.902298: step 5760, loss 0.191722, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:37:41.103242: step 5760, loss 0.216069, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5760

2017-10-10T13:37:42.793981: step 5761, loss 0.133177, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:43.340076: step 5762, loss 0.120612, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:43.893782: step 5763, loss 0.200633, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:44.426911: step 5764, loss 0.252778, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:44.964847: step 5765, loss 0.127863, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:45.502831: step 5766, loss 0.194757, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:46.038925: step 5767, loss 0.244728, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:46.572853: step 5768, loss 0.221359, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:47.093745: step 5769, loss 0.0491953, acc 1, learning_rate 0.0001
2017-10-10T13:37:47.663664: step 5770, loss 0.13447, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:48.193190: step 5771, loss 0.0915282, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:48.749019: step 5772, loss 0.158819, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:49.200868: step 5773, loss 0.141982, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:49.712970: step 5774, loss 0.252721, acc 0.875, learning_rate 0.0001
2017-10-10T13:37:50.268900: step 5775, loss 0.122013, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:50.697017: step 5776, loss 0.127523, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:51.153284: step 5777, loss 0.1113, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:51.575674: step 5778, loss 0.141705, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:52.120768: step 5779, loss 0.0670199, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:52.744743: step 5780, loss 0.131305, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:53.188022: step 5781, loss 0.141241, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:53.593435: step 5782, loss 0.14401, acc 0.960784, learning_rate 0.0001
2017-10-10T13:37:54.049107: step 5783, loss 0.122276, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:54.601364: step 5784, loss 0.17392, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:55.246268: step 5785, loss 0.164722, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:55.696525: step 5786, loss 0.372755, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:56.191800: step 5787, loss 0.0977925, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:56.690993: step 5788, loss 0.16414, acc 0.984375, learning_rate 0.0001
2017-10-10T13:37:57.296981: step 5789, loss 0.215696, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:57.676429: step 5790, loss 0.207943, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:58.130532: step 5791, loss 0.310958, acc 0.859375, learning_rate 0.0001
2017-10-10T13:37:58.624323: step 5792, loss 0.121051, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:59.184708: step 5793, loss 0.121469, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:59.633929: step 5794, loss 0.132295, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:00.187936: step 5795, loss 0.142151, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:00.740874: step 5796, loss 0.102739, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:01.332536: step 5797, loss 0.132194, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:01.824871: step 5798, loss 0.10973, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:02.340826: step 5799, loss 0.184706, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:02.863883: step 5800, loss 0.106147, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:38:04.046661: step 5800, loss 0.216466, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5800

2017-10-10T13:38:05.905000: step 5801, loss 0.100826, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:06.432239: step 5802, loss 0.144726, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:06.973850: step 5803, loss 0.185879, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:07.593031: step 5804, loss 0.0942681, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:08.080617: step 5805, loss 0.111272, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:08.568847: step 5806, loss 0.0981332, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:09.116905: step 5807, loss 0.171493, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:09.669817: step 5808, loss 0.202833, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:10.223009: step 5809, loss 0.242329, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:10.788731: step 5810, loss 0.215565, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:11.338122: step 5811, loss 0.151466, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:11.873973: step 5812, loss 0.0801152, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:12.382508: step 5813, loss 0.203538, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:12.897644: step 5814, loss 0.0709771, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:13.393522: step 5815, loss 0.167554, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:13.904045: step 5816, loss 0.155517, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:14.468629: step 5817, loss 0.0778308, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:15.008839: step 5818, loss 0.184117, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:15.509014: step 5819, loss 0.106149, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:16.130305: step 5820, loss 0.222942, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:16.587755: step 5821, loss 0.094494, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:16.970713: step 5822, loss 0.173877, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:17.496795: step 5823, loss 0.193576, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:18.073024: step 5824, loss 0.175309, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:18.625029: step 5825, loss 0.217664, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:19.112864: step 5826, loss 0.128896, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:19.592856: step 5827, loss 0.10794, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:20.232949: step 5828, loss 0.290316, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:20.805315: step 5829, loss 0.163497, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:21.240075: step 5830, loss 0.107828, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:21.644888: step 5831, loss 0.184585, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:22.190117: step 5832, loss 0.164928, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:22.693111: step 5833, loss 0.234487, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:23.202890: step 5834, loss 0.160313, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:23.629154: step 5835, loss 0.174372, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:24.145136: step 5836, loss 0.297056, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:24.630188: step 5837, loss 0.133651, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:25.173087: step 5838, loss 0.0851771, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:25.596985: step 5839, loss 0.0990507, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:26.170472: step 5840, loss 0.160488, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:38:27.427296: step 5840, loss 0.214331, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5840

2017-10-10T13:38:28.927227: step 5841, loss 0.134122, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:29.469552: step 5842, loss 0.132817, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:29.929293: step 5843, loss 0.205039, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:30.522142: step 5844, loss 0.15468, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:31.095177: step 5845, loss 0.216539, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:31.592547: step 5846, loss 0.0992161, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:32.087457: step 5847, loss 0.153079, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:32.692364: step 5848, loss 0.135719, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:33.239103: step 5849, loss 0.0666233, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:33.804849: step 5850, loss 0.150188, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:34.388995: step 5851, loss 0.221921, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:34.952850: step 5852, loss 0.323117, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:35.457054: step 5853, loss 0.147734, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:36.042488: step 5854, loss 0.159837, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:36.573373: step 5855, loss 0.189353, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:37.131427: step 5856, loss 0.0370222, acc 1, learning_rate 0.0001
2017-10-10T13:38:37.713085: step 5857, loss 0.11571, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:38.292948: step 5858, loss 0.178517, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:38.736565: step 5859, loss 0.137043, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:39.212946: step 5860, loss 0.234558, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:39.712867: step 5861, loss 0.111423, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:40.328820: step 5862, loss 0.0676059, acc 1, learning_rate 0.0001
2017-10-10T13:38:40.824903: step 5863, loss 0.0658479, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:41.436875: step 5864, loss 0.102104, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:41.951370: step 5865, loss 0.0838248, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:42.405062: step 5866, loss 0.188819, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:42.892888: step 5867, loss 0.131681, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:43.387335: step 5868, loss 0.196892, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:43.967246: step 5869, loss 0.103996, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:44.484003: step 5870, loss 0.150283, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:44.890694: step 5871, loss 0.135033, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:45.330699: step 5872, loss 0.182792, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:45.890535: step 5873, loss 0.187186, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:46.452845: step 5874, loss 0.266615, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:47.008418: step 5875, loss 0.156404, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:47.516776: step 5876, loss 0.291249, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:48.001376: step 5877, loss 0.0741518, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:48.570995: step 5878, loss 0.0565393, acc 1, learning_rate 0.0001
2017-10-10T13:38:49.094891: step 5879, loss 0.185769, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:49.536277: step 5880, loss 0.19488, acc 0.921569, learning_rate 0.0001

Evaluation:
2017-10-10T13:38:50.716991: step 5880, loss 0.213922, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5880

2017-10-10T13:38:52.272325: step 5881, loss 0.314375, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:52.764927: step 5882, loss 0.157551, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:53.324997: step 5883, loss 0.123554, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:53.947815: step 5884, loss 0.233922, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:54.462470: step 5885, loss 0.131975, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:54.989792: step 5886, loss 0.190517, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:55.538512: step 5887, loss 0.050934, acc 1, learning_rate 0.0001
2017-10-10T13:38:56.142691: step 5888, loss 0.10657, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:56.670157: step 5889, loss 0.0942386, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:57.192824: step 5890, loss 0.202647, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:57.718385: step 5891, loss 0.099756, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:58.257870: step 5892, loss 0.103911, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:58.762503: step 5893, loss 0.171436, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:59.248940: step 5894, loss 0.0712036, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:59.832823: step 5895, loss 0.0874338, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:00.429568: step 5896, loss 0.181361, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:01.056055: step 5897, loss 0.138285, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:01.492823: step 5898, loss 0.106463, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:01.941980: step 5899, loss 0.101052, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:02.418080: step 5900, loss 0.164227, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:02.956867: step 5901, loss 0.199855, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:03.500510: step 5902, loss 0.221652, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:04.092860: step 5903, loss 0.188884, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:04.682154: step 5904, loss 0.262271, acc 0.875, learning_rate 0.0001
2017-10-10T13:39:05.110479: step 5905, loss 0.214078, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:05.571417: step 5906, loss 0.12769, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:06.093118: step 5907, loss 0.0962979, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:06.660925: step 5908, loss 0.182639, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:07.276889: step 5909, loss 0.100807, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:07.764831: step 5910, loss 0.135097, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:08.208591: step 5911, loss 0.156853, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:08.720907: step 5912, loss 0.127225, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:09.232922: step 5913, loss 0.0591307, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:09.789054: step 5914, loss 0.134324, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:10.349449: step 5915, loss 0.0865316, acc 1, learning_rate 0.0001
2017-10-10T13:39:10.851707: step 5916, loss 0.149386, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:11.396921: step 5917, loss 0.160003, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:11.909046: step 5918, loss 0.176111, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:12.489469: step 5919, loss 0.0964346, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:12.956900: step 5920, loss 0.222881, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:39:14.202795: step 5920, loss 0.213569, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5920

2017-10-10T13:39:15.939221: step 5921, loss 0.1645, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:16.468821: step 5922, loss 0.10194, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:17.009025: step 5923, loss 0.135238, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:17.528856: step 5924, loss 0.148774, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:18.091811: step 5925, loss 0.212143, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:18.627110: step 5926, loss 0.241177, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:19.176929: step 5927, loss 0.133821, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:19.715307: step 5928, loss 0.169318, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:20.251849: step 5929, loss 0.108563, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:20.796509: step 5930, loss 0.41381, acc 0.84375, learning_rate 0.0001
2017-10-10T13:39:21.318290: step 5931, loss 0.138298, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:21.763713: step 5932, loss 0.258344, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:22.201016: step 5933, loss 0.171608, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:22.693122: step 5934, loss 0.10146, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:23.237019: step 5935, loss 0.275372, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:23.867905: step 5936, loss 0.208226, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:24.359070: step 5937, loss 0.190845, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:24.786835: step 5938, loss 0.163582, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:25.283927: step 5939, loss 0.157768, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:25.808892: step 5940, loss 0.117776, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:26.391319: step 5941, loss 0.20149, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:26.932995: step 5942, loss 0.121104, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:27.477997: step 5943, loss 0.211618, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:27.939040: step 5944, loss 0.0895539, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:28.426783: step 5945, loss 0.22726, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:28.862833: step 5946, loss 0.127295, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:29.479862: step 5947, loss 0.106818, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:30.031858: step 5948, loss 0.152349, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:30.446016: step 5949, loss 0.157904, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:30.898719: step 5950, loss 0.12387, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:31.304711: step 5951, loss 0.13448, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:31.843589: step 5952, loss 0.115094, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:32.440286: step 5953, loss 0.0747787, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:32.983174: step 5954, loss 0.123122, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:33.504462: step 5955, loss 0.129694, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:34.011691: step 5956, loss 0.112161, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:34.485044: step 5957, loss 0.216482, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:35.025009: step 5958, loss 0.305556, acc 0.875, learning_rate 0.0001
2017-10-10T13:39:35.532930: step 5959, loss 0.17383, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:36.002974: step 5960, loss 0.0297235, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:39:37.213070: step 5960, loss 0.213054, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-5960

2017-10-10T13:39:38.676885: step 5961, loss 0.0736429, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:39.222018: step 5962, loss 0.170819, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:39.725648: step 5963, loss 0.220564, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:40.282824: step 5964, loss 0.439001, acc 0.84375, learning_rate 0.0001
2017-10-10T13:39:40.772914: step 5965, loss 0.194626, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:41.250636: step 5966, loss 0.138834, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:41.749996: step 5967, loss 0.264092, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:42.248976: step 5968, loss 0.168312, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:42.689169: step 5969, loss 0.1119, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:43.146348: step 5970, loss 0.124142, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:43.744942: step 5971, loss 0.187997, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:44.165083: step 5972, loss 0.233913, acc 0.875, learning_rate 0.0001
2017-10-10T13:39:44.705008: step 5973, loss 0.109283, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:45.257074: step 5974, loss 0.101796, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:45.789338: step 5975, loss 0.225592, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:46.312904: step 5976, loss 0.2321, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:46.957151: step 5977, loss 0.243997, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:47.392154: step 5978, loss 0.147109, acc 0.941176, learning_rate 0.0001
2017-10-10T13:39:47.831530: step 5979, loss 0.162823, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:48.280702: step 5980, loss 0.0837946, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:48.837918: step 5981, loss 0.167917, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:49.418336: step 5982, loss 0.0867731, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:50.038780: step 5983, loss 0.0880273, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:50.625195: step 5984, loss 0.228635, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:51.059284: step 5985, loss 0.160738, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:51.553107: step 5986, loss 0.150913, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:52.180957: step 5987, loss 0.208771, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:52.765216: step 5988, loss 0.160543, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:53.469103: step 5989, loss 0.242709, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:53.977180: step 5990, loss 0.0671714, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:54.385426: step 5991, loss 0.121285, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:54.893505: step 5992, loss 0.087716, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:55.417044: step 5993, loss 0.137011, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:55.980815: step 5994, loss 0.110378, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:56.527051: step 5995, loss 0.315626, acc 0.859375, learning_rate 0.0001
2017-10-10T13:39:57.087570: step 5996, loss 0.189438, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:57.640913: step 5997, loss 0.120918, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:58.209215: step 5998, loss 0.120294, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:58.737032: step 5999, loss 0.1595, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:59.261007: step 6000, loss 0.0856116, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:40:00.465619: step 6000, loss 0.214435, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6000

2017-10-10T13:40:02.078184: step 6001, loss 0.249358, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:02.610185: step 6002, loss 0.128696, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:03.177006: step 6003, loss 0.153738, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:03.752514: step 6004, loss 0.277781, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:04.294518: step 6005, loss 0.113566, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:04.801464: step 6006, loss 0.179463, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:05.260993: step 6007, loss 0.172736, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:05.679650: step 6008, loss 0.141594, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:06.221055: step 6009, loss 0.103006, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:06.716252: step 6010, loss 0.164847, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:07.245132: step 6011, loss 0.322612, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:07.724880: step 6012, loss 0.262652, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:08.274111: step 6013, loss 0.238762, acc 0.875, learning_rate 0.0001
2017-10-10T13:40:08.773539: step 6014, loss 0.108236, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:09.323299: step 6015, loss 0.0814946, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:09.793100: step 6016, loss 0.197685, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:10.341057: step 6017, loss 0.18147, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:10.916841: step 6018, loss 0.121272, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:11.391651: step 6019, loss 0.0631927, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:11.807014: step 6020, loss 0.11796, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:12.377232: step 6021, loss 0.138311, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:12.969092: step 6022, loss 0.146661, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:13.357029: step 6023, loss 0.130468, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:13.800397: step 6024, loss 0.0778164, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:14.257038: step 6025, loss 0.170446, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:14.741211: step 6026, loss 0.179387, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:15.243719: step 6027, loss 0.149051, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:15.765051: step 6028, loss 0.144163, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:16.255723: step 6029, loss 0.125462, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:16.824942: step 6030, loss 0.102878, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:17.413080: step 6031, loss 0.189641, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:17.787063: step 6032, loss 0.155121, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:18.236820: step 6033, loss 0.135634, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:18.736934: step 6034, loss 0.169379, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:19.193815: step 6035, loss 0.0700643, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:19.728903: step 6036, loss 0.185401, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:20.243132: step 6037, loss 0.0917071, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:20.770918: step 6038, loss 0.177759, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:21.304993: step 6039, loss 0.212748, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:21.888973: step 6040, loss 0.253974, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:40:23.062241: step 6040, loss 0.211615, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6040

2017-10-10T13:40:24.864116: step 6041, loss 0.0959631, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:25.384866: step 6042, loss 0.118281, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:25.902853: step 6043, loss 0.167959, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:26.393437: step 6044, loss 0.235757, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:26.924857: step 6045, loss 0.196481, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:27.459183: step 6046, loss 0.129845, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:27.977364: step 6047, loss 0.072936, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:28.529359: step 6048, loss 0.189559, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:29.071753: step 6049, loss 0.118982, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:29.593179: step 6050, loss 0.128462, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:30.102472: step 6051, loss 0.0987208, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:30.792263: step 6052, loss 0.135059, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:31.348862: step 6053, loss 0.29556, acc 0.859375, learning_rate 0.0001
2017-10-10T13:40:31.888824: step 6054, loss 0.206623, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:32.396971: step 6055, loss 0.0578198, acc 1, learning_rate 0.0001
2017-10-10T13:40:32.970717: step 6056, loss 0.197223, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:33.557028: step 6057, loss 0.0643866, acc 1, learning_rate 0.0001
2017-10-10T13:40:34.142744: step 6058, loss 0.162353, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:34.544841: step 6059, loss 0.116837, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:35.021523: step 6060, loss 0.140389, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:35.464520: step 6061, loss 0.108852, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:36.077115: step 6062, loss 0.213054, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:36.612758: step 6063, loss 0.126641, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:37.077377: step 6064, loss 0.218745, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:37.510436: step 6065, loss 0.129882, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:38.040853: step 6066, loss 0.149335, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:38.554502: step 6067, loss 0.111696, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:39.126719: step 6068, loss 0.0975904, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:39.653010: step 6069, loss 0.0434534, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:40.313219: step 6070, loss 0.219727, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:40.613121: step 6071, loss 0.124762, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:41.082458: step 6072, loss 0.477133, acc 0.875, learning_rate 0.0001
2017-10-10T13:40:41.545699: step 6073, loss 0.0763855, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:42.095139: step 6074, loss 0.176704, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:42.637114: step 6075, loss 0.0898817, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:43.135959: step 6076, loss 0.110411, acc 0.960784, learning_rate 0.0001
2017-10-10T13:40:43.630395: step 6077, loss 0.111593, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:44.178368: step 6078, loss 0.142226, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:44.724972: step 6079, loss 0.0674314, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:45.265331: step 6080, loss 0.129119, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:40:46.511403: step 6080, loss 0.212247, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6080

2017-10-10T13:40:47.929592: step 6081, loss 0.115988, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:48.416932: step 6082, loss 0.199116, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:48.968842: step 6083, loss 0.191847, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:49.526609: step 6084, loss 0.199354, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:49.976877: step 6085, loss 0.24097, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:50.488988: step 6086, loss 0.168553, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:51.044925: step 6087, loss 0.262581, acc 0.859375, learning_rate 0.0001
2017-10-10T13:40:51.477504: step 6088, loss 0.0866173, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:51.963047: step 6089, loss 0.0497037, acc 1, learning_rate 0.0001
2017-10-10T13:40:52.426222: step 6090, loss 0.197283, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:52.964936: step 6091, loss 0.106982, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:53.473772: step 6092, loss 0.156471, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:53.941132: step 6093, loss 0.123222, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:54.473048: step 6094, loss 0.254829, acc 0.875, learning_rate 0.0001
2017-10-10T13:40:54.993799: step 6095, loss 0.152195, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:55.473189: step 6096, loss 0.196817, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:56.045236: step 6097, loss 0.121395, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:56.651077: step 6098, loss 0.217166, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:57.049121: step 6099, loss 0.217443, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:57.516113: step 6100, loss 0.0813169, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:57.972994: step 6101, loss 0.124959, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:58.529146: step 6102, loss 0.201551, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:59.057181: step 6103, loss 0.159512, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:59.609152: step 6104, loss 0.113043, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:00.032912: step 6105, loss 0.0688783, acc 1, learning_rate 0.0001
2017-10-10T13:41:00.485038: step 6106, loss 0.0598988, acc 1, learning_rate 0.0001
2017-10-10T13:41:01.048746: step 6107, loss 0.240342, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:01.567889: step 6108, loss 0.0881193, acc 1, learning_rate 0.0001
2017-10-10T13:41:02.050270: step 6109, loss 0.17076, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:02.572871: step 6110, loss 0.212517, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:03.198016: step 6111, loss 0.118648, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:03.800771: step 6112, loss 0.141857, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:04.272842: step 6113, loss 0.0719248, acc 1, learning_rate 0.0001
2017-10-10T13:41:04.828954: step 6114, loss 0.180905, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:05.328876: step 6115, loss 0.162498, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:05.912068: step 6116, loss 0.116858, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:06.398001: step 6117, loss 0.101859, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:06.918893: step 6118, loss 0.24233, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:07.456962: step 6119, loss 0.118676, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:07.995360: step 6120, loss 0.242997, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:41:09.082991: step 6120, loss 0.213481, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6120

2017-10-10T13:41:10.704212: step 6121, loss 0.170413, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:11.266748: step 6122, loss 0.124656, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:11.799318: step 6123, loss 0.150735, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:12.333123: step 6124, loss 0.133139, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:12.859291: step 6125, loss 0.201944, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:13.376933: step 6126, loss 0.233676, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:13.932276: step 6127, loss 0.232012, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:14.509123: step 6128, loss 0.0999134, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:15.037049: step 6129, loss 0.0581031, acc 1, learning_rate 0.0001
2017-10-10T13:41:15.515014: step 6130, loss 0.113308, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:16.022105: step 6131, loss 0.21855, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:16.517260: step 6132, loss 0.233339, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:16.960880: step 6133, loss 0.201262, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:17.493583: step 6134, loss 0.271749, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:18.035702: step 6135, loss 0.325002, acc 0.875, learning_rate 0.0001
2017-10-10T13:41:18.685101: step 6136, loss 0.105135, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:19.235678: step 6137, loss 0.294215, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:19.810988: step 6138, loss 0.135812, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:20.259219: step 6139, loss 0.101655, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:20.700234: step 6140, loss 0.182132, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:21.283201: step 6141, loss 0.0523242, acc 1, learning_rate 0.0001
2017-10-10T13:41:21.884285: step 6142, loss 0.232525, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:22.485034: step 6143, loss 0.121944, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:22.942431: step 6144, loss 0.195097, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:23.404014: step 6145, loss 0.202963, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:23.960953: step 6146, loss 0.213478, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:24.496764: step 6147, loss 0.146729, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:25.037265: step 6148, loss 0.103634, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:25.615467: step 6149, loss 0.105237, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:26.207819: step 6150, loss 0.134316, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:26.802988: step 6151, loss 0.108201, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:27.256566: step 6152, loss 0.198595, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:27.690832: step 6153, loss 0.106003, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:28.174441: step 6154, loss 0.0987044, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:28.669519: step 6155, loss 0.172849, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:29.185163: step 6156, loss 0.117452, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:29.726771: step 6157, loss 0.103517, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:30.223630: step 6158, loss 0.132124, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:30.720822: step 6159, loss 0.130519, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:31.248820: step 6160, loss 0.146689, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:41:32.370618: step 6160, loss 0.213284, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6160

2017-10-10T13:41:33.910998: step 6161, loss 0.126557, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:34.436843: step 6162, loss 0.0980119, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:34.972332: step 6163, loss 0.0536299, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:35.540917: step 6164, loss 0.0589161, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:36.088407: step 6165, loss 0.199419, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:36.650606: step 6166, loss 0.182678, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:37.224256: step 6167, loss 0.137018, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:37.765118: step 6168, loss 0.108626, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:38.329631: step 6169, loss 0.151643, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:38.872960: step 6170, loss 0.169739, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:39.407377: step 6171, loss 0.3291, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:39.956843: step 6172, loss 0.0908795, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:40.456830: step 6173, loss 0.132655, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:40.906559: step 6174, loss 0.0888286, acc 0.960784, learning_rate 0.0001
2017-10-10T13:41:41.445086: step 6175, loss 0.127027, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:41.969337: step 6176, loss 0.089609, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:42.545932: step 6177, loss 0.108684, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:43.020840: step 6178, loss 0.185979, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:43.424909: step 6179, loss 0.192241, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:43.788861: step 6180, loss 0.271149, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:44.296328: step 6181, loss 0.199135, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:44.960907: step 6182, loss 0.135205, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:45.526605: step 6183, loss 0.229415, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:45.973614: step 6184, loss 0.281397, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:46.360884: step 6185, loss 0.215767, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:46.890804: step 6186, loss 0.128419, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:47.408855: step 6187, loss 0.107698, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:47.864484: step 6188, loss 0.133579, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:48.391701: step 6189, loss 0.129237, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:48.928986: step 6190, loss 0.163311, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:49.381048: step 6191, loss 0.0730554, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:49.967530: step 6192, loss 0.118202, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:50.572863: step 6193, loss 0.124845, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:51.021725: step 6194, loss 0.199945, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:51.470487: step 6195, loss 0.119723, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:51.949333: step 6196, loss 0.0999608, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:52.448989: step 6197, loss 0.146927, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:52.965002: step 6198, loss 0.180828, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:53.491465: step 6199, loss 0.127907, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:54.023978: step 6200, loss 0.073498, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:41:55.160904: step 6200, loss 0.211682, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6200

2017-10-10T13:41:56.880277: step 6201, loss 0.20293, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:57.402552: step 6202, loss 0.0558627, acc 1, learning_rate 0.0001
2017-10-10T13:41:57.908666: step 6203, loss 0.268864, acc 0.875, learning_rate 0.0001
2017-10-10T13:41:58.406580: step 6204, loss 0.204161, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:58.926958: step 6205, loss 0.119471, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:59.439440: step 6206, loss 0.149993, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:59.945003: step 6207, loss 0.211901, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:00.500968: step 6208, loss 0.231061, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:01.054276: step 6209, loss 0.2032, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:01.576979: step 6210, loss 0.132953, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:01.996663: step 6211, loss 0.160515, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:02.512965: step 6212, loss 0.0784902, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:03.025897: step 6213, loss 0.167812, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:03.583126: step 6214, loss 0.124129, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:04.086078: step 6215, loss 0.0905756, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:04.583461: step 6216, loss 0.160663, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:05.246894: step 6217, loss 0.226625, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:06.047456: step 6218, loss 0.12865, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:06.519852: step 6219, loss 0.221309, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:07.081849: step 6220, loss 0.186161, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:07.660885: step 6221, loss 0.141363, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:08.297375: step 6222, loss 0.09822, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:08.723314: step 6223, loss 0.131507, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:09.136340: step 6224, loss 0.137094, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:09.675278: step 6225, loss 0.098645, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:10.184886: step 6226, loss 0.0548336, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:10.592422: step 6227, loss 0.220759, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:11.148838: step 6228, loss 0.0964367, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:11.721048: step 6229, loss 0.157326, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:12.310973: step 6230, loss 0.100905, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:12.828976: step 6231, loss 0.191551, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:13.454682: step 6232, loss 0.0876687, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:13.972602: step 6233, loss 0.21688, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:14.514436: step 6234, loss 0.0885948, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:14.941014: step 6235, loss 0.107474, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:15.500990: step 6236, loss 0.176669, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:16.021138: step 6237, loss 0.168208, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:16.501047: step 6238, loss 0.298715, acc 0.84375, learning_rate 0.0001
2017-10-10T13:42:17.013018: step 6239, loss 0.094892, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:17.600851: step 6240, loss 0.175295, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:42:18.811198: step 6240, loss 0.210756, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6240

2017-10-10T13:42:20.321199: step 6241, loss 0.143774, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:20.832808: step 6242, loss 0.295281, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:21.312844: step 6243, loss 0.133688, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:21.769075: step 6244, loss 0.10625, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:22.236899: step 6245, loss 0.122146, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:22.804984: step 6246, loss 0.1116, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:23.364861: step 6247, loss 0.145457, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:23.838089: step 6248, loss 0.159039, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:24.341445: step 6249, loss 0.113828, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:24.877427: step 6250, loss 0.148694, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:25.389710: step 6251, loss 0.164699, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:25.956957: step 6252, loss 0.15685, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:26.560821: step 6253, loss 0.22505, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:27.128116: step 6254, loss 0.31318, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:27.699854: step 6255, loss 0.194735, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:28.236947: step 6256, loss 0.200827, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:28.857039: step 6257, loss 0.177, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:29.342703: step 6258, loss 0.0732872, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:29.781082: step 6259, loss 0.156351, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:30.368420: step 6260, loss 0.139971, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:30.964938: step 6261, loss 0.19084, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:31.424317: step 6262, loss 0.240646, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:31.848822: step 6263, loss 0.151864, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:32.328424: step 6264, loss 0.136115, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:32.830080: step 6265, loss 0.145799, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:33.309069: step 6266, loss 0.108381, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:33.861920: step 6267, loss 0.104476, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:34.380678: step 6268, loss 0.134683, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:34.917037: step 6269, loss 0.227183, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:35.344988: step 6270, loss 0.121283, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:35.824969: step 6271, loss 0.101719, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:36.365047: step 6272, loss 0.156703, acc 0.921569, learning_rate 0.0001
2017-10-10T13:42:36.957116: step 6273, loss 0.206476, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:37.349712: step 6274, loss 0.18976, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:37.716835: step 6275, loss 0.194658, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:38.224729: step 6276, loss 0.212885, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:38.799645: step 6277, loss 0.123963, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:39.338826: step 6278, loss 0.195889, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:39.837303: step 6279, loss 0.0839171, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:40.388858: step 6280, loss 0.269913, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:42:41.533603: step 6280, loss 0.211366, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6280

2017-10-10T13:42:43.275386: step 6281, loss 0.193648, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:43.860866: step 6282, loss 0.114863, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:44.399043: step 6283, loss 0.183543, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:44.960266: step 6284, loss 0.0813009, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:45.454498: step 6285, loss 0.254447, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:45.984917: step 6286, loss 0.128271, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:46.449139: step 6287, loss 0.203385, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:46.980194: step 6288, loss 0.110466, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:47.504991: step 6289, loss 0.174297, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:47.985074: step 6290, loss 0.140215, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:48.490226: step 6291, loss 0.147925, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:48.977020: step 6292, loss 0.0648202, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:49.506140: step 6293, loss 0.0645288, acc 1, learning_rate 0.0001
2017-10-10T13:42:49.989329: step 6294, loss 0.282503, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:50.494002: step 6295, loss 0.0702511, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:51.028809: step 6296, loss 0.22676, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:51.569613: step 6297, loss 0.258286, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:52.126953: step 6298, loss 0.10248, acc 1, learning_rate 0.0001
2017-10-10T13:42:52.564525: step 6299, loss 0.114181, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:53.108990: step 6300, loss 0.0885777, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:53.644134: step 6301, loss 0.206662, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:54.076987: step 6302, loss 0.0937453, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:54.533098: step 6303, loss 0.0825582, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:55.021059: step 6304, loss 0.184876, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:55.624319: step 6305, loss 0.163619, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:56.087294: step 6306, loss 0.174261, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:56.580899: step 6307, loss 0.132832, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:57.136164: step 6308, loss 0.213206, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:57.648034: step 6309, loss 0.360524, acc 0.859375, learning_rate 0.0001
2017-10-10T13:42:58.151079: step 6310, loss 0.141847, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:58.679089: step 6311, loss 0.167069, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:59.179118: step 6312, loss 0.174641, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:59.744174: step 6313, loss 0.0641319, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:00.140047: step 6314, loss 0.136034, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:00.521282: step 6315, loss 0.199266, acc 0.890625, learning_rate 0.0001
2017-10-10T13:43:00.923519: step 6316, loss 0.190696, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:01.450441: step 6317, loss 0.077143, acc 1, learning_rate 0.0001
2017-10-10T13:43:02.012828: step 6318, loss 0.159015, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:02.544489: step 6319, loss 0.0935012, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:03.052138: step 6320, loss 0.0996021, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:43:04.115158: step 6320, loss 0.210577, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6320

2017-10-10T13:43:05.796353: step 6321, loss 0.100988, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:06.367923: step 6322, loss 0.137862, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:06.892875: step 6323, loss 0.112956, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:07.474885: step 6324, loss 0.145002, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:08.028620: step 6325, loss 0.056177, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:08.560684: step 6326, loss 0.116244, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:09.096657: step 6327, loss 0.234662, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:09.669532: step 6328, loss 0.0941558, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:10.227821: step 6329, loss 0.102571, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:10.769666: step 6330, loss 0.10304, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:11.329258: step 6331, loss 0.331697, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:11.881089: step 6332, loss 0.0844598, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:12.399694: step 6333, loss 0.215216, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:13.040454: step 6334, loss 0.0947511, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:13.565065: step 6335, loss 0.120216, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:14.172903: step 6336, loss 0.0992572, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:14.700906: step 6337, loss 0.119657, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:15.225754: step 6338, loss 0.0502873, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:15.693347: step 6339, loss 0.132025, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:16.187000: step 6340, loss 0.242081, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:16.706150: step 6341, loss 0.0952656, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:17.122333: step 6342, loss 0.125521, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:17.558078: step 6343, loss 0.225333, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:18.057024: step 6344, loss 0.214889, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:18.629020: step 6345, loss 0.135778, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:19.161028: step 6346, loss 0.144048, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:19.625159: step 6347, loss 0.109776, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:20.241239: step 6348, loss 0.152419, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:20.772971: step 6349, loss 0.162191, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:21.293015: step 6350, loss 0.191328, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:21.597393: step 6351, loss 0.189545, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:22.148210: step 6352, loss 0.142776, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:22.720876: step 6353, loss 0.2018, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:23.393265: step 6354, loss 0.231506, acc 0.890625, learning_rate 0.0001
2017-10-10T13:43:23.873100: step 6355, loss 0.181438, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:24.283117: step 6356, loss 0.227741, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:24.805060: step 6357, loss 0.128651, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:25.224792: step 6358, loss 0.161632, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:25.825149: step 6359, loss 0.0465154, acc 1, learning_rate 0.0001
2017-10-10T13:43:26.377654: step 6360, loss 0.16623, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:43:27.561597: step 6360, loss 0.211345, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6360

2017-10-10T13:43:28.872988: step 6361, loss 0.169648, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:29.357118: step 6362, loss 0.16863, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:29.876966: step 6363, loss 0.159765, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:30.456898: step 6364, loss 0.215365, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:31.024016: step 6365, loss 0.230332, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:31.452435: step 6366, loss 0.121491, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:31.963837: step 6367, loss 0.124663, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:32.453059: step 6368, loss 0.0859032, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:32.975358: step 6369, loss 0.179724, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:33.398992: step 6370, loss 0.0720133, acc 1, learning_rate 0.0001
2017-10-10T13:43:33.940703: step 6371, loss 0.145753, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:34.450707: step 6372, loss 0.0969528, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:34.981745: step 6373, loss 0.167769, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:35.537545: step 6374, loss 0.151125, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:36.080423: step 6375, loss 0.133816, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:36.603406: step 6376, loss 0.18806, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:37.105022: step 6377, loss 0.158477, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:37.653143: step 6378, loss 0.188671, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:38.141412: step 6379, loss 0.158908, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:38.632902: step 6380, loss 0.215749, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:39.225119: step 6381, loss 0.163122, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:39.698752: step 6382, loss 0.139962, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:40.077151: step 6383, loss 0.116044, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:40.615955: step 6384, loss 0.0689806, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:41.053079: step 6385, loss 0.304417, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:41.604572: step 6386, loss 0.0886941, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:42.127902: step 6387, loss 0.147519, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:42.642791: step 6388, loss 0.146387, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:43.142341: step 6389, loss 0.19717, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:43.752873: step 6390, loss 0.109041, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:44.277285: step 6391, loss 0.133224, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:44.849003: step 6392, loss 0.127735, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:45.332845: step 6393, loss 0.0741596, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:45.893246: step 6394, loss 0.120767, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:46.440867: step 6395, loss 0.0645711, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:47.091922: step 6396, loss 0.172636, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:47.468835: step 6397, loss 0.146742, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:47.892380: step 6398, loss 0.19389, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:48.393837: step 6399, loss 0.0818985, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:48.899758: step 6400, loss 0.142486, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:43:50.056959: step 6400, loss 0.211339, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6400

2017-10-10T13:43:51.720993: step 6401, loss 0.160261, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:52.313607: step 6402, loss 0.104419, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:52.867695: step 6403, loss 0.155884, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:53.405676: step 6404, loss 0.209596, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:53.950341: step 6405, loss 0.0722356, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:54.522849: step 6406, loss 0.141598, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:55.062995: step 6407, loss 0.17524, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:55.622153: step 6408, loss 0.150873, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:56.156889: step 6409, loss 0.0658254, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:56.672837: step 6410, loss 0.205115, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:57.204849: step 6411, loss 0.211187, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:57.728844: step 6412, loss 0.0764087, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:58.276824: step 6413, loss 0.12634, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:58.780892: step 6414, loss 0.0486424, acc 0.984375, learning_rate 0.0001
2017-10-10T13:43:59.267390: step 6415, loss 0.206324, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:59.677280: step 6416, loss 0.166598, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:00.269325: step 6417, loss 0.14187, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:00.821015: step 6418, loss 0.267775, acc 0.90625, learning_rate 0.0001
2017-10-10T13:44:01.456935: step 6419, loss 0.151053, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:01.805181: step 6420, loss 0.170976, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:02.165922: step 6421, loss 0.135549, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:02.480816: step 6422, loss 0.175214, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:02.949807: step 6423, loss 0.103461, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:03.481586: step 6424, loss 0.0815736, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:04.046576: step 6425, loss 0.347441, acc 0.890625, learning_rate 0.0001
2017-10-10T13:44:04.588924: step 6426, loss 0.128846, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:05.161442: step 6427, loss 0.0851097, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:05.800963: step 6428, loss 0.0506467, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:06.316875: step 6429, loss 0.0461303, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:06.861231: step 6430, loss 0.156967, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:07.339332: step 6431, loss 0.238145, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:07.820446: step 6432, loss 0.0614681, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:08.394197: step 6433, loss 0.0862355, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:08.900734: step 6434, loss 0.0612624, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:09.397514: step 6435, loss 0.275261, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:09.991183: step 6436, loss 0.227715, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:10.499491: step 6437, loss 0.158582, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:10.974939: step 6438, loss 0.12982, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:11.395155: step 6439, loss 0.0953459, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:11.939576: step 6440, loss 0.142326, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:44:13.272110: step 6440, loss 0.210389, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6440

2017-10-10T13:44:14.933540: step 6441, loss 0.0519568, acc 1, learning_rate 0.0001
2017-10-10T13:44:15.429062: step 6442, loss 0.184949, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:16.051323: step 6443, loss 0.125472, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:16.528953: step 6444, loss 0.130634, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:17.021058: step 6445, loss 0.130477, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:17.521342: step 6446, loss 0.141679, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:18.077021: step 6447, loss 0.0647486, acc 1, learning_rate 0.0001
2017-10-10T13:44:18.625018: step 6448, loss 0.141975, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:19.181134: step 6449, loss 0.142886, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:19.705227: step 6450, loss 0.115687, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:20.165113: step 6451, loss 0.151747, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:20.705208: step 6452, loss 0.111172, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:21.193227: step 6453, loss 0.122403, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:21.765658: step 6454, loss 0.0659341, acc 1, learning_rate 0.0001
2017-10-10T13:44:22.258094: step 6455, loss 0.251476, acc 0.90625, learning_rate 0.0001
2017-10-10T13:44:22.813083: step 6456, loss 0.151679, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:23.384493: step 6457, loss 0.124111, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:23.984931: step 6458, loss 0.109513, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:24.331001: step 6459, loss 0.150981, acc 0.953125, learning_rate 0.0001
2017-10-10T13:45:17.226059: step 6460, loss 0.235785, acc 0.890625, learning_rate 0.0001
2017-10-10T13:45:35.894582: step 6461, loss 0.251769, acc 0.9375, learning_rate 0.0001
2017-10-10T13:45:38.308568: step 6462, loss 0.126406, acc 0.9375, learning_rate 0.0001
2017-10-10T13:45:40.476861: step 6463, loss 0.198193, acc 0.9375, learning_rate 0.0001
2017-10-10T13:45:41.086635: step 6464, loss 0.137862, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:42.658002: step 6465, loss 0.0584968, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:43.410440: step 6466, loss 0.13572, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:44.195688: step 6467, loss 0.168278, acc 0.921875, learning_rate 0.0001
2017-10-10T13:45:44.963810: step 6468, loss 0.162091, acc 0.941176, learning_rate 0.0001
2017-10-10T13:45:46.123662: step 6469, loss 0.139462, acc 0.953125, learning_rate 0.0001
2017-10-10T13:45:46.972548: step 6470, loss 0.177929, acc 0.921875, learning_rate 0.0001
2017-10-10T13:45:47.454635: step 6471, loss 0.0880666, acc 0.96875, learning_rate 0.0001
2017-10-10T13:45:48.509831: step 6472, loss 0.162477, acc 0.9375, learning_rate 0.0001
2017-10-10T13:45:49.113560: step 6473, loss 0.110013, acc 0.953125, learning_rate 0.0001
2017-10-10T13:45:49.429774: step 6474, loss 0.230446, acc 0.921875, learning_rate 0.0001
2017-10-10T13:45:50.205163: step 6475, loss 0.163918, acc 0.90625, learning_rate 0.0001
2017-10-10T13:45:50.578368: step 6476, loss 0.216569, acc 0.9375, learning_rate 0.0001
2017-10-10T13:45:51.015692: step 6477, loss 0.234711, acc 0.90625, learning_rate 0.0001
2017-10-10T13:45:51.598135: step 6478, loss 0.106984, acc 0.984375, learning_rate 0.0001
2017-10-10T13:45:52.036438: step 6479, loss 0.122813, acc 0.953125, learning_rate 0.0001
2017-10-10T13:45:52.367362: step 6480, loss 0.176422, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:45:53.716499: step 6480, loss 0.210125, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6480

2017-10-10T13:46:07.634959: step 6481, loss 0.121668, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:07.950239: step 6482, loss 0.16184, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:08.265001: step 6483, loss 0.196238, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:08.530908: step 6484, loss 0.188653, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:08.797665: step 6485, loss 0.101994, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:09.026333: step 6486, loss 0.152834, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:09.255451: step 6487, loss 0.0634239, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:09.531697: step 6488, loss 0.139066, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:09.869935: step 6489, loss 0.129346, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:10.076461: step 6490, loss 0.122088, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:10.356226: step 6491, loss 0.20279, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:10.600935: step 6492, loss 0.158007, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:10.866002: step 6493, loss 0.166492, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:11.098482: step 6494, loss 0.191768, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:11.365072: step 6495, loss 0.217192, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:11.638772: step 6496, loss 0.226252, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:11.835697: step 6497, loss 0.0732848, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:12.456432: step 6498, loss 0.125959, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:12.799901: step 6499, loss 0.115103, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:13.143437: step 6500, loss 0.247051, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:13.478377: step 6501, loss 0.134966, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:13.861969: step 6502, loss 0.0885628, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:14.302642: step 6503, loss 0.128406, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:14.606840: step 6504, loss 0.172504, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:15.102874: step 6505, loss 0.164516, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:15.378843: step 6506, loss 0.19013, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:15.740849: step 6507, loss 0.136097, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:16.168990: step 6508, loss 0.129546, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:16.455912: step 6509, loss 0.189681, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:16.798922: step 6510, loss 0.100054, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:17.018276: step 6511, loss 0.132461, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:17.183585: step 6512, loss 0.142555, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:17.504015: step 6513, loss 0.148203, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:17.891009: step 6514, loss 0.123058, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:18.195421: step 6515, loss 0.159154, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:18.494142: step 6516, loss 0.130815, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:18.806245: step 6517, loss 0.186851, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:19.073269: step 6518, loss 0.128468, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:19.474895: step 6519, loss 0.0704432, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:19.835687: step 6520, loss 0.147001, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:46:20.713232: step 6520, loss 0.210153, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6520

2017-10-10T13:46:53.874574: step 6521, loss 0.135049, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:54.293361: step 6522, loss 0.0969784, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:54.747027: step 6523, loss 0.119321, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:55.346476: step 6524, loss 0.335227, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:55.780832: step 6525, loss 0.11453, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:56.176533: step 6526, loss 0.220921, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:56.585088: step 6527, loss 0.149937, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:56.993596: step 6528, loss 0.127483, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:57.439374: step 6529, loss 0.142201, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:57.964029: step 6530, loss 0.234881, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:58.370322: step 6531, loss 0.190214, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:58.807949: step 6532, loss 0.225178, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:59.234669: step 6533, loss 0.152612, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:59.636893: step 6534, loss 0.122037, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:00.114740: step 6535, loss 0.135145, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:00.488979: step 6536, loss 0.0741598, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:00.806675: step 6537, loss 0.0684512, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:01.132857: step 6538, loss 0.0998484, acc 1, learning_rate 0.0001
2017-10-10T13:47:01.548191: step 6539, loss 0.0707655, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:01.949935: step 6540, loss 0.144771, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:02.416815: step 6541, loss 0.058613, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:02.827427: step 6542, loss 0.0778195, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:03.175950: step 6543, loss 0.172767, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:03.545269: step 6544, loss 0.134333, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:03.960147: step 6545, loss 0.0805104, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:04.405088: step 6546, loss 0.153585, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:04.800852: step 6547, loss 0.119656, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:05.243465: step 6548, loss 0.153957, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:05.744915: step 6549, loss 0.0727436, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:06.063589: step 6550, loss 0.19573, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:06.440941: step 6551, loss 0.118586, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:06.739972: step 6552, loss 0.275458, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:07.174213: step 6553, loss 0.143797, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:07.568999: step 6554, loss 0.118624, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:07.982069: step 6555, loss 0.261973, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:08.403074: step 6556, loss 0.0781531, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:08.809324: step 6557, loss 0.190509, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:09.233563: step 6558, loss 0.151325, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:09.668906: step 6559, loss 0.0683189, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:10.032202: step 6560, loss 0.172802, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:47:10.915574: step 6560, loss 0.209847, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6560

2017-10-10T13:47:12.380830: step 6561, loss 0.211051, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:12.787332: step 6562, loss 0.255581, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:13.264952: step 6563, loss 0.112528, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:13.616196: step 6564, loss 0.12678, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:14.048046: step 6565, loss 0.160998, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:14.397282: step 6566, loss 0.112988, acc 0.960784, learning_rate 0.0001
2017-10-10T13:47:14.864920: step 6567, loss 0.185776, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:15.276956: step 6568, loss 0.349788, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:15.670583: step 6569, loss 0.111997, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:16.091031: step 6570, loss 0.122914, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:16.517386: step 6571, loss 0.209027, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:16.900914: step 6572, loss 0.132454, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:17.583919: step 6573, loss 0.134835, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:17.860569: step 6574, loss 0.127264, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:18.180899: step 6575, loss 0.0919285, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:18.472873: step 6576, loss 0.137867, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:18.872869: step 6577, loss 0.146767, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:19.183144: step 6578, loss 0.0748925, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:19.581218: step 6579, loss 0.088275, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:19.982723: step 6580, loss 0.0747531, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:20.304879: step 6581, loss 0.176629, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:20.721034: step 6582, loss 0.188856, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:21.095500: step 6583, loss 0.0860656, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:21.506850: step 6584, loss 0.186327, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:21.899157: step 6585, loss 0.132609, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:22.272552: step 6586, loss 0.0694259, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:22.668874: step 6587, loss 0.124336, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:23.104906: step 6588, loss 0.159271, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:23.505922: step 6589, loss 0.107259, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:23.817064: step 6590, loss 0.207783, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:24.147447: step 6591, loss 0.20932, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:24.467538: step 6592, loss 0.162433, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:24.910274: step 6593, loss 0.147143, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:25.308471: step 6594, loss 0.153247, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:25.728745: step 6595, loss 0.226517, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:26.144863: step 6596, loss 0.150528, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:26.556834: step 6597, loss 0.218304, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:26.956865: step 6598, loss 0.238114, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:27.350599: step 6599, loss 0.232993, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:27.756840: step 6600, loss 0.209012, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:47:28.662022: step 6600, loss 0.20877, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6600

2017-10-10T13:47:30.152300: step 6601, loss 0.07633, acc 1, learning_rate 0.0001
2017-10-10T13:47:30.577048: step 6602, loss 0.15352, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:30.941042: step 6603, loss 0.0638045, acc 1, learning_rate 0.0001
2017-10-10T13:47:31.311616: step 6604, loss 0.10767, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:31.674276: step 6605, loss 0.12241, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:32.066957: step 6606, loss 0.148618, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:32.477175: step 6607, loss 0.0816867, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:32.886055: step 6608, loss 0.219233, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:33.288956: step 6609, loss 0.107822, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:33.718166: step 6610, loss 0.210678, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:34.151865: step 6611, loss 0.174379, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:34.657067: step 6612, loss 0.208936, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:35.111363: step 6613, loss 0.22762, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:35.419600: step 6614, loss 0.233387, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:35.738395: step 6615, loss 0.154558, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:36.152903: step 6616, loss 0.112674, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:36.550652: step 6617, loss 0.125498, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:36.995007: step 6618, loss 0.0922346, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:37.385020: step 6619, loss 0.247644, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:37.805923: step 6620, loss 0.0857687, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:38.161789: step 6621, loss 0.0979285, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:38.548335: step 6622, loss 0.0951233, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:38.929013: step 6623, loss 0.130097, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:39.331376: step 6624, loss 0.0765963, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:39.701141: step 6625, loss 0.12387, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:40.004356: step 6626, loss 0.151861, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:40.436900: step 6627, loss 0.124171, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:40.958080: step 6628, loss 0.0808519, acc 1, learning_rate 0.0001
2017-10-10T13:47:41.291170: step 6629, loss 0.11752, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:41.629972: step 6630, loss 0.0552662, acc 1, learning_rate 0.0001
2017-10-10T13:47:41.880490: step 6631, loss 0.168602, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:42.233066: step 6632, loss 0.147354, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:42.615054: step 6633, loss 0.145893, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:43.036958: step 6634, loss 0.230183, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:43.457255: step 6635, loss 0.145674, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:43.853704: step 6636, loss 0.246862, acc 0.859375, learning_rate 0.0001
2017-10-10T13:47:44.286319: step 6637, loss 0.0819001, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:44.674473: step 6638, loss 0.122679, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:45.097473: step 6639, loss 0.0536266, acc 1, learning_rate 0.0001
2017-10-10T13:47:45.511864: step 6640, loss 0.074507, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:47:46.423990: step 6640, loss 0.208469, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6640

2017-10-10T13:47:47.519340: step 6641, loss 0.085096, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:47.933124: step 6642, loss 0.167418, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:48.343074: step 6643, loss 0.206832, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:48.750531: step 6644, loss 0.173282, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:49.182081: step 6645, loss 0.134396, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:49.612301: step 6646, loss 0.164716, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:50.022953: step 6647, loss 0.184591, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:50.410125: step 6648, loss 0.0706136, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:50.814182: step 6649, loss 0.199269, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:51.217163: step 6650, loss 0.164194, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:51.660938: step 6651, loss 0.158984, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:52.148869: step 6652, loss 0.0860657, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:52.628295: step 6653, loss 0.152797, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:52.942074: step 6654, loss 0.174469, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:53.256805: step 6655, loss 0.178005, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:53.564629: step 6656, loss 0.127892, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:53.972171: step 6657, loss 0.0624555, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:54.388255: step 6658, loss 0.0722934, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:54.794234: step 6659, loss 0.132793, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:55.229130: step 6660, loss 0.088604, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:55.595052: step 6661, loss 0.2977, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:55.989114: step 6662, loss 0.0428916, acc 1, learning_rate 0.0001
2017-10-10T13:47:56.412246: step 6663, loss 0.183816, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:56.785217: step 6664, loss 0.142551, acc 0.941176, learning_rate 0.0001
2017-10-10T13:47:57.190243: step 6665, loss 0.309424, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:57.613114: step 6666, loss 0.0678186, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:58.034188: step 6667, loss 0.0761016, acc 1, learning_rate 0.0001
2017-10-10T13:47:58.456416: step 6668, loss 0.307491, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:58.940834: step 6669, loss 0.0591935, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:59.253634: step 6670, loss 0.236219, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:59.559085: step 6671, loss 0.0627396, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:59.878780: step 6672, loss 0.133833, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:00.301687: step 6673, loss 0.149545, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:00.717657: step 6674, loss 0.193993, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:01.131190: step 6675, loss 0.154749, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:01.557493: step 6676, loss 0.15185, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:01.966372: step 6677, loss 0.102028, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:02.382517: step 6678, loss 0.13979, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:02.792657: step 6679, loss 0.0943455, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:03.209506: step 6680, loss 0.206242, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:48:03.954854: step 6680, loss 0.209582, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6680

2017-10-10T13:48:05.321572: step 6681, loss 0.195037, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:05.665785: step 6682, loss 0.192216, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:06.105336: step 6683, loss 0.170732, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:06.478417: step 6684, loss 0.320031, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:06.847660: step 6685, loss 0.154612, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:07.242991: step 6686, loss 0.140892, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:07.617085: step 6687, loss 0.197126, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:08.004869: step 6688, loss 0.0874762, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:08.423915: step 6689, loss 0.103879, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:08.841062: step 6690, loss 0.240645, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:09.240885: step 6691, loss 0.105491, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:09.608994: step 6692, loss 0.305301, acc 0.859375, learning_rate 0.0001
2017-10-10T13:48:10.133520: step 6693, loss 0.197989, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:10.456448: step 6694, loss 0.20628, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:10.744992: step 6695, loss 0.151869, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:11.060878: step 6696, loss 0.114984, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:11.348887: step 6697, loss 0.108835, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:11.696832: step 6698, loss 0.0925425, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:12.100545: step 6699, loss 0.116273, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:12.516472: step 6700, loss 0.185624, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:12.910917: step 6701, loss 0.198508, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:13.303958: step 6702, loss 0.105484, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:13.712774: step 6703, loss 0.128047, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:14.126952: step 6704, loss 0.170305, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:14.554270: step 6705, loss 0.0894539, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:14.948845: step 6706, loss 0.197811, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:15.364285: step 6707, loss 0.16605, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:15.774029: step 6708, loss 0.140203, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:16.141112: step 6709, loss 0.204361, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:16.567281: step 6710, loss 0.0741744, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:17.093322: step 6711, loss 0.036687, acc 1, learning_rate 0.0001
2017-10-10T13:48:17.395234: step 6712, loss 0.0767789, acc 1, learning_rate 0.0001
2017-10-10T13:48:17.652835: step 6713, loss 0.248225, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:17.944853: step 6714, loss 0.174494, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:18.252866: step 6715, loss 0.13693, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:18.652210: step 6716, loss 0.241812, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:19.055464: step 6717, loss 0.213219, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:19.412924: step 6718, loss 0.106381, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:19.804728: step 6719, loss 0.118691, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:20.248986: step 6720, loss 0.212793, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:48:21.125814: step 6720, loss 0.210137, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6720

2017-10-10T13:48:22.685357: step 6721, loss 0.0597058, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:23.006147: step 6722, loss 0.103643, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:23.393346: step 6723, loss 0.172303, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:23.829143: step 6724, loss 0.312321, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:24.243599: step 6725, loss 0.116641, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:24.651694: step 6726, loss 0.0771498, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:25.075542: step 6727, loss 0.108482, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:25.492831: step 6728, loss 0.11448, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:25.910198: step 6729, loss 0.13062, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:26.311715: step 6730, loss 0.147757, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:26.694260: step 6731, loss 0.212448, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:27.136924: step 6732, loss 0.0525023, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:27.602315: step 6733, loss 0.218807, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:27.926496: step 6734, loss 0.121651, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:28.243393: step 6735, loss 0.185757, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:28.553723: step 6736, loss 0.138415, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:28.979469: step 6737, loss 0.248057, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:29.378159: step 6738, loss 0.167427, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:29.804458: step 6739, loss 0.070047, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:30.242729: step 6740, loss 0.134845, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:30.640905: step 6741, loss 0.0791932, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:31.054495: step 6742, loss 0.0833491, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:31.455306: step 6743, loss 0.161727, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:31.880882: step 6744, loss 0.231694, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:32.288126: step 6745, loss 0.258272, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:32.716229: step 6746, loss 0.0807961, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:33.133136: step 6747, loss 0.249125, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:33.524583: step 6748, loss 0.0911754, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:33.947712: step 6749, loss 0.115311, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:34.350100: step 6750, loss 0.17881, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:34.752842: step 6751, loss 0.178789, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:35.272391: step 6752, loss 0.0957493, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:35.591682: step 6753, loss 0.0921101, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:35.912565: step 6754, loss 0.15259, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:36.234196: step 6755, loss 0.0556291, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:36.635704: step 6756, loss 0.124488, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:37.048181: step 6757, loss 0.136388, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:37.432173: step 6758, loss 0.186928, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:37.824348: step 6759, loss 0.181519, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:38.208689: step 6760, loss 0.162944, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:48:39.136496: step 6760, loss 0.206775, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6760

2017-10-10T13:48:40.400825: step 6761, loss 0.108698, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:40.732851: step 6762, loss 0.229363, acc 0.921569, learning_rate 0.0001
2017-10-10T13:48:41.122992: step 6763, loss 0.11504, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:41.487362: step 6764, loss 0.177293, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:41.908955: step 6765, loss 0.150163, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:42.284917: step 6766, loss 0.109209, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:42.736832: step 6767, loss 0.135945, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:43.160368: step 6768, loss 0.114988, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:43.580558: step 6769, loss 0.212898, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:43.998541: step 6770, loss 0.127339, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:44.398289: step 6771, loss 0.0843341, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:44.860946: step 6772, loss 0.248405, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:45.355491: step 6773, loss 0.121415, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:45.664220: step 6774, loss 0.170048, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:45.984331: step 6775, loss 0.217617, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:46.360891: step 6776, loss 0.0871801, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:46.772852: step 6777, loss 0.112392, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:47.188898: step 6778, loss 0.1524, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:47.609836: step 6779, loss 0.193021, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:48.029212: step 6780, loss 0.236585, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:48.457052: step 6781, loss 0.0955353, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:48.907842: step 6782, loss 0.174766, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:49.311612: step 6783, loss 0.174627, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:49.726220: step 6784, loss 0.0974573, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:50.128871: step 6785, loss 0.144967, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:50.552682: step 6786, loss 0.173476, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:50.993437: step 6787, loss 0.0471958, acc 1, learning_rate 0.0001
2017-10-10T13:48:51.414125: step 6788, loss 0.090026, acc 1, learning_rate 0.0001
2017-10-10T13:48:51.823369: step 6789, loss 0.142711, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:52.252826: step 6790, loss 0.141542, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:52.771034: step 6791, loss 0.227764, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:53.074098: step 6792, loss 0.198958, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:53.390391: step 6793, loss 0.253859, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:53.692887: step 6794, loss 0.0894913, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:54.100826: step 6795, loss 0.100366, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:54.520868: step 6796, loss 0.0849912, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:54.908861: step 6797, loss 0.199366, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:55.311540: step 6798, loss 0.173245, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:55.736744: step 6799, loss 0.166789, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:56.144831: step 6800, loss 0.115394, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:48:57.015844: step 6800, loss 0.208355, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6800

2017-10-10T13:48:58.430029: step 6801, loss 0.0901875, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:58.853971: step 6802, loss 0.117905, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:59.276458: step 6803, loss 0.1118, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:59.693770: step 6804, loss 0.139793, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:00.122638: step 6805, loss 0.066145, acc 1, learning_rate 0.0001
2017-10-10T13:49:00.541604: step 6806, loss 0.110975, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:00.942074: step 6807, loss 0.160896, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:01.353735: step 6808, loss 0.0793509, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:01.749699: step 6809, loss 0.114008, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:02.154091: step 6810, loss 0.133775, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:02.540896: step 6811, loss 0.0615357, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:02.993303: step 6812, loss 0.212105, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:03.269158: step 6813, loss 0.139033, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:03.548475: step 6814, loss 0.0997779, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:03.860274: step 6815, loss 0.0697197, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:04.162728: step 6816, loss 0.112641, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:04.559007: step 6817, loss 0.204632, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:04.935494: step 6818, loss 0.123925, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:05.351577: step 6819, loss 0.0844568, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:05.780058: step 6820, loss 0.0522089, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:06.195526: step 6821, loss 0.212123, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:06.613077: step 6822, loss 0.141028, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:07.036340: step 6823, loss 0.20452, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:07.439438: step 6824, loss 0.144941, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:07.911656: step 6825, loss 0.0686802, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:08.281738: step 6826, loss 0.112127, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:08.679112: step 6827, loss 0.223256, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:09.087784: step 6828, loss 0.0860858, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:09.520876: step 6829, loss 0.169626, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:09.941861: step 6830, loss 0.0989048, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:10.418060: step 6831, loss 0.0783449, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:10.838435: step 6832, loss 0.208844, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:11.153583: step 6833, loss 0.160029, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:11.468423: step 6834, loss 0.180795, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:11.804229: step 6835, loss 0.10209, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:12.213150: step 6836, loss 0.0803036, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:12.888895: step 6837, loss 0.169565, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:13.328924: step 6838, loss 0.077485, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:13.696391: step 6839, loss 0.13179, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:14.095634: step 6840, loss 0.167934, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:49:15.022231: step 6840, loss 0.206723, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6840

2017-10-10T13:49:16.508896: step 6841, loss 0.217721, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:16.880354: step 6842, loss 0.100819, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:17.296438: step 6843, loss 0.136129, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:17.697409: step 6844, loss 0.122969, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:18.115736: step 6845, loss 0.188223, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:18.542922: step 6846, loss 0.177392, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:18.941748: step 6847, loss 0.0652139, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:19.356963: step 6848, loss 0.168281, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:19.757869: step 6849, loss 0.218531, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:20.180939: step 6850, loss 0.189441, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:20.688330: step 6851, loss 0.161191, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:20.992147: step 6852, loss 0.144149, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:21.302403: step 6853, loss 0.140106, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:21.600663: step 6854, loss 0.121123, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:22.001360: step 6855, loss 0.220262, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:22.337845: step 6856, loss 0.167324, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:22.708610: step 6857, loss 0.0999653, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:23.124856: step 6858, loss 0.140832, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:23.533077: step 6859, loss 0.115315, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:23.919314: step 6860, loss 0.140781, acc 0.980392, learning_rate 0.0001
2017-10-10T13:49:24.361313: step 6861, loss 0.160712, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:24.742741: step 6862, loss 0.22047, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:25.156404: step 6863, loss 0.140953, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:25.548055: step 6864, loss 0.1732, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:25.954186: step 6865, loss 0.140897, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:26.360845: step 6866, loss 0.0510591, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:26.789043: step 6867, loss 0.120726, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:27.220685: step 6868, loss 0.0461532, acc 1, learning_rate 0.0001
2017-10-10T13:49:27.684886: step 6869, loss 0.10161, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:28.177176: step 6870, loss 0.137063, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:28.508851: step 6871, loss 0.171449, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:28.824912: step 6872, loss 0.115698, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:29.143805: step 6873, loss 0.196404, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:29.474916: step 6874, loss 0.176971, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:29.892946: step 6875, loss 0.281585, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:30.237021: step 6876, loss 0.116483, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:30.695005: step 6877, loss 0.0596705, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:31.069114: step 6878, loss 0.153735, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:31.355854: step 6879, loss 0.220953, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:31.777070: step 6880, loss 0.146702, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:49:32.610640: step 6880, loss 0.206025, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6880

2017-10-10T13:49:33.892906: step 6881, loss 0.135082, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:34.300846: step 6882, loss 0.210777, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:34.694872: step 6883, loss 0.219238, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:35.105768: step 6884, loss 0.124907, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:35.508532: step 6885, loss 0.204378, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:35.905329: step 6886, loss 0.183783, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:36.325896: step 6887, loss 0.189641, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:36.732552: step 6888, loss 0.146559, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:37.136750: step 6889, loss 0.289431, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:37.562782: step 6890, loss 0.261097, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:38.096162: step 6891, loss 0.140393, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:38.486729: step 6892, loss 0.125222, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:38.808029: step 6893, loss 0.193177, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:39.124087: step 6894, loss 0.159446, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:39.512794: step 6895, loss 0.097873, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:39.867554: step 6896, loss 0.0877354, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:40.252969: step 6897, loss 0.0857735, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:40.652948: step 6898, loss 0.166904, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:41.087384: step 6899, loss 0.0878106, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:41.520237: step 6900, loss 0.084996, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:41.938118: step 6901, loss 0.186694, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:42.352067: step 6902, loss 0.209119, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:42.748547: step 6903, loss 0.225275, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:43.139027: step 6904, loss 0.1205, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:43.546053: step 6905, loss 0.225995, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:43.955689: step 6906, loss 0.116631, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:44.371951: step 6907, loss 0.155681, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:44.789800: step 6908, loss 0.119355, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:45.189299: step 6909, loss 0.176717, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:45.508868: step 6910, loss 0.143158, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:46.053040: step 6911, loss 0.15131, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:46.382671: step 6912, loss 0.101157, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:46.714497: step 6913, loss 0.192352, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:47.016407: step 6914, loss 0.11036, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:47.441483: step 6915, loss 0.102702, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:47.864905: step 6916, loss 0.158806, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:48.276197: step 6917, loss 0.10348, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:48.675652: step 6918, loss 0.102155, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:49.092813: step 6919, loss 0.0835936, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:49.506679: step 6920, loss 0.116758, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:49:50.400863: step 6920, loss 0.206832, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6920

2017-10-10T13:49:51.828959: step 6921, loss 0.134629, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:52.200692: step 6922, loss 0.0913563, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:52.585558: step 6923, loss 0.131588, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:53.012442: step 6924, loss 0.104842, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:53.409590: step 6925, loss 0.201456, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:53.825228: step 6926, loss 0.140606, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:54.236232: step 6927, loss 0.150928, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:54.648537: step 6928, loss 0.265344, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:55.052332: step 6929, loss 0.125963, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:55.510245: step 6930, loss 0.287222, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:55.959623: step 6931, loss 0.0761743, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:56.275704: step 6932, loss 0.227147, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:56.570567: step 6933, loss 0.133138, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:56.882752: step 6934, loss 0.122994, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:57.296313: step 6935, loss 0.204506, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:57.709157: step 6936, loss 0.159916, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:58.130091: step 6937, loss 0.146103, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:58.565607: step 6938, loss 0.175888, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:58.993926: step 6939, loss 0.117806, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:59.399126: step 6940, loss 0.189727, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:59.814019: step 6941, loss 0.229741, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:00.200853: step 6942, loss 0.16728, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:00.635617: step 6943, loss 0.152896, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:01.059669: step 6944, loss 0.0773718, acc 1, learning_rate 0.0001
2017-10-10T13:50:01.441006: step 6945, loss 0.201708, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:01.822237: step 6946, loss 0.150125, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:02.217552: step 6947, loss 0.159902, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:02.592913: step 6948, loss 0.0903309, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:02.863933: step 6949, loss 0.154969, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:03.100864: step 6950, loss 0.0957674, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:03.456853: step 6951, loss 0.208629, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:03.872948: step 6952, loss 0.100346, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:04.168322: step 6953, loss 0.176246, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:04.462070: step 6954, loss 0.068727, acc 1, learning_rate 0.0001
2017-10-10T13:50:04.740995: step 6955, loss 0.100802, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:05.138473: step 6956, loss 0.0801683, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:05.501076: step 6957, loss 0.110726, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:05.834624: step 6958, loss 0.244368, acc 0.941176, learning_rate 0.0001
2017-10-10T13:50:06.277024: step 6959, loss 0.100147, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:06.656911: step 6960, loss 0.146627, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:50:07.448447: step 6960, loss 0.206832, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-6960

2017-10-10T13:50:09.072468: step 6961, loss 0.0954572, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:09.503017: step 6962, loss 0.155893, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:09.920852: step 6963, loss 0.0792073, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:10.322751: step 6964, loss 0.265815, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:10.722720: step 6965, loss 0.087133, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:11.152739: step 6966, loss 0.155684, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:11.593606: step 6967, loss 0.190435, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:11.960989: step 6968, loss 0.0955755, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:12.369161: step 6969, loss 0.125366, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:12.792372: step 6970, loss 0.213447, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:13.188702: step 6971, loss 0.177553, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:13.610286: step 6972, loss 0.177069, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:14.069249: step 6973, loss 0.082257, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:14.428839: step 6974, loss 0.118251, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:14.755124: step 6975, loss 0.0737253, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:15.066122: step 6976, loss 0.0747134, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:15.421389: step 6977, loss 0.150795, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:15.782325: step 6978, loss 0.177427, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:16.184984: step 6979, loss 0.312971, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:16.591478: step 6980, loss 0.180084, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:16.924947: step 6981, loss 0.0885708, acc 1, learning_rate 0.0001
2017-10-10T13:50:17.320863: step 6982, loss 0.114954, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:17.753250: step 6983, loss 0.204226, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:18.181762: step 6984, loss 0.161317, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:18.553014: step 6985, loss 0.195259, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:18.960862: step 6986, loss 0.132279, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:19.398445: step 6987, loss 0.0429743, acc 1, learning_rate 0.0001
2017-10-10T13:50:19.784883: step 6988, loss 0.0956021, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:20.164882: step 6989, loss 0.178176, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:20.484983: step 6990, loss 0.115228, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:21.000615: step 6991, loss 0.0919539, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:21.296303: step 6992, loss 0.220685, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:21.612273: step 6993, loss 0.0659081, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:21.932822: step 6994, loss 0.106609, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:22.300886: step 6995, loss 0.111959, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:22.771317: step 6996, loss 0.0792453, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:23.171304: step 6997, loss 0.171775, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:23.564437: step 6998, loss 0.155288, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:23.945236: step 6999, loss 0.137663, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:24.334703: step 7000, loss 0.153085, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:50:25.164960: step 7000, loss 0.206819, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7000

2017-10-10T13:50:26.444018: step 7001, loss 0.0931368, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:26.880333: step 7002, loss 0.0289608, acc 1, learning_rate 0.0001
2017-10-10T13:50:27.292913: step 7003, loss 0.145422, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:27.692735: step 7004, loss 0.273151, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:28.118582: step 7005, loss 0.120298, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:28.528111: step 7006, loss 0.211851, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:28.928872: step 7007, loss 0.0487367, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:29.324446: step 7008, loss 0.149494, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:29.748876: step 7009, loss 0.228809, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:30.165902: step 7010, loss 0.152022, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:30.589211: step 7011, loss 0.188663, acc 0.875, learning_rate 0.0001
2017-10-10T13:50:31.021778: step 7012, loss 0.14288, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:31.481228: step 7013, loss 0.158034, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:31.866503: step 7014, loss 0.297049, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:32.197975: step 7015, loss 0.0730631, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:32.515684: step 7016, loss 0.176359, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:32.921754: step 7017, loss 0.0765455, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:33.352122: step 7018, loss 0.123366, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:33.782849: step 7019, loss 0.166157, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:34.201757: step 7020, loss 0.139776, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:34.623594: step 7021, loss 0.0954607, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:35.046242: step 7022, loss 0.17447, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:35.464826: step 7023, loss 0.134664, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:35.910411: step 7024, loss 0.138139, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:36.292999: step 7025, loss 0.393377, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:36.712855: step 7026, loss 0.117422, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:37.112836: step 7027, loss 0.274781, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:37.515592: step 7028, loss 0.185286, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:37.904798: step 7029, loss 0.0873619, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:38.308908: step 7030, loss 0.177851, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:38.792311: step 7031, loss 0.0849383, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:39.229292: step 7032, loss 0.136801, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:39.550511: step 7033, loss 0.111439, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:39.876710: step 7034, loss 0.264787, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:40.279241: step 7035, loss 0.0754616, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:40.691078: step 7036, loss 0.0862339, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:41.126998: step 7037, loss 0.061802, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:41.541633: step 7038, loss 0.167523, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:41.933986: step 7039, loss 0.157331, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:42.324943: step 7040, loss 0.0668645, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:50:43.248829: step 7040, loss 0.205963, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7040

2017-10-10T13:50:44.689768: step 7041, loss 0.13978, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:45.073179: step 7042, loss 0.0707866, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:45.505344: step 7043, loss 0.0905699, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:45.907465: step 7044, loss 0.17044, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:46.328082: step 7045, loss 0.176578, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:46.731150: step 7046, loss 0.132252, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:47.168105: step 7047, loss 0.165004, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:47.575138: step 7048, loss 0.0729435, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:48.001536: step 7049, loss 0.186601, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:48.398993: step 7050, loss 0.169573, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:48.748907: step 7051, loss 0.164201, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:49.181298: step 7052, loss 0.0958443, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:49.564945: step 7053, loss 0.0602993, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:49.926238: step 7054, loss 0.237259, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:50.222261: step 7055, loss 0.273155, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:50.464876: step 7056, loss 0.178676, acc 0.941176, learning_rate 0.0001
2017-10-10T13:50:50.817039: step 7057, loss 0.0400835, acc 1, learning_rate 0.0001
2017-10-10T13:50:51.201574: step 7058, loss 0.10746, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:51.573128: step 7059, loss 0.228257, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:51.950638: step 7060, loss 0.173007, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:52.354453: step 7061, loss 0.0986923, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:52.730925: step 7062, loss 0.105339, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:53.112913: step 7063, loss 0.0584964, acc 1, learning_rate 0.0001
2017-10-10T13:50:53.524847: step 7064, loss 0.106095, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:53.920899: step 7065, loss 0.0556688, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:54.330296: step 7066, loss 0.112936, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:54.758605: step 7067, loss 0.127126, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:55.143336: step 7068, loss 0.120812, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:55.557772: step 7069, loss 0.0872374, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:56.129193: step 7070, loss 0.0638119, acc 1, learning_rate 0.0001
2017-10-10T13:50:56.455135: step 7071, loss 0.204715, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:56.774467: step 7072, loss 0.0675095, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:57.081227: step 7073, loss 0.121674, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:57.484853: step 7074, loss 0.148187, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:57.913393: step 7075, loss 0.116482, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:58.339603: step 7076, loss 0.0678129, acc 0.984375, learning_rate 0.0001
2017-10-10T13:50:58.762747: step 7077, loss 0.110392, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:59.187147: step 7078, loss 0.123338, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:59.601328: step 7079, loss 0.218147, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:00.045150: step 7080, loss 0.133731, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:51:00.936051: step 7080, loss 0.205862, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7080

2017-10-10T13:51:02.401291: step 7081, loss 0.0944607, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:02.734981: step 7082, loss 0.151191, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:03.145165: step 7083, loss 0.0929517, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:03.564857: step 7084, loss 0.0717776, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:03.960849: step 7085, loss 0.122386, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:04.362447: step 7086, loss 0.188297, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:04.782238: step 7087, loss 0.0924259, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:05.206831: step 7088, loss 0.136013, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:05.614315: step 7089, loss 0.205126, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:06.028473: step 7090, loss 0.191419, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:06.437640: step 7091, loss 0.0711894, acc 1, learning_rate 0.0001
2017-10-10T13:51:06.868884: step 7092, loss 0.0937292, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:07.357026: step 7093, loss 0.139263, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:07.625209: step 7094, loss 0.185333, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:07.929639: step 7095, loss 0.208489, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:08.256894: step 7096, loss 0.188309, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:08.632843: step 7097, loss 0.187938, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:09.013430: step 7098, loss 0.271987, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:09.344918: step 7099, loss 0.103765, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:09.702088: step 7100, loss 0.104642, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:10.051628: step 7101, loss 0.167349, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:10.443747: step 7102, loss 0.179878, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:10.822587: step 7103, loss 0.111686, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:11.183796: step 7104, loss 0.176863, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:11.556852: step 7105, loss 0.0938237, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:11.936817: step 7106, loss 0.160198, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:12.353094: step 7107, loss 0.100005, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:12.769503: step 7108, loss 0.117136, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:13.191868: step 7109, loss 0.0997891, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:13.660885: step 7110, loss 0.121793, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:14.160411: step 7111, loss 0.181253, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:14.479762: step 7112, loss 0.111741, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:14.796552: step 7113, loss 0.168591, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:15.119937: step 7114, loss 0.404825, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:15.529019: step 7115, loss 0.152357, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:15.905632: step 7116, loss 0.201732, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:16.305841: step 7117, loss 0.0675902, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:16.716475: step 7118, loss 0.215617, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:17.154048: step 7119, loss 0.170146, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:17.519035: step 7120, loss 0.0834072, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:51:18.367165: step 7120, loss 0.206155, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7120

2017-10-10T13:51:19.516964: step 7121, loss 0.22686, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:19.913874: step 7122, loss 0.173574, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:20.328814: step 7123, loss 0.115662, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:20.744390: step 7124, loss 0.0974807, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:21.141033: step 7125, loss 0.0950285, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:21.536842: step 7126, loss 0.190442, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:21.948159: step 7127, loss 0.0490596, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:22.337120: step 7128, loss 0.16425, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:22.765121: step 7129, loss 0.065138, acc 1, learning_rate 0.0001
2017-10-10T13:51:23.172672: step 7130, loss 0.199127, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:23.587619: step 7131, loss 0.0968125, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:23.946930: step 7132, loss 0.209576, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:24.344831: step 7133, loss 0.160688, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:24.857291: step 7134, loss 0.0886607, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:25.198223: step 7135, loss 0.144843, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:25.506645: step 7136, loss 0.191725, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:25.821052: step 7137, loss 0.130636, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:26.125814: step 7138, loss 0.119554, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:26.535414: step 7139, loss 0.111108, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:26.924693: step 7140, loss 0.155048, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:27.368704: step 7141, loss 0.143025, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:27.774772: step 7142, loss 0.251675, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:28.189085: step 7143, loss 0.125494, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:28.596104: step 7144, loss 0.153551, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:29.029815: step 7145, loss 0.101446, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:29.442676: step 7146, loss 0.163648, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:29.880449: step 7147, loss 0.136684, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:30.252827: step 7148, loss 0.151275, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:30.651838: step 7149, loss 0.161318, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:31.072982: step 7150, loss 0.118844, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:31.556871: step 7151, loss 0.234724, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:32.019786: step 7152, loss 0.24459, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:32.332018: step 7153, loss 0.0820233, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:32.608606: step 7154, loss 0.1414, acc 0.941176, learning_rate 0.0001
2017-10-10T13:51:32.977559: step 7155, loss 0.135447, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:33.353570: step 7156, loss 0.0856546, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:33.763908: step 7157, loss 0.146533, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:34.104943: step 7158, loss 0.132523, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:34.524945: step 7159, loss 0.159312, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:34.933025: step 7160, loss 0.232588, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:51:35.857277: step 7160, loss 0.206553, acc 0.923741

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7160

2017-10-10T13:51:37.302700: step 7161, loss 0.0804636, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:37.760920: step 7162, loss 0.0814917, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:38.173751: step 7163, loss 0.111212, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:38.620879: step 7164, loss 0.093277, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:39.040159: step 7165, loss 0.155277, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:39.446713: step 7166, loss 0.160014, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:39.805042: step 7167, loss 0.112204, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:40.275687: step 7168, loss 0.224348, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:40.729242: step 7169, loss 0.139704, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:41.140904: step 7170, loss 0.0964775, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:41.557064: step 7171, loss 0.186008, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:41.988788: step 7172, loss 0.132238, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:42.385103: step 7173, loss 0.0479382, acc 1, learning_rate 0.0001
2017-10-10T13:51:42.852451: step 7174, loss 0.133811, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:43.105563: step 7175, loss 0.0689839, acc 1, learning_rate 0.0001
2017-10-10T13:51:43.456109: step 7176, loss 0.147875, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:43.768792: step 7177, loss 0.183745, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:44.190067: step 7178, loss 0.0810297, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:44.601743: step 7179, loss 0.100074, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:44.999388: step 7180, loss 0.122182, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:45.432397: step 7181, loss 0.153184, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:45.851353: step 7182, loss 0.0697718, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:46.276788: step 7183, loss 0.165934, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:46.684062: step 7184, loss 0.0744383, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:47.105966: step 7185, loss 0.197731, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:47.514357: step 7186, loss 0.123976, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:47.959458: step 7187, loss 0.156412, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:48.376875: step 7188, loss 0.148642, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:48.848215: step 7189, loss 0.0501659, acc 1, learning_rate 0.0001
2017-10-10T13:51:49.302834: step 7190, loss 0.100438, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:49.597918: step 7191, loss 0.142658, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:49.911097: step 7192, loss 0.209452, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:50.206076: step 7193, loss 0.166402, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:50.571928: step 7194, loss 0.0587284, acc 1, learning_rate 0.0001
2017-10-10T13:51:51.003971: step 7195, loss 0.160776, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:51.421511: step 7196, loss 0.0986372, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:51.851019: step 7197, loss 0.128338, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:52.303796: step 7198, loss 0.0819966, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:52.719946: step 7199, loss 0.0427252, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:53.144852: step 7200, loss 0.170983, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:51:54.048039: step 7200, loss 0.20606, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7200

2017-10-10T13:51:55.459771: step 7201, loss 0.177016, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:55.880038: step 7202, loss 0.210227, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:56.345626: step 7203, loss 0.16675, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:56.808886: step 7204, loss 0.127618, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:57.165848: step 7205, loss 0.0985963, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:57.596953: step 7206, loss 0.200094, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:58.004942: step 7207, loss 0.133441, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:58.384883: step 7208, loss 0.201558, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:58.747686: step 7209, loss 0.115281, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:59.163837: step 7210, loss 0.163306, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:59.538486: step 7211, loss 0.189372, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:59.961107: step 7212, loss 0.0541159, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:00.481312: step 7213, loss 0.183984, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:00.851907: step 7214, loss 0.145513, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:01.157615: step 7215, loss 0.178918, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:01.473264: step 7216, loss 0.200998, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:01.884897: step 7217, loss 0.1495, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:02.284816: step 7218, loss 0.21535, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:02.697116: step 7219, loss 0.152306, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:03.095743: step 7220, loss 0.134929, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:03.497166: step 7221, loss 0.132845, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:03.908190: step 7222, loss 0.170736, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:04.343271: step 7223, loss 0.129806, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:04.763292: step 7224, loss 0.132708, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:05.188894: step 7225, loss 0.112013, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:05.676829: step 7226, loss 0.101203, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:06.052981: step 7227, loss 0.131329, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:06.436267: step 7228, loss 0.114031, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:06.904881: step 7229, loss 0.159688, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:07.241818: step 7230, loss 0.19907, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:07.540848: step 7231, loss 0.083744, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:07.824842: step 7232, loss 0.155332, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:08.144845: step 7233, loss 0.135847, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:08.524433: step 7234, loss 0.247616, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:08.941137: step 7235, loss 0.15179, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:09.364111: step 7236, loss 0.3267, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:09.778613: step 7237, loss 0.266237, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:10.178420: step 7238, loss 0.24823, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:10.596951: step 7239, loss 0.188633, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:11.027163: step 7240, loss 0.068239, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:52:11.896908: step 7240, loss 0.205608, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7240

2017-10-10T13:52:13.431223: step 7241, loss 0.0659253, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:13.776977: step 7242, loss 0.0917427, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:14.176851: step 7243, loss 0.146284, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:14.721917: step 7244, loss 0.174503, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:15.082706: step 7245, loss 0.156616, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:15.469181: step 7246, loss 0.131455, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:15.825101: step 7247, loss 0.042312, acc 1, learning_rate 0.0001
2017-10-10T13:52:16.257614: step 7248, loss 0.139085, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:16.644835: step 7249, loss 0.078977, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:17.052873: step 7250, loss 0.10978, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:17.432850: step 7251, loss 0.0755921, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:17.865138: step 7252, loss 0.0881129, acc 1, learning_rate 0.0001
2017-10-10T13:52:18.241908: step 7253, loss 0.165198, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:18.550733: step 7254, loss 0.112412, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:18.873931: step 7255, loss 0.138564, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:19.196052: step 7256, loss 0.0875037, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:19.610880: step 7257, loss 0.164567, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:20.035623: step 7258, loss 0.0896507, acc 1, learning_rate 0.0001
2017-10-10T13:52:20.465710: step 7259, loss 0.101436, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:20.885262: step 7260, loss 0.195214, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:21.308370: step 7261, loss 0.116337, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:21.745090: step 7262, loss 0.172804, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:22.168961: step 7263, loss 0.0879438, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:22.560887: step 7264, loss 0.0491857, acc 1, learning_rate 0.0001
2017-10-10T13:52:22.934382: step 7265, loss 0.187702, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:23.324593: step 7266, loss 0.277202, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:23.791413: step 7267, loss 0.0902307, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:24.163735: step 7268, loss 0.152489, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:24.644919: step 7269, loss 0.170227, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:25.144023: step 7270, loss 0.108252, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:25.465127: step 7271, loss 0.0730144, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:25.764921: step 7272, loss 0.100504, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:26.160813: step 7273, loss 0.129988, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:26.575039: step 7274, loss 0.190369, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:27.002579: step 7275, loss 0.121149, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:27.413310: step 7276, loss 0.181143, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:27.793007: step 7277, loss 0.12676, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:28.128844: step 7278, loss 0.0929849, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:28.541216: step 7279, loss 0.0474868, acc 1, learning_rate 0.0001
2017-10-10T13:52:28.896924: step 7280, loss 0.215894, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:52:29.737028: step 7280, loss 0.203888, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7280

2017-10-10T13:52:31.032954: step 7281, loss 0.0739961, acc 1, learning_rate 0.0001
2017-10-10T13:52:31.429533: step 7282, loss 0.122489, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:31.824936: step 7283, loss 0.0868137, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:32.249292: step 7284, loss 0.0794028, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:32.625552: step 7285, loss 0.167181, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:33.017066: step 7286, loss 0.381106, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:33.429036: step 7287, loss 0.124836, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:33.889680: step 7288, loss 0.097955, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:34.321192: step 7289, loss 0.215072, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:34.746128: step 7290, loss 0.196969, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:35.245285: step 7291, loss 0.172938, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:35.670871: step 7292, loss 0.105013, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:35.969346: step 7293, loss 0.126784, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:36.260815: step 7294, loss 0.0410204, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:36.598402: step 7295, loss 0.225435, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:37.044802: step 7296, loss 0.0859518, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:37.480829: step 7297, loss 0.138558, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:37.919206: step 7298, loss 0.187955, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:38.344294: step 7299, loss 0.101811, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:38.762849: step 7300, loss 0.0881061, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:39.175127: step 7301, loss 0.230465, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:39.601722: step 7302, loss 0.1204, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:40.020585: step 7303, loss 0.0542388, acc 1, learning_rate 0.0001
2017-10-10T13:52:40.445042: step 7304, loss 0.0945883, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:40.862127: step 7305, loss 0.292189, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:41.299867: step 7306, loss 0.284726, acc 0.84375, learning_rate 0.0001
2017-10-10T13:52:41.710974: step 7307, loss 0.152927, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:42.172881: step 7308, loss 0.259976, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:42.637725: step 7309, loss 0.062979, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:42.925664: step 7310, loss 0.0648391, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:43.248913: step 7311, loss 0.101202, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:43.539969: step 7312, loss 0.0710212, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:43.918791: step 7313, loss 0.113316, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:44.320872: step 7314, loss 0.125512, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:44.751224: step 7315, loss 0.168482, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:45.140629: step 7316, loss 0.131116, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:45.540304: step 7317, loss 0.177677, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:45.948570: step 7318, loss 0.0978709, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:46.367716: step 7319, loss 0.103718, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:46.811832: step 7320, loss 0.190692, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:52:47.717986: step 7320, loss 0.204535, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7320

2017-10-10T13:52:49.081108: step 7321, loss 0.147556, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:49.507053: step 7322, loss 0.0902561, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:49.960943: step 7323, loss 0.133975, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:50.365744: step 7324, loss 0.18591, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:50.692325: step 7325, loss 0.12923, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:51.103579: step 7326, loss 0.0882265, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:51.505077: step 7327, loss 0.10293, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:51.946928: step 7328, loss 0.113641, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:52.372839: step 7329, loss 0.175548, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:52.926412: step 7330, loss 0.0609539, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:53.250203: step 7331, loss 0.0721603, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:53.556312: step 7332, loss 0.139485, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:53.855182: step 7333, loss 0.0745246, acc 1, learning_rate 0.0001
2017-10-10T13:52:54.160024: step 7334, loss 0.149695, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:54.459029: step 7335, loss 0.15908, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:54.688863: step 7336, loss 0.19431, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:55.009866: step 7337, loss 0.0727651, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:55.319794: step 7338, loss 0.144669, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:55.628898: step 7339, loss 0.208949, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:55.927819: step 7340, loss 0.101681, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:56.223621: step 7341, loss 0.120941, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:56.516988: step 7342, loss 0.311579, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:56.806823: step 7343, loss 0.0948405, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:57.099310: step 7344, loss 0.0717616, acc 1, learning_rate 0.0001
2017-10-10T13:52:57.396712: step 7345, loss 0.038408, acc 1, learning_rate 0.0001
2017-10-10T13:52:57.667937: step 7346, loss 0.0856006, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:57.972877: step 7347, loss 0.223618, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:58.334136: step 7348, loss 0.076278, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:58.510461: step 7349, loss 0.0973094, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:58.659881: step 7350, loss 0.0824608, acc 0.960784, learning_rate 0.0001
2017-10-10T13:52:58.848391: step 7351, loss 0.144598, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:59.028935: step 7352, loss 0.290654, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:59.202052: step 7353, loss 0.118354, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:59.394954: step 7354, loss 0.221155, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:59.677019: step 7355, loss 0.120844, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:59.936732: step 7356, loss 0.111709, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:00.196873: step 7357, loss 0.194372, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:00.456881: step 7358, loss 0.267905, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:00.740539: step 7359, loss 0.16759, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:01.027160: step 7360, loss 0.0290791, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:53:01.625063: step 7360, loss 0.205314, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7360

2017-10-10T13:53:02.968695: step 7361, loss 0.232233, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:03.252541: step 7362, loss 0.164927, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:03.532562: step 7363, loss 0.102621, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:03.820716: step 7364, loss 0.225115, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:04.110124: step 7365, loss 0.149228, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:04.400529: step 7366, loss 0.173042, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:04.669948: step 7367, loss 0.179448, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:04.966342: step 7368, loss 0.139643, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:05.260153: step 7369, loss 0.069212, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:05.541958: step 7370, loss 0.0589778, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:05.810657: step 7371, loss 0.197446, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:06.094881: step 7372, loss 0.128623, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:06.370989: step 7373, loss 0.265873, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:06.656065: step 7374, loss 0.171634, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:06.952243: step 7375, loss 0.133466, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:07.240162: step 7376, loss 0.0829508, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:07.521885: step 7377, loss 0.206382, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:07.807575: step 7378, loss 0.114916, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:08.101995: step 7379, loss 0.154631, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:08.402100: step 7380, loss 0.067168, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:08.679393: step 7381, loss 0.112245, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:08.959153: step 7382, loss 0.0734071, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:09.248585: step 7383, loss 0.0994758, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:09.529489: step 7384, loss 0.150643, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:09.814731: step 7385, loss 0.0854857, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:10.091949: step 7386, loss 0.160797, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:10.400889: step 7387, loss 0.150999, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:10.788795: step 7388, loss 0.158042, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:10.983578: step 7389, loss 0.102459, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:11.169000: step 7390, loss 0.0534777, acc 1, learning_rate 0.0001
2017-10-10T13:53:11.343588: step 7391, loss 0.24505, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:11.516434: step 7392, loss 0.222262, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:11.808026: step 7393, loss 0.169838, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:12.097179: step 7394, loss 0.199699, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:12.390199: step 7395, loss 0.0851563, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:12.671768: step 7396, loss 0.119746, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:12.958138: step 7397, loss 0.243018, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:13.226423: step 7398, loss 0.185348, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:13.502813: step 7399, loss 0.215639, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:13.777543: step 7400, loss 0.0742129, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:53:14.440168: step 7400, loss 0.204194, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7400

2017-10-10T13:53:15.463959: step 7401, loss 0.12513, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:15.746569: step 7402, loss 0.132223, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:16.026654: step 7403, loss 0.167762, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:16.312083: step 7404, loss 0.136254, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:16.593977: step 7405, loss 0.10281, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:16.880071: step 7406, loss 0.122886, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:17.171499: step 7407, loss 0.0861946, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:17.466501: step 7408, loss 0.0723351, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:17.762431: step 7409, loss 0.1162, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:18.058998: step 7410, loss 0.0849382, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:18.325028: step 7411, loss 0.107035, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:18.610618: step 7412, loss 0.108159, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:18.891656: step 7413, loss 0.131314, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:19.194747: step 7414, loss 0.189129, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:19.498209: step 7415, loss 0.146827, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:19.803506: step 7416, loss 0.154845, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:20.098783: step 7417, loss 0.142261, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:20.390244: step 7418, loss 0.113234, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:20.676295: step 7419, loss 0.0852959, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:20.929690: step 7420, loss 0.0630551, acc 1, learning_rate 0.0001
2017-10-10T13:53:21.209087: step 7421, loss 0.0797565, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:21.494139: step 7422, loss 0.150143, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:21.780981: step 7423, loss 0.16975, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:22.069452: step 7424, loss 0.15342, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:22.354538: step 7425, loss 0.101124, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:22.645502: step 7426, loss 0.137317, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:22.980221: step 7427, loss 0.130243, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:23.359432: step 7428, loss 0.11822, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:23.536855: step 7429, loss 0.141344, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:23.713794: step 7430, loss 0.106045, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:23.897286: step 7431, loss 0.148358, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:24.071567: step 7432, loss 0.119812, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:24.301420: step 7433, loss 0.099714, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:24.550774: step 7434, loss 0.192136, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:24.787973: step 7435, loss 0.0704233, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:25.053858: step 7436, loss 0.196238, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:25.331712: step 7437, loss 0.22645, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:25.622780: step 7438, loss 0.154156, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:25.923986: step 7439, loss 0.243713, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:26.217469: step 7440, loss 0.152652, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:53:26.866557: step 7440, loss 0.203776, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7440

2017-10-10T13:53:28.025149: step 7441, loss 0.0816183, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:28.313486: step 7442, loss 0.107628, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:28.603940: step 7443, loss 0.130618, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:28.906452: step 7444, loss 0.185164, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:29.175106: step 7445, loss 0.103829, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:29.459634: step 7446, loss 0.0861536, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:29.752311: step 7447, loss 0.118052, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:29.995913: step 7448, loss 0.0997779, acc 0.960784, learning_rate 0.0001
2017-10-10T13:53:30.268542: step 7449, loss 0.0701104, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:30.558747: step 7450, loss 0.097455, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:31.044215: step 7451, loss 0.0987007, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:31.318989: step 7452, loss 0.100615, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:31.587390: step 7453, loss 0.138399, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:31.876208: step 7454, loss 0.143036, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:32.152466: step 7455, loss 0.197375, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:32.437360: step 7456, loss 0.126446, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:32.721024: step 7457, loss 0.126343, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:33.008895: step 7458, loss 0.1705, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:33.321359: step 7459, loss 0.0852049, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:33.617552: step 7460, loss 0.0881924, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:33.914154: step 7461, loss 0.0604435, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:34.213214: step 7462, loss 0.115978, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:34.500599: step 7463, loss 0.158628, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:34.787379: step 7464, loss 0.115809, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:35.079801: step 7465, loss 0.107468, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:35.420388: step 7466, loss 0.135011, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:35.765904: step 7467, loss 0.136197, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:35.955934: step 7468, loss 0.150796, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:36.141194: step 7469, loss 0.0994935, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:36.327053: step 7470, loss 0.0731762, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:36.511766: step 7471, loss 0.107669, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:36.708897: step 7472, loss 0.0499071, acc 1, learning_rate 0.0001
2017-10-10T13:53:36.985406: step 7473, loss 0.110222, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:37.242111: step 7474, loss 0.0909063, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:37.498332: step 7475, loss 0.119824, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:37.774021: step 7476, loss 0.0671854, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:38.057639: step 7477, loss 0.0327899, acc 1, learning_rate 0.0001
2017-10-10T13:53:38.344491: step 7478, loss 0.199328, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:38.627921: step 7479, loss 0.0489353, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:38.920252: step 7480, loss 0.152332, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:53:39.518527: step 7480, loss 0.204789, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7480

2017-10-10T13:53:40.668282: step 7481, loss 0.121641, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:40.955964: step 7482, loss 0.117699, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:41.242596: step 7483, loss 0.199935, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:41.532228: step 7484, loss 0.144853, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:41.817789: step 7485, loss 0.0871861, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:42.099942: step 7486, loss 0.142204, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:42.390702: step 7487, loss 0.153842, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:42.677086: step 7488, loss 0.171856, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:42.955960: step 7489, loss 0.19335, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:43.238302: step 7490, loss 0.07747, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:43.517384: step 7491, loss 0.169642, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:43.803980: step 7492, loss 0.229716, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:44.088297: step 7493, loss 0.111923, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:44.379749: step 7494, loss 0.14242, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:44.669443: step 7495, loss 0.0984069, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:44.966485: step 7496, loss 0.109877, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:45.254583: step 7497, loss 0.100077, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:45.539005: step 7498, loss 0.177905, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:45.835548: step 7499, loss 0.0900432, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:46.126548: step 7500, loss 0.188385, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:46.413422: step 7501, loss 0.224488, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:46.709664: step 7502, loss 0.0874864, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:47.000273: step 7503, loss 0.174445, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:47.292451: step 7504, loss 0.180883, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:47.584543: step 7505, loss 0.0681648, acc 1, learning_rate 0.0001
2017-10-10T13:53:47.875397: step 7506, loss 0.205864, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:48.197482: step 7507, loss 0.211521, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:48.540969: step 7508, loss 0.216214, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:48.718883: step 7509, loss 0.172182, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:48.906710: step 7510, loss 0.169661, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:49.085885: step 7511, loss 0.112513, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:49.257255: step 7512, loss 0.188969, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:49.427545: step 7513, loss 0.0713895, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:49.634467: step 7514, loss 0.116484, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:49.901178: step 7515, loss 0.106578, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:50.140868: step 7516, loss 0.193868, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:50.410361: step 7517, loss 0.139864, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:50.693586: step 7518, loss 0.107377, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:50.980597: step 7519, loss 0.0881395, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:51.278285: step 7520, loss 0.0788281, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:53:51.889194: step 7520, loss 0.20442, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7520

2017-10-10T13:53:52.929959: step 7521, loss 0.0625974, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:53.215316: step 7522, loss 0.205428, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:53.492534: step 7523, loss 0.119416, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:53.774328: step 7524, loss 0.100105, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:54.053525: step 7525, loss 0.269187, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:54.354229: step 7526, loss 0.161904, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:54.645805: step 7527, loss 0.17591, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:54.935650: step 7528, loss 0.104083, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:55.223161: step 7529, loss 0.169022, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:55.511758: step 7530, loss 0.252012, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:55.801451: step 7531, loss 0.135476, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:56.090031: step 7532, loss 0.207647, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:56.376144: step 7533, loss 0.119936, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:56.677445: step 7534, loss 0.134066, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:56.959900: step 7535, loss 0.184998, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:57.251514: step 7536, loss 0.0874483, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:57.555713: step 7537, loss 0.102746, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:57.855148: step 7538, loss 0.121371, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:58.152891: step 7539, loss 0.0859307, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:58.447251: step 7540, loss 0.0604987, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:58.737354: step 7541, loss 0.215183, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:59.016171: step 7542, loss 0.114367, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:59.307880: step 7543, loss 0.155031, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:59.587702: step 7544, loss 0.0690183, acc 1, learning_rate 0.0001
2017-10-10T13:53:59.859942: step 7545, loss 0.163413, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:00.105817: step 7546, loss 0.219864, acc 0.941176, learning_rate 0.0001
2017-10-10T13:54:00.398964: step 7547, loss 0.216025, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:00.693020: step 7548, loss 0.0744907, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:01.072213: step 7549, loss 0.116951, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:01.374151: step 7550, loss 0.0815228, acc 1, learning_rate 0.0001
2017-10-10T13:54:01.558814: step 7551, loss 0.107605, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:01.738411: step 7552, loss 0.0995341, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:01.915648: step 7553, loss 0.239107, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:02.090335: step 7554, loss 0.0694122, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:02.384797: step 7555, loss 0.146768, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:02.668180: step 7556, loss 0.231473, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:02.965350: step 7557, loss 0.153977, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:03.252601: step 7558, loss 0.0486805, acc 1, learning_rate 0.0001
2017-10-10T13:54:03.537984: step 7559, loss 0.109109, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:03.828756: step 7560, loss 0.237016, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:54:04.471278: step 7560, loss 0.202299, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7560

2017-10-10T13:54:05.534780: step 7561, loss 0.15724, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:05.817440: step 7562, loss 0.0983869, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:06.066432: step 7563, loss 0.106638, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:06.601016: step 7564, loss 0.0939609, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:06.865354: step 7565, loss 0.164991, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:07.116858: step 7566, loss 0.168302, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:07.384834: step 7567, loss 0.116296, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:07.655652: step 7568, loss 0.0853147, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:07.941530: step 7569, loss 0.187692, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:08.228822: step 7570, loss 0.148683, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:08.514502: step 7571, loss 0.114069, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:08.804768: step 7572, loss 0.122788, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:09.090214: step 7573, loss 0.0932635, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:09.372304: step 7574, loss 0.0518067, acc 1, learning_rate 0.0001
2017-10-10T13:54:09.650131: step 7575, loss 0.0725305, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:09.944888: step 7576, loss 0.135829, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:10.227903: step 7577, loss 0.074912, acc 1, learning_rate 0.0001
2017-10-10T13:54:10.518525: step 7578, loss 0.158402, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:10.814141: step 7579, loss 0.0886011, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:11.097772: step 7580, loss 0.0900185, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:11.384554: step 7581, loss 0.0988953, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:11.676938: step 7582, loss 0.0619052, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:11.975207: step 7583, loss 0.120119, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:12.270314: step 7584, loss 0.155675, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:12.561234: step 7585, loss 0.20816, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:12.851072: step 7586, loss 0.240922, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:13.139139: step 7587, loss 0.0933303, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:13.525069: step 7588, loss 0.107703, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:13.821762: step 7589, loss 0.0761108, acc 1, learning_rate 0.0001
2017-10-10T13:54:14.010456: step 7590, loss 0.180289, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:14.186607: step 7591, loss 0.112956, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:14.363025: step 7592, loss 0.151934, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:14.539997: step 7593, loss 0.150321, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:14.778562: step 7594, loss 0.14051, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:15.029065: step 7595, loss 0.153042, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:15.305095: step 7596, loss 0.21919, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:15.592873: step 7597, loss 0.152893, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:15.858834: step 7598, loss 0.22378, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:16.064224: step 7599, loss 0.0618963, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:16.336486: step 7600, loss 0.0790165, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T13:54:16.959259: step 7600, loss 0.201619, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7600

2017-10-10T13:54:18.035224: step 7601, loss 0.100032, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:18.320924: step 7602, loss 0.193809, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:18.600967: step 7603, loss 0.0728811, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:18.909976: step 7604, loss 0.232188, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:19.210337: step 7605, loss 0.101025, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:19.493289: step 7606, loss 0.189974, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:19.780360: step 7607, loss 0.153998, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:20.067934: step 7608, loss 0.118103, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:20.347861: step 7609, loss 0.079124, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:20.640887: step 7610, loss 0.156202, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:20.939012: step 7611, loss 0.0452106, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:21.221106: step 7612, loss 0.105569, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:21.518967: step 7613, loss 0.110146, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:21.815064: step 7614, loss 0.124247, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:22.112140: step 7615, loss 0.187317, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:22.411778: step 7616, loss 0.122154, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:22.707345: step 7617, loss 0.0607291, acc 1, learning_rate 0.0001
2017-10-10T13:54:22.968594: step 7618, loss 0.224829, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:23.269108: step 7619, loss 0.10282, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:23.554030: step 7620, loss 0.0642828, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:23.839874: step 7621, loss 0.126805, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:24.130886: step 7622, loss 0.294954, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:24.414917: step 7623, loss 0.144328, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:24.693974: step 7624, loss 0.0831138, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:24.967343: step 7625, loss 0.0607076, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:25.245214: step 7626, loss 0.0840377, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:25.532875: step 7627, loss 0.14277, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:25.802092: step 7628, loss 0.179572, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:26.193136: step 7629, loss 0.0902825, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:26.475064: step 7630, loss 0.0982627, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:26.649533: step 7631, loss 0.206938, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:26.825173: step 7632, loss 0.113011, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:27.022185: step 7633, loss 0.0914413, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:27.211848: step 7634, loss 0.234544, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:27.509993: step 7635, loss 0.157489, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:27.799027: step 7636, loss 0.212451, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:28.094006: step 7637, loss 0.161189, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:28.399195: step 7638, loss 0.124446, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:28.693635: step 7639, loss 0.136478, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:28.985867: step 7640, loss 0.143557, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:54:29.594235: step 7640, loss 0.203468, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7640

2017-10-10T13:54:30.863772: step 7641, loss 0.0913029, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:31.156741: step 7642, loss 0.0886188, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:31.430887: step 7643, loss 0.133853, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:31.682314: step 7644, loss 0.184912, acc 0.960784, learning_rate 0.0001
2017-10-10T13:54:31.975536: step 7645, loss 0.105206, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:32.250352: step 7646, loss 0.153153, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:32.548348: step 7647, loss 0.190643, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:32.841432: step 7648, loss 0.0400335, acc 1, learning_rate 0.0001
2017-10-10T13:54:33.132423: step 7649, loss 0.220669, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:33.425732: step 7650, loss 0.11365, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:33.712856: step 7651, loss 0.166903, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:33.998704: step 7652, loss 0.0531887, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:34.286043: step 7653, loss 0.145329, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:34.583030: step 7654, loss 0.259902, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:34.868483: step 7655, loss 0.0532366, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:35.154603: step 7656, loss 0.13579, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:35.438608: step 7657, loss 0.086131, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:35.722439: step 7658, loss 0.197298, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:36.024795: step 7659, loss 0.160818, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:36.328924: step 7660, loss 0.0951408, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:36.605704: step 7661, loss 0.168338, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:36.896461: step 7662, loss 0.152323, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:37.177858: step 7663, loss 0.0861748, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:37.469886: step 7664, loss 0.0427847, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:37.745946: step 7665, loss 0.32321, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:38.038634: step 7666, loss 0.176457, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:38.333916: step 7667, loss 0.0631824, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:38.668332: step 7668, loss 0.191112, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:38.997491: step 7669, loss 0.195433, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:39.183540: step 7670, loss 0.0894057, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:39.375732: step 7671, loss 0.145769, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:39.565073: step 7672, loss 0.0777126, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:39.739328: step 7673, loss 0.268089, acc 0.875, learning_rate 0.0001
2017-10-10T13:54:40.031359: step 7674, loss 0.0938599, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:40.322493: step 7675, loss 0.0532118, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:40.605995: step 7676, loss 0.142017, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:40.878000: step 7677, loss 0.0977812, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:41.167381: step 7678, loss 0.0735792, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:41.446583: step 7679, loss 0.151293, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:41.728150: step 7680, loss 0.147037, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:54:42.394476: step 7680, loss 0.202251, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7680

2017-10-10T13:54:43.420974: step 7681, loss 0.159938, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:43.712173: step 7682, loss 0.174073, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:44.011788: step 7683, loss 0.175696, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:44.307633: step 7684, loss 0.200582, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:44.592021: step 7685, loss 0.0543397, acc 1, learning_rate 0.0001
2017-10-10T13:54:44.871287: step 7686, loss 0.118865, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:45.169406: step 7687, loss 0.0914439, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:45.456377: step 7688, loss 0.113079, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:45.736723: step 7689, loss 0.245662, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:46.030221: step 7690, loss 0.0834823, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:46.307829: step 7691, loss 0.124835, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:46.588350: step 7692, loss 0.0713221, acc 1, learning_rate 0.0001
2017-10-10T13:54:46.880051: step 7693, loss 0.219576, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:47.180098: step 7694, loss 0.26351, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:47.452075: step 7695, loss 0.184294, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:47.743517: step 7696, loss 0.187938, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:48.024356: step 7697, loss 0.112037, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:48.306653: step 7698, loss 0.156847, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:48.586088: step 7699, loss 0.193324, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:48.872563: step 7700, loss 0.235201, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:49.153908: step 7701, loss 0.162731, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:49.412582: step 7702, loss 0.134887, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:49.703780: step 7703, loss 0.13018, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:49.994416: step 7704, loss 0.188803, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:50.270458: step 7705, loss 0.164828, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:50.552357: step 7706, loss 0.206903, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:50.833783: step 7707, loss 0.0953921, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:51.188861: step 7708, loss 0.159399, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:51.501462: step 7709, loss 0.172183, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:51.687207: step 7710, loss 0.159604, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:51.866595: step 7711, loss 0.0781638, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:52.045838: step 7712, loss 0.1333, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:52.221079: step 7713, loss 0.108114, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:52.441062: step 7714, loss 0.0991954, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:52.717156: step 7715, loss 0.0931179, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:52.989045: step 7716, loss 0.0834028, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:53.250561: step 7717, loss 0.0704649, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:53.514519: step 7718, loss 0.0724064, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:53.761396: step 7719, loss 0.219501, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:54.024892: step 7720, loss 0.25259, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:54:54.633855: step 7720, loss 0.202489, acc 0.92518

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7720

2017-10-10T13:54:55.773956: step 7721, loss 0.165017, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:56.076528: step 7722, loss 0.0693786, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:56.376432: step 7723, loss 0.0605612, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:56.678318: step 7724, loss 0.178693, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:56.978242: step 7725, loss 0.0393677, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:57.270529: step 7726, loss 0.101817, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:57.566480: step 7727, loss 0.170639, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:57.849012: step 7728, loss 0.0605222, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:58.144429: step 7729, loss 0.0372435, acc 1, learning_rate 0.0001
2017-10-10T13:54:58.423566: step 7730, loss 0.104413, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:58.697288: step 7731, loss 0.201532, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:58.983964: step 7732, loss 0.0939173, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:59.267515: step 7733, loss 0.109807, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:59.553731: step 7734, loss 0.212676, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:59.839732: step 7735, loss 0.130814, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:00.128437: step 7736, loss 0.16705, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:00.406718: step 7737, loss 0.0542064, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:00.694832: step 7738, loss 0.0628583, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:00.990562: step 7739, loss 0.0526214, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:01.269317: step 7740, loss 0.0878746, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:01.564687: step 7741, loss 0.052669, acc 1, learning_rate 0.0001
2017-10-10T13:55:01.806341: step 7742, loss 0.0843491, acc 0.980392, learning_rate 0.0001
2017-10-10T13:55:02.101295: step 7743, loss 0.0658239, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:02.402840: step 7744, loss 0.10646, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:02.697209: step 7745, loss 0.107641, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:02.991071: step 7746, loss 0.08544, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:03.285548: step 7747, loss 0.0981997, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:03.737474: step 7748, loss 0.100753, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:03.943322: step 7749, loss 0.204957, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:04.139706: step 7750, loss 0.0792123, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:04.334321: step 7751, loss 0.115479, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:04.529012: step 7752, loss 0.0587717, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:04.718765: step 7753, loss 0.257669, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:05.015243: step 7754, loss 0.156567, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:05.303126: step 7755, loss 0.134321, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:05.579994: step 7756, loss 0.0757384, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:05.872078: step 7757, loss 0.20834, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:06.170646: step 7758, loss 0.10547, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:06.465203: step 7759, loss 0.0864993, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:06.755743: step 7760, loss 0.127373, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:55:07.382184: step 7760, loss 0.200963, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7760

2017-10-10T13:55:08.651346: step 7761, loss 0.075678, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:08.949084: step 7762, loss 0.148817, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:09.250404: step 7763, loss 0.10631, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:09.543466: step 7764, loss 0.129497, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:09.840835: step 7765, loss 0.161694, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:10.137008: step 7766, loss 0.144552, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:10.428501: step 7767, loss 0.167856, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:10.700564: step 7768, loss 0.225494, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:10.983734: step 7769, loss 0.175867, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:11.269266: step 7770, loss 0.223324, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:11.550786: step 7771, loss 0.0686139, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:11.833968: step 7772, loss 0.0760217, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:12.137332: step 7773, loss 0.129226, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:12.431725: step 7774, loss 0.241474, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:12.704151: step 7775, loss 0.275744, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:12.983178: step 7776, loss 0.175203, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:13.263011: step 7777, loss 0.141319, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:13.554547: step 7778, loss 0.268851, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:13.838838: step 7779, loss 0.264623, acc 0.859375, learning_rate 0.0001
2017-10-10T13:55:14.129702: step 7780, loss 0.153922, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:14.411286: step 7781, loss 0.127375, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:14.695828: step 7782, loss 0.180951, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:14.958770: step 7783, loss 0.0925584, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:15.225325: step 7784, loss 0.175579, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:15.514695: step 7785, loss 0.0782479, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:15.798267: step 7786, loss 0.0996554, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:16.176890: step 7787, loss 0.126263, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:16.480725: step 7788, loss 0.135858, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:16.655815: step 7789, loss 0.0878521, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:16.825021: step 7790, loss 0.170356, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:17.008328: step 7791, loss 0.106198, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:17.184081: step 7792, loss 0.166266, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:17.491332: step 7793, loss 0.136626, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:17.779713: step 7794, loss 0.174986, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:18.069984: step 7795, loss 0.144872, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:18.349134: step 7796, loss 0.159416, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:18.627175: step 7797, loss 0.0767477, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:18.927770: step 7798, loss 0.0738499, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:19.207150: step 7799, loss 0.218556, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:19.506470: step 7800, loss 0.183386, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:55:20.152036: step 7800, loss 0.200408, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7800

2017-10-10T13:55:21.120835: step 7801, loss 0.114041, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:21.388834: step 7802, loss 0.0832162, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:21.660858: step 7803, loss 0.080577, acc 1, learning_rate 0.0001
2017-10-10T13:55:21.942366: step 7804, loss 0.196519, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:22.237339: step 7805, loss 0.206201, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:22.516793: step 7806, loss 0.0671256, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:22.800216: step 7807, loss 0.11015, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:23.082529: step 7808, loss 0.129317, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:23.368712: step 7809, loss 0.0814354, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:23.647518: step 7810, loss 0.0564056, acc 1, learning_rate 0.0001
2017-10-10T13:55:23.932355: step 7811, loss 0.152941, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:24.201458: step 7812, loss 0.10735, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:24.477767: step 7813, loss 0.145749, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:24.782349: step 7814, loss 0.0760412, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:25.044874: step 7815, loss 0.123557, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:25.301487: step 7816, loss 0.140044, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:25.588993: step 7817, loss 0.11034, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:25.860901: step 7818, loss 0.0826514, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:26.136907: step 7819, loss 0.164358, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:26.405078: step 7820, loss 0.156701, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:26.682964: step 7821, loss 0.166881, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:26.974096: step 7822, loss 0.107264, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:27.251154: step 7823, loss 0.129961, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:27.536852: step 7824, loss 0.185647, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:27.821508: step 7825, loss 0.122263, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:28.095896: step 7826, loss 0.0994553, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:28.460874: step 7827, loss 0.160427, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:28.813095: step 7828, loss 0.156211, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:28.992623: step 7829, loss 0.10247, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:29.167256: step 7830, loss 0.0340384, acc 1, learning_rate 0.0001
2017-10-10T13:55:29.339111: step 7831, loss 0.119243, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:29.512230: step 7832, loss 0.210065, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:29.741069: step 7833, loss 0.099391, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:29.970145: step 7834, loss 0.143313, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:30.269029: step 7835, loss 0.170896, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:30.584873: step 7836, loss 0.149362, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:30.884529: step 7837, loss 0.129488, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:31.172851: step 7838, loss 0.141033, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:31.467729: step 7839, loss 0.126989, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:31.733475: step 7840, loss 0.123263, acc 0.921569, learning_rate 0.0001

Evaluation:
2017-10-10T13:55:32.360887: step 7840, loss 0.202195, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507657355/checkpoints/model-7840

