
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=4
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=128

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507664605

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-10T14:43:30.844486: step 1, loss 6.98698, acc 0.21875, learning_rate 0.005
2017-10-10T14:43:31.030475: step 2, loss 6.67137, acc 0.265625, learning_rate 0.00498
2017-10-10T14:43:31.245158: step 3, loss 5.82974, acc 0.15625, learning_rate 0.00496008
2017-10-10T14:43:31.502623: step 4, loss 4.07673, acc 0.28125, learning_rate 0.00494024
2017-10-10T14:43:31.671887: step 5, loss 3.91606, acc 0.359375, learning_rate 0.00492049
2017-10-10T14:43:31.933960: step 6, loss 3.71339, acc 0.359375, learning_rate 0.00490081
2017-10-10T14:43:32.165337: step 7, loss 4.3558, acc 0.390625, learning_rate 0.00488121
2017-10-10T14:43:32.371190: step 8, loss 4.61053, acc 0.28125, learning_rate 0.0048617
2017-10-10T14:43:32.677178: step 9, loss 3.62118, acc 0.4375, learning_rate 0.00484226
2017-10-10T14:43:32.942613: step 10, loss 4.42468, acc 0.390625, learning_rate 0.00482291
2017-10-10T14:43:33.180863: step 11, loss 3.18871, acc 0.453125, learning_rate 0.00480363
2017-10-10T14:43:33.468831: step 12, loss 2.90093, acc 0.46875, learning_rate 0.00478443
2017-10-10T14:43:33.749316: step 13, loss 2.85888, acc 0.453125, learning_rate 0.00476531
2017-10-10T14:43:34.057008: step 14, loss 3.02307, acc 0.515625, learning_rate 0.00474627
2017-10-10T14:43:34.306457: step 15, loss 3.93294, acc 0.3125, learning_rate 0.0047273
2017-10-10T14:43:34.581141: step 16, loss 2.70638, acc 0.515625, learning_rate 0.00470841
2017-10-10T14:43:34.833068: step 17, loss 2.76533, acc 0.484375, learning_rate 0.0046896
2017-10-10T14:43:35.010463: step 18, loss 2.51753, acc 0.453125, learning_rate 0.00467087
2017-10-10T14:43:35.303143: step 19, loss 3.15802, acc 0.421875, learning_rate 0.00465221
2017-10-10T14:43:35.560847: step 20, loss 1.99048, acc 0.53125, learning_rate 0.00463363
2017-10-10T14:43:35.768059: step 21, loss 2.86718, acc 0.515625, learning_rate 0.00461513
2017-10-10T14:43:36.052828: step 22, loss 1.88217, acc 0.5625, learning_rate 0.0045967
2017-10-10T14:43:36.341996: step 23, loss 1.56216, acc 0.625, learning_rate 0.00457834
2017-10-10T14:43:36.596996: step 24, loss 2.42282, acc 0.5, learning_rate 0.00456006
2017-10-10T14:43:36.844560: step 25, loss 1.72626, acc 0.71875, learning_rate 0.00454186
2017-10-10T14:43:37.132922: step 26, loss 1.89722, acc 0.59375, learning_rate 0.00452373
2017-10-10T14:43:37.384111: step 27, loss 1.74317, acc 0.609375, learning_rate 0.00450567
2017-10-10T14:43:37.611394: step 28, loss 1.54438, acc 0.65625, learning_rate 0.00448769
2017-10-10T14:43:37.885617: step 29, loss 1.06951, acc 0.609375, learning_rate 0.00446978
2017-10-10T14:43:38.123732: step 30, loss 1.48791, acc 0.640625, learning_rate 0.00445194
2017-10-10T14:43:38.352836: step 31, loss 1.20498, acc 0.65625, learning_rate 0.00443418
2017-10-10T14:43:38.577185: step 32, loss 2.05296, acc 0.5625, learning_rate 0.00441649
2017-10-10T14:43:38.877934: step 33, loss 1.21539, acc 0.65625, learning_rate 0.00439887
2017-10-10T14:43:39.067461: step 34, loss 1.05613, acc 0.640625, learning_rate 0.00438132
2017-10-10T14:43:39.359029: step 35, loss 1.0286, acc 0.703125, learning_rate 0.00436385
2017-10-10T14:43:39.594650: step 36, loss 1.45603, acc 0.609375, learning_rate 0.00434644
2017-10-10T14:43:39.817856: step 37, loss 1.44913, acc 0.65625, learning_rate 0.00432911
2017-10-10T14:43:40.106260: step 38, loss 1.46719, acc 0.578125, learning_rate 0.00431185
2017-10-10T14:43:40.352327: step 39, loss 1.04792, acc 0.75, learning_rate 0.00429465
2017-10-10T14:43:40.619503: step 40, loss 0.941586, acc 0.703125, learning_rate 0.00427753

Evaluation:
2017-10-10T14:43:41.407787: step 40, loss 0.382799, acc 0.847482

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-40

2017-10-10T14:43:42.388295: step 41, loss 1.66044, acc 0.703125, learning_rate 0.00426048
2017-10-10T14:43:42.643829: step 42, loss 0.905967, acc 0.796875, learning_rate 0.0042435
2017-10-10T14:43:42.848151: step 43, loss 1.04443, acc 0.75, learning_rate 0.00422659
2017-10-10T14:43:43.116522: step 44, loss 1.0679, acc 0.734375, learning_rate 0.00420974
2017-10-10T14:43:43.361477: step 45, loss 1.18428, acc 0.6875, learning_rate 0.00419297
2017-10-10T14:43:43.567311: step 46, loss 1.14719, acc 0.640625, learning_rate 0.00417626
2017-10-10T14:43:43.834440: step 47, loss 1.35526, acc 0.671875, learning_rate 0.00415962
2017-10-10T14:43:44.119383: step 48, loss 1.11019, acc 0.703125, learning_rate 0.00414305
2017-10-10T14:43:44.376938: step 49, loss 0.827203, acc 0.75, learning_rate 0.00412655
2017-10-10T14:43:44.664448: step 50, loss 1.34557, acc 0.6875, learning_rate 0.00411011
2017-10-10T14:43:44.922878: step 51, loss 0.718233, acc 0.8125, learning_rate 0.00409375
2017-10-10T14:43:45.241042: step 52, loss 0.879446, acc 0.765625, learning_rate 0.00407744
2017-10-10T14:43:45.491343: step 53, loss 0.985002, acc 0.765625, learning_rate 0.00406121
2017-10-10T14:43:45.734617: step 54, loss 0.811363, acc 0.734375, learning_rate 0.00404504
2017-10-10T14:43:45.924961: step 55, loss 0.532753, acc 0.84375, learning_rate 0.00402894
2017-10-10T14:43:46.118187: step 56, loss 0.523213, acc 0.796875, learning_rate 0.0040129
2017-10-10T14:43:46.313455: step 57, loss 0.888745, acc 0.734375, learning_rate 0.00399693
2017-10-10T14:43:46.550847: step 58, loss 0.858924, acc 0.734375, learning_rate 0.00398102
2017-10-10T14:43:46.811515: step 59, loss 0.778584, acc 0.75, learning_rate 0.00396518
2017-10-10T14:43:47.091845: step 60, loss 0.82644, acc 0.75, learning_rate 0.00394941
2017-10-10T14:43:47.349052: step 61, loss 1.01213, acc 0.765625, learning_rate 0.00393369
2017-10-10T14:43:47.596611: step 62, loss 0.8517, acc 0.828125, learning_rate 0.00391804
2017-10-10T14:43:47.900831: step 63, loss 0.956637, acc 0.8125, learning_rate 0.00390246
2017-10-10T14:43:48.137048: step 64, loss 0.995364, acc 0.765625, learning_rate 0.00388694
2017-10-10T14:43:48.352829: step 65, loss 0.40635, acc 0.890625, learning_rate 0.00387148
2017-10-10T14:43:48.650745: step 66, loss 0.693312, acc 0.78125, learning_rate 0.00385609
2017-10-10T14:43:48.959043: step 67, loss 0.320992, acc 0.859375, learning_rate 0.00384076
2017-10-10T14:43:49.248889: step 68, loss 0.663265, acc 0.875, learning_rate 0.00382549
2017-10-10T14:43:49.508786: step 69, loss 0.649592, acc 0.8125, learning_rate 0.00381028
2017-10-10T14:43:49.786489: step 70, loss 0.850117, acc 0.71875, learning_rate 0.00379514
2017-10-10T14:43:50.100888: step 71, loss 0.662574, acc 0.828125, learning_rate 0.00378005
2017-10-10T14:43:50.413124: step 72, loss 0.341962, acc 0.890625, learning_rate 0.00376503
2017-10-10T14:43:50.685949: step 73, loss 0.701996, acc 0.8125, learning_rate 0.00375007
2017-10-10T14:43:50.985747: step 74, loss 0.67755, acc 0.8125, learning_rate 0.00373517
2017-10-10T14:43:51.261071: step 75, loss 0.433156, acc 0.828125, learning_rate 0.00372034
2017-10-10T14:43:51.544983: step 76, loss 0.91892, acc 0.84375, learning_rate 0.00370556
2017-10-10T14:43:51.820264: step 77, loss 0.456466, acc 0.828125, learning_rate 0.00369084
2017-10-10T14:43:52.088978: step 78, loss 0.506407, acc 0.859375, learning_rate 0.00367619
2017-10-10T14:43:52.350385: step 79, loss 0.462288, acc 0.859375, learning_rate 0.00366159
2017-10-10T14:43:52.617118: step 80, loss 0.644123, acc 0.796875, learning_rate 0.00364705

Evaluation:
2017-10-10T14:43:53.156942: step 80, loss 0.338694, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-80

2017-10-10T14:43:54.277316: step 81, loss 0.687306, acc 0.765625, learning_rate 0.00363257
2017-10-10T14:43:54.542441: step 82, loss 0.690299, acc 0.765625, learning_rate 0.00361815
2017-10-10T14:43:54.834719: step 83, loss 0.680664, acc 0.765625, learning_rate 0.00360379
2017-10-10T14:43:55.172895: step 84, loss 0.630915, acc 0.84375, learning_rate 0.00358949
2017-10-10T14:43:55.459443: step 85, loss 0.76028, acc 0.71875, learning_rate 0.00357525
2017-10-10T14:43:55.678316: step 86, loss 0.258447, acc 0.875, learning_rate 0.00356106
2017-10-10T14:43:55.965419: step 87, loss 0.636711, acc 0.796875, learning_rate 0.00354694
2017-10-10T14:43:56.221435: step 88, loss 1.07396, acc 0.734375, learning_rate 0.00353287
2017-10-10T14:43:56.516967: step 89, loss 0.907956, acc 0.75, learning_rate 0.00351885
2017-10-10T14:43:56.776977: step 90, loss 0.902558, acc 0.765625, learning_rate 0.0035049
2017-10-10T14:43:57.020943: step 91, loss 0.680551, acc 0.765625, learning_rate 0.003491
2017-10-10T14:43:57.341864: step 92, loss 0.309747, acc 0.890625, learning_rate 0.00347716
2017-10-10T14:43:57.622428: step 93, loss 0.538127, acc 0.828125, learning_rate 0.00346338
2017-10-10T14:43:57.920964: step 94, loss 0.486228, acc 0.875, learning_rate 0.00344965
2017-10-10T14:43:58.183517: step 95, loss 0.621074, acc 0.78125, learning_rate 0.00343597
2017-10-10T14:43:58.432876: step 96, loss 0.426689, acc 0.890625, learning_rate 0.00342236
2017-10-10T14:43:58.764468: step 97, loss 0.898823, acc 0.734375, learning_rate 0.0034088
2017-10-10T14:43:59.017165: step 98, loss 0.835313, acc 0.666667, learning_rate 0.00339529
2017-10-10T14:43:59.288026: step 99, loss 0.420041, acc 0.859375, learning_rate 0.00338184
2017-10-10T14:43:59.568567: step 100, loss 0.724962, acc 0.859375, learning_rate 0.00336844
2017-10-10T14:43:59.826929: step 101, loss 0.638722, acc 0.78125, learning_rate 0.0033551
2017-10-10T14:44:00.083663: step 102, loss 0.613619, acc 0.796875, learning_rate 0.00334182
2017-10-10T14:44:00.332592: step 103, loss 0.64389, acc 0.78125, learning_rate 0.00332858
2017-10-10T14:44:00.572825: step 104, loss 0.555483, acc 0.765625, learning_rate 0.00331541
2017-10-10T14:44:00.783228: step 105, loss 0.665526, acc 0.78125, learning_rate 0.00330228
2017-10-10T14:44:01.030421: step 106, loss 0.475757, acc 0.828125, learning_rate 0.00328921
2017-10-10T14:44:01.131341: step 107, loss 0.519806, acc 0.796875, learning_rate 0.00327619
2017-10-10T14:44:01.400688: step 108, loss 0.478304, acc 0.8125, learning_rate 0.00326323
2017-10-10T14:44:01.752871: step 109, loss 0.323487, acc 0.875, learning_rate 0.00325032
2017-10-10T14:44:02.100540: step 110, loss 0.879784, acc 0.734375, learning_rate 0.00323746
2017-10-10T14:44:02.350551: step 111, loss 0.441698, acc 0.828125, learning_rate 0.00322465
2017-10-10T14:44:02.568302: step 112, loss 0.293139, acc 0.921875, learning_rate 0.0032119
2017-10-10T14:44:02.756881: step 113, loss 0.344352, acc 0.859375, learning_rate 0.0031992
2017-10-10T14:44:02.999333: step 114, loss 0.620386, acc 0.78125, learning_rate 0.00318655
2017-10-10T14:44:03.229146: step 115, loss 0.430948, acc 0.796875, learning_rate 0.00317395
2017-10-10T14:44:03.518622: step 116, loss 0.750201, acc 0.734375, learning_rate 0.0031614
2017-10-10T14:44:03.823313: step 117, loss 0.485989, acc 0.828125, learning_rate 0.0031489
2017-10-10T14:44:04.087843: step 118, loss 0.5109, acc 0.859375, learning_rate 0.00313646
2017-10-10T14:44:04.353740: step 119, loss 0.69325, acc 0.796875, learning_rate 0.00312407
2017-10-10T14:44:04.633010: step 120, loss 0.582003, acc 0.796875, learning_rate 0.00311172

Evaluation:
2017-10-10T14:44:05.104841: step 120, loss 0.319677, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-120

2017-10-10T14:44:06.177113: step 121, loss 0.204392, acc 0.9375, learning_rate 0.00309943
2017-10-10T14:44:06.446067: step 122, loss 0.397259, acc 0.890625, learning_rate 0.00308719
2017-10-10T14:44:06.700708: step 123, loss 0.340743, acc 0.875, learning_rate 0.00307499
2017-10-10T14:44:06.988861: step 124, loss 0.410246, acc 0.859375, learning_rate 0.00306285
2017-10-10T14:44:07.268910: step 125, loss 0.40915, acc 0.90625, learning_rate 0.00305076
2017-10-10T14:44:07.515379: step 126, loss 0.387302, acc 0.921875, learning_rate 0.00303871
2017-10-10T14:44:07.735921: step 127, loss 0.559615, acc 0.765625, learning_rate 0.00302672
2017-10-10T14:44:08.037761: step 128, loss 0.542628, acc 0.8125, learning_rate 0.00301477
2017-10-10T14:44:08.269145: step 129, loss 0.394304, acc 0.875, learning_rate 0.00300287
2017-10-10T14:44:08.557048: step 130, loss 0.432897, acc 0.859375, learning_rate 0.00299102
2017-10-10T14:44:08.820828: step 131, loss 0.228723, acc 0.90625, learning_rate 0.00297922
2017-10-10T14:44:09.085116: step 132, loss 0.436555, acc 0.8125, learning_rate 0.00296747
2017-10-10T14:44:09.380857: step 133, loss 0.343126, acc 0.890625, learning_rate 0.00295577
2017-10-10T14:44:09.674594: step 134, loss 0.290018, acc 0.921875, learning_rate 0.00294411
2017-10-10T14:44:09.945003: step 135, loss 0.33874, acc 0.859375, learning_rate 0.0029325
2017-10-10T14:44:10.209079: step 136, loss 0.370675, acc 0.890625, learning_rate 0.00292094
2017-10-10T14:44:10.521569: step 137, loss 0.402504, acc 0.8125, learning_rate 0.00290943
2017-10-10T14:44:10.836906: step 138, loss 0.539351, acc 0.8125, learning_rate 0.00289796
2017-10-10T14:44:11.159301: step 139, loss 0.685508, acc 0.828125, learning_rate 0.00288654
2017-10-10T14:44:11.429178: step 140, loss 0.303662, acc 0.890625, learning_rate 0.00287516
2017-10-10T14:44:11.725345: step 141, loss 0.307229, acc 0.890625, learning_rate 0.00286384
2017-10-10T14:44:12.040866: step 142, loss 0.549097, acc 0.796875, learning_rate 0.00285256
2017-10-10T14:44:12.240419: step 143, loss 0.433234, acc 0.84375, learning_rate 0.00284132
2017-10-10T14:44:12.531661: step 144, loss 0.303706, acc 0.890625, learning_rate 0.00283013
2017-10-10T14:44:12.780967: step 145, loss 0.278839, acc 0.921875, learning_rate 0.00281899
2017-10-10T14:44:13.067570: step 146, loss 0.688902, acc 0.765625, learning_rate 0.00280789
2017-10-10T14:44:13.389130: step 147, loss 0.700183, acc 0.796875, learning_rate 0.00279684
2017-10-10T14:44:13.617308: step 148, loss 0.563474, acc 0.875, learning_rate 0.00278583
2017-10-10T14:44:13.950125: step 149, loss 0.34543, acc 0.859375, learning_rate 0.00277486
2017-10-10T14:44:14.221107: step 150, loss 0.614567, acc 0.84375, learning_rate 0.00276395
2017-10-10T14:44:14.476828: step 151, loss 0.48307, acc 0.8125, learning_rate 0.00275307
2017-10-10T14:44:14.779899: step 152, loss 0.575458, acc 0.8125, learning_rate 0.00274224
2017-10-10T14:44:14.979109: step 153, loss 0.414156, acc 0.890625, learning_rate 0.00273146
2017-10-10T14:44:15.256403: step 154, loss 0.414394, acc 0.828125, learning_rate 0.00272072
2017-10-10T14:44:15.533136: step 155, loss 0.282579, acc 0.90625, learning_rate 0.00271002
2017-10-10T14:44:15.817140: step 156, loss 0.563641, acc 0.78125, learning_rate 0.00269937
2017-10-10T14:44:16.058161: step 157, loss 0.245592, acc 0.90625, learning_rate 0.00268876
2017-10-10T14:44:16.334908: step 158, loss 0.341241, acc 0.90625, learning_rate 0.00267819
2017-10-10T14:44:16.652888: step 159, loss 0.824067, acc 0.828125, learning_rate 0.00266767
2017-10-10T14:44:17.001125: step 160, loss 0.393292, acc 0.828125, learning_rate 0.00265719

Evaluation:
2017-10-10T14:44:17.638917: step 160, loss 0.304895, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-160

2017-10-10T14:44:18.783958: step 161, loss 0.146657, acc 0.953125, learning_rate 0.00264675
2017-10-10T14:44:19.088706: step 162, loss 0.464607, acc 0.8125, learning_rate 0.00263635
2017-10-10T14:44:19.321469: step 163, loss 0.500225, acc 0.828125, learning_rate 0.002626
2017-10-10T14:44:19.501323: step 164, loss 0.563898, acc 0.828125, learning_rate 0.00261569
2017-10-10T14:44:19.772854: step 165, loss 0.47068, acc 0.828125, learning_rate 0.00260542
2017-10-10T14:44:19.996537: step 166, loss 0.527745, acc 0.84375, learning_rate 0.0025952
2017-10-10T14:44:20.236106: step 167, loss 0.235146, acc 0.953125, learning_rate 0.00258501
2017-10-10T14:44:20.548967: step 168, loss 0.384793, acc 0.859375, learning_rate 0.00257487
2017-10-10T14:44:20.871117: step 169, loss 0.310327, acc 0.90625, learning_rate 0.00256477
2017-10-10T14:44:21.225039: step 170, loss 0.500445, acc 0.84375, learning_rate 0.0025547
2017-10-10T14:44:21.508572: step 171, loss 0.313636, acc 0.875, learning_rate 0.00254469
2017-10-10T14:44:21.832676: step 172, loss 0.372244, acc 0.890625, learning_rate 0.00253471
2017-10-10T14:44:22.152894: step 173, loss 0.422109, acc 0.859375, learning_rate 0.00252477
2017-10-10T14:44:22.436420: step 174, loss 0.482305, acc 0.84375, learning_rate 0.00251487
2017-10-10T14:44:22.752429: step 175, loss 0.263628, acc 0.90625, learning_rate 0.00250501
2017-10-10T14:44:23.097171: step 176, loss 0.337169, acc 0.8125, learning_rate 0.0024952
2017-10-10T14:44:23.392845: step 177, loss 0.407942, acc 0.828125, learning_rate 0.00248542
2017-10-10T14:44:23.684833: step 178, loss 0.351138, acc 0.859375, learning_rate 0.00247568
2017-10-10T14:44:23.969798: step 179, loss 0.336772, acc 0.875, learning_rate 0.00246599
2017-10-10T14:44:24.228872: step 180, loss 0.447318, acc 0.84375, learning_rate 0.00245633
2017-10-10T14:44:24.520556: step 181, loss 0.570833, acc 0.765625, learning_rate 0.00244671
2017-10-10T14:44:24.847283: step 182, loss 0.670496, acc 0.828125, learning_rate 0.00243713
2017-10-10T14:44:25.092903: step 183, loss 0.261176, acc 0.921875, learning_rate 0.00242759
2017-10-10T14:44:25.328519: step 184, loss 0.63669, acc 0.859375, learning_rate 0.00241809
2017-10-10T14:44:25.593159: step 185, loss 0.420864, acc 0.90625, learning_rate 0.00240863
2017-10-10T14:44:25.876873: step 186, loss 0.253023, acc 0.9375, learning_rate 0.00239921
2017-10-10T14:44:26.179924: step 187, loss 0.466837, acc 0.84375, learning_rate 0.00238982
2017-10-10T14:44:26.484539: step 188, loss 0.261565, acc 0.890625, learning_rate 0.00238048
2017-10-10T14:44:26.742135: step 189, loss 0.245081, acc 0.890625, learning_rate 0.00237117
2017-10-10T14:44:27.076223: step 190, loss 0.343929, acc 0.859375, learning_rate 0.0023619
2017-10-10T14:44:27.305746: step 191, loss 0.561819, acc 0.84375, learning_rate 0.00235267
2017-10-10T14:44:27.491289: step 192, loss 0.389113, acc 0.890625, learning_rate 0.00234347
2017-10-10T14:44:27.755717: step 193, loss 0.259386, acc 0.90625, learning_rate 0.00233431
2017-10-10T14:44:27.972483: step 194, loss 0.239571, acc 0.921875, learning_rate 0.00232519
2017-10-10T14:44:28.248845: step 195, loss 0.24814, acc 0.90625, learning_rate 0.00231611
2017-10-10T14:44:28.464962: step 196, loss 0.515892, acc 0.882353, learning_rate 0.00230707
2017-10-10T14:44:28.738718: step 197, loss 0.213828, acc 0.921875, learning_rate 0.00229806
2017-10-10T14:44:29.057177: step 198, loss 0.254701, acc 0.921875, learning_rate 0.00228908
2017-10-10T14:44:29.335701: step 199, loss 0.192112, acc 0.90625, learning_rate 0.00228015
2017-10-10T14:44:29.552941: step 200, loss 0.474379, acc 0.828125, learning_rate 0.00227125

Evaluation:
2017-10-10T14:44:30.028169: step 200, loss 0.297988, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-200

2017-10-10T14:44:30.951716: step 201, loss 0.55212, acc 0.78125, learning_rate 0.00226239
2017-10-10T14:44:31.253680: step 202, loss 0.499548, acc 0.8125, learning_rate 0.00225356
2017-10-10T14:44:31.600856: step 203, loss 0.332321, acc 0.890625, learning_rate 0.00224477
2017-10-10T14:44:31.896906: step 204, loss 0.330511, acc 0.859375, learning_rate 0.00223602
2017-10-10T14:44:32.147836: step 205, loss 0.270547, acc 0.921875, learning_rate 0.0022273
2017-10-10T14:44:32.404797: step 206, loss 0.296081, acc 0.90625, learning_rate 0.00221862
2017-10-10T14:44:32.713677: step 207, loss 0.258401, acc 0.921875, learning_rate 0.00220997
2017-10-10T14:44:32.972922: step 208, loss 0.20503, acc 0.9375, learning_rate 0.00220136
2017-10-10T14:44:33.229129: step 209, loss 0.289067, acc 0.875, learning_rate 0.00219278
2017-10-10T14:44:33.560744: step 210, loss 0.233438, acc 0.875, learning_rate 0.00218424
2017-10-10T14:44:33.874649: step 211, loss 0.204225, acc 0.953125, learning_rate 0.00217573
2017-10-10T14:44:34.154332: step 212, loss 0.287853, acc 0.890625, learning_rate 0.00216726
2017-10-10T14:44:34.455955: step 213, loss 0.154907, acc 0.9375, learning_rate 0.00215882
2017-10-10T14:44:34.692542: step 214, loss 0.48265, acc 0.84375, learning_rate 0.00215041
2017-10-10T14:44:35.036919: step 215, loss 0.359751, acc 0.890625, learning_rate 0.00214204
2017-10-10T14:44:35.343204: step 216, loss 0.397824, acc 0.84375, learning_rate 0.00213371
2017-10-10T14:44:35.659387: step 217, loss 0.272103, acc 0.890625, learning_rate 0.00212541
2017-10-10T14:44:35.925751: step 218, loss 0.408932, acc 0.875, learning_rate 0.00211714
2017-10-10T14:44:36.005108: step 219, loss 0.545319, acc 0.890625, learning_rate 0.00210891
2017-10-10T14:44:36.098144: step 220, loss 0.241942, acc 0.9375, learning_rate 0.00210071
2017-10-10T14:44:36.256517: step 221, loss 0.37046, acc 0.84375, learning_rate 0.00209254
2017-10-10T14:44:36.443587: step 222, loss 0.303699, acc 0.90625, learning_rate 0.00208441
2017-10-10T14:44:36.637703: step 223, loss 0.317156, acc 0.890625, learning_rate 0.00207631
2017-10-10T14:44:36.845614: step 224, loss 0.198177, acc 0.921875, learning_rate 0.00206824
2017-10-10T14:44:37.085730: step 225, loss 0.318576, acc 0.921875, learning_rate 0.00206021
2017-10-10T14:44:37.406944: step 226, loss 0.270542, acc 0.875, learning_rate 0.00205221
2017-10-10T14:44:37.712844: step 227, loss 0.404397, acc 0.90625, learning_rate 0.00204424
2017-10-10T14:44:37.932957: step 228, loss 0.335626, acc 0.9375, learning_rate 0.0020363
2017-10-10T14:44:38.213935: step 229, loss 0.246324, acc 0.921875, learning_rate 0.0020284
2017-10-10T14:44:38.518236: step 230, loss 0.491093, acc 0.796875, learning_rate 0.00202053
2017-10-10T14:44:38.769613: step 231, loss 0.344336, acc 0.875, learning_rate 0.00201269
2017-10-10T14:44:39.058657: step 232, loss 0.350295, acc 0.859375, learning_rate 0.00200488
2017-10-10T14:44:39.400255: step 233, loss 0.242747, acc 0.875, learning_rate 0.00199711
2017-10-10T14:44:39.707591: step 234, loss 0.374767, acc 0.890625, learning_rate 0.00198936
2017-10-10T14:44:39.954266: step 235, loss 0.24005, acc 0.921875, learning_rate 0.00198165
2017-10-10T14:44:40.284846: step 236, loss 0.366096, acc 0.84375, learning_rate 0.00197397
2017-10-10T14:44:40.583150: step 237, loss 0.557657, acc 0.828125, learning_rate 0.00196632
2017-10-10T14:44:40.800887: step 238, loss 0.243363, acc 0.9375, learning_rate 0.0019587
2017-10-10T14:44:41.033556: step 239, loss 0.282313, acc 0.921875, learning_rate 0.00195112
2017-10-10T14:44:41.321503: step 240, loss 0.249623, acc 0.890625, learning_rate 0.00194356

Evaluation:
2017-10-10T14:44:41.819057: step 240, loss 0.273379, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-240

2017-10-10T14:44:42.993317: step 241, loss 0.22338, acc 0.890625, learning_rate 0.00193604
2017-10-10T14:44:43.244115: step 242, loss 0.363643, acc 0.84375, learning_rate 0.00192854
2017-10-10T14:44:43.500261: step 243, loss 0.492393, acc 0.8125, learning_rate 0.00192108
2017-10-10T14:44:43.785635: step 244, loss 0.39924, acc 0.859375, learning_rate 0.00191364
2017-10-10T14:44:44.065314: step 245, loss 0.320618, acc 0.90625, learning_rate 0.00190624
2017-10-10T14:44:44.360930: step 246, loss 0.262485, acc 0.9375, learning_rate 0.00189887
2017-10-10T14:44:44.631653: step 247, loss 0.391642, acc 0.890625, learning_rate 0.00189153
2017-10-10T14:44:44.900904: step 248, loss 0.30799, acc 0.90625, learning_rate 0.00188421
2017-10-10T14:44:45.201077: step 249, loss 0.328559, acc 0.890625, learning_rate 0.00187693
2017-10-10T14:44:45.470136: step 250, loss 0.462375, acc 0.8125, learning_rate 0.00186968
2017-10-10T14:44:45.761913: step 251, loss 0.322886, acc 0.890625, learning_rate 0.00186245
2017-10-10T14:44:46.014539: step 252, loss 0.149345, acc 0.953125, learning_rate 0.00185526
2017-10-10T14:44:46.256927: step 253, loss 0.297885, acc 0.90625, learning_rate 0.0018481
2017-10-10T14:44:46.570002: step 254, loss 0.47784, acc 0.859375, learning_rate 0.00184096
2017-10-10T14:44:46.798077: step 255, loss 0.123001, acc 0.96875, learning_rate 0.00183385
2017-10-10T14:44:47.085664: step 256, loss 0.483209, acc 0.84375, learning_rate 0.00182678
2017-10-10T14:44:47.382694: step 257, loss 0.3198, acc 0.921875, learning_rate 0.00181973
2017-10-10T14:44:47.680993: step 258, loss 0.219099, acc 0.921875, learning_rate 0.00181271
2017-10-10T14:44:47.905336: step 259, loss 0.22929, acc 0.890625, learning_rate 0.00180572
2017-10-10T14:44:48.204071: step 260, loss 0.285115, acc 0.921875, learning_rate 0.00179876
2017-10-10T14:44:48.454864: step 261, loss 0.398415, acc 0.890625, learning_rate 0.00179182
2017-10-10T14:44:48.687516: step 262, loss 0.356947, acc 0.9375, learning_rate 0.00178492
2017-10-10T14:44:48.976902: step 263, loss 0.361566, acc 0.828125, learning_rate 0.00177804
2017-10-10T14:44:49.291738: step 264, loss 0.253622, acc 0.953125, learning_rate 0.00177119
2017-10-10T14:44:49.573235: step 265, loss 0.216261, acc 0.890625, learning_rate 0.00176437
2017-10-10T14:44:49.875246: step 266, loss 0.219031, acc 0.953125, learning_rate 0.00175758
2017-10-10T14:44:50.149493: step 267, loss 0.279416, acc 0.890625, learning_rate 0.00175081
2017-10-10T14:44:50.432385: step 268, loss 0.174495, acc 0.953125, learning_rate 0.00174407
2017-10-10T14:44:50.730984: step 269, loss 0.371892, acc 0.84375, learning_rate 0.00173736
2017-10-10T14:44:50.953099: step 270, loss 0.130225, acc 0.96875, learning_rate 0.00173068
2017-10-10T14:44:51.228887: step 271, loss 0.322727, acc 0.890625, learning_rate 0.00172402
2017-10-10T14:44:51.516538: step 272, loss 0.431824, acc 0.890625, learning_rate 0.00171739
2017-10-10T14:44:51.705025: step 273, loss 0.146032, acc 0.953125, learning_rate 0.00171079
2017-10-10T14:44:51.994495: step 274, loss 0.535468, acc 0.796875, learning_rate 0.00170422
2017-10-10T14:44:52.292865: step 275, loss 0.317695, acc 0.875, learning_rate 0.00169767
2017-10-10T14:44:52.548794: step 276, loss 0.223559, acc 0.90625, learning_rate 0.00169115
2017-10-10T14:44:52.869603: step 277, loss 0.361759, acc 0.84375, learning_rate 0.00168465
2017-10-10T14:44:53.298161: step 278, loss 0.383951, acc 0.875, learning_rate 0.00167818
2017-10-10T14:44:53.549770: step 279, loss 0.304036, acc 0.890625, learning_rate 0.00167174
2017-10-10T14:44:53.791362: step 280, loss 0.202807, acc 0.9375, learning_rate 0.00166533

Evaluation:
2017-10-10T14:44:54.162102: step 280, loss 0.274833, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-280

2017-10-10T14:44:55.166651: step 281, loss 0.46505, acc 0.84375, learning_rate 0.00165894
2017-10-10T14:44:55.433395: step 282, loss 0.677664, acc 0.8125, learning_rate 0.00165257
2017-10-10T14:44:55.674864: step 283, loss 0.200675, acc 0.953125, learning_rate 0.00164624
2017-10-10T14:44:55.961734: step 284, loss 0.388383, acc 0.875, learning_rate 0.00163993
2017-10-10T14:44:56.203368: step 285, loss 0.26921, acc 0.84375, learning_rate 0.00163364
2017-10-10T14:44:56.471802: step 286, loss 0.204183, acc 0.890625, learning_rate 0.00162738
2017-10-10T14:44:56.796240: step 287, loss 0.295566, acc 0.859375, learning_rate 0.00162115
2017-10-10T14:44:57.097051: step 288, loss 0.331074, acc 0.828125, learning_rate 0.00161494
2017-10-10T14:44:57.293987: step 289, loss 0.315894, acc 0.875, learning_rate 0.00160875
2017-10-10T14:44:57.592861: step 290, loss 0.456573, acc 0.796875, learning_rate 0.00160259
2017-10-10T14:44:57.895127: step 291, loss 0.260852, acc 0.859375, learning_rate 0.00159646
2017-10-10T14:44:58.150650: step 292, loss 0.301026, acc 0.859375, learning_rate 0.00159035
2017-10-10T14:44:58.444586: step 293, loss 0.22273, acc 0.921875, learning_rate 0.00158427
2017-10-10T14:44:58.740841: step 294, loss 0.196988, acc 0.901961, learning_rate 0.00157821
2017-10-10T14:44:59.005447: step 295, loss 0.237411, acc 0.90625, learning_rate 0.00157218
2017-10-10T14:44:59.297384: step 296, loss 0.246164, acc 0.921875, learning_rate 0.00156617
2017-10-10T14:44:59.625849: step 297, loss 0.249277, acc 0.90625, learning_rate 0.00156018
2017-10-10T14:44:59.849871: step 298, loss 0.191164, acc 0.921875, learning_rate 0.00155422
2017-10-10T14:45:00.124927: step 299, loss 0.477626, acc 0.84375, learning_rate 0.00154829
2017-10-10T14:45:00.380999: step 300, loss 0.228134, acc 0.921875, learning_rate 0.00154238
2017-10-10T14:45:00.681695: step 301, loss 0.571701, acc 0.828125, learning_rate 0.00153649
2017-10-10T14:45:00.956851: step 302, loss 0.328699, acc 0.875, learning_rate 0.00153063
2017-10-10T14:45:01.265009: step 303, loss 0.288345, acc 0.875, learning_rate 0.00152479
2017-10-10T14:45:01.552879: step 304, loss 0.384406, acc 0.875, learning_rate 0.00151897
2017-10-10T14:45:01.872476: step 305, loss 0.209582, acc 0.96875, learning_rate 0.00151318
2017-10-10T14:45:02.140202: step 306, loss 0.522636, acc 0.828125, learning_rate 0.00150741
2017-10-10T14:45:02.361326: step 307, loss 0.25331, acc 0.921875, learning_rate 0.00150167
2017-10-10T14:45:02.643401: step 308, loss 0.324438, acc 0.890625, learning_rate 0.00149594
2017-10-10T14:45:02.858285: step 309, loss 0.166976, acc 0.9375, learning_rate 0.00149025
2017-10-10T14:45:03.144851: step 310, loss 0.160875, acc 0.9375, learning_rate 0.00148457
2017-10-10T14:45:03.472223: step 311, loss 0.240756, acc 0.90625, learning_rate 0.00147892
2017-10-10T14:45:03.804845: step 312, loss 0.229413, acc 0.9375, learning_rate 0.00147329
2017-10-10T14:45:04.081252: step 313, loss 0.163314, acc 0.9375, learning_rate 0.00146769
2017-10-10T14:45:04.350451: step 314, loss 0.375156, acc 0.84375, learning_rate 0.0014621
2017-10-10T14:45:04.656882: step 315, loss 0.197083, acc 0.890625, learning_rate 0.00145654
2017-10-10T14:45:04.945938: step 316, loss 0.499764, acc 0.859375, learning_rate 0.00145101
2017-10-10T14:45:05.255037: step 317, loss 0.334148, acc 0.90625, learning_rate 0.00144549
2017-10-10T14:45:05.548999: step 318, loss 0.474469, acc 0.859375, learning_rate 0.00144
2017-10-10T14:45:05.768817: step 319, loss 0.234589, acc 0.921875, learning_rate 0.00143453
2017-10-10T14:45:06.026850: step 320, loss 0.370204, acc 0.84375, learning_rate 0.00142908

Evaluation:
2017-10-10T14:45:06.471638: step 320, loss 0.278212, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-320

2017-10-10T14:45:07.496265: step 321, loss 0.476233, acc 0.84375, learning_rate 0.00142366
2017-10-10T14:45:07.721806: step 322, loss 0.189443, acc 0.9375, learning_rate 0.00141826
2017-10-10T14:45:07.968718: step 323, loss 0.219331, acc 0.921875, learning_rate 0.00141288
2017-10-10T14:45:08.248859: step 324, loss 0.264471, acc 0.921875, learning_rate 0.00140752
2017-10-10T14:45:08.522616: step 325, loss 0.299812, acc 0.84375, learning_rate 0.00140218
2017-10-10T14:45:08.744967: step 326, loss 0.247292, acc 0.90625, learning_rate 0.00139686
2017-10-10T14:45:09.035130: step 327, loss 0.281038, acc 0.90625, learning_rate 0.00139157
2017-10-10T14:45:09.349335: step 328, loss 0.286089, acc 0.859375, learning_rate 0.0013863
2017-10-10T14:45:09.592820: step 329, loss 0.145022, acc 0.953125, learning_rate 0.00138105
2017-10-10T14:45:09.843817: step 330, loss 0.295306, acc 0.90625, learning_rate 0.00137582
2017-10-10T14:45:10.104821: step 331, loss 0.276256, acc 0.890625, learning_rate 0.00137061
2017-10-10T14:45:10.340834: step 332, loss 0.323241, acc 0.875, learning_rate 0.00136543
2017-10-10T14:45:10.628518: step 333, loss 0.250699, acc 0.921875, learning_rate 0.00136026
2017-10-10T14:45:10.962440: step 334, loss 0.394007, acc 0.84375, learning_rate 0.00135512
2017-10-10T14:45:11.176051: step 335, loss 0.140898, acc 0.96875, learning_rate 0.00134999
2017-10-10T14:45:11.439758: step 336, loss 0.436446, acc 0.828125, learning_rate 0.00134489
2017-10-10T14:45:11.714776: step 337, loss 0.219811, acc 0.9375, learning_rate 0.00133981
2017-10-10T14:45:11.988979: step 338, loss 0.478542, acc 0.890625, learning_rate 0.00133475
2017-10-10T14:45:12.328045: step 339, loss 0.274532, acc 0.890625, learning_rate 0.00132971
2017-10-10T14:45:12.556945: step 340, loss 0.436905, acc 0.875, learning_rate 0.00132469
2017-10-10T14:45:12.795832: step 341, loss 0.237157, acc 0.921875, learning_rate 0.00131969
2017-10-10T14:45:13.060841: step 342, loss 0.195636, acc 0.921875, learning_rate 0.00131471
2017-10-10T14:45:13.320386: step 343, loss 0.120611, acc 0.984375, learning_rate 0.00130975
2017-10-10T14:45:13.585018: step 344, loss 0.179682, acc 0.921875, learning_rate 0.00130482
2017-10-10T14:45:13.785811: step 345, loss 0.436529, acc 0.828125, learning_rate 0.0012999
2017-10-10T14:45:14.073999: step 346, loss 0.297115, acc 0.890625, learning_rate 0.001295
2017-10-10T14:45:14.320279: step 347, loss 0.37545, acc 0.890625, learning_rate 0.00129012
2017-10-10T14:45:14.585152: step 348, loss 0.271421, acc 0.90625, learning_rate 0.00128527
2017-10-10T14:45:14.903663: step 349, loss 0.190969, acc 0.953125, learning_rate 0.00128043
2017-10-10T14:45:15.225597: step 350, loss 0.261137, acc 0.90625, learning_rate 0.00127561
2017-10-10T14:45:15.517085: step 351, loss 0.197111, acc 0.921875, learning_rate 0.00127081
2017-10-10T14:45:15.776974: step 352, loss 0.186654, acc 0.9375, learning_rate 0.00126603
2017-10-10T14:45:16.004808: step 353, loss 0.330862, acc 0.90625, learning_rate 0.00126127
2017-10-10T14:45:16.335807: step 354, loss 0.226648, acc 0.90625, learning_rate 0.00125653
2017-10-10T14:45:16.580999: step 355, loss 0.122301, acc 0.96875, learning_rate 0.00125181
2017-10-10T14:45:16.816809: step 356, loss 0.195592, acc 0.953125, learning_rate 0.00124711
2017-10-10T14:45:17.091379: step 357, loss 0.212924, acc 0.921875, learning_rate 0.00124243
2017-10-10T14:45:17.352702: step 358, loss 0.323735, acc 0.890625, learning_rate 0.00123777
2017-10-10T14:45:17.608326: step 359, loss 0.244267, acc 0.859375, learning_rate 0.00123312
2017-10-10T14:45:17.876163: step 360, loss 0.270581, acc 0.921875, learning_rate 0.0012285

Evaluation:
2017-10-10T14:45:18.336011: step 360, loss 0.272708, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-360

2017-10-10T14:45:19.240836: step 361, loss 0.249267, acc 0.90625, learning_rate 0.00122389
2017-10-10T14:45:19.517702: step 362, loss 0.215377, acc 0.921875, learning_rate 0.0012193
2017-10-10T14:45:19.782689: step 363, loss 0.209833, acc 0.90625, learning_rate 0.00121473
2017-10-10T14:45:20.102991: step 364, loss 0.330418, acc 0.84375, learning_rate 0.00121018
2017-10-10T14:45:20.467840: step 365, loss 0.272972, acc 0.921875, learning_rate 0.00120565
2017-10-10T14:45:20.679499: step 366, loss 0.309278, acc 0.921875, learning_rate 0.00120114
2017-10-10T14:45:20.923146: step 367, loss 0.281675, acc 0.875, learning_rate 0.00119664
2017-10-10T14:45:21.229529: step 368, loss 0.148202, acc 0.96875, learning_rate 0.00119217
2017-10-10T14:45:21.533022: step 369, loss 0.267875, acc 0.90625, learning_rate 0.00118771
2017-10-10T14:45:21.842958: step 370, loss 0.277651, acc 0.859375, learning_rate 0.00118327
2017-10-10T14:45:22.081168: step 371, loss 0.153064, acc 0.96875, learning_rate 0.00117885
2017-10-10T14:45:22.338821: step 372, loss 0.211923, acc 0.90625, learning_rate 0.00117445
2017-10-10T14:45:22.558745: step 373, loss 0.271517, acc 0.90625, learning_rate 0.00117006
2017-10-10T14:45:22.829539: step 374, loss 0.425307, acc 0.859375, learning_rate 0.00116569
2017-10-10T14:45:23.110614: step 375, loss 0.21804, acc 0.921875, learning_rate 0.00116134
2017-10-10T14:45:23.341166: step 376, loss 0.133721, acc 0.953125, learning_rate 0.00115701
2017-10-10T14:45:23.621528: step 377, loss 0.302364, acc 0.90625, learning_rate 0.0011527
2017-10-10T14:45:23.933190: step 378, loss 0.17548, acc 0.953125, learning_rate 0.0011484
2017-10-10T14:45:24.227869: step 379, loss 0.174548, acc 0.9375, learning_rate 0.00114412
2017-10-10T14:45:24.437008: step 380, loss 0.196134, acc 0.921875, learning_rate 0.00113986
2017-10-10T14:45:24.652784: step 381, loss 0.220105, acc 0.953125, learning_rate 0.00113561
2017-10-10T14:45:24.943969: step 382, loss 0.311436, acc 0.921875, learning_rate 0.00113139
2017-10-10T14:45:25.223578: step 383, loss 0.243469, acc 0.90625, learning_rate 0.00112718
2017-10-10T14:45:25.504883: step 384, loss 0.275087, acc 0.890625, learning_rate 0.00112298
2017-10-10T14:45:25.853950: step 385, loss 0.236245, acc 0.953125, learning_rate 0.00111881
2017-10-10T14:45:26.186119: step 386, loss 0.25074, acc 0.90625, learning_rate 0.00111465
2017-10-10T14:45:26.461946: step 387, loss 0.166406, acc 0.953125, learning_rate 0.00111051
2017-10-10T14:45:26.683346: step 388, loss 0.235059, acc 0.921875, learning_rate 0.00110638
2017-10-10T14:45:26.895198: step 389, loss 0.35498, acc 0.890625, learning_rate 0.00110228
2017-10-10T14:45:27.127982: step 390, loss 0.154119, acc 0.953125, learning_rate 0.00109818
2017-10-10T14:45:27.344034: step 391, loss 0.186824, acc 0.9375, learning_rate 0.00109411
2017-10-10T14:45:27.553402: step 392, loss 0.248315, acc 0.901961, learning_rate 0.00109005
2017-10-10T14:45:27.843207: step 393, loss 0.221955, acc 0.921875, learning_rate 0.00108601
2017-10-10T14:45:28.118022: step 394, loss 0.157801, acc 0.953125, learning_rate 0.00108199
2017-10-10T14:45:28.440910: step 395, loss 0.20635, acc 0.921875, learning_rate 0.00107798
2017-10-10T14:45:28.787810: step 396, loss 0.154222, acc 0.9375, learning_rate 0.00107399
2017-10-10T14:45:29.057238: step 397, loss 0.264822, acc 0.90625, learning_rate 0.00107001
2017-10-10T14:45:29.328119: step 398, loss 0.162998, acc 0.90625, learning_rate 0.00106605
2017-10-10T14:45:29.629412: step 399, loss 0.160298, acc 0.953125, learning_rate 0.00106211
2017-10-10T14:45:29.905508: step 400, loss 0.31941, acc 0.90625, learning_rate 0.00105818

Evaluation:
2017-10-10T14:45:30.457842: step 400, loss 0.262142, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-400

2017-10-10T14:45:31.584841: step 401, loss 0.136672, acc 0.9375, learning_rate 0.00105427
2017-10-10T14:45:31.816952: step 402, loss 0.387216, acc 0.84375, learning_rate 0.00105037
2017-10-10T14:45:32.088917: step 403, loss 0.131461, acc 0.953125, learning_rate 0.0010465
2017-10-10T14:45:32.393793: step 404, loss 0.296213, acc 0.859375, learning_rate 0.00104263
2017-10-10T14:45:32.624132: step 405, loss 0.313154, acc 0.875, learning_rate 0.00103878
2017-10-10T14:45:32.933046: step 406, loss 0.244465, acc 0.890625, learning_rate 0.00103495
2017-10-10T14:45:33.204770: step 407, loss 0.495268, acc 0.875, learning_rate 0.00103114
2017-10-10T14:45:33.524839: step 408, loss 0.14323, acc 0.953125, learning_rate 0.00102734
2017-10-10T14:45:33.868837: step 409, loss 0.243138, acc 0.9375, learning_rate 0.00102355
2017-10-10T14:45:34.068820: step 410, loss 0.235436, acc 0.90625, learning_rate 0.00101978
2017-10-10T14:45:34.317008: step 411, loss 0.294023, acc 0.890625, learning_rate 0.00101603
2017-10-10T14:45:34.608447: step 412, loss 0.216579, acc 0.9375, learning_rate 0.00101229
2017-10-10T14:45:34.967131: step 413, loss 0.288277, acc 0.90625, learning_rate 0.00100856
2017-10-10T14:45:35.160849: step 414, loss 0.178859, acc 0.921875, learning_rate 0.00100486
2017-10-10T14:45:35.455233: step 415, loss 0.258289, acc 0.921875, learning_rate 0.00100116
2017-10-10T14:45:35.717455: step 416, loss 0.249294, acc 0.9375, learning_rate 0.000997483
2017-10-10T14:45:35.951852: step 417, loss 0.219296, acc 0.921875, learning_rate 0.00099382
2017-10-10T14:45:36.274253: step 418, loss 0.279382, acc 0.921875, learning_rate 0.000990172
2017-10-10T14:45:36.514285: step 419, loss 0.136377, acc 0.96875, learning_rate 0.000986538
2017-10-10T14:45:36.786911: step 420, loss 0.27961, acc 0.9375, learning_rate 0.00098292
2017-10-10T14:45:37.081847: step 421, loss 0.123402, acc 0.953125, learning_rate 0.000979316
2017-10-10T14:45:37.336957: step 422, loss 0.200856, acc 0.921875, learning_rate 0.000975727
2017-10-10T14:45:37.594921: step 423, loss 0.198857, acc 0.921875, learning_rate 0.000972152
2017-10-10T14:45:37.892848: step 424, loss 0.113714, acc 0.96875, learning_rate 0.000968592
2017-10-10T14:45:38.177472: step 425, loss 0.220917, acc 0.90625, learning_rate 0.000965047
2017-10-10T14:45:38.444967: step 426, loss 0.182258, acc 0.921875, learning_rate 0.000961516
2017-10-10T14:45:38.712037: step 427, loss 0.0936516, acc 0.984375, learning_rate 0.000958
2017-10-10T14:45:38.982118: step 428, loss 0.236659, acc 0.90625, learning_rate 0.000954497
2017-10-10T14:45:39.257345: step 429, loss 0.376375, acc 0.890625, learning_rate 0.00095101
2017-10-10T14:45:39.539630: step 430, loss 0.2655, acc 0.875, learning_rate 0.000947536
2017-10-10T14:45:39.801175: step 431, loss 0.158922, acc 0.96875, learning_rate 0.000944076
2017-10-10T14:45:40.108832: step 432, loss 0.172541, acc 0.96875, learning_rate 0.000940631
2017-10-10T14:45:40.399511: step 433, loss 0.224572, acc 0.9375, learning_rate 0.0009372
2017-10-10T14:45:40.655638: step 434, loss 0.148146, acc 0.953125, learning_rate 0.000933783
2017-10-10T14:45:40.938550: step 435, loss 0.16549, acc 0.953125, learning_rate 0.000930379
2017-10-10T14:45:41.249855: step 436, loss 0.175895, acc 0.9375, learning_rate 0.00092699
2017-10-10T14:45:41.460871: step 437, loss 0.163895, acc 0.921875, learning_rate 0.000923614
2017-10-10T14:45:41.755544: step 438, loss 0.213225, acc 0.90625, learning_rate 0.000920253
2017-10-10T14:45:42.050319: step 439, loss 0.214252, acc 0.9375, learning_rate 0.000916905
2017-10-10T14:45:42.344960: step 440, loss 0.220513, acc 0.96875, learning_rate 0.00091357

Evaluation:
2017-10-10T14:45:42.960848: step 440, loss 0.259341, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-440

2017-10-10T14:45:44.172024: step 441, loss 0.187506, acc 0.921875, learning_rate 0.000910249
2017-10-10T14:45:44.459324: step 442, loss 0.253421, acc 0.90625, learning_rate 0.000906942
2017-10-10T14:45:44.760111: step 443, loss 0.152653, acc 0.9375, learning_rate 0.000903648
2017-10-10T14:45:44.985037: step 444, loss 0.190375, acc 0.953125, learning_rate 0.000900368
2017-10-10T14:45:45.260212: step 445, loss 0.335237, acc 0.890625, learning_rate 0.000897101
2017-10-10T14:45:45.578974: step 446, loss 0.309412, acc 0.859375, learning_rate 0.000893848
2017-10-10T14:45:45.898614: step 447, loss 0.192101, acc 0.9375, learning_rate 0.000890607
2017-10-10T14:45:46.167512: step 448, loss 0.294535, acc 0.90625, learning_rate 0.00088738
2017-10-10T14:45:46.484922: step 449, loss 0.340751, acc 0.953125, learning_rate 0.000884166
2017-10-10T14:45:46.761126: step 450, loss 0.240747, acc 0.90625, learning_rate 0.000880966
2017-10-10T14:45:47.049168: step 451, loss 0.0991518, acc 0.984375, learning_rate 0.000877778
2017-10-10T14:45:47.336546: step 452, loss 0.243276, acc 0.90625, learning_rate 0.000874603
2017-10-10T14:45:47.577004: step 453, loss 0.382626, acc 0.890625, learning_rate 0.000871441
2017-10-10T14:45:47.849810: step 454, loss 0.132975, acc 0.96875, learning_rate 0.000868293
2017-10-10T14:45:48.151892: step 455, loss 0.373723, acc 0.84375, learning_rate 0.000865157
2017-10-10T14:45:48.452742: step 456, loss 0.223072, acc 0.9375, learning_rate 0.000862033
2017-10-10T14:45:48.723171: step 457, loss 0.218067, acc 0.921875, learning_rate 0.000858923
2017-10-10T14:45:49.028877: step 458, loss 0.246839, acc 0.9375, learning_rate 0.000855825
2017-10-10T14:45:49.217986: step 459, loss 0.140774, acc 0.9375, learning_rate 0.00085274
2017-10-10T14:45:49.435545: step 460, loss 0.126529, acc 0.953125, learning_rate 0.000849668
2017-10-10T14:45:49.665273: step 461, loss 0.247922, acc 0.90625, learning_rate 0.000846608
2017-10-10T14:45:49.980899: step 462, loss 0.222428, acc 0.9375, learning_rate 0.00084356
2017-10-10T14:45:50.197129: step 463, loss 0.291158, acc 0.921875, learning_rate 0.000840525
2017-10-10T14:45:50.502818: step 464, loss 0.0948876, acc 0.96875, learning_rate 0.000837502
2017-10-10T14:45:50.823398: step 465, loss 0.29993, acc 0.890625, learning_rate 0.000834492
2017-10-10T14:45:51.116225: step 466, loss 0.291806, acc 0.90625, learning_rate 0.000831494
2017-10-10T14:45:51.401933: step 467, loss 0.200172, acc 0.90625, learning_rate 0.000828508
2017-10-10T14:45:51.635842: step 468, loss 0.204539, acc 0.9375, learning_rate 0.000825535
2017-10-10T14:45:51.905126: step 469, loss 0.229136, acc 0.921875, learning_rate 0.000822573
2017-10-10T14:45:52.193045: step 470, loss 0.166809, acc 0.9375, learning_rate 0.000819624
2017-10-10T14:45:52.477014: step 471, loss 0.242892, acc 0.90625, learning_rate 0.000816687
2017-10-10T14:45:52.724547: step 472, loss 0.156269, acc 0.9375, learning_rate 0.000813761
2017-10-10T14:45:53.061160: step 473, loss 0.15623, acc 0.953125, learning_rate 0.000810848
2017-10-10T14:45:53.348551: step 474, loss 0.144234, acc 0.984375, learning_rate 0.000807946
2017-10-10T14:45:53.577477: step 475, loss 0.302471, acc 0.875, learning_rate 0.000805057
2017-10-10T14:45:53.855028: step 476, loss 0.142689, acc 0.96875, learning_rate 0.000802179
2017-10-10T14:45:54.158286: step 477, loss 0.173876, acc 0.953125, learning_rate 0.000799313
2017-10-10T14:45:54.382545: step 478, loss 0.199809, acc 0.9375, learning_rate 0.000796458
2017-10-10T14:45:54.674982: step 479, loss 0.279885, acc 0.90625, learning_rate 0.000793616
2017-10-10T14:45:54.995993: step 480, loss 0.286563, acc 0.921875, learning_rate 0.000790784

Evaluation:
2017-10-10T14:45:55.427124: step 480, loss 0.266574, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-480

2017-10-10T14:45:56.532908: step 481, loss 0.278741, acc 0.90625, learning_rate 0.000787965
2017-10-10T14:45:56.771298: step 482, loss 0.41394, acc 0.875, learning_rate 0.000785157
2017-10-10T14:45:57.022668: step 483, loss 0.182425, acc 0.9375, learning_rate 0.00078236
2017-10-10T14:45:57.267683: step 484, loss 0.195636, acc 0.9375, learning_rate 0.000779575
2017-10-10T14:45:57.482754: step 485, loss 0.100761, acc 0.96875, learning_rate 0.000776801
2017-10-10T14:45:57.787650: step 486, loss 0.253888, acc 0.9375, learning_rate 0.000774038
2017-10-10T14:45:58.064334: step 487, loss 0.141125, acc 0.96875, learning_rate 0.000771287
2017-10-10T14:45:58.320664: step 488, loss 0.215606, acc 0.890625, learning_rate 0.000768547
2017-10-10T14:45:58.580905: step 489, loss 0.365261, acc 0.890625, learning_rate 0.000765818
2017-10-10T14:45:58.843253: step 490, loss 0.23283, acc 0.901961, learning_rate 0.000763101
2017-10-10T14:45:59.098338: step 491, loss 0.184781, acc 0.921875, learning_rate 0.000760394
2017-10-10T14:45:59.381180: step 492, loss 0.199844, acc 0.9375, learning_rate 0.000757698
2017-10-10T14:45:59.768909: step 493, loss 0.166535, acc 0.953125, learning_rate 0.000755014
2017-10-10T14:46:00.121290: step 494, loss 0.326684, acc 0.84375, learning_rate 0.00075234
2017-10-10T14:46:00.347121: step 495, loss 0.293878, acc 0.90625, learning_rate 0.000749677
2017-10-10T14:46:00.580875: step 496, loss 0.281411, acc 0.90625, learning_rate 0.000747026
2017-10-10T14:46:00.756884: step 497, loss 0.219874, acc 0.890625, learning_rate 0.000744385
2017-10-10T14:46:00.966440: step 498, loss 0.2473, acc 0.90625, learning_rate 0.000741754
2017-10-10T14:46:01.226959: step 499, loss 0.107022, acc 0.953125, learning_rate 0.000739135
2017-10-10T14:46:01.472745: step 500, loss 0.167998, acc 0.921875, learning_rate 0.000736526
2017-10-10T14:46:01.769969: step 501, loss 0.113831, acc 0.984375, learning_rate 0.000733928
2017-10-10T14:46:02.001059: step 502, loss 0.327977, acc 0.90625, learning_rate 0.00073134
2017-10-10T14:46:02.230775: step 503, loss 0.275763, acc 0.875, learning_rate 0.000728763
2017-10-10T14:46:02.484620: step 504, loss 0.188221, acc 0.9375, learning_rate 0.000726197
2017-10-10T14:46:02.762297: step 505, loss 0.171311, acc 0.953125, learning_rate 0.000723641
2017-10-10T14:46:03.046402: step 506, loss 0.197522, acc 0.9375, learning_rate 0.000721095
2017-10-10T14:46:03.361151: step 507, loss 0.199842, acc 0.9375, learning_rate 0.00071856
2017-10-10T14:46:03.648905: step 508, loss 0.189994, acc 0.921875, learning_rate 0.000716036
2017-10-10T14:46:03.904967: step 509, loss 0.247119, acc 0.90625, learning_rate 0.000713521
2017-10-10T14:46:04.206901: step 510, loss 0.134834, acc 0.96875, learning_rate 0.000711017
2017-10-10T14:46:04.540803: step 511, loss 0.188975, acc 0.921875, learning_rate 0.000708523
2017-10-10T14:46:04.838009: step 512, loss 0.218131, acc 0.921875, learning_rate 0.000706039
2017-10-10T14:46:05.121255: step 513, loss 0.333549, acc 0.890625, learning_rate 0.000703565
2017-10-10T14:46:05.354016: step 514, loss 0.0646681, acc 1, learning_rate 0.000701102
2017-10-10T14:46:05.609151: step 515, loss 0.232157, acc 0.90625, learning_rate 0.000698648
2017-10-10T14:46:05.921129: step 516, loss 0.254634, acc 0.90625, learning_rate 0.000696204
2017-10-10T14:46:06.236971: step 517, loss 0.21406, acc 0.921875, learning_rate 0.000693771
2017-10-10T14:46:06.514641: step 518, loss 0.245379, acc 0.890625, learning_rate 0.000691347
2017-10-10T14:46:06.811146: step 519, loss 0.325419, acc 0.859375, learning_rate 0.000688934
2017-10-10T14:46:07.009096: step 520, loss 0.209967, acc 0.953125, learning_rate 0.00068653

Evaluation:
2017-10-10T14:46:07.380806: step 520, loss 0.257174, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-520

2017-10-10T14:46:08.348019: step 521, loss 0.212179, acc 0.9375, learning_rate 0.000684136
2017-10-10T14:46:08.656776: step 522, loss 0.320394, acc 0.859375, learning_rate 0.000681751
2017-10-10T14:46:08.923817: step 523, loss 0.262968, acc 0.890625, learning_rate 0.000679377
2017-10-10T14:46:09.240401: step 524, loss 0.299868, acc 0.890625, learning_rate 0.000677012
2017-10-10T14:46:09.533873: step 525, loss 0.218562, acc 0.921875, learning_rate 0.000674657
2017-10-10T14:46:09.784898: step 526, loss 0.303665, acc 0.890625, learning_rate 0.000672311
2017-10-10T14:46:10.130084: step 527, loss 0.166869, acc 0.953125, learning_rate 0.000669975
2017-10-10T14:46:10.439395: step 528, loss 0.186834, acc 0.90625, learning_rate 0.000667648
2017-10-10T14:46:10.699120: step 529, loss 0.15614, acc 0.921875, learning_rate 0.000665331
2017-10-10T14:46:10.938250: step 530, loss 0.0976345, acc 0.984375, learning_rate 0.000663024
2017-10-10T14:46:11.277384: step 531, loss 0.229888, acc 0.890625, learning_rate 0.000660726
2017-10-10T14:46:11.614858: step 532, loss 0.381636, acc 0.90625, learning_rate 0.000658437
2017-10-10T14:46:11.834953: step 533, loss 0.214474, acc 0.9375, learning_rate 0.000656158
2017-10-10T14:46:12.119243: step 534, loss 0.31774, acc 0.890625, learning_rate 0.000653888
2017-10-10T14:46:12.355666: step 535, loss 0.170358, acc 0.953125, learning_rate 0.000651627
2017-10-10T14:46:12.645092: step 536, loss 0.154514, acc 0.96875, learning_rate 0.000649375
2017-10-10T14:46:12.892882: step 537, loss 0.149097, acc 0.953125, learning_rate 0.000647133
2017-10-10T14:46:13.190440: step 538, loss 0.155286, acc 0.9375, learning_rate 0.000644899
2017-10-10T14:46:13.534496: step 539, loss 0.19097, acc 0.921875, learning_rate 0.000642675
2017-10-10T14:46:13.765258: step 540, loss 0.169302, acc 0.953125, learning_rate 0.00064046
2017-10-10T14:46:14.027710: step 541, loss 0.214522, acc 0.90625, learning_rate 0.000638254
2017-10-10T14:46:14.324840: step 542, loss 0.223082, acc 0.921875, learning_rate 0.000636057
2017-10-10T14:46:14.551144: step 543, loss 0.0725645, acc 1, learning_rate 0.000633869
2017-10-10T14:46:14.820961: step 544, loss 0.310532, acc 0.875, learning_rate 0.00063169
2017-10-10T14:46:15.116826: step 545, loss 0.252801, acc 0.921875, learning_rate 0.00062952
2017-10-10T14:46:15.416847: step 546, loss 0.220461, acc 0.921875, learning_rate 0.000627358
2017-10-10T14:46:15.727185: step 547, loss 0.189533, acc 0.9375, learning_rate 0.000625206
2017-10-10T14:46:16.022287: step 548, loss 0.277596, acc 0.859375, learning_rate 0.000623062
2017-10-10T14:46:16.270167: step 549, loss 0.263694, acc 0.921875, learning_rate 0.000620927
2017-10-10T14:46:16.565083: step 550, loss 0.101732, acc 0.96875, learning_rate 0.000618801
2017-10-10T14:46:16.885051: step 551, loss 0.211619, acc 0.953125, learning_rate 0.000616683
2017-10-10T14:46:17.180594: step 552, loss 0.171266, acc 0.953125, learning_rate 0.000614574
2017-10-10T14:46:17.431540: step 553, loss 0.211196, acc 0.921875, learning_rate 0.000612474
2017-10-10T14:46:17.673242: step 554, loss 0.187909, acc 0.90625, learning_rate 0.000610382
2017-10-10T14:46:17.938402: step 555, loss 0.0848727, acc 1, learning_rate 0.000608299
2017-10-10T14:46:18.145122: step 556, loss 0.253756, acc 0.890625, learning_rate 0.000606224
2017-10-10T14:46:18.472120: step 557, loss 0.186917, acc 0.953125, learning_rate 0.000604158
2017-10-10T14:46:18.740455: step 558, loss 0.261126, acc 0.90625, learning_rate 0.0006021
2017-10-10T14:46:18.980898: step 559, loss 0.246727, acc 0.921875, learning_rate 0.00060005
2017-10-10T14:46:19.223540: step 560, loss 0.109853, acc 0.96875, learning_rate 0.000598009

Evaluation:
2017-10-10T14:46:19.696834: step 560, loss 0.257201, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-560

2017-10-10T14:46:20.770425: step 561, loss 0.163547, acc 0.96875, learning_rate 0.000595977
2017-10-10T14:46:21.004321: step 562, loss 0.362623, acc 0.859375, learning_rate 0.000593952
2017-10-10T14:46:21.271566: step 563, loss 0.223406, acc 0.921875, learning_rate 0.000591936
2017-10-10T14:46:21.568474: step 564, loss 0.207894, acc 0.921875, learning_rate 0.000589928
2017-10-10T14:46:21.841099: step 565, loss 0.209702, acc 0.890625, learning_rate 0.000587928
2017-10-10T14:46:22.110716: step 566, loss 0.315561, acc 0.890625, learning_rate 0.000585937
2017-10-10T14:46:22.404058: step 567, loss 0.333095, acc 0.90625, learning_rate 0.000583953
2017-10-10T14:46:22.659565: step 568, loss 0.23892, acc 0.921875, learning_rate 0.000581978
2017-10-10T14:46:22.992944: step 569, loss 0.222566, acc 0.90625, learning_rate 0.00058001
2017-10-10T14:46:23.254453: step 570, loss 0.19533, acc 0.921875, learning_rate 0.000578051
2017-10-10T14:46:23.506774: step 571, loss 0.132828, acc 0.96875, learning_rate 0.0005761
2017-10-10T14:46:23.770746: step 572, loss 0.240479, acc 0.921875, learning_rate 0.000574157
2017-10-10T14:46:24.056885: step 573, loss 0.27454, acc 0.90625, learning_rate 0.000572221
2017-10-10T14:46:24.312852: step 574, loss 0.115045, acc 1, learning_rate 0.000570294
2017-10-10T14:46:24.628944: step 575, loss 0.151108, acc 0.953125, learning_rate 0.000568374
2017-10-10T14:46:24.916920: step 576, loss 0.255321, acc 0.9375, learning_rate 0.000566462
2017-10-10T14:46:25.164852: step 577, loss 0.267793, acc 0.890625, learning_rate 0.000564558
2017-10-10T14:46:25.402097: step 578, loss 0.16573, acc 0.96875, learning_rate 0.000562662
2017-10-10T14:46:25.644586: step 579, loss 0.17013, acc 0.921875, learning_rate 0.000560774
2017-10-10T14:46:25.856983: step 580, loss 0.218274, acc 0.921875, learning_rate 0.000558893
2017-10-10T14:46:26.129076: step 581, loss 0.238368, acc 0.890625, learning_rate 0.00055702
2017-10-10T14:46:26.424433: step 582, loss 0.109429, acc 0.96875, learning_rate 0.000555154
2017-10-10T14:46:26.749021: step 583, loss 0.129965, acc 0.953125, learning_rate 0.000553296
2017-10-10T14:46:27.002681: step 584, loss 0.257415, acc 0.90625, learning_rate 0.000551446
2017-10-10T14:46:27.264870: step 585, loss 0.172788, acc 0.96875, learning_rate 0.000549604
2017-10-10T14:46:27.544110: step 586, loss 0.229099, acc 0.890625, learning_rate 0.000547768
2017-10-10T14:46:27.816889: step 587, loss 0.144509, acc 0.953125, learning_rate 0.000545941
2017-10-10T14:46:28.061133: step 588, loss 0.34096, acc 0.882353, learning_rate 0.00054412
2017-10-10T14:46:28.360928: step 589, loss 0.249302, acc 0.9375, learning_rate 0.000542308
2017-10-10T14:46:28.660897: step 590, loss 0.238117, acc 0.90625, learning_rate 0.000540502
2017-10-10T14:46:28.923165: step 591, loss 0.109383, acc 0.953125, learning_rate 0.000538704
2017-10-10T14:46:29.216022: step 592, loss 0.182725, acc 0.9375, learning_rate 0.000536914
2017-10-10T14:46:29.521424: step 593, loss 0.294546, acc 0.875, learning_rate 0.00053513
2017-10-10T14:46:29.750556: step 594, loss 0.164512, acc 0.953125, learning_rate 0.000533354
2017-10-10T14:46:30.032880: step 595, loss 0.103092, acc 0.984375, learning_rate 0.000531585
2017-10-10T14:46:30.272980: step 596, loss 0.205605, acc 0.921875, learning_rate 0.000529824
2017-10-10T14:46:30.550428: step 597, loss 0.189373, acc 0.953125, learning_rate 0.000528069
2017-10-10T14:46:30.808947: step 598, loss 0.215876, acc 0.921875, learning_rate 0.000526322
2017-10-10T14:46:31.065011: step 599, loss 0.264347, acc 0.921875, learning_rate 0.000524582
2017-10-10T14:46:31.346950: step 600, loss 0.125265, acc 0.96875, learning_rate 0.000522849

Evaluation:
2017-10-10T14:46:31.784066: step 600, loss 0.255599, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-600

2017-10-10T14:46:32.914135: step 601, loss 0.205968, acc 0.9375, learning_rate 0.000521123
2017-10-10T14:46:33.188949: step 602, loss 0.1929, acc 0.9375, learning_rate 0.000519404
2017-10-10T14:46:33.471315: step 603, loss 0.12392, acc 0.96875, learning_rate 0.000517692
2017-10-10T14:46:33.793808: step 604, loss 0.235747, acc 0.90625, learning_rate 0.000515987
2017-10-10T14:46:34.064082: step 605, loss 0.19768, acc 0.9375, learning_rate 0.000514289
2017-10-10T14:46:34.280863: step 606, loss 0.23657, acc 0.875, learning_rate 0.000512598
2017-10-10T14:46:34.518077: step 607, loss 0.187812, acc 0.90625, learning_rate 0.000510914
2017-10-10T14:46:34.718173: step 608, loss 0.233056, acc 0.90625, learning_rate 0.000509237
2017-10-10T14:46:34.976196: step 609, loss 0.281647, acc 0.890625, learning_rate 0.000507566
2017-10-10T14:46:35.262144: step 610, loss 0.210691, acc 0.90625, learning_rate 0.000505903
2017-10-10T14:46:35.517579: step 611, loss 0.135705, acc 0.9375, learning_rate 0.000504246
2017-10-10T14:46:35.768972: step 612, loss 0.242352, acc 0.9375, learning_rate 0.000502596
2017-10-10T14:46:36.082631: step 613, loss 0.165722, acc 0.953125, learning_rate 0.000500953
2017-10-10T14:46:36.336676: step 614, loss 0.253523, acc 0.90625, learning_rate 0.000499316
2017-10-10T14:46:36.615669: step 615, loss 0.171903, acc 0.921875, learning_rate 0.000497686
2017-10-10T14:46:36.912155: step 616, loss 0.151808, acc 0.9375, learning_rate 0.000496063
2017-10-10T14:46:37.124938: step 617, loss 0.195345, acc 0.9375, learning_rate 0.000494446
2017-10-10T14:46:37.458904: step 618, loss 0.239252, acc 0.9375, learning_rate 0.000492836
2017-10-10T14:46:37.766354: step 619, loss 0.210523, acc 0.90625, learning_rate 0.000491233
2017-10-10T14:46:38.007753: step 620, loss 0.278825, acc 0.90625, learning_rate 0.000489636
2017-10-10T14:46:38.240851: step 621, loss 0.122429, acc 0.96875, learning_rate 0.000488045
2017-10-10T14:46:38.602431: step 622, loss 0.229672, acc 0.875, learning_rate 0.000486461
2017-10-10T14:46:38.865104: step 623, loss 0.123936, acc 0.96875, learning_rate 0.000484884
2017-10-10T14:46:39.159225: step 624, loss 0.100242, acc 1, learning_rate 0.000483313
2017-10-10T14:46:39.438769: step 625, loss 0.158905, acc 0.9375, learning_rate 0.000481748
2017-10-10T14:46:39.689193: step 626, loss 0.289976, acc 0.921875, learning_rate 0.00048019
2017-10-10T14:46:39.963118: step 627, loss 0.226856, acc 0.90625, learning_rate 0.000478638
2017-10-10T14:46:40.213119: step 628, loss 0.207341, acc 0.953125, learning_rate 0.000477093
2017-10-10T14:46:40.473010: step 629, loss 0.121562, acc 0.953125, learning_rate 0.000475554
2017-10-10T14:46:40.720987: step 630, loss 0.106706, acc 0.96875, learning_rate 0.000474021
2017-10-10T14:46:40.961001: step 631, loss 0.163612, acc 0.96875, learning_rate 0.000472494
2017-10-10T14:46:41.179643: step 632, loss 0.211113, acc 0.9375, learning_rate 0.000470974
2017-10-10T14:46:41.415413: step 633, loss 0.231206, acc 0.921875, learning_rate 0.000469459
2017-10-10T14:46:41.681049: step 634, loss 0.301517, acc 0.90625, learning_rate 0.000467951
2017-10-10T14:46:41.974749: step 635, loss 0.256269, acc 0.9375, learning_rate 0.000466449
2017-10-10T14:46:42.281135: step 636, loss 0.113486, acc 0.984375, learning_rate 0.000464954
2017-10-10T14:46:42.545429: step 637, loss 0.293923, acc 0.9375, learning_rate 0.000463464
2017-10-10T14:46:42.852900: step 638, loss 0.0933889, acc 0.953125, learning_rate 0.00046198
2017-10-10T14:46:43.175746: step 639, loss 0.282684, acc 0.859375, learning_rate 0.000460503
2017-10-10T14:46:43.402052: step 640, loss 0.165992, acc 0.9375, learning_rate 0.000459031

Evaluation:
2017-10-10T14:46:43.787687: step 640, loss 0.254525, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-640

2017-10-10T14:46:44.924792: step 641, loss 0.149146, acc 0.96875, learning_rate 0.000457566
2017-10-10T14:46:45.240975: step 642, loss 0.155916, acc 0.953125, learning_rate 0.000456106
2017-10-10T14:46:45.529743: step 643, loss 0.299243, acc 0.90625, learning_rate 0.000454653
2017-10-10T14:46:45.761389: step 644, loss 0.328493, acc 0.875, learning_rate 0.000453205
2017-10-10T14:46:46.066682: step 645, loss 0.27256, acc 0.890625, learning_rate 0.000451764
2017-10-10T14:46:46.357721: step 646, loss 0.388541, acc 0.84375, learning_rate 0.000450328
2017-10-10T14:46:46.551085: step 647, loss 0.119011, acc 0.9375, learning_rate 0.000448898
2017-10-10T14:46:46.863960: step 648, loss 0.239275, acc 0.875, learning_rate 0.000447474
2017-10-10T14:46:47.114011: step 649, loss 0.153318, acc 0.953125, learning_rate 0.000446055
2017-10-10T14:46:47.368856: step 650, loss 0.135665, acc 0.96875, learning_rate 0.000444643
2017-10-10T14:46:47.680855: step 651, loss 0.341244, acc 0.890625, learning_rate 0.000443236
2017-10-10T14:46:47.968897: step 652, loss 0.195325, acc 0.96875, learning_rate 0.000441835
2017-10-10T14:46:48.221058: step 653, loss 0.233806, acc 0.953125, learning_rate 0.00044044
2017-10-10T14:46:48.542386: step 654, loss 0.136822, acc 0.96875, learning_rate 0.00043905
2017-10-10T14:46:48.808961: step 655, loss 0.181208, acc 0.953125, learning_rate 0.000437666
2017-10-10T14:46:49.063722: step 656, loss 0.176255, acc 0.953125, learning_rate 0.000436288
2017-10-10T14:46:49.362759: step 657, loss 0.215064, acc 0.953125, learning_rate 0.000434915
2017-10-10T14:46:49.652907: step 658, loss 0.263287, acc 0.921875, learning_rate 0.000433548
2017-10-10T14:46:49.908178: step 659, loss 0.272055, acc 0.90625, learning_rate 0.000432187
2017-10-10T14:46:50.229414: step 660, loss 0.226111, acc 0.921875, learning_rate 0.000430831
2017-10-10T14:46:50.536814: step 661, loss 0.14097, acc 0.953125, learning_rate 0.000429481
2017-10-10T14:46:50.826942: step 662, loss 0.237148, acc 0.90625, learning_rate 0.000428136
2017-10-10T14:46:51.125383: step 663, loss 0.174888, acc 0.9375, learning_rate 0.000426796
2017-10-10T14:46:51.364841: step 664, loss 0.212423, acc 0.90625, learning_rate 0.000425463
2017-10-10T14:46:51.590960: step 665, loss 0.17185, acc 0.9375, learning_rate 0.000424134
2017-10-10T14:46:51.950402: step 666, loss 0.329373, acc 0.828125, learning_rate 0.000422811
2017-10-10T14:46:52.269820: step 667, loss 0.191343, acc 0.953125, learning_rate 0.000421493
2017-10-10T14:46:52.515823: step 668, loss 0.16129, acc 0.96875, learning_rate 0.000420181
2017-10-10T14:46:52.835560: step 669, loss 0.168582, acc 0.953125, learning_rate 0.000418874
2017-10-10T14:46:53.130908: step 670, loss 0.158117, acc 0.9375, learning_rate 0.000417573
2017-10-10T14:46:53.408816: step 671, loss 0.212194, acc 0.9375, learning_rate 0.000416276
2017-10-10T14:46:53.704846: step 672, loss 0.124831, acc 0.96875, learning_rate 0.000414985
2017-10-10T14:46:53.905134: step 673, loss 0.218278, acc 0.9375, learning_rate 0.0004137
2017-10-10T14:46:54.187307: step 674, loss 0.182306, acc 0.921875, learning_rate 0.000412419
2017-10-10T14:46:54.540691: step 675, loss 0.185792, acc 0.921875, learning_rate 0.000411144
2017-10-10T14:46:54.775089: step 676, loss 0.173223, acc 0.953125, learning_rate 0.000409874
2017-10-10T14:46:55.013675: step 677, loss 0.25691, acc 0.890625, learning_rate 0.000408609
2017-10-10T14:46:55.335034: step 678, loss 0.223828, acc 0.9375, learning_rate 0.00040735
2017-10-10T14:46:55.593207: step 679, loss 0.21578, acc 0.890625, learning_rate 0.000406095
2017-10-10T14:46:55.818147: step 680, loss 0.168315, acc 0.953125, learning_rate 0.000404846

Evaluation:
2017-10-10T14:46:56.288791: step 680, loss 0.253333, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-680

2017-10-10T14:46:57.220820: step 681, loss 0.179503, acc 0.9375, learning_rate 0.000403601
2017-10-10T14:46:57.540953: step 682, loss 0.173437, acc 0.953125, learning_rate 0.000402362
2017-10-10T14:46:57.887744: step 683, loss 0.161595, acc 0.90625, learning_rate 0.000401128
2017-10-10T14:46:58.173626: step 684, loss 0.227832, acc 0.921875, learning_rate 0.000399899
2017-10-10T14:46:58.404833: step 685, loss 0.150437, acc 0.953125, learning_rate 0.000398675
2017-10-10T14:46:58.689157: step 686, loss 0.193788, acc 0.980392, learning_rate 0.000397456
2017-10-10T14:46:58.937185: step 687, loss 0.156924, acc 0.953125, learning_rate 0.000396241
2017-10-10T14:46:59.202073: step 688, loss 0.190837, acc 0.9375, learning_rate 0.000395032
2017-10-10T14:46:59.539049: step 689, loss 0.132295, acc 0.984375, learning_rate 0.000393828
2017-10-10T14:46:59.849123: step 690, loss 0.115837, acc 0.953125, learning_rate 0.000392629
2017-10-10T14:47:00.120996: step 691, loss 0.243642, acc 0.9375, learning_rate 0.000391434
2017-10-10T14:47:00.386274: step 692, loss 0.203035, acc 0.921875, learning_rate 0.000390245
2017-10-10T14:47:00.677379: step 693, loss 0.153767, acc 0.96875, learning_rate 0.00038906
2017-10-10T14:47:00.982289: step 694, loss 0.19574, acc 0.921875, learning_rate 0.00038788
2017-10-10T14:47:01.274377: step 695, loss 0.110637, acc 0.96875, learning_rate 0.000386705
2017-10-10T14:47:01.533011: step 696, loss 0.241069, acc 0.890625, learning_rate 0.000385535
2017-10-10T14:47:01.792893: step 697, loss 0.122477, acc 0.96875, learning_rate 0.000384369
2017-10-10T14:47:02.022898: step 698, loss 0.161132, acc 0.953125, learning_rate 0.000383209
2017-10-10T14:47:02.231043: step 699, loss 0.212949, acc 0.921875, learning_rate 0.000382053
2017-10-10T14:47:02.529089: step 700, loss 0.248429, acc 0.90625, learning_rate 0.000380901
2017-10-10T14:47:02.856886: step 701, loss 0.140109, acc 0.9375, learning_rate 0.000379755
2017-10-10T14:47:03.104865: step 702, loss 0.210126, acc 0.9375, learning_rate 0.000378613
2017-10-10T14:47:03.332317: step 703, loss 0.157247, acc 0.953125, learning_rate 0.000377476
2017-10-10T14:47:03.613115: step 704, loss 0.0869902, acc 1, learning_rate 0.000376343
2017-10-10T14:47:03.889733: step 705, loss 0.194414, acc 0.921875, learning_rate 0.000375215
2017-10-10T14:47:04.137112: step 706, loss 0.296604, acc 0.9375, learning_rate 0.000374092
2017-10-10T14:47:04.457033: step 707, loss 0.124136, acc 0.96875, learning_rate 0.000372973
2017-10-10T14:47:04.659615: step 708, loss 0.189579, acc 0.953125, learning_rate 0.000371859
2017-10-10T14:47:04.959223: step 709, loss 0.123286, acc 0.984375, learning_rate 0.000370749
2017-10-10T14:47:05.232416: step 710, loss 0.181464, acc 0.9375, learning_rate 0.000369644
2017-10-10T14:47:05.493969: step 711, loss 0.175605, acc 0.921875, learning_rate 0.000368543
2017-10-10T14:47:05.784811: step 712, loss 0.245755, acc 0.90625, learning_rate 0.000367447
2017-10-10T14:47:06.067812: step 713, loss 0.144718, acc 0.96875, learning_rate 0.000366356
2017-10-10T14:47:06.305711: step 714, loss 0.243191, acc 0.9375, learning_rate 0.000365268
2017-10-10T14:47:06.614164: step 715, loss 0.227362, acc 0.890625, learning_rate 0.000364186
2017-10-10T14:47:06.859065: step 716, loss 0.275731, acc 0.875, learning_rate 0.000363107
2017-10-10T14:47:07.117896: step 717, loss 0.222223, acc 0.9375, learning_rate 0.000362033
2017-10-10T14:47:07.437065: step 718, loss 0.192144, acc 0.921875, learning_rate 0.000360964
2017-10-10T14:47:07.660833: step 719, loss 0.232079, acc 0.90625, learning_rate 0.000359899
2017-10-10T14:47:07.924381: step 720, loss 0.155076, acc 0.9375, learning_rate 0.000358838

Evaluation:
2017-10-10T14:47:08.337048: step 720, loss 0.248793, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-720

2017-10-10T14:47:09.468060: step 721, loss 0.261816, acc 0.90625, learning_rate 0.000357781
2017-10-10T14:47:09.709478: step 722, loss 0.210954, acc 0.9375, learning_rate 0.000356729
2017-10-10T14:47:10.069647: step 723, loss 0.225327, acc 0.921875, learning_rate 0.000355681
2017-10-10T14:47:10.326370: step 724, loss 0.144602, acc 0.953125, learning_rate 0.000354637
2017-10-10T14:47:10.609048: step 725, loss 0.253332, acc 0.90625, learning_rate 0.000353598
2017-10-10T14:47:10.868447: step 726, loss 0.295351, acc 0.875, learning_rate 0.000352563
2017-10-10T14:47:11.178963: step 727, loss 0.255596, acc 0.90625, learning_rate 0.000351532
2017-10-10T14:47:11.436997: step 728, loss 0.208651, acc 0.921875, learning_rate 0.000350505
2017-10-10T14:47:11.721710: step 729, loss 0.158862, acc 0.953125, learning_rate 0.000349483
2017-10-10T14:47:12.063230: step 730, loss 0.191823, acc 0.921875, learning_rate 0.000348465
2017-10-10T14:47:12.324423: step 731, loss 0.297506, acc 0.90625, learning_rate 0.00034745
2017-10-10T14:47:12.556829: step 732, loss 0.132315, acc 0.96875, learning_rate 0.00034644
2017-10-10T14:47:12.843627: step 733, loss 0.107882, acc 0.96875, learning_rate 0.000345434
2017-10-10T14:47:13.121197: step 734, loss 0.164178, acc 0.953125, learning_rate 0.000344433
2017-10-10T14:47:13.360555: step 735, loss 0.122962, acc 0.984375, learning_rate 0.000343435
2017-10-10T14:47:13.681750: step 736, loss 0.261937, acc 0.921875, learning_rate 0.000342441
2017-10-10T14:47:13.917219: step 737, loss 0.17163, acc 0.921875, learning_rate 0.000341452
2017-10-10T14:47:14.147795: step 738, loss 0.144309, acc 0.953125, learning_rate 0.000340466
2017-10-10T14:47:14.469370: step 739, loss 0.179492, acc 0.96875, learning_rate 0.000339485
2017-10-10T14:47:14.756963: step 740, loss 0.172799, acc 0.953125, learning_rate 0.000338507
2017-10-10T14:47:15.054597: step 741, loss 0.190903, acc 0.90625, learning_rate 0.000337534
2017-10-10T14:47:15.321181: step 742, loss 0.148483, acc 0.953125, learning_rate 0.000336564
2017-10-10T14:47:15.617335: step 743, loss 0.181564, acc 0.953125, learning_rate 0.000335598
2017-10-10T14:47:15.898644: step 744, loss 0.217558, acc 0.9375, learning_rate 0.000334637
2017-10-10T14:47:16.149209: step 745, loss 0.30837, acc 0.890625, learning_rate 0.000333679
2017-10-10T14:47:16.407194: step 746, loss 0.158022, acc 0.953125, learning_rate 0.000332725
2017-10-10T14:47:16.683806: step 747, loss 0.131244, acc 0.96875, learning_rate 0.000331775
2017-10-10T14:47:17.028874: step 748, loss 0.308027, acc 0.84375, learning_rate 0.000330829
2017-10-10T14:47:17.316926: step 749, loss 0.210065, acc 0.96875, learning_rate 0.000329887
2017-10-10T14:47:17.592950: step 750, loss 0.0776142, acc 0.984375, learning_rate 0.000328949
2017-10-10T14:47:17.833064: step 751, loss 0.191879, acc 0.953125, learning_rate 0.000328014
2017-10-10T14:47:18.088059: step 752, loss 0.127653, acc 0.96875, learning_rate 0.000327083
2017-10-10T14:47:18.331718: step 753, loss 0.266911, acc 0.921875, learning_rate 0.000326157
2017-10-10T14:47:18.579258: step 754, loss 0.399025, acc 0.875, learning_rate 0.000325233
2017-10-10T14:47:18.866677: step 755, loss 0.186052, acc 0.90625, learning_rate 0.000324314
2017-10-10T14:47:19.099399: step 756, loss 0.156971, acc 0.953125, learning_rate 0.000323399
2017-10-10T14:47:19.457312: step 757, loss 0.149911, acc 0.96875, learning_rate 0.000322487
2017-10-10T14:47:19.790903: step 758, loss 0.207716, acc 0.921875, learning_rate 0.000321579
2017-10-10T14:47:20.163030: step 759, loss 0.209264, acc 0.9375, learning_rate 0.000320674
2017-10-10T14:47:20.368827: step 760, loss 0.171012, acc 0.921875, learning_rate 0.000319773

Evaluation:
2017-10-10T14:47:20.732856: step 760, loss 0.249019, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-760

2017-10-10T14:47:21.768838: step 761, loss 0.14088, acc 0.96875, learning_rate 0.000318876
2017-10-10T14:47:22.109004: step 762, loss 0.208559, acc 0.953125, learning_rate 0.000317983
2017-10-10T14:47:22.406531: step 763, loss 0.220173, acc 0.921875, learning_rate 0.000317093
2017-10-10T14:47:22.651975: step 764, loss 0.0892596, acc 0.96875, learning_rate 0.000316207
2017-10-10T14:47:22.940877: step 765, loss 0.240602, acc 0.90625, learning_rate 0.000315325
2017-10-10T14:47:23.257127: step 766, loss 0.159216, acc 0.9375, learning_rate 0.000314446
2017-10-10T14:47:23.633529: step 767, loss 0.0875392, acc 1, learning_rate 0.00031357
2017-10-10T14:47:23.828789: step 768, loss 0.162629, acc 0.921875, learning_rate 0.000312699
2017-10-10T14:47:24.054537: step 769, loss 0.289239, acc 0.90625, learning_rate 0.00031183
2017-10-10T14:47:24.328774: step 770, loss 0.275327, acc 0.875, learning_rate 0.000310966
2017-10-10T14:47:24.537989: step 771, loss 0.270404, acc 0.921875, learning_rate 0.000310105
2017-10-10T14:47:24.789116: step 772, loss 0.25886, acc 0.90625, learning_rate 0.000309247
2017-10-10T14:47:25.052847: step 773, loss 0.296613, acc 0.921875, learning_rate 0.000308393
2017-10-10T14:47:25.272365: step 774, loss 0.170804, acc 0.921875, learning_rate 0.000307542
2017-10-10T14:47:25.572827: step 775, loss 0.256987, acc 0.90625, learning_rate 0.000306695
2017-10-10T14:47:25.877633: step 776, loss 0.130688, acc 0.953125, learning_rate 0.000305852
2017-10-10T14:47:26.143670: step 777, loss 0.159491, acc 0.96875, learning_rate 0.000305011
2017-10-10T14:47:26.376885: step 778, loss 0.123165, acc 0.96875, learning_rate 0.000304174
2017-10-10T14:47:26.652858: step 779, loss 0.175382, acc 0.9375, learning_rate 0.000303341
2017-10-10T14:47:26.936916: step 780, loss 0.158275, acc 0.9375, learning_rate 0.000302511
2017-10-10T14:47:27.264903: step 781, loss 0.118737, acc 0.984375, learning_rate 0.000301684
2017-10-10T14:47:27.549038: step 782, loss 0.20244, acc 0.953125, learning_rate 0.000300861
2017-10-10T14:47:27.872493: step 783, loss 0.198048, acc 0.9375, learning_rate 0.000300041
2017-10-10T14:47:28.084451: step 784, loss 0.106992, acc 0.941176, learning_rate 0.000299225
2017-10-10T14:47:28.380868: step 785, loss 0.113476, acc 0.96875, learning_rate 0.000298412
2017-10-10T14:47:28.627293: step 786, loss 0.140173, acc 0.984375, learning_rate 0.000297602
2017-10-10T14:47:28.876833: step 787, loss 0.124882, acc 0.96875, learning_rate 0.000296795
2017-10-10T14:47:29.196011: step 788, loss 0.184855, acc 0.953125, learning_rate 0.000295992
2017-10-10T14:47:29.464404: step 789, loss 0.166883, acc 0.90625, learning_rate 0.000295192
2017-10-10T14:47:29.716925: step 790, loss 0.230591, acc 0.921875, learning_rate 0.000294395
2017-10-10T14:47:30.016856: step 791, loss 0.256656, acc 0.921875, learning_rate 0.000293602
2017-10-10T14:47:30.310159: step 792, loss 0.120599, acc 0.96875, learning_rate 0.000292812
2017-10-10T14:47:30.586315: step 793, loss 0.233505, acc 0.90625, learning_rate 0.000292025
2017-10-10T14:47:30.856078: step 794, loss 0.175659, acc 0.90625, learning_rate 0.000291241
2017-10-10T14:47:31.130500: step 795, loss 0.162794, acc 0.921875, learning_rate 0.00029046
2017-10-10T14:47:31.433928: step 796, loss 0.169841, acc 0.921875, learning_rate 0.000289683
2017-10-10T14:47:31.700542: step 797, loss 0.182878, acc 0.953125, learning_rate 0.000288908
2017-10-10T14:47:32.012904: step 798, loss 0.240071, acc 0.90625, learning_rate 0.000288137
2017-10-10T14:47:32.240578: step 799, loss 0.198716, acc 0.890625, learning_rate 0.000287369
2017-10-10T14:47:32.514241: step 800, loss 0.168056, acc 0.9375, learning_rate 0.000286605

Evaluation:
2017-10-10T14:47:33.004677: step 800, loss 0.245995, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-800

2017-10-10T14:47:34.168830: step 801, loss 0.171142, acc 0.9375, learning_rate 0.000285843
2017-10-10T14:47:34.428800: step 802, loss 0.119703, acc 0.953125, learning_rate 0.000285084
2017-10-10T14:47:34.705130: step 803, loss 0.134549, acc 0.96875, learning_rate 0.000284329
2017-10-10T14:47:35.039578: step 804, loss 0.198728, acc 0.9375, learning_rate 0.000283577
2017-10-10T14:47:35.344938: step 805, loss 0.283818, acc 0.9375, learning_rate 0.000282827
2017-10-10T14:47:35.608341: step 806, loss 0.135094, acc 0.96875, learning_rate 0.000282081
2017-10-10T14:47:35.875071: step 807, loss 0.123841, acc 0.953125, learning_rate 0.000281338
2017-10-10T14:47:36.139057: step 808, loss 0.168276, acc 0.96875, learning_rate 0.000280598
2017-10-10T14:47:36.317075: step 809, loss 0.198355, acc 0.9375, learning_rate 0.00027986
2017-10-10T14:47:36.558878: step 810, loss 0.229123, acc 0.921875, learning_rate 0.000279126
2017-10-10T14:47:36.837133: step 811, loss 0.135032, acc 0.96875, learning_rate 0.000278395
2017-10-10T14:47:37.112829: step 812, loss 0.160193, acc 0.9375, learning_rate 0.000277667
2017-10-10T14:47:37.374089: step 813, loss 0.330004, acc 0.890625, learning_rate 0.000276942
2017-10-10T14:47:37.692589: step 814, loss 0.142105, acc 0.953125, learning_rate 0.00027622
2017-10-10T14:47:38.004857: step 815, loss 0.169342, acc 0.9375, learning_rate 0.0002755
2017-10-10T14:47:38.396801: step 816, loss 0.162064, acc 0.953125, learning_rate 0.000274784
2017-10-10T14:47:38.619263: step 817, loss 0.0680804, acc 0.984375, learning_rate 0.000274071
2017-10-10T14:47:38.877800: step 818, loss 0.22782, acc 0.890625, learning_rate 0.00027336
2017-10-10T14:47:39.128063: step 819, loss 0.27751, acc 0.921875, learning_rate 0.000272652
2017-10-10T14:47:39.397006: step 820, loss 0.108187, acc 0.984375, learning_rate 0.000271948
2017-10-10T14:47:39.703338: step 821, loss 0.180354, acc 0.90625, learning_rate 0.000271246
2017-10-10T14:47:40.053896: step 822, loss 0.234449, acc 0.9375, learning_rate 0.000270547
2017-10-10T14:47:40.385074: step 823, loss 0.206564, acc 0.9375, learning_rate 0.000269851
2017-10-10T14:47:40.599149: step 824, loss 0.287187, acc 0.921875, learning_rate 0.000269157
2017-10-10T14:47:40.830698: step 825, loss 0.143007, acc 0.953125, learning_rate 0.000268467
2017-10-10T14:47:41.086405: step 826, loss 0.309921, acc 0.890625, learning_rate 0.000267779
2017-10-10T14:47:41.375537: step 827, loss 0.324032, acc 0.890625, learning_rate 0.000267094
2017-10-10T14:47:41.620133: step 828, loss 0.14851, acc 0.96875, learning_rate 0.000266412
2017-10-10T14:47:41.888805: step 829, loss 0.0527598, acc 1, learning_rate 0.000265733
2017-10-10T14:47:42.144888: step 830, loss 0.242123, acc 0.90625, learning_rate 0.000265057
2017-10-10T14:47:42.390667: step 831, loss 0.275806, acc 0.90625, learning_rate 0.000264383
2017-10-10T14:47:42.631023: step 832, loss 0.149722, acc 0.953125, learning_rate 0.000263712
2017-10-10T14:47:42.877388: step 833, loss 0.163847, acc 0.921875, learning_rate 0.000263044
2017-10-10T14:47:43.155287: step 834, loss 0.149339, acc 0.984375, learning_rate 0.000262378
2017-10-10T14:47:43.400542: step 835, loss 0.101435, acc 0.96875, learning_rate 0.000261715
2017-10-10T14:47:43.676923: step 836, loss 0.145671, acc 0.953125, learning_rate 0.000261055
2017-10-10T14:47:43.912265: step 837, loss 0.273482, acc 0.9375, learning_rate 0.000260398
2017-10-10T14:47:44.192979: step 838, loss 0.190336, acc 0.9375, learning_rate 0.000259743
2017-10-10T14:47:44.497813: step 839, loss 0.13193, acc 0.953125, learning_rate 0.000259091
2017-10-10T14:47:44.783360: step 840, loss 0.184976, acc 0.921875, learning_rate 0.000258442

Evaluation:
2017-10-10T14:47:45.159919: step 840, loss 0.245056, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-840

2017-10-10T14:47:46.140888: step 841, loss 0.190506, acc 0.890625, learning_rate 0.000257795
2017-10-10T14:47:46.428107: step 842, loss 0.322753, acc 0.890625, learning_rate 0.000257151
2017-10-10T14:47:46.701139: step 843, loss 0.19968, acc 0.90625, learning_rate 0.00025651
2017-10-10T14:47:46.993059: step 844, loss 0.143806, acc 0.953125, learning_rate 0.000255871
2017-10-10T14:47:47.237135: step 845, loss 0.135114, acc 0.9375, learning_rate 0.000255235
2017-10-10T14:47:47.420439: step 846, loss 0.171657, acc 0.953125, learning_rate 0.000254601
2017-10-10T14:47:47.661009: step 847, loss 0.126774, acc 0.9375, learning_rate 0.00025397
2017-10-10T14:47:47.960684: step 848, loss 0.165928, acc 0.953125, learning_rate 0.000253341
2017-10-10T14:47:48.245026: step 849, loss 0.108519, acc 0.9375, learning_rate 0.000252716
2017-10-10T14:47:48.469385: step 850, loss 0.167666, acc 0.9375, learning_rate 0.000252092
2017-10-10T14:47:48.789129: step 851, loss 0.181191, acc 0.9375, learning_rate 0.000251471
2017-10-10T14:47:49.090812: step 852, loss 0.136758, acc 0.953125, learning_rate 0.000250853
2017-10-10T14:47:49.341382: step 853, loss 0.217551, acc 0.9375, learning_rate 0.000250237
2017-10-10T14:47:49.652680: step 854, loss 0.15952, acc 0.96875, learning_rate 0.000249624
2017-10-10T14:47:49.921997: step 855, loss 0.237996, acc 0.90625, learning_rate 0.000249013
2017-10-10T14:47:50.184762: step 856, loss 0.13743, acc 0.9375, learning_rate 0.000248405
2017-10-10T14:47:50.430600: step 857, loss 0.286299, acc 0.921875, learning_rate 0.000247799
2017-10-10T14:47:50.713359: step 858, loss 0.213881, acc 0.921875, learning_rate 0.000247196
2017-10-10T14:47:51.046022: step 859, loss 0.102414, acc 0.96875, learning_rate 0.000246595
2017-10-10T14:47:51.365483: step 860, loss 0.178024, acc 0.953125, learning_rate 0.000245997
2017-10-10T14:47:51.602271: step 861, loss 0.133629, acc 0.953125, learning_rate 0.000245401
2017-10-10T14:47:51.875398: step 862, loss 0.268528, acc 0.90625, learning_rate 0.000244808
2017-10-10T14:47:52.194974: step 863, loss 0.0876534, acc 0.96875, learning_rate 0.000244216
2017-10-10T14:47:52.469491: step 864, loss 0.258115, acc 0.9375, learning_rate 0.000243628
2017-10-10T14:47:52.730680: step 865, loss 0.15409, acc 0.9375, learning_rate 0.000243042
2017-10-10T14:47:53.025942: step 866, loss 0.171666, acc 0.953125, learning_rate 0.000242458
2017-10-10T14:47:53.280611: step 867, loss 0.213251, acc 0.90625, learning_rate 0.000241876
2017-10-10T14:47:53.537816: step 868, loss 0.0838807, acc 0.984375, learning_rate 0.000241297
2017-10-10T14:47:53.765269: step 869, loss 0.138345, acc 0.96875, learning_rate 0.00024072
2017-10-10T14:47:54.127248: step 870, loss 0.223736, acc 0.9375, learning_rate 0.000240146
2017-10-10T14:47:54.435335: step 871, loss 0.232274, acc 0.921875, learning_rate 0.000239574
2017-10-10T14:47:54.756969: step 872, loss 0.193898, acc 0.953125, learning_rate 0.000239004
2017-10-10T14:47:54.982529: step 873, loss 0.231309, acc 0.875, learning_rate 0.000238437
2017-10-10T14:47:55.245248: step 874, loss 0.0698647, acc 1, learning_rate 0.000237872
2017-10-10T14:47:55.532993: step 875, loss 0.301846, acc 0.921875, learning_rate 0.000237309
2017-10-10T14:47:55.812913: step 876, loss 0.133967, acc 0.9375, learning_rate 0.000236749
2017-10-10T14:47:56.136997: step 877, loss 0.153349, acc 0.9375, learning_rate 0.00023619
2017-10-10T14:47:56.454277: step 878, loss 0.177708, acc 0.9375, learning_rate 0.000235635
2017-10-10T14:47:56.748953: step 879, loss 0.262612, acc 0.90625, learning_rate 0.000235081
2017-10-10T14:47:57.036000: step 880, loss 0.273532, acc 0.890625, learning_rate 0.00023453

Evaluation:
2017-10-10T14:47:57.385630: step 880, loss 0.245771, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-880

2017-10-10T14:47:58.400814: step 881, loss 0.225285, acc 0.9375, learning_rate 0.00023398
2017-10-10T14:47:58.565587: step 882, loss 0.232499, acc 0.941176, learning_rate 0.000233434
2017-10-10T14:47:58.842327: step 883, loss 0.162141, acc 0.921875, learning_rate 0.000232889
2017-10-10T14:47:59.125064: step 884, loss 0.275386, acc 0.890625, learning_rate 0.000232346
2017-10-10T14:47:59.334320: step 885, loss 0.152442, acc 0.953125, learning_rate 0.000231806
2017-10-10T14:47:59.634592: step 886, loss 0.132127, acc 0.953125, learning_rate 0.000231268
2017-10-10T14:47:59.928958: step 887, loss 0.149998, acc 0.9375, learning_rate 0.000230732
2017-10-10T14:48:00.148673: step 888, loss 0.285891, acc 0.921875, learning_rate 0.000230199
2017-10-10T14:48:00.382590: step 889, loss 0.344202, acc 0.90625, learning_rate 0.000229667
2017-10-10T14:48:00.608530: step 890, loss 0.148903, acc 0.953125, learning_rate 0.000229138
2017-10-10T14:48:00.934446: step 891, loss 0.232208, acc 0.9375, learning_rate 0.000228611
2017-10-10T14:48:01.199757: step 892, loss 0.282049, acc 0.90625, learning_rate 0.000228086
2017-10-10T14:48:01.431062: step 893, loss 0.109991, acc 0.96875, learning_rate 0.000227563
2017-10-10T14:48:01.788851: step 894, loss 0.206877, acc 0.921875, learning_rate 0.000227043
2017-10-10T14:48:02.036542: step 895, loss 0.18462, acc 0.9375, learning_rate 0.000226524
2017-10-10T14:48:02.339232: step 896, loss 0.108819, acc 0.96875, learning_rate 0.000226008
2017-10-10T14:48:02.627819: step 897, loss 0.217977, acc 0.921875, learning_rate 0.000225493
2017-10-10T14:48:02.948891: step 898, loss 0.0829457, acc 0.984375, learning_rate 0.000224981
2017-10-10T14:48:03.239812: step 899, loss 0.190107, acc 0.90625, learning_rate 0.000224471
2017-10-10T14:48:03.531837: step 900, loss 0.273265, acc 0.875, learning_rate 0.000223963
2017-10-10T14:48:03.820970: step 901, loss 0.117754, acc 0.9375, learning_rate 0.000223457
2017-10-10T14:48:04.076132: step 902, loss 0.144572, acc 0.9375, learning_rate 0.000222953
2017-10-10T14:48:04.405416: step 903, loss 0.19363, acc 0.953125, learning_rate 0.000222451
2017-10-10T14:48:04.720855: step 904, loss 0.192881, acc 0.9375, learning_rate 0.000221951
2017-10-10T14:48:04.968313: step 905, loss 0.193401, acc 0.90625, learning_rate 0.000221453
2017-10-10T14:48:05.188877: step 906, loss 0.210683, acc 0.921875, learning_rate 0.000220958
2017-10-10T14:48:05.537031: step 907, loss 0.192875, acc 0.921875, learning_rate 0.000220464
2017-10-10T14:48:05.848159: step 908, loss 0.187902, acc 0.9375, learning_rate 0.000219972
2017-10-10T14:48:06.065011: step 909, loss 0.158102, acc 0.953125, learning_rate 0.000219483
2017-10-10T14:48:06.310588: step 910, loss 0.258051, acc 0.90625, learning_rate 0.000218995
2017-10-10T14:48:06.601723: step 911, loss 0.128791, acc 0.96875, learning_rate 0.000218509
2017-10-10T14:48:06.932636: step 912, loss 0.205969, acc 0.90625, learning_rate 0.000218025
2017-10-10T14:48:07.236892: step 913, loss 0.202254, acc 0.921875, learning_rate 0.000217544
2017-10-10T14:48:07.493025: step 914, loss 0.182896, acc 0.96875, learning_rate 0.000217064
2017-10-10T14:48:07.748885: step 915, loss 0.115798, acc 0.984375, learning_rate 0.000216586
2017-10-10T14:48:08.072935: step 916, loss 0.18719, acc 0.921875, learning_rate 0.00021611
2017-10-10T14:48:08.345092: step 917, loss 0.15926, acc 0.9375, learning_rate 0.000215636
2017-10-10T14:48:08.613992: step 918, loss 0.0993449, acc 0.984375, learning_rate 0.000215164
2017-10-10T14:48:08.934776: step 919, loss 0.0951848, acc 0.984375, learning_rate 0.000214694
2017-10-10T14:48:09.252827: step 920, loss 0.215395, acc 0.9375, learning_rate 0.000214226

Evaluation:
2017-10-10T14:48:09.647758: step 920, loss 0.242813, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-920

2017-10-10T14:48:10.748927: step 921, loss 0.165746, acc 0.9375, learning_rate 0.00021376
2017-10-10T14:48:11.081086: step 922, loss 0.199547, acc 0.9375, learning_rate 0.000213295
2017-10-10T14:48:11.376881: step 923, loss 0.140493, acc 0.9375, learning_rate 0.000212833
2017-10-10T14:48:11.616886: step 924, loss 0.218542, acc 0.9375, learning_rate 0.000212372
2017-10-10T14:48:11.929105: step 925, loss 0.146478, acc 0.953125, learning_rate 0.000211914
2017-10-10T14:48:12.262125: step 926, loss 0.138049, acc 0.96875, learning_rate 0.000211457
2017-10-10T14:48:12.533133: step 927, loss 0.108667, acc 0.953125, learning_rate 0.000211002
2017-10-10T14:48:12.829721: step 928, loss 0.196543, acc 0.90625, learning_rate 0.000210549
2017-10-10T14:48:13.236833: step 929, loss 0.139301, acc 0.953125, learning_rate 0.000210098
2017-10-10T14:48:13.516901: step 930, loss 0.208835, acc 0.921875, learning_rate 0.000209648
2017-10-10T14:48:13.817355: step 931, loss 0.302986, acc 0.890625, learning_rate 0.000209201
2017-10-10T14:48:14.048822: step 932, loss 0.155324, acc 0.96875, learning_rate 0.000208755
2017-10-10T14:48:14.372624: step 933, loss 0.172442, acc 0.953125, learning_rate 0.000208311
2017-10-10T14:48:14.637634: step 934, loss 0.167163, acc 0.953125, learning_rate 0.000207869
2017-10-10T14:48:14.836939: step 935, loss 0.268545, acc 0.90625, learning_rate 0.000207429
2017-10-10T14:48:15.100877: step 936, loss 0.383973, acc 0.875, learning_rate 0.00020699
2017-10-10T14:48:15.360879: step 937, loss 0.0432359, acc 1, learning_rate 0.000206554
2017-10-10T14:48:15.634718: step 938, loss 0.159233, acc 0.9375, learning_rate 0.000206119
2017-10-10T14:48:15.881042: step 939, loss 0.217478, acc 0.953125, learning_rate 0.000205685
2017-10-10T14:48:16.129945: step 940, loss 0.189889, acc 0.9375, learning_rate 0.000205254
2017-10-10T14:48:16.377867: step 941, loss 0.155443, acc 0.96875, learning_rate 0.000204824
2017-10-10T14:48:16.701047: step 942, loss 0.259661, acc 0.90625, learning_rate 0.000204397
2017-10-10T14:48:16.931284: step 943, loss 0.140878, acc 0.953125, learning_rate 0.00020397
2017-10-10T14:48:17.208883: step 944, loss 0.147651, acc 0.953125, learning_rate 0.000203546
2017-10-10T14:48:17.560979: step 945, loss 0.169075, acc 0.9375, learning_rate 0.000203123
2017-10-10T14:48:17.817226: step 946, loss 0.287291, acc 0.875, learning_rate 0.000202702
2017-10-10T14:48:18.118299: step 947, loss 0.292998, acc 0.875, learning_rate 0.000202283
2017-10-10T14:48:18.400994: step 948, loss 0.0998004, acc 0.984375, learning_rate 0.000201866
2017-10-10T14:48:18.704904: step 949, loss 0.152282, acc 0.953125, learning_rate 0.00020145
2017-10-10T14:48:18.971098: step 950, loss 0.176967, acc 0.90625, learning_rate 0.000201036
2017-10-10T14:48:19.268423: step 951, loss 0.189966, acc 0.9375, learning_rate 0.000200623
2017-10-10T14:48:19.524417: step 952, loss 0.12954, acc 0.953125, learning_rate 0.000200213
2017-10-10T14:48:19.758856: step 953, loss 0.284027, acc 0.90625, learning_rate 0.000199804
2017-10-10T14:48:20.027263: step 954, loss 0.254216, acc 0.890625, learning_rate 0.000199396
2017-10-10T14:48:20.320742: step 955, loss 0.216218, acc 0.96875, learning_rate 0.000198991
2017-10-10T14:48:20.529063: step 956, loss 0.194592, acc 0.9375, learning_rate 0.000198587
2017-10-10T14:48:20.830176: step 957, loss 0.128505, acc 0.984375, learning_rate 0.000198184
2017-10-10T14:48:21.061390: step 958, loss 0.142887, acc 0.96875, learning_rate 0.000197783
2017-10-10T14:48:21.358390: step 959, loss 0.23739, acc 0.90625, learning_rate 0.000197384
2017-10-10T14:48:21.610721: step 960, loss 0.2468, acc 0.921875, learning_rate 0.000196987

Evaluation:
2017-10-10T14:48:22.039325: step 960, loss 0.247404, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-960

2017-10-10T14:48:22.925000: step 961, loss 0.203753, acc 0.921875, learning_rate 0.000196591
2017-10-10T14:48:23.247379: step 962, loss 0.185536, acc 0.9375, learning_rate 0.000196197
2017-10-10T14:48:23.536940: step 963, loss 0.208721, acc 0.953125, learning_rate 0.000195804
2017-10-10T14:48:23.804885: step 964, loss 0.188715, acc 0.9375, learning_rate 0.000195413
2017-10-10T14:48:24.080899: step 965, loss 0.206485, acc 0.921875, learning_rate 0.000195023
2017-10-10T14:48:24.310993: step 966, loss 0.162521, acc 0.9375, learning_rate 0.000194636
2017-10-10T14:48:24.594486: step 967, loss 0.200666, acc 0.953125, learning_rate 0.000194249
2017-10-10T14:48:24.936832: step 968, loss 0.17996, acc 0.953125, learning_rate 0.000193865
2017-10-10T14:48:25.175618: step 969, loss 0.249363, acc 0.90625, learning_rate 0.000193482
2017-10-10T14:48:25.464461: step 970, loss 0.205755, acc 0.90625, learning_rate 0.0001931
2017-10-10T14:48:25.744859: step 971, loss 0.163962, acc 0.96875, learning_rate 0.00019272
2017-10-10T14:48:26.071055: step 972, loss 0.143021, acc 0.96875, learning_rate 0.000192341
2017-10-10T14:48:26.345251: step 973, loss 0.0989852, acc 0.96875, learning_rate 0.000191965
2017-10-10T14:48:26.577470: step 974, loss 0.2401, acc 0.953125, learning_rate 0.000191589
2017-10-10T14:48:26.826889: step 975, loss 0.168251, acc 0.9375, learning_rate 0.000191215
2017-10-10T14:48:27.074849: step 976, loss 0.127848, acc 0.953125, learning_rate 0.000190843
2017-10-10T14:48:27.332953: step 977, loss 0.198473, acc 0.9375, learning_rate 0.000190472
2017-10-10T14:48:27.651756: step 978, loss 0.220169, acc 0.921875, learning_rate 0.000190103
2017-10-10T14:48:27.880870: step 979, loss 0.121834, acc 0.953125, learning_rate 0.000189735
2017-10-10T14:48:28.083130: step 980, loss 0.291186, acc 0.921569, learning_rate 0.000189369
2017-10-10T14:48:28.376077: step 981, loss 0.19929, acc 0.9375, learning_rate 0.000189004
2017-10-10T14:48:28.610933: step 982, loss 0.238088, acc 0.890625, learning_rate 0.000188641
2017-10-10T14:48:28.926951: step 983, loss 0.0838069, acc 0.984375, learning_rate 0.000188279
2017-10-10T14:48:29.224090: step 984, loss 0.137138, acc 0.96875, learning_rate 0.000187919
2017-10-10T14:48:29.530880: step 985, loss 0.106658, acc 0.953125, learning_rate 0.00018756
2017-10-10T14:48:29.741020: step 986, loss 0.160416, acc 0.953125, learning_rate 0.000187202
2017-10-10T14:48:30.064059: step 987, loss 0.0886745, acc 0.984375, learning_rate 0.000186846
2017-10-10T14:48:30.365272: step 988, loss 0.117105, acc 0.953125, learning_rate 0.000186492
2017-10-10T14:48:30.589090: step 989, loss 0.142081, acc 0.953125, learning_rate 0.000186139
2017-10-10T14:48:30.880940: step 990, loss 0.157704, acc 0.921875, learning_rate 0.000185787
2017-10-10T14:48:31.160936: step 991, loss 0.236055, acc 0.921875, learning_rate 0.000185437
2017-10-10T14:48:31.437006: step 992, loss 0.232195, acc 0.9375, learning_rate 0.000185088
2017-10-10T14:48:31.668470: step 993, loss 0.123636, acc 0.953125, learning_rate 0.000184741
2017-10-10T14:48:31.907588: step 994, loss 0.114473, acc 0.96875, learning_rate 0.000184395
2017-10-10T14:48:32.224855: step 995, loss 0.241161, acc 0.90625, learning_rate 0.000184051
2017-10-10T14:48:32.531663: step 996, loss 0.244668, acc 0.875, learning_rate 0.000183708
2017-10-10T14:48:32.849195: step 997, loss 0.165759, acc 0.921875, learning_rate 0.000183366
2017-10-10T14:48:33.131411: step 998, loss 0.306195, acc 0.859375, learning_rate 0.000183026
2017-10-10T14:48:33.356879: step 999, loss 0.319015, acc 0.890625, learning_rate 0.000182687
2017-10-10T14:48:33.623508: step 1000, loss 0.122943, acc 0.96875, learning_rate 0.000182349

Evaluation:
2017-10-10T14:48:33.997040: step 1000, loss 0.244249, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1000

2017-10-10T14:48:35.048696: step 1001, loss 0.129911, acc 0.984375, learning_rate 0.000182013
2017-10-10T14:48:35.363727: step 1002, loss 0.214795, acc 0.90625, learning_rate 0.000181678
2017-10-10T14:48:35.616110: step 1003, loss 0.100495, acc 0.984375, learning_rate 0.000181345
2017-10-10T14:48:35.926946: step 1004, loss 0.224474, acc 0.921875, learning_rate 0.000181013
2017-10-10T14:48:36.224830: step 1005, loss 0.116237, acc 0.96875, learning_rate 0.000180682
2017-10-10T14:48:36.556423: step 1006, loss 0.147254, acc 0.953125, learning_rate 0.000180353
2017-10-10T14:48:36.807689: step 1007, loss 0.299367, acc 0.90625, learning_rate 0.000180025
2017-10-10T14:48:37.067782: step 1008, loss 0.0859971, acc 0.984375, learning_rate 0.000179698
2017-10-10T14:48:37.347045: step 1009, loss 0.138399, acc 0.953125, learning_rate 0.000179373
2017-10-10T14:48:37.673105: step 1010, loss 0.246658, acc 0.90625, learning_rate 0.000179049
2017-10-10T14:48:37.977044: step 1011, loss 0.125905, acc 0.953125, learning_rate 0.000178726
2017-10-10T14:48:38.264132: step 1012, loss 0.0916278, acc 0.953125, learning_rate 0.000178405
2017-10-10T14:48:38.489357: step 1013, loss 0.296339, acc 0.890625, learning_rate 0.000178085
2017-10-10T14:48:38.795381: step 1014, loss 0.170799, acc 0.953125, learning_rate 0.000177766
2017-10-10T14:48:39.032236: step 1015, loss 0.199645, acc 0.9375, learning_rate 0.000177449
2017-10-10T14:48:39.377125: step 1016, loss 0.194907, acc 0.9375, learning_rate 0.000177133
2017-10-10T14:48:39.662362: step 1017, loss 0.194381, acc 0.90625, learning_rate 0.000176818
2017-10-10T14:48:39.948011: step 1018, loss 0.185405, acc 0.953125, learning_rate 0.000176504
2017-10-10T14:48:40.206015: step 1019, loss 0.154664, acc 0.953125, learning_rate 0.000176192
2017-10-10T14:48:40.452528: step 1020, loss 0.113692, acc 0.96875, learning_rate 0.000175881
2017-10-10T14:48:40.692066: step 1021, loss 0.152166, acc 0.9375, learning_rate 0.000175571
2017-10-10T14:48:40.984865: step 1022, loss 0.212499, acc 0.9375, learning_rate 0.000175263
2017-10-10T14:48:41.274204: step 1023, loss 0.110554, acc 0.984375, learning_rate 0.000174956
2017-10-10T14:48:41.584908: step 1024, loss 0.136049, acc 0.984375, learning_rate 0.00017465
2017-10-10T14:48:41.881512: step 1025, loss 0.191514, acc 0.9375, learning_rate 0.000174345
2017-10-10T14:48:42.151791: step 1026, loss 0.0736049, acc 0.984375, learning_rate 0.000174042
2017-10-10T14:48:42.392453: step 1027, loss 0.0880309, acc 0.984375, learning_rate 0.000173739
2017-10-10T14:48:42.628890: step 1028, loss 0.0978837, acc 0.953125, learning_rate 0.000173438
2017-10-10T14:48:42.892124: step 1029, loss 0.112114, acc 0.96875, learning_rate 0.000173139
2017-10-10T14:48:43.145860: step 1030, loss 0.186416, acc 0.9375, learning_rate 0.00017284
2017-10-10T14:48:43.424821: step 1031, loss 0.288359, acc 0.90625, learning_rate 0.000172543
2017-10-10T14:48:43.668817: step 1032, loss 0.243802, acc 0.9375, learning_rate 0.000172247
2017-10-10T14:48:44.003047: step 1033, loss 0.234491, acc 0.921875, learning_rate 0.000171952
2017-10-10T14:48:44.288808: step 1034, loss 0.107554, acc 0.953125, learning_rate 0.000171658
2017-10-10T14:48:44.628224: step 1035, loss 0.312068, acc 0.875, learning_rate 0.000171366
2017-10-10T14:48:44.900830: step 1036, loss 0.213202, acc 0.921875, learning_rate 0.000171074
2017-10-10T14:48:45.186342: step 1037, loss 0.16132, acc 0.953125, learning_rate 0.000170784
2017-10-10T14:48:45.481465: step 1038, loss 0.204021, acc 0.953125, learning_rate 0.000170495
2017-10-10T14:48:45.795426: step 1039, loss 0.18609, acc 0.953125, learning_rate 0.000170208
2017-10-10T14:48:46.071049: step 1040, loss 0.251234, acc 0.9375, learning_rate 0.000169921

Evaluation:
2017-10-10T14:48:46.546976: step 1040, loss 0.242913, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1040

2017-10-10T14:48:48.109711: step 1041, loss 0.148236, acc 0.9375, learning_rate 0.000169636
2017-10-10T14:48:48.384173: step 1042, loss 0.148219, acc 0.984375, learning_rate 0.000169351
2017-10-10T14:48:48.669954: step 1043, loss 0.176371, acc 0.953125, learning_rate 0.000169068
2017-10-10T14:48:48.961654: step 1044, loss 0.201731, acc 0.890625, learning_rate 0.000168786
2017-10-10T14:48:49.257758: step 1045, loss 0.158917, acc 0.9375, learning_rate 0.000168506
2017-10-10T14:48:49.478386: step 1046, loss 0.14865, acc 0.96875, learning_rate 0.000168226
2017-10-10T14:48:49.771184: step 1047, loss 0.176845, acc 0.9375, learning_rate 0.000167947
2017-10-10T14:48:50.045066: step 1048, loss 0.145191, acc 0.953125, learning_rate 0.00016767
2017-10-10T14:48:50.298937: step 1049, loss 0.265529, acc 0.90625, learning_rate 0.000167394
2017-10-10T14:48:50.615894: step 1050, loss 0.156661, acc 0.953125, learning_rate 0.000167119
2017-10-10T14:48:50.987001: step 1051, loss 0.128735, acc 0.9375, learning_rate 0.000166845
2017-10-10T14:48:51.232889: step 1052, loss 0.219521, acc 0.90625, learning_rate 0.000166572
2017-10-10T14:48:51.450064: step 1053, loss 0.150192, acc 0.90625, learning_rate 0.0001663
2017-10-10T14:48:51.671475: step 1054, loss 0.144596, acc 0.9375, learning_rate 0.00016603
2017-10-10T14:48:51.895411: step 1055, loss 0.179169, acc 0.9375, learning_rate 0.00016576
2017-10-10T14:48:52.126458: step 1056, loss 0.166776, acc 0.9375, learning_rate 0.000165492
2017-10-10T14:48:52.472851: step 1057, loss 0.147633, acc 0.984375, learning_rate 0.000165224
2017-10-10T14:48:52.763908: step 1058, loss 0.229773, acc 0.921875, learning_rate 0.000164958
2017-10-10T14:48:53.004189: step 1059, loss 0.177603, acc 0.9375, learning_rate 0.000164693
2017-10-10T14:48:53.253287: step 1060, loss 0.152423, acc 0.9375, learning_rate 0.000164429
2017-10-10T14:48:53.434629: step 1061, loss 0.183059, acc 0.9375, learning_rate 0.000164166
2017-10-10T14:48:53.644919: step 1062, loss 0.130074, acc 0.9375, learning_rate 0.000163904
2017-10-10T14:48:53.944947: step 1063, loss 0.191536, acc 0.921875, learning_rate 0.000163643
2017-10-10T14:48:54.232935: step 1064, loss 0.0728762, acc 1, learning_rate 0.000163383
2017-10-10T14:48:54.507304: step 1065, loss 0.107004, acc 0.96875, learning_rate 0.000163125
2017-10-10T14:48:54.808866: step 1066, loss 0.192672, acc 0.9375, learning_rate 0.000162867
2017-10-10T14:48:55.078855: step 1067, loss 0.126134, acc 0.953125, learning_rate 0.00016261
2017-10-10T14:48:55.332591: step 1068, loss 0.11265, acc 0.953125, learning_rate 0.000162355
2017-10-10T14:48:55.638653: step 1069, loss 0.0932899, acc 0.96875, learning_rate 0.0001621
2017-10-10T14:48:55.906702: step 1070, loss 0.175175, acc 0.921875, learning_rate 0.000161847
2017-10-10T14:48:56.162490: step 1071, loss 0.195, acc 0.921875, learning_rate 0.000161594
2017-10-10T14:48:56.420251: step 1072, loss 0.169453, acc 0.921875, learning_rate 0.000161343
2017-10-10T14:48:56.700955: step 1073, loss 0.107783, acc 0.984375, learning_rate 0.000161093
2017-10-10T14:48:56.961032: step 1074, loss 0.150179, acc 0.953125, learning_rate 0.000160843
2017-10-10T14:48:57.227697: step 1075, loss 0.22084, acc 0.9375, learning_rate 0.000160595
2017-10-10T14:48:57.504876: step 1076, loss 0.175241, acc 0.9375, learning_rate 0.000160348
2017-10-10T14:48:57.732355: step 1077, loss 0.0915964, acc 0.96875, learning_rate 0.000160101
2017-10-10T14:48:57.974726: step 1078, loss 0.140444, acc 0.960784, learning_rate 0.000159856
2017-10-10T14:48:58.206905: step 1079, loss 0.165726, acc 0.9375, learning_rate 0.000159612
2017-10-10T14:48:58.512958: step 1080, loss 0.160138, acc 0.96875, learning_rate 0.000159368

Evaluation:
2017-10-10T14:48:58.947960: step 1080, loss 0.243076, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1080

2017-10-10T14:49:00.072420: step 1081, loss 0.147086, acc 0.953125, learning_rate 0.000159126
2017-10-10T14:49:00.296319: step 1082, loss 0.152631, acc 0.921875, learning_rate 0.000158885
2017-10-10T14:49:00.577000: step 1083, loss 0.205166, acc 0.921875, learning_rate 0.000158644
2017-10-10T14:49:00.904953: step 1084, loss 0.132581, acc 0.953125, learning_rate 0.000158405
2017-10-10T14:49:01.164929: step 1085, loss 0.216639, acc 0.921875, learning_rate 0.000158167
2017-10-10T14:49:01.512823: step 1086, loss 0.112636, acc 0.96875, learning_rate 0.000157929
2017-10-10T14:49:01.720442: step 1087, loss 0.149047, acc 0.9375, learning_rate 0.000157693
2017-10-10T14:49:01.991642: step 1088, loss 0.238657, acc 0.90625, learning_rate 0.000157457
2017-10-10T14:49:02.220942: step 1089, loss 0.183319, acc 0.953125, learning_rate 0.000157223
2017-10-10T14:49:02.528847: step 1090, loss 0.230274, acc 0.890625, learning_rate 0.000156989
2017-10-10T14:49:02.803668: step 1091, loss 0.196366, acc 0.921875, learning_rate 0.000156757
2017-10-10T14:49:03.153088: step 1092, loss 0.19167, acc 0.9375, learning_rate 0.000156525
2017-10-10T14:49:03.476389: step 1093, loss 0.137966, acc 0.96875, learning_rate 0.000156294
2017-10-10T14:49:03.717013: step 1094, loss 0.252635, acc 0.90625, learning_rate 0.000156064
2017-10-10T14:49:03.973145: step 1095, loss 0.185646, acc 0.9375, learning_rate 0.000155836
2017-10-10T14:49:04.221440: step 1096, loss 0.349392, acc 0.90625, learning_rate 0.000155608
2017-10-10T14:49:04.470576: step 1097, loss 0.114019, acc 0.953125, learning_rate 0.000155381
2017-10-10T14:49:04.745703: step 1098, loss 0.195991, acc 0.9375, learning_rate 0.000155155
2017-10-10T14:49:04.980897: step 1099, loss 0.127307, acc 0.96875, learning_rate 0.000154929
2017-10-10T14:49:05.229098: step 1100, loss 0.366846, acc 0.90625, learning_rate 0.000154705
2017-10-10T14:49:05.489953: step 1101, loss 0.134381, acc 0.953125, learning_rate 0.000154482
2017-10-10T14:49:05.780918: step 1102, loss 0.224965, acc 0.90625, learning_rate 0.00015426
2017-10-10T14:49:06.084055: step 1103, loss 0.336427, acc 0.890625, learning_rate 0.000154038
2017-10-10T14:49:06.343081: step 1104, loss 0.140692, acc 0.953125, learning_rate 0.000153818
2017-10-10T14:49:06.584825: step 1105, loss 0.203366, acc 0.921875, learning_rate 0.000153598
2017-10-10T14:49:06.891410: step 1106, loss 0.225638, acc 0.9375, learning_rate 0.000153379
2017-10-10T14:49:07.095511: step 1107, loss 0.281796, acc 0.875, learning_rate 0.000153161
2017-10-10T14:49:07.378397: step 1108, loss 0.328485, acc 0.90625, learning_rate 0.000152944
2017-10-10T14:49:07.666445: step 1109, loss 0.0635506, acc 0.984375, learning_rate 0.000152728
2017-10-10T14:49:07.982188: step 1110, loss 0.0933268, acc 0.984375, learning_rate 0.000152513
2017-10-10T14:49:08.273205: step 1111, loss 0.125436, acc 0.96875, learning_rate 0.000152299
2017-10-10T14:49:08.586548: step 1112, loss 0.141395, acc 0.953125, learning_rate 0.000152085
2017-10-10T14:49:08.912685: step 1113, loss 0.187123, acc 0.953125, learning_rate 0.000151872
2017-10-10T14:49:09.232498: step 1114, loss 0.296455, acc 0.890625, learning_rate 0.000151661
2017-10-10T14:49:09.549893: step 1115, loss 0.153985, acc 0.9375, learning_rate 0.00015145
2017-10-10T14:49:09.811961: step 1116, loss 0.203875, acc 0.90625, learning_rate 0.00015124
2017-10-10T14:49:10.060992: step 1117, loss 0.2328, acc 0.890625, learning_rate 0.000151031
2017-10-10T14:49:10.288323: step 1118, loss 0.186105, acc 0.921875, learning_rate 0.000150822
2017-10-10T14:49:10.544962: step 1119, loss 0.295992, acc 0.890625, learning_rate 0.000150615
2017-10-10T14:49:10.861585: step 1120, loss 0.201299, acc 0.921875, learning_rate 0.000150408

Evaluation:
2017-10-10T14:49:11.308933: step 1120, loss 0.243029, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1120

2017-10-10T14:49:12.322555: step 1121, loss 0.155172, acc 0.96875, learning_rate 0.000150203
2017-10-10T14:49:12.595669: step 1122, loss 0.0691328, acc 1, learning_rate 0.000149998
2017-10-10T14:49:12.828633: step 1123, loss 0.0964585, acc 0.96875, learning_rate 0.000149794
2017-10-10T14:49:13.100836: step 1124, loss 0.151714, acc 0.953125, learning_rate 0.00014959
2017-10-10T14:49:13.369406: step 1125, loss 0.18894, acc 0.953125, learning_rate 0.000149388
2017-10-10T14:49:13.660453: step 1126, loss 0.168691, acc 0.9375, learning_rate 0.000149186
2017-10-10T14:49:13.905615: step 1127, loss 0.14539, acc 0.9375, learning_rate 0.000148986
2017-10-10T14:49:14.164228: step 1128, loss 0.242346, acc 0.90625, learning_rate 0.000148786
2017-10-10T14:49:14.492292: step 1129, loss 0.174662, acc 0.921875, learning_rate 0.000148587
2017-10-10T14:49:14.729718: step 1130, loss 0.214876, acc 0.9375, learning_rate 0.000148388
2017-10-10T14:49:14.952941: step 1131, loss 0.282527, acc 0.90625, learning_rate 0.000148191
2017-10-10T14:49:15.283620: step 1132, loss 0.196458, acc 0.953125, learning_rate 0.000147994
2017-10-10T14:49:15.643489: step 1133, loss 0.146328, acc 0.96875, learning_rate 0.000147798
2017-10-10T14:49:15.889063: step 1134, loss 0.265307, acc 0.90625, learning_rate 0.000147603
2017-10-10T14:49:16.112839: step 1135, loss 0.140118, acc 0.96875, learning_rate 0.000147409
2017-10-10T14:49:16.440862: step 1136, loss 0.101798, acc 1, learning_rate 0.000147215
2017-10-10T14:49:16.748822: step 1137, loss 0.125011, acc 0.9375, learning_rate 0.000147022
2017-10-10T14:49:17.010613: step 1138, loss 0.104542, acc 0.96875, learning_rate 0.000146831
2017-10-10T14:49:17.277611: step 1139, loss 0.112979, acc 0.953125, learning_rate 0.000146639
2017-10-10T14:49:17.550758: step 1140, loss 0.162844, acc 0.96875, learning_rate 0.000146449
2017-10-10T14:49:17.796096: step 1141, loss 0.162627, acc 0.9375, learning_rate 0.000146259
2017-10-10T14:49:18.051259: step 1142, loss 0.0982083, acc 1, learning_rate 0.000146071
2017-10-10T14:49:18.339663: step 1143, loss 0.131275, acc 0.9375, learning_rate 0.000145883
2017-10-10T14:49:18.637018: step 1144, loss 0.18521, acc 0.9375, learning_rate 0.000145695
2017-10-10T14:49:18.881236: step 1145, loss 0.395472, acc 0.875, learning_rate 0.000145509
2017-10-10T14:49:19.221172: step 1146, loss 0.232967, acc 0.9375, learning_rate 0.000145323
2017-10-10T14:49:19.564950: step 1147, loss 0.197031, acc 0.9375, learning_rate 0.000145138
2017-10-10T14:49:19.938548: step 1148, loss 0.137884, acc 0.921875, learning_rate 0.000144954
2017-10-10T14:49:20.133794: step 1149, loss 0.204888, acc 0.9375, learning_rate 0.00014477
2017-10-10T14:49:20.305877: step 1150, loss 0.117292, acc 0.96875, learning_rate 0.000144588
2017-10-10T14:49:20.498774: step 1151, loss 0.124161, acc 0.96875, learning_rate 0.000144406
2017-10-10T14:49:20.704084: step 1152, loss 0.211447, acc 0.90625, learning_rate 0.000144224
2017-10-10T14:49:20.932878: step 1153, loss 0.123519, acc 0.953125, learning_rate 0.000144044
2017-10-10T14:49:21.160175: step 1154, loss 0.103224, acc 0.96875, learning_rate 0.000143864
2017-10-10T14:49:21.451858: step 1155, loss 0.118125, acc 0.96875, learning_rate 0.000143685
2017-10-10T14:49:21.685023: step 1156, loss 0.174795, acc 0.953125, learning_rate 0.000143507
2017-10-10T14:49:21.972873: step 1157, loss 0.198104, acc 0.9375, learning_rate 0.000143329
2017-10-10T14:49:22.259794: step 1158, loss 0.118895, acc 0.96875, learning_rate 0.000143152
2017-10-10T14:49:22.539613: step 1159, loss 0.0961489, acc 0.984375, learning_rate 0.000142976
2017-10-10T14:49:22.842392: step 1160, loss 0.0949106, acc 0.96875, learning_rate 0.000142801

Evaluation:
2017-10-10T14:49:23.334894: step 1160, loss 0.240885, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1160

2017-10-10T14:49:24.448393: step 1161, loss 0.21407, acc 0.90625, learning_rate 0.000142626
2017-10-10T14:49:24.715557: step 1162, loss 0.0642702, acc 1, learning_rate 0.000142452
2017-10-10T14:49:24.945176: step 1163, loss 0.158836, acc 0.9375, learning_rate 0.000142279
2017-10-10T14:49:25.248085: step 1164, loss 0.0899882, acc 0.96875, learning_rate 0.000142106
2017-10-10T14:49:25.483209: step 1165, loss 0.227298, acc 0.921875, learning_rate 0.000141934
2017-10-10T14:49:25.753436: step 1166, loss 0.134463, acc 0.9375, learning_rate 0.000141763
2017-10-10T14:49:26.081781: step 1167, loss 0.187018, acc 0.953125, learning_rate 0.000141593
2017-10-10T14:49:26.404974: step 1168, loss 0.189549, acc 0.921875, learning_rate 0.000141423
2017-10-10T14:49:26.581039: step 1169, loss 0.185458, acc 0.9375, learning_rate 0.000141254
2017-10-10T14:49:26.870201: step 1170, loss 0.207221, acc 0.953125, learning_rate 0.000141085
2017-10-10T14:49:27.192432: step 1171, loss 0.127789, acc 0.984375, learning_rate 0.000140918
2017-10-10T14:49:27.501094: step 1172, loss 0.187672, acc 0.953125, learning_rate 0.000140751
2017-10-10T14:49:27.688234: step 1173, loss 0.176488, acc 0.9375, learning_rate 0.000140584
2017-10-10T14:49:27.916791: step 1174, loss 0.216754, acc 0.90625, learning_rate 0.000140419
2017-10-10T14:49:28.143184: step 1175, loss 0.221511, acc 0.9375, learning_rate 0.000140254
2017-10-10T14:49:28.317242: step 1176, loss 0.137098, acc 0.960784, learning_rate 0.000140089
2017-10-10T14:49:28.564846: step 1177, loss 0.137268, acc 0.9375, learning_rate 0.000139926
2017-10-10T14:49:28.814878: step 1178, loss 0.213764, acc 0.953125, learning_rate 0.000139763
2017-10-10T14:49:29.101586: step 1179, loss 0.144115, acc 0.953125, learning_rate 0.0001396
2017-10-10T14:49:29.395846: step 1180, loss 0.243133, acc 0.90625, learning_rate 0.000139439
2017-10-10T14:49:29.656961: step 1181, loss 0.0829484, acc 0.96875, learning_rate 0.000139278
2017-10-10T14:49:29.890551: step 1182, loss 0.244621, acc 0.921875, learning_rate 0.000139118
2017-10-10T14:49:30.209033: step 1183, loss 0.0838775, acc 0.984375, learning_rate 0.000138958
2017-10-10T14:49:30.450418: step 1184, loss 0.0553811, acc 1, learning_rate 0.000138799
2017-10-10T14:49:30.720596: step 1185, loss 0.214374, acc 0.90625, learning_rate 0.00013864
2017-10-10T14:49:31.064954: step 1186, loss 0.177237, acc 0.953125, learning_rate 0.000138483
2017-10-10T14:49:31.389812: step 1187, loss 0.267233, acc 0.890625, learning_rate 0.000138326
2017-10-10T14:49:31.648721: step 1188, loss 0.28237, acc 0.859375, learning_rate 0.000138169
2017-10-10T14:49:31.945632: step 1189, loss 0.24096, acc 0.921875, learning_rate 0.000138013
2017-10-10T14:49:32.265095: step 1190, loss 0.130679, acc 0.96875, learning_rate 0.000137858
2017-10-10T14:49:32.517119: step 1191, loss 0.152938, acc 0.953125, learning_rate 0.000137704
2017-10-10T14:49:32.907925: step 1192, loss 0.356509, acc 0.890625, learning_rate 0.00013755
2017-10-10T14:49:33.188581: step 1193, loss 0.211886, acc 0.921875, learning_rate 0.000137397
2017-10-10T14:49:33.440846: step 1194, loss 0.300955, acc 0.90625, learning_rate 0.000137244
2017-10-10T14:49:33.665644: step 1195, loss 0.121961, acc 0.9375, learning_rate 0.000137092
2017-10-10T14:49:33.898006: step 1196, loss 0.198818, acc 0.9375, learning_rate 0.000136941
2017-10-10T14:49:34.243387: step 1197, loss 0.176103, acc 0.921875, learning_rate 0.00013679
2017-10-10T14:49:34.501077: step 1198, loss 0.157137, acc 0.953125, learning_rate 0.00013664
2017-10-10T14:49:34.901971: step 1199, loss 0.160505, acc 0.984375, learning_rate 0.00013649
2017-10-10T14:49:35.136602: step 1200, loss 0.272615, acc 0.90625, learning_rate 0.000136341

Evaluation:
2017-10-10T14:49:35.641457: step 1200, loss 0.240252, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1200

2017-10-10T14:49:36.808927: step 1201, loss 0.187583, acc 0.9375, learning_rate 0.000136193
2017-10-10T14:49:37.037177: step 1202, loss 0.0900292, acc 0.984375, learning_rate 0.000136045
2017-10-10T14:49:37.248652: step 1203, loss 0.129139, acc 0.921875, learning_rate 0.000135898
2017-10-10T14:49:37.464872: step 1204, loss 0.182039, acc 0.9375, learning_rate 0.000135751
2017-10-10T14:49:37.710463: step 1205, loss 0.267743, acc 0.90625, learning_rate 0.000135605
2017-10-10T14:49:37.984983: step 1206, loss 0.157197, acc 0.96875, learning_rate 0.00013546
2017-10-10T14:49:38.208033: step 1207, loss 0.185926, acc 0.953125, learning_rate 0.000135315
2017-10-10T14:49:38.408833: step 1208, loss 0.141348, acc 0.9375, learning_rate 0.000135171
2017-10-10T14:49:38.698477: step 1209, loss 0.174521, acc 0.953125, learning_rate 0.000135028
2017-10-10T14:49:38.945169: step 1210, loss 0.14133, acc 0.953125, learning_rate 0.000134885
2017-10-10T14:49:39.257107: step 1211, loss 0.215766, acc 0.9375, learning_rate 0.000134742
2017-10-10T14:49:39.546169: step 1212, loss 0.121272, acc 0.953125, learning_rate 0.0001346
2017-10-10T14:49:39.836862: step 1213, loss 0.101571, acc 0.96875, learning_rate 0.000134459
2017-10-10T14:49:40.077311: step 1214, loss 0.194462, acc 0.953125, learning_rate 0.000134319
2017-10-10T14:49:40.359749: step 1215, loss 0.16802, acc 0.921875, learning_rate 0.000134178
2017-10-10T14:49:40.611561: step 1216, loss 0.136595, acc 0.953125, learning_rate 0.000134039
2017-10-10T14:49:40.833889: step 1217, loss 0.229781, acc 0.890625, learning_rate 0.0001339
2017-10-10T14:49:41.157634: step 1218, loss 0.194781, acc 0.953125, learning_rate 0.000133762
2017-10-10T14:49:41.423024: step 1219, loss 0.141461, acc 0.96875, learning_rate 0.000133624
2017-10-10T14:49:41.646083: step 1220, loss 0.133807, acc 0.953125, learning_rate 0.000133487
2017-10-10T14:49:41.965896: step 1221, loss 0.118924, acc 0.9375, learning_rate 0.00013335
2017-10-10T14:49:42.245106: step 1222, loss 0.114642, acc 0.953125, learning_rate 0.000133214
2017-10-10T14:49:42.503280: step 1223, loss 0.166537, acc 0.921875, learning_rate 0.000133078
2017-10-10T14:49:42.715265: step 1224, loss 0.191598, acc 0.921875, learning_rate 0.000132943
2017-10-10T14:49:43.009749: step 1225, loss 0.153044, acc 0.9375, learning_rate 0.000132809
2017-10-10T14:49:43.308866: step 1226, loss 0.0829853, acc 1, learning_rate 0.000132675
2017-10-10T14:49:43.590918: step 1227, loss 0.102139, acc 0.984375, learning_rate 0.000132541
2017-10-10T14:49:43.851267: step 1228, loss 0.100077, acc 0.96875, learning_rate 0.000132409
2017-10-10T14:49:44.135256: step 1229, loss 0.0892733, acc 0.984375, learning_rate 0.000132276
2017-10-10T14:49:44.460826: step 1230, loss 0.220104, acc 0.921875, learning_rate 0.000132145
2017-10-10T14:49:44.735309: step 1231, loss 0.131872, acc 0.96875, learning_rate 0.000132013
2017-10-10T14:49:45.044837: step 1232, loss 0.125505, acc 0.953125, learning_rate 0.000131883
2017-10-10T14:49:45.275830: step 1233, loss 0.239825, acc 0.921875, learning_rate 0.000131753
2017-10-10T14:49:45.605006: step 1234, loss 0.109739, acc 0.96875, learning_rate 0.000131623
2017-10-10T14:49:45.928932: step 1235, loss 0.156733, acc 0.921875, learning_rate 0.000131494
2017-10-10T14:49:46.267573: step 1236, loss 0.17742, acc 0.921875, learning_rate 0.000131365
2017-10-10T14:49:46.511016: step 1237, loss 0.131248, acc 0.96875, learning_rate 0.000131237
2017-10-10T14:49:46.664962: step 1238, loss 0.141366, acc 0.96875, learning_rate 0.00013111
2017-10-10T14:49:46.874025: step 1239, loss 0.208537, acc 0.9375, learning_rate 0.000130983
2017-10-10T14:49:47.088830: step 1240, loss 0.123249, acc 0.96875, learning_rate 0.000130856

Evaluation:
2017-10-10T14:49:47.507673: step 1240, loss 0.23831, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1240

2017-10-10T14:49:48.502015: step 1241, loss 0.149384, acc 0.96875, learning_rate 0.00013073
2017-10-10T14:49:48.835276: step 1242, loss 0.268842, acc 0.921875, learning_rate 0.000130605
2017-10-10T14:49:49.120075: step 1243, loss 0.121311, acc 0.96875, learning_rate 0.00013048
2017-10-10T14:49:49.365405: step 1244, loss 0.134213, acc 0.9375, learning_rate 0.000130356
2017-10-10T14:49:49.632929: step 1245, loss 0.193194, acc 0.921875, learning_rate 0.000130232
2017-10-10T14:49:49.917039: step 1246, loss 0.126619, acc 0.984375, learning_rate 0.000130108
2017-10-10T14:49:50.156330: step 1247, loss 0.0689545, acc 1, learning_rate 0.000129985
2017-10-10T14:49:50.493092: step 1248, loss 0.186352, acc 0.96875, learning_rate 0.000129863
2017-10-10T14:49:50.799793: step 1249, loss 0.10376, acc 0.96875, learning_rate 0.000129741
2017-10-10T14:49:51.144920: step 1250, loss 0.121942, acc 0.96875, learning_rate 0.00012962
2017-10-10T14:49:51.402253: step 1251, loss 0.189427, acc 0.921875, learning_rate 0.000129499
2017-10-10T14:49:51.710650: step 1252, loss 0.184292, acc 0.9375, learning_rate 0.000129378
2017-10-10T14:49:52.021257: step 1253, loss 0.122317, acc 0.96875, learning_rate 0.000129259
2017-10-10T14:49:52.223497: step 1254, loss 0.119449, acc 0.953125, learning_rate 0.000129139
2017-10-10T14:49:52.565994: step 1255, loss 0.0849064, acc 0.96875, learning_rate 0.00012902
2017-10-10T14:49:52.860875: step 1256, loss 0.17625, acc 0.9375, learning_rate 0.000128902
2017-10-10T14:49:53.168963: step 1257, loss 0.131964, acc 0.96875, learning_rate 0.000128784
2017-10-10T14:49:53.437477: step 1258, loss 0.0942199, acc 0.96875, learning_rate 0.000128666
2017-10-10T14:49:53.793961: step 1259, loss 0.206083, acc 0.953125, learning_rate 0.000128549
2017-10-10T14:49:54.063047: step 1260, loss 0.176753, acc 0.921875, learning_rate 0.000128433
2017-10-10T14:49:54.320918: step 1261, loss 0.109145, acc 0.984375, learning_rate 0.000128317
2017-10-10T14:49:54.564890: step 1262, loss 0.125147, acc 0.953125, learning_rate 0.000128201
2017-10-10T14:49:54.818175: step 1263, loss 0.158883, acc 0.953125, learning_rate 0.000128086
2017-10-10T14:49:55.135168: step 1264, loss 0.204187, acc 0.953125, learning_rate 0.000127971
2017-10-10T14:49:55.402789: step 1265, loss 0.106904, acc 0.96875, learning_rate 0.000127857
2017-10-10T14:49:55.657576: step 1266, loss 0.287953, acc 0.921875, learning_rate 0.000127743
2017-10-10T14:49:55.963192: step 1267, loss 0.133173, acc 0.953125, learning_rate 0.00012763
2017-10-10T14:49:56.303956: step 1268, loss 0.21033, acc 0.9375, learning_rate 0.000127517
2017-10-10T14:49:56.567156: step 1269, loss 0.115705, acc 0.96875, learning_rate 0.000127405
2017-10-10T14:49:56.856952: step 1270, loss 0.154707, acc 0.96875, learning_rate 0.000127293
2017-10-10T14:49:57.171016: step 1271, loss 0.213174, acc 0.9375, learning_rate 0.000127182
2017-10-10T14:49:57.449585: step 1272, loss 0.128221, acc 0.953125, learning_rate 0.000127071
2017-10-10T14:49:57.748952: step 1273, loss 0.142962, acc 0.9375, learning_rate 0.00012696
2017-10-10T14:49:58.006293: step 1274, loss 0.304156, acc 0.921569, learning_rate 0.00012685
2017-10-10T14:49:58.316904: step 1275, loss 0.138544, acc 0.96875, learning_rate 0.000126741
2017-10-10T14:49:58.548806: step 1276, loss 0.0965798, acc 0.96875, learning_rate 0.000126632
2017-10-10T14:49:58.820606: step 1277, loss 0.187129, acc 0.96875, learning_rate 0.000126523
2017-10-10T14:49:59.120901: step 1278, loss 0.118946, acc 0.953125, learning_rate 0.000126415
2017-10-10T14:49:59.337651: step 1279, loss 0.245109, acc 0.921875, learning_rate 0.000126307
2017-10-10T14:49:59.767239: step 1280, loss 0.045951, acc 1, learning_rate 0.000126199

Evaluation:
2017-10-10T14:50:00.247387: step 1280, loss 0.240228, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1280

2017-10-10T14:50:01.308914: step 1281, loss 0.111976, acc 0.96875, learning_rate 0.000126093
2017-10-10T14:50:01.607636: step 1282, loss 0.1577, acc 0.96875, learning_rate 0.000125986
2017-10-10T14:50:01.913101: step 1283, loss 0.159931, acc 0.953125, learning_rate 0.00012588
2017-10-10T14:50:02.161352: step 1284, loss 0.201831, acc 0.9375, learning_rate 0.000125774
2017-10-10T14:50:02.458727: step 1285, loss 0.100722, acc 0.96875, learning_rate 0.000125669
2017-10-10T14:50:02.750732: step 1286, loss 0.183555, acc 0.9375, learning_rate 0.000125564
2017-10-10T14:50:03.082480: step 1287, loss 0.104426, acc 0.96875, learning_rate 0.00012546
2017-10-10T14:50:03.286052: step 1288, loss 0.295, acc 0.890625, learning_rate 0.000125356
2017-10-10T14:50:03.524896: step 1289, loss 0.233632, acc 0.9375, learning_rate 0.000125253
2017-10-10T14:50:03.813778: step 1290, loss 0.118567, acc 0.96875, learning_rate 0.00012515
2017-10-10T14:50:04.165932: step 1291, loss 0.323583, acc 0.875, learning_rate 0.000125047
2017-10-10T14:50:04.414367: step 1292, loss 0.31171, acc 0.890625, learning_rate 0.000124945
2017-10-10T14:50:04.660855: step 1293, loss 0.206643, acc 0.921875, learning_rate 0.000124843
2017-10-10T14:50:04.916936: step 1294, loss 0.172269, acc 0.953125, learning_rate 0.000124741
2017-10-10T14:50:05.139701: step 1295, loss 0.080349, acc 1, learning_rate 0.00012464
2017-10-10T14:50:05.434509: step 1296, loss 0.21552, acc 0.921875, learning_rate 0.00012454
2017-10-10T14:50:05.665609: step 1297, loss 0.230525, acc 0.9375, learning_rate 0.00012444
2017-10-10T14:50:05.965334: step 1298, loss 0.208762, acc 0.96875, learning_rate 0.00012434
2017-10-10T14:50:06.220285: step 1299, loss 0.101167, acc 0.96875, learning_rate 0.000124241
2017-10-10T14:50:06.538094: step 1300, loss 0.0642709, acc 0.984375, learning_rate 0.000124142
2017-10-10T14:50:06.848816: step 1301, loss 0.107359, acc 0.984375, learning_rate 0.000124043
2017-10-10T14:50:07.144924: step 1302, loss 0.153062, acc 0.953125, learning_rate 0.000123945
2017-10-10T14:50:07.416316: step 1303, loss 0.182955, acc 0.9375, learning_rate 0.000123847
2017-10-10T14:50:07.709031: step 1304, loss 0.149952, acc 0.953125, learning_rate 0.00012375
2017-10-10T14:50:08.023894: step 1305, loss 0.231251, acc 0.96875, learning_rate 0.000123653
2017-10-10T14:50:08.244929: step 1306, loss 0.096944, acc 0.96875, learning_rate 0.000123556
2017-10-10T14:50:08.557691: step 1307, loss 0.14787, acc 0.953125, learning_rate 0.00012346
2017-10-10T14:50:08.838880: step 1308, loss 0.112113, acc 0.96875, learning_rate 0.000123364
2017-10-10T14:50:09.096997: step 1309, loss 0.100465, acc 0.96875, learning_rate 0.000123269
2017-10-10T14:50:09.367247: step 1310, loss 0.132172, acc 0.921875, learning_rate 0.000123174
2017-10-10T14:50:09.684981: step 1311, loss 0.176287, acc 0.9375, learning_rate 0.00012308
2017-10-10T14:50:09.981061: step 1312, loss 0.172461, acc 0.9375, learning_rate 0.000122985
2017-10-10T14:50:10.216968: step 1313, loss 0.180744, acc 0.90625, learning_rate 0.000122892
2017-10-10T14:50:10.469132: step 1314, loss 0.169419, acc 0.984375, learning_rate 0.000122798
2017-10-10T14:50:10.669903: step 1315, loss 0.168539, acc 0.9375, learning_rate 0.000122705
2017-10-10T14:50:10.915621: step 1316, loss 0.135594, acc 0.96875, learning_rate 0.000122612
2017-10-10T14:50:11.121512: step 1317, loss 0.126008, acc 0.953125, learning_rate 0.00012252
2017-10-10T14:50:11.391012: step 1318, loss 0.181935, acc 0.96875, learning_rate 0.000122428
2017-10-10T14:50:11.676353: step 1319, loss 0.325629, acc 0.84375, learning_rate 0.000122337
2017-10-10T14:50:11.905501: step 1320, loss 0.165355, acc 0.9375, learning_rate 0.000122245

Evaluation:
2017-10-10T14:50:12.406436: step 1320, loss 0.237874, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1320

2017-10-10T14:50:13.796855: step 1321, loss 0.235008, acc 0.90625, learning_rate 0.000122155
2017-10-10T14:50:14.061031: step 1322, loss 0.296677, acc 0.9375, learning_rate 0.000122064
2017-10-10T14:50:14.356463: step 1323, loss 0.146956, acc 0.953125, learning_rate 0.000121974
2017-10-10T14:50:14.637007: step 1324, loss 0.208328, acc 0.9375, learning_rate 0.000121884
2017-10-10T14:50:14.880850: step 1325, loss 0.167846, acc 0.953125, learning_rate 0.000121795
2017-10-10T14:50:15.156943: step 1326, loss 0.187474, acc 0.96875, learning_rate 0.000121706
2017-10-10T14:50:15.467793: step 1327, loss 0.123987, acc 0.96875, learning_rate 0.000121618
2017-10-10T14:50:15.719530: step 1328, loss 0.121769, acc 0.953125, learning_rate 0.000121529
2017-10-10T14:50:16.052937: step 1329, loss 0.195854, acc 0.921875, learning_rate 0.000121441
2017-10-10T14:50:16.361823: step 1330, loss 0.119276, acc 0.953125, learning_rate 0.000121354
2017-10-10T14:50:16.643748: step 1331, loss 0.151651, acc 0.9375, learning_rate 0.000121267
2017-10-10T14:50:16.942286: step 1332, loss 0.187712, acc 0.984375, learning_rate 0.00012118
2017-10-10T14:50:17.175046: step 1333, loss 0.148114, acc 0.96875, learning_rate 0.000121093
2017-10-10T14:50:17.440869: step 1334, loss 0.166944, acc 0.953125, learning_rate 0.000121007
2017-10-10T14:50:17.716840: step 1335, loss 0.159215, acc 0.9375, learning_rate 0.000120922
2017-10-10T14:50:17.986094: step 1336, loss 0.107822, acc 0.984375, learning_rate 0.000120836
2017-10-10T14:50:18.220899: step 1337, loss 0.137418, acc 0.9375, learning_rate 0.000120751
2017-10-10T14:50:18.504271: step 1338, loss 0.169072, acc 0.953125, learning_rate 0.000120666
2017-10-10T14:50:18.684977: step 1339, loss 0.173352, acc 0.9375, learning_rate 0.000120582
2017-10-10T14:50:18.983265: step 1340, loss 0.231929, acc 0.9375, learning_rate 0.000120498
2017-10-10T14:50:19.234009: step 1341, loss 0.0773867, acc 0.984375, learning_rate 0.000120414
2017-10-10T14:50:19.515234: step 1342, loss 0.0933061, acc 0.96875, learning_rate 0.000120331
2017-10-10T14:50:19.800815: step 1343, loss 0.119232, acc 0.984375, learning_rate 0.000120248
2017-10-10T14:50:20.077690: step 1344, loss 0.114868, acc 0.96875, learning_rate 0.000120165
2017-10-10T14:50:20.387025: step 1345, loss 0.105572, acc 0.953125, learning_rate 0.000120083
2017-10-10T14:50:20.700860: step 1346, loss 0.167951, acc 0.921875, learning_rate 0.000120001
2017-10-10T14:50:20.975332: step 1347, loss 0.140738, acc 0.9375, learning_rate 0.00011992
2017-10-10T14:50:21.243470: step 1348, loss 0.221758, acc 0.953125, learning_rate 0.000119838
2017-10-10T14:50:21.524889: step 1349, loss 0.237992, acc 0.90625, learning_rate 0.000119757
2017-10-10T14:50:21.862300: step 1350, loss 0.144442, acc 0.96875, learning_rate 0.000119677
2017-10-10T14:50:22.160919: step 1351, loss 0.247899, acc 0.90625, learning_rate 0.000119596
2017-10-10T14:50:22.465119: step 1352, loss 0.173775, acc 0.9375, learning_rate 0.000119516
2017-10-10T14:50:22.693051: step 1353, loss 0.172244, acc 0.953125, learning_rate 0.000119437
2017-10-10T14:50:22.897642: step 1354, loss 0.229067, acc 0.9375, learning_rate 0.000119357
2017-10-10T14:50:23.137321: step 1355, loss 0.13893, acc 0.9375, learning_rate 0.000119278
2017-10-10T14:50:23.412856: step 1356, loss 0.10468, acc 0.96875, learning_rate 0.0001192
2017-10-10T14:50:23.708707: step 1357, loss 0.123147, acc 0.953125, learning_rate 0.000119121
2017-10-10T14:50:23.957044: step 1358, loss 0.114281, acc 0.96875, learning_rate 0.000119043
2017-10-10T14:50:24.283828: step 1359, loss 0.140151, acc 0.984375, learning_rate 0.000118965
2017-10-10T14:50:24.547174: step 1360, loss 0.103125, acc 1, learning_rate 0.000118888

Evaluation:
2017-10-10T14:50:25.016564: step 1360, loss 0.239091, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1360

2017-10-10T14:50:26.209223: step 1361, loss 0.215248, acc 0.921875, learning_rate 0.000118811
2017-10-10T14:50:26.480911: step 1362, loss 0.110769, acc 0.984375, learning_rate 0.000118734
2017-10-10T14:50:26.823800: step 1363, loss 0.174199, acc 0.9375, learning_rate 0.000118658
2017-10-10T14:50:27.011470: step 1364, loss 0.135431, acc 0.96875, learning_rate 0.000118582
2017-10-10T14:50:27.195158: step 1365, loss 0.216258, acc 0.9375, learning_rate 0.000118506
2017-10-10T14:50:27.472800: step 1366, loss 0.125696, acc 0.953125, learning_rate 0.00011843
2017-10-10T14:50:27.668739: step 1367, loss 0.146195, acc 0.953125, learning_rate 0.000118355
2017-10-10T14:50:27.920308: step 1368, loss 0.0986493, acc 0.984375, learning_rate 0.00011828
2017-10-10T14:50:28.134913: step 1369, loss 0.382758, acc 0.828125, learning_rate 0.000118205
2017-10-10T14:50:28.405707: step 1370, loss 0.143455, acc 0.96875, learning_rate 0.000118131
2017-10-10T14:50:28.709937: step 1371, loss 0.0969941, acc 0.984375, learning_rate 0.000118057
2017-10-10T14:50:28.954295: step 1372, loss 0.0952191, acc 0.960784, learning_rate 0.000117983
2017-10-10T14:50:29.229681: step 1373, loss 0.185086, acc 0.953125, learning_rate 0.00011791
2017-10-10T14:50:29.531513: step 1374, loss 0.142832, acc 0.921875, learning_rate 0.000117837
2017-10-10T14:50:29.762454: step 1375, loss 0.0906729, acc 0.984375, learning_rate 0.000117764
2017-10-10T14:50:30.047580: step 1376, loss 0.159154, acc 0.921875, learning_rate 0.000117692
2017-10-10T14:50:30.313053: step 1377, loss 0.0907019, acc 0.984375, learning_rate 0.000117619
2017-10-10T14:50:30.608797: step 1378, loss 0.149137, acc 0.96875, learning_rate 0.000117547
2017-10-10T14:50:30.896882: step 1379, loss 0.0896159, acc 0.984375, learning_rate 0.000117476
2017-10-10T14:50:31.176828: step 1380, loss 0.0936745, acc 0.953125, learning_rate 0.000117404
2017-10-10T14:50:31.463719: step 1381, loss 0.140588, acc 0.953125, learning_rate 0.000117333
2017-10-10T14:50:31.765961: step 1382, loss 0.138101, acc 0.9375, learning_rate 0.000117263
2017-10-10T14:50:32.028039: step 1383, loss 0.139963, acc 0.953125, learning_rate 0.000117192
2017-10-10T14:50:32.320831: step 1384, loss 0.116248, acc 0.953125, learning_rate 0.000117122
2017-10-10T14:50:32.653032: step 1385, loss 0.143377, acc 0.984375, learning_rate 0.000117052
2017-10-10T14:50:32.885715: step 1386, loss 0.135876, acc 0.953125, learning_rate 0.000116983
2017-10-10T14:50:33.137303: step 1387, loss 0.196384, acc 0.9375, learning_rate 0.000116913
2017-10-10T14:50:33.428839: step 1388, loss 0.101415, acc 0.984375, learning_rate 0.000116844
2017-10-10T14:50:33.641019: step 1389, loss 0.0702753, acc 1, learning_rate 0.000116775
2017-10-10T14:50:33.960927: step 1390, loss 0.231867, acc 0.90625, learning_rate 0.000116707
2017-10-10T14:50:34.279268: step 1391, loss 0.176732, acc 0.921875, learning_rate 0.000116639
2017-10-10T14:50:34.542649: step 1392, loss 0.226427, acc 0.90625, learning_rate 0.000116571
2017-10-10T14:50:34.824129: step 1393, loss 0.128636, acc 0.96875, learning_rate 0.000116503
2017-10-10T14:50:35.091109: step 1394, loss 0.165968, acc 0.96875, learning_rate 0.000116436
2017-10-10T14:50:35.443905: step 1395, loss 0.142862, acc 0.96875, learning_rate 0.000116369
2017-10-10T14:50:35.663266: step 1396, loss 0.202837, acc 0.921875, learning_rate 0.000116302
2017-10-10T14:50:35.919970: step 1397, loss 0.143243, acc 0.9375, learning_rate 0.000116235
2017-10-10T14:50:36.125528: step 1398, loss 0.268332, acc 0.90625, learning_rate 0.000116169
2017-10-10T14:50:36.348000: step 1399, loss 0.170335, acc 0.9375, learning_rate 0.000116103
2017-10-10T14:50:36.663346: step 1400, loss 0.155751, acc 0.921875, learning_rate 0.000116037

Evaluation:
2017-10-10T14:50:37.170481: step 1400, loss 0.238976, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1400

2017-10-10T14:50:38.105264: step 1401, loss 0.245186, acc 0.921875, learning_rate 0.000115972
2017-10-10T14:50:38.404051: step 1402, loss 0.122085, acc 0.96875, learning_rate 0.000115907
2017-10-10T14:50:38.718989: step 1403, loss 0.120358, acc 0.96875, learning_rate 0.000115842
2017-10-10T14:50:38.959447: step 1404, loss 0.102508, acc 0.984375, learning_rate 0.000115777
2017-10-10T14:50:39.193243: step 1405, loss 0.181295, acc 0.953125, learning_rate 0.000115713
2017-10-10T14:50:39.561075: step 1406, loss 0.147011, acc 0.953125, learning_rate 0.000115649
2017-10-10T14:50:39.812948: step 1407, loss 0.150327, acc 0.953125, learning_rate 0.000115585
2017-10-10T14:50:40.068158: step 1408, loss 0.208491, acc 0.90625, learning_rate 0.000115521
2017-10-10T14:50:40.388898: step 1409, loss 0.142151, acc 0.9375, learning_rate 0.000115458
2017-10-10T14:50:40.703477: step 1410, loss 0.110383, acc 0.953125, learning_rate 0.000115395
2017-10-10T14:50:41.024866: step 1411, loss 0.175399, acc 0.9375, learning_rate 0.000115332
2017-10-10T14:50:41.280294: step 1412, loss 0.17463, acc 0.9375, learning_rate 0.000115269
2017-10-10T14:50:41.560341: step 1413, loss 0.097248, acc 0.984375, learning_rate 0.000115207
2017-10-10T14:50:41.774744: step 1414, loss 0.122214, acc 0.953125, learning_rate 0.000115145
2017-10-10T14:50:42.035110: step 1415, loss 0.129686, acc 0.953125, learning_rate 0.000115083
2017-10-10T14:50:42.332902: step 1416, loss 0.13685, acc 0.9375, learning_rate 0.000115022
2017-10-10T14:50:42.638902: step 1417, loss 0.218007, acc 0.921875, learning_rate 0.00011496
2017-10-10T14:50:42.904889: step 1418, loss 0.0766243, acc 1, learning_rate 0.000114899
2017-10-10T14:50:43.229242: step 1419, loss 0.257025, acc 0.890625, learning_rate 0.000114838
2017-10-10T14:50:43.549030: step 1420, loss 0.104511, acc 0.96875, learning_rate 0.000114778
2017-10-10T14:50:43.827560: step 1421, loss 0.19727, acc 0.953125, learning_rate 0.000114717
2017-10-10T14:50:44.044837: step 1422, loss 0.232301, acc 0.921875, learning_rate 0.000114657
2017-10-10T14:50:44.293599: step 1423, loss 0.0588001, acc 0.984375, learning_rate 0.000114598
2017-10-10T14:50:44.468303: step 1424, loss 0.115949, acc 0.96875, learning_rate 0.000114538
2017-10-10T14:50:44.757420: step 1425, loss 0.183085, acc 0.9375, learning_rate 0.000114479
2017-10-10T14:50:45.023281: step 1426, loss 0.0749147, acc 1, learning_rate 0.00011442
2017-10-10T14:50:45.232975: step 1427, loss 0.132732, acc 0.9375, learning_rate 0.000114361
2017-10-10T14:50:45.541511: step 1428, loss 0.0958215, acc 0.9375, learning_rate 0.000114302
2017-10-10T14:50:45.760878: step 1429, loss 0.21204, acc 0.90625, learning_rate 0.000114244
2017-10-10T14:50:46.018978: step 1430, loss 0.147945, acc 0.96875, learning_rate 0.000114186
2017-10-10T14:50:46.324948: step 1431, loss 0.283991, acc 0.921875, learning_rate 0.000114128
2017-10-10T14:50:46.624814: step 1432, loss 0.103526, acc 0.96875, learning_rate 0.00011407
2017-10-10T14:50:46.944921: step 1433, loss 0.204419, acc 0.953125, learning_rate 0.000114013
2017-10-10T14:50:47.259353: step 1434, loss 0.325445, acc 0.875, learning_rate 0.000113955
2017-10-10T14:50:47.557215: step 1435, loss 0.214309, acc 0.9375, learning_rate 0.000113898
2017-10-10T14:50:47.792998: step 1436, loss 0.0933394, acc 0.96875, learning_rate 0.000113842
2017-10-10T14:50:48.112272: step 1437, loss 0.186188, acc 0.9375, learning_rate 0.000113785
2017-10-10T14:50:48.352855: step 1438, loss 0.181016, acc 0.921875, learning_rate 0.000113729
2017-10-10T14:50:48.648440: step 1439, loss 0.204186, acc 0.953125, learning_rate 0.000113673
2017-10-10T14:50:48.938464: step 1440, loss 0.125354, acc 0.984375, learning_rate 0.000113617

Evaluation:
2017-10-10T14:50:49.374645: step 1440, loss 0.239065, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1440

2017-10-10T14:50:50.480555: step 1441, loss 0.146013, acc 0.953125, learning_rate 0.000113561
2017-10-10T14:50:50.781657: step 1442, loss 0.1486, acc 0.953125, learning_rate 0.000113506
2017-10-10T14:50:51.080021: step 1443, loss 0.303491, acc 0.875, learning_rate 0.000113451
2017-10-10T14:50:51.368866: step 1444, loss 0.183549, acc 0.9375, learning_rate 0.000113396
2017-10-10T14:50:51.653613: step 1445, loss 0.148481, acc 0.96875, learning_rate 0.000113341
2017-10-10T14:50:51.889043: step 1446, loss 0.169472, acc 0.9375, learning_rate 0.000113287
2017-10-10T14:50:52.142741: step 1447, loss 0.141018, acc 0.953125, learning_rate 0.000113233
2017-10-10T14:50:52.475332: step 1448, loss 0.212967, acc 0.90625, learning_rate 0.000113179
2017-10-10T14:50:52.824921: step 1449, loss 0.125644, acc 0.96875, learning_rate 0.000113125
2017-10-10T14:50:53.108940: step 1450, loss 0.150758, acc 0.953125, learning_rate 0.000113071
2017-10-10T14:50:53.321193: step 1451, loss 0.120983, acc 0.984375, learning_rate 0.000113018
2017-10-10T14:50:53.588049: step 1452, loss 0.163209, acc 0.96875, learning_rate 0.000112965
2017-10-10T14:50:53.904781: step 1453, loss 0.10295, acc 0.96875, learning_rate 0.000112912
2017-10-10T14:50:54.161257: step 1454, loss 0.219181, acc 0.90625, learning_rate 0.000112859
2017-10-10T14:50:54.448840: step 1455, loss 0.175933, acc 0.9375, learning_rate 0.000112807
2017-10-10T14:50:54.720569: step 1456, loss 0.148199, acc 0.96875, learning_rate 0.000112754
2017-10-10T14:50:54.996536: step 1457, loss 0.172908, acc 0.9375, learning_rate 0.000112702
2017-10-10T14:50:55.192580: step 1458, loss 0.167736, acc 0.921875, learning_rate 0.000112651
2017-10-10T14:50:55.452195: step 1459, loss 0.213113, acc 0.921875, learning_rate 0.000112599
2017-10-10T14:50:55.736963: step 1460, loss 0.194549, acc 0.953125, learning_rate 0.000112547
2017-10-10T14:50:55.946358: step 1461, loss 0.293683, acc 0.890625, learning_rate 0.000112496
2017-10-10T14:50:56.236218: step 1462, loss 0.137933, acc 0.953125, learning_rate 0.000112445
2017-10-10T14:50:56.485735: step 1463, loss 0.286168, acc 0.921875, learning_rate 0.000112394
2017-10-10T14:50:56.732871: step 1464, loss 0.206059, acc 0.953125, learning_rate 0.000112344
2017-10-10T14:50:57.004921: step 1465, loss 0.194114, acc 0.90625, learning_rate 0.000112293
2017-10-10T14:50:57.268956: step 1466, loss 0.178373, acc 0.9375, learning_rate 0.000112243
2017-10-10T14:50:57.508969: step 1467, loss 0.175351, acc 0.9375, learning_rate 0.000112193
2017-10-10T14:50:57.812064: step 1468, loss 0.0446602, acc 0.984375, learning_rate 0.000112144
2017-10-10T14:50:58.143897: step 1469, loss 0.184592, acc 0.953125, learning_rate 0.000112094
2017-10-10T14:50:58.346773: step 1470, loss 0.0854988, acc 0.960784, learning_rate 0.000112045
2017-10-10T14:50:58.679969: step 1471, loss 0.164621, acc 0.96875, learning_rate 0.000111995
2017-10-10T14:50:58.960837: step 1472, loss 0.133255, acc 0.953125, learning_rate 0.000111946
2017-10-10T14:50:59.278329: step 1473, loss 0.172361, acc 0.921875, learning_rate 0.000111898
2017-10-10T14:50:59.558585: step 1474, loss 0.198736, acc 0.921875, learning_rate 0.000111849
2017-10-10T14:50:59.802601: step 1475, loss 0.167625, acc 0.9375, learning_rate 0.000111801
2017-10-10T14:51:00.093799: step 1476, loss 0.0999127, acc 0.96875, learning_rate 0.000111753
2017-10-10T14:51:00.399639: step 1477, loss 0.171045, acc 0.9375, learning_rate 0.000111705
2017-10-10T14:51:00.633107: step 1478, loss 0.149505, acc 0.953125, learning_rate 0.000111657
2017-10-10T14:51:00.857373: step 1479, loss 0.101771, acc 0.96875, learning_rate 0.000111609
2017-10-10T14:51:01.118175: step 1480, loss 0.190214, acc 0.921875, learning_rate 0.000111562

Evaluation:
2017-10-10T14:51:01.573229: step 1480, loss 0.238421, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1480

2017-10-10T14:51:02.728850: step 1481, loss 0.101545, acc 0.953125, learning_rate 0.000111515
2017-10-10T14:51:03.019929: step 1482, loss 0.174352, acc 0.9375, learning_rate 0.000111468
2017-10-10T14:51:03.299098: step 1483, loss 0.25591, acc 0.90625, learning_rate 0.000111421
2017-10-10T14:51:03.608426: step 1484, loss 0.0891272, acc 0.984375, learning_rate 0.000111374
2017-10-10T14:51:03.921671: step 1485, loss 0.223688, acc 0.9375, learning_rate 0.000111328
2017-10-10T14:51:04.150420: step 1486, loss 0.197959, acc 0.890625, learning_rate 0.000111282
2017-10-10T14:51:04.465269: step 1487, loss 0.130711, acc 0.96875, learning_rate 0.000111236
2017-10-10T14:51:04.659829: step 1488, loss 0.0868455, acc 0.984375, learning_rate 0.00011119
2017-10-10T14:51:04.964911: step 1489, loss 0.130658, acc 0.96875, learning_rate 0.000111144
2017-10-10T14:51:05.213138: step 1490, loss 0.213449, acc 0.9375, learning_rate 0.000111099
2017-10-10T14:51:05.556926: step 1491, loss 0.114592, acc 0.96875, learning_rate 0.000111053
2017-10-10T14:51:05.823591: step 1492, loss 0.133871, acc 0.953125, learning_rate 0.000111008
2017-10-10T14:51:06.144422: step 1493, loss 0.278435, acc 0.90625, learning_rate 0.000110963
2017-10-10T14:51:06.392832: step 1494, loss 0.294901, acc 0.921875, learning_rate 0.000110918
2017-10-10T14:51:06.604949: step 1495, loss 0.132379, acc 0.96875, learning_rate 0.000110874
2017-10-10T14:51:06.802825: step 1496, loss 0.111854, acc 0.96875, learning_rate 0.00011083
2017-10-10T14:51:07.071431: step 1497, loss 0.185424, acc 0.953125, learning_rate 0.000110785
2017-10-10T14:51:07.244905: step 1498, loss 0.160073, acc 0.96875, learning_rate 0.000110741
2017-10-10T14:51:07.539525: step 1499, loss 0.108324, acc 0.953125, learning_rate 0.000110697
2017-10-10T14:51:07.848902: step 1500, loss 0.176958, acc 0.9375, learning_rate 0.000110654
2017-10-10T14:51:08.149066: step 1501, loss 0.156612, acc 0.9375, learning_rate 0.00011061
2017-10-10T14:51:08.388928: step 1502, loss 0.116101, acc 0.96875, learning_rate 0.000110567
2017-10-10T14:51:08.676297: step 1503, loss 0.0883991, acc 0.96875, learning_rate 0.000110524
2017-10-10T14:51:08.945000: step 1504, loss 0.0858237, acc 0.96875, learning_rate 0.000110481
2017-10-10T14:51:09.249816: step 1505, loss 0.150695, acc 0.953125, learning_rate 0.000110438
2017-10-10T14:51:09.581929: step 1506, loss 0.114757, acc 0.953125, learning_rate 0.000110396
2017-10-10T14:51:09.857290: step 1507, loss 0.154276, acc 0.953125, learning_rate 0.000110353
2017-10-10T14:51:10.103834: step 1508, loss 0.0983281, acc 0.953125, learning_rate 0.000110311
2017-10-10T14:51:10.400926: step 1509, loss 0.147886, acc 0.9375, learning_rate 0.000110269
2017-10-10T14:51:10.692838: step 1510, loss 0.088998, acc 0.96875, learning_rate 0.000110227
2017-10-10T14:51:10.926464: step 1511, loss 0.158999, acc 0.96875, learning_rate 0.000110185
2017-10-10T14:51:11.203369: step 1512, loss 0.162041, acc 0.96875, learning_rate 0.000110144
2017-10-10T14:51:11.532861: step 1513, loss 0.106663, acc 0.96875, learning_rate 0.000110102
2017-10-10T14:51:11.797119: step 1514, loss 0.252896, acc 0.875, learning_rate 0.000110061
2017-10-10T14:51:12.104994: step 1515, loss 0.213367, acc 0.9375, learning_rate 0.00011002
2017-10-10T14:51:12.418730: step 1516, loss 0.230747, acc 0.90625, learning_rate 0.000109979
2017-10-10T14:51:12.725866: step 1517, loss 0.29637, acc 0.890625, learning_rate 0.000109938
2017-10-10T14:51:13.014716: step 1518, loss 0.144951, acc 0.9375, learning_rate 0.000109898
2017-10-10T14:51:13.311171: step 1519, loss 0.238353, acc 0.90625, learning_rate 0.000109857
2017-10-10T14:51:13.604851: step 1520, loss 0.249227, acc 0.9375, learning_rate 0.000109817

Evaluation:
2017-10-10T14:51:14.036577: step 1520, loss 0.237801, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1520

2017-10-10T14:51:14.984541: step 1521, loss 0.206745, acc 0.9375, learning_rate 0.000109777
2017-10-10T14:51:15.236865: step 1522, loss 0.130674, acc 0.984375, learning_rate 0.000109737
2017-10-10T14:51:15.531751: step 1523, loss 0.0708124, acc 1, learning_rate 0.000109697
2017-10-10T14:51:15.787852: step 1524, loss 0.171352, acc 0.953125, learning_rate 0.000109658
2017-10-10T14:51:16.043303: step 1525, loss 0.201656, acc 0.9375, learning_rate 0.000109618
2017-10-10T14:51:16.384824: step 1526, loss 0.06948, acc 0.96875, learning_rate 0.000109579
2017-10-10T14:51:16.594564: step 1527, loss 0.165706, acc 0.9375, learning_rate 0.00010954
2017-10-10T14:52:40.933516: step 1528, loss 0.115875, acc 0.96875, learning_rate 0.000109501
2017-10-10T14:53:47.538851: step 1529, loss 0.160322, acc 0.953125, learning_rate 0.000109462
2017-10-10T14:53:54.483615: step 1530, loss 0.427992, acc 0.859375, learning_rate 0.000109424
2017-10-10T14:54:07.595865: step 1531, loss 0.0901488, acc 1, learning_rate 0.000109385
2017-10-10T14:54:15.663846: step 1532, loss 0.133328, acc 0.96875, learning_rate 0.000109347
2017-10-10T14:54:19.254734: step 1533, loss 0.131234, acc 0.96875, learning_rate 0.000109309
2017-10-10T14:54:24.403045: step 1534, loss 0.310121, acc 0.953125, learning_rate 0.000109271
2017-10-10T14:54:30.668960: step 1535, loss 0.118593, acc 0.96875, learning_rate 0.000109233
2017-10-10T14:54:36.983220: step 1536, loss 0.268513, acc 0.921875, learning_rate 0.000109195
2017-10-10T14:54:42.757437: step 1537, loss 0.252569, acc 0.921875, learning_rate 0.000109158
2017-10-10T14:54:48.431841: step 1538, loss 0.124101, acc 0.96875, learning_rate 0.00010912
2017-10-10T14:54:55.499271: step 1539, loss 0.192909, acc 0.953125, learning_rate 0.000109083
2017-10-10T14:55:01.242009: step 1540, loss 0.0817243, acc 0.984375, learning_rate 0.000109046
2017-10-10T14:55:06.307931: step 1541, loss 0.207545, acc 0.9375, learning_rate 0.000109009
2017-10-10T14:55:09.031822: step 1542, loss 0.15889, acc 0.984375, learning_rate 0.000108972
2017-10-10T14:55:13.536420: step 1543, loss 0.204476, acc 0.9375, learning_rate 0.000108936
2017-10-10T14:55:16.651746: step 1544, loss 0.122373, acc 0.96875, learning_rate 0.000108899
2017-10-10T14:55:19.168583: step 1545, loss 0.119399, acc 0.96875, learning_rate 0.000108863
2017-10-10T14:55:22.066836: step 1546, loss 0.288658, acc 0.9375, learning_rate 0.000108827
2017-10-10T14:55:24.733465: step 1547, loss 0.159476, acc 0.96875, learning_rate 0.000108791
2017-10-10T14:55:26.761841: step 1548, loss 0.124927, acc 0.953125, learning_rate 0.000108755
2017-10-10T14:55:28.375356: step 1549, loss 0.145534, acc 0.953125, learning_rate 0.000108719
2017-10-10T14:55:31.098403: step 1550, loss 0.127384, acc 0.953125, learning_rate 0.000108683
2017-10-10T14:55:32.949919: step 1551, loss 0.169473, acc 0.9375, learning_rate 0.000108648
2017-10-10T14:55:34.325719: step 1552, loss 0.103923, acc 0.984375, learning_rate 0.000108613
2017-10-10T14:55:36.881925: step 1553, loss 0.17306, acc 0.9375, learning_rate 0.000108577
2017-10-10T14:55:39.289558: step 1554, loss 0.309638, acc 0.875, learning_rate 0.000108542
2017-10-10T14:55:41.233329: step 1555, loss 0.17356, acc 0.9375, learning_rate 0.000108508
2017-10-10T14:55:42.968146: step 1556, loss 0.108046, acc 0.953125, learning_rate 0.000108473
2017-10-10T14:55:44.477702: step 1557, loss 0.0863194, acc 0.984375, learning_rate 0.000108438
2017-10-10T14:55:45.992329: step 1558, loss 0.124824, acc 0.9375, learning_rate 0.000108404
2017-10-10T14:55:47.162548: step 1559, loss 0.155686, acc 0.96875, learning_rate 0.00010837
2017-10-10T14:55:48.677643: step 1560, loss 0.051023, acc 1, learning_rate 0.000108335

Evaluation:
2017-10-10T14:55:51.350868: step 1560, loss 0.238474, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1560

2017-10-10T14:56:56.151479: step 1561, loss 0.0739464, acc 0.96875, learning_rate 0.000108301
2017-10-10T14:56:57.717438: step 1562, loss 0.13661, acc 0.96875, learning_rate 0.000108267
2017-10-10T14:56:59.168381: step 1563, loss 0.181814, acc 0.9375, learning_rate 0.000108234
2017-10-10T14:57:00.607679: step 1564, loss 0.249924, acc 0.90625, learning_rate 0.0001082
2017-10-10T14:57:01.493300: step 1565, loss 0.102369, acc 0.96875, learning_rate 0.000108167
2017-10-10T14:57:03.142187: step 1566, loss 0.0875209, acc 0.96875, learning_rate 0.000108133
2017-10-10T14:57:03.931168: step 1567, loss 0.195305, acc 0.9375, learning_rate 0.0001081
2017-10-10T14:57:05.953517: step 1568, loss 0.174858, acc 0.941176, learning_rate 0.000108067
2017-10-10T14:57:13.555372: step 1569, loss 0.172374, acc 0.9375, learning_rate 0.000108034
2017-10-10T14:57:15.133830: step 1570, loss 0.169313, acc 0.9375, learning_rate 0.000108001
2017-10-10T14:57:16.431680: step 1571, loss 0.301637, acc 0.875, learning_rate 0.000107969
2017-10-10T14:57:17.727418: step 1572, loss 0.136314, acc 0.96875, learning_rate 0.000107936
2017-10-10T14:57:18.996343: step 1573, loss 0.151007, acc 0.9375, learning_rate 0.000107904
2017-10-10T14:57:23.640688: step 1574, loss 0.142083, acc 0.953125, learning_rate 0.000107871
2017-10-10T14:57:24.932554: step 1575, loss 0.157505, acc 0.96875, learning_rate 0.000107839
2017-10-10T14:57:26.380601: step 1576, loss 0.182003, acc 0.953125, learning_rate 0.000107807
2017-10-10T14:57:27.859127: step 1577, loss 0.231233, acc 0.953125, learning_rate 0.000107775
2017-10-10T14:57:28.668671: step 1578, loss 0.149941, acc 0.953125, learning_rate 0.000107744
2017-10-10T14:57:29.939649: step 1579, loss 0.143533, acc 0.96875, learning_rate 0.000107712
2017-10-10T14:57:30.295013: step 1580, loss 0.140152, acc 0.953125, learning_rate 0.000107681
2017-10-10T14:57:30.875984: step 1581, loss 0.0725573, acc 0.984375, learning_rate 0.000107649
2017-10-10T14:57:31.640760: step 1582, loss 0.0883394, acc 0.984375, learning_rate 0.000107618
2017-10-10T14:57:32.420619: step 1583, loss 0.196278, acc 0.921875, learning_rate 0.000107587
2017-10-10T14:57:33.905518: step 1584, loss 0.0912645, acc 0.984375, learning_rate 0.000107556
2017-10-10T14:57:34.885904: step 1585, loss 0.199666, acc 0.921875, learning_rate 0.000107525
2017-10-10T14:57:35.221146: step 1586, loss 0.115078, acc 0.984375, learning_rate 0.000107494
2017-10-10T14:57:35.824805: step 1587, loss 0.0948704, acc 0.96875, learning_rate 0.000107464
2017-10-10T14:57:36.272995: step 1588, loss 0.169498, acc 0.953125, learning_rate 0.000107433
2017-10-10T14:57:37.530333: step 1589, loss 0.294649, acc 0.921875, learning_rate 0.000107403
2017-10-10T14:57:38.475905: step 1590, loss 0.114639, acc 0.953125, learning_rate 0.000107373
2017-10-10T14:57:39.051305: step 1591, loss 0.222149, acc 0.90625, learning_rate 0.000107343
2017-10-10T14:57:39.597729: step 1592, loss 0.128016, acc 0.953125, learning_rate 0.000107313
2017-10-10T14:57:40.050424: step 1593, loss 0.12947, acc 0.96875, learning_rate 0.000107283
2017-10-10T14:57:40.957695: step 1594, loss 0.150946, acc 0.9375, learning_rate 0.000107253
2017-10-10T14:57:41.648586: step 1595, loss 0.156305, acc 0.953125, learning_rate 0.000107224
2017-10-10T14:57:42.000359: step 1596, loss 0.161042, acc 0.9375, learning_rate 0.000107194
2017-10-10T14:57:42.405188: step 1597, loss 0.20392, acc 0.9375, learning_rate 0.000107165
2017-10-10T14:57:42.609233: step 1598, loss 0.0764837, acc 0.96875, learning_rate 0.000107136
2017-10-10T14:57:42.746730: step 1599, loss 0.184514, acc 0.953125, learning_rate 0.000107106
2017-10-10T14:57:43.440120: step 1600, loss 0.137014, acc 0.953125, learning_rate 0.000107077

Evaluation:
2017-10-10T14:57:43.595142: step 1600, loss 0.236248, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1600

2017-10-10T14:57:44.966531: step 1601, loss 0.243494, acc 0.921875, learning_rate 0.000107048
2017-10-10T14:57:45.579315: step 1602, loss 0.120073, acc 0.9375, learning_rate 0.00010702
2017-10-10T14:57:45.851315: step 1603, loss 0.122425, acc 0.96875, learning_rate 0.000106991
2017-10-10T14:57:46.436241: step 1604, loss 0.152111, acc 0.96875, learning_rate 0.000106963
2017-10-10T14:57:46.827001: step 1605, loss 0.12103, acc 0.96875, learning_rate 0.000106934
2017-10-10T14:57:47.040386: step 1606, loss 0.135362, acc 0.953125, learning_rate 0.000106906
2017-10-10T14:57:47.586088: step 1607, loss 0.271981, acc 0.890625, learning_rate 0.000106878
2017-10-10T14:57:48.155937: step 1608, loss 0.117966, acc 0.96875, learning_rate 0.00010685
2017-10-10T14:57:48.826619: step 1609, loss 0.141295, acc 0.96875, learning_rate 0.000106822
2017-10-10T14:57:49.195273: step 1610, loss 0.153064, acc 0.96875, learning_rate 0.000106794
2017-10-10T14:57:49.856831: step 1611, loss 0.137002, acc 0.9375, learning_rate 0.000106766
2017-10-10T14:57:50.273639: step 1612, loss 0.148842, acc 0.953125, learning_rate 0.000106738
2017-10-10T14:57:50.632880: step 1613, loss 0.152718, acc 0.96875, learning_rate 0.000106711
2017-10-10T14:57:50.827379: step 1614, loss 0.137317, acc 0.953125, learning_rate 0.000106684
2017-10-10T14:57:50.930947: step 1615, loss 0.135556, acc 0.953125, learning_rate 0.000106656
2017-10-10T14:57:51.457608: step 1616, loss 0.0927297, acc 0.96875, learning_rate 0.000106629
2017-10-10T14:57:51.629087: step 1617, loss 0.110848, acc 0.96875, learning_rate 0.000106602
2017-10-10T14:57:51.811932: step 1618, loss 0.251374, acc 0.921875, learning_rate 0.000106575
2017-10-10T14:57:52.196559: step 1619, loss 0.091223, acc 0.984375, learning_rate 0.000106548
2017-10-10T14:57:52.544441: step 1620, loss 0.168177, acc 0.953125, learning_rate 0.000106521
2017-10-10T14:57:52.766248: step 1621, loss 0.0936859, acc 0.953125, learning_rate 0.000106495
2017-10-10T14:57:52.920978: step 1622, loss 0.176882, acc 0.921875, learning_rate 0.000106468
2017-10-10T14:57:53.010625: step 1623, loss 0.123627, acc 0.984375, learning_rate 0.000106442
2017-10-10T14:57:53.244953: step 1624, loss 0.161736, acc 0.921875, learning_rate 0.000106416
2017-10-10T14:57:53.690931: step 1625, loss 0.208248, acc 0.90625, learning_rate 0.000106389
2017-10-10T14:57:54.090837: step 1626, loss 0.106515, acc 0.96875, learning_rate 0.000106363
2017-10-10T14:57:54.372827: step 1627, loss 0.179039, acc 0.96875, learning_rate 0.000106337
2017-10-10T14:57:54.627882: step 1628, loss 0.2283, acc 0.921875, learning_rate 0.000106312
2017-10-10T14:57:54.961992: step 1629, loss 0.221732, acc 0.90625, learning_rate 0.000106286
2017-10-10T14:57:55.510013: step 1630, loss 0.0853217, acc 0.96875, learning_rate 0.00010626
2017-10-10T14:57:56.094564: step 1631, loss 0.251654, acc 0.9375, learning_rate 0.000106235
2017-10-10T14:57:56.365003: step 1632, loss 0.152932, acc 0.953125, learning_rate 0.000106209
2017-10-10T14:57:56.596013: step 1633, loss 0.320809, acc 0.859375, learning_rate 0.000106184
2017-10-10T14:57:56.784065: step 1634, loss 0.167959, acc 0.9375, learning_rate 0.000106159
2017-10-10T14:57:56.915240: step 1635, loss 0.0642741, acc 1, learning_rate 0.000106133
2017-10-10T14:57:57.225406: step 1636, loss 0.087929, acc 0.984375, learning_rate 0.000106108
2017-10-10T14:57:57.448806: step 1637, loss 0.123371, acc 0.984375, learning_rate 0.000106083
2017-10-10T14:57:57.581531: step 1638, loss 0.24152, acc 0.875, learning_rate 0.000106059
2017-10-10T14:57:57.769795: step 1639, loss 0.132706, acc 0.96875, learning_rate 0.000106034
2017-10-10T14:57:57.854924: step 1640, loss 0.0985868, acc 0.984375, learning_rate 0.000106009

Evaluation:
2017-10-10T14:57:58.076059: step 1640, loss 0.236275, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1640

2017-10-10T14:57:59.044443: step 1641, loss 0.198577, acc 0.953125, learning_rate 0.000105985
2017-10-10T14:57:59.227425: step 1642, loss 0.113782, acc 0.9375, learning_rate 0.00010596
2017-10-10T14:57:59.376402: step 1643, loss 0.196268, acc 0.921875, learning_rate 0.000105936
2017-10-10T14:57:59.529022: step 1644, loss 0.139852, acc 0.984375, learning_rate 0.000105912
2017-10-10T14:57:59.968094: step 1645, loss 0.157834, acc 0.9375, learning_rate 0.000105888
2017-10-10T14:58:00.168500: step 1646, loss 0.182625, acc 0.96875, learning_rate 0.000105864
2017-10-10T14:58:00.400318: step 1647, loss 0.0529826, acc 1, learning_rate 0.00010584
2017-10-10T14:58:00.566729: step 1648, loss 0.187377, acc 0.953125, learning_rate 0.000105816
2017-10-10T14:58:00.874794: step 1649, loss 0.142498, acc 0.953125, learning_rate 0.000105792
2017-10-10T14:58:01.051387: step 1650, loss 0.156611, acc 0.9375, learning_rate 0.000105768
2017-10-10T14:58:01.240053: step 1651, loss 0.0760219, acc 0.984375, learning_rate 0.000105745
2017-10-10T14:58:01.559551: step 1652, loss 0.0803291, acc 0.984375, learning_rate 0.000105721
2017-10-10T14:58:01.650272: step 1653, loss 0.172013, acc 0.921875, learning_rate 0.000105698
2017-10-10T14:58:01.962341: step 1654, loss 0.179068, acc 0.90625, learning_rate 0.000105675
2017-10-10T14:58:02.176905: step 1655, loss 0.174165, acc 0.953125, learning_rate 0.000105652
2017-10-10T14:58:02.329652: step 1656, loss 0.148368, acc 0.953125, learning_rate 0.000105629
2017-10-10T14:58:02.523571: step 1657, loss 0.204954, acc 0.953125, learning_rate 0.000105606
2017-10-10T14:58:02.812242: step 1658, loss 0.183676, acc 0.90625, learning_rate 0.000105583
2017-10-10T14:58:02.992790: step 1659, loss 0.194585, acc 0.9375, learning_rate 0.00010556
2017-10-10T14:58:03.137384: step 1660, loss 0.198433, acc 0.921875, learning_rate 0.000105537
2017-10-10T14:58:03.447572: step 1661, loss 0.174242, acc 0.9375, learning_rate 0.000105515
2017-10-10T14:58:03.531697: step 1662, loss 0.0793533, acc 0.984375, learning_rate 0.000105492
2017-10-10T14:58:03.671706: step 1663, loss 0.128778, acc 0.9375, learning_rate 0.00010547
2017-10-10T14:58:03.934578: step 1664, loss 0.201529, acc 0.90625, learning_rate 0.000105447
2017-10-10T14:58:04.015817: step 1665, loss 0.0477212, acc 1, learning_rate 0.000105425
2017-10-10T14:58:04.135341: step 1666, loss 0.161287, acc 0.960784, learning_rate 0.000105403
2017-10-10T14:58:04.386995: step 1667, loss 0.134356, acc 0.9375, learning_rate 0.000105381
2017-10-10T14:58:04.535806: step 1668, loss 0.23632, acc 0.90625, learning_rate 0.000105359
2017-10-10T14:58:04.611498: step 1669, loss 0.218524, acc 0.9375, learning_rate 0.000105337
2017-10-10T14:58:04.685786: step 1670, loss 0.0957841, acc 0.984375, learning_rate 0.000105315
2017-10-10T14:58:04.760803: step 1671, loss 0.142269, acc 0.9375, learning_rate 0.000105294
2017-10-10T14:58:04.888545: step 1672, loss 0.159401, acc 0.953125, learning_rate 0.000105272
2017-10-10T14:58:04.962422: step 1673, loss 0.179706, acc 0.921875, learning_rate 0.000105251
2017-10-10T14:58:05.079949: step 1674, loss 0.157815, acc 0.9375, learning_rate 0.000105229
2017-10-10T14:58:05.189881: step 1675, loss 0.183576, acc 0.9375, learning_rate 0.000105208
2017-10-10T14:58:05.282335: step 1676, loss 0.118777, acc 0.96875, learning_rate 0.000105186
2017-10-10T14:58:05.377310: step 1677, loss 0.203908, acc 0.921875, learning_rate 0.000105165
2017-10-10T14:58:05.670465: step 1678, loss 0.116736, acc 0.953125, learning_rate 0.000105144
2017-10-10T14:58:05.815761: step 1679, loss 0.105593, acc 0.96875, learning_rate 0.000105123
2017-10-10T14:58:05.973288: step 1680, loss 0.232213, acc 0.9375, learning_rate 0.000105102

Evaluation:
2017-10-10T14:58:06.352856: step 1680, loss 0.237145, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1680

2017-10-10T14:58:07.137946: step 1681, loss 0.169546, acc 0.953125, learning_rate 0.000105081
2017-10-10T14:58:07.255737: step 1682, loss 0.102743, acc 0.96875, learning_rate 0.000105061
2017-10-10T14:58:07.340067: step 1683, loss 0.063116, acc 1, learning_rate 0.00010504
2017-10-10T14:58:07.513614: step 1684, loss 0.19163, acc 0.921875, learning_rate 0.00010502
2017-10-10T14:58:07.629605: step 1685, loss 0.193508, acc 0.9375, learning_rate 0.000104999
2017-10-10T14:58:07.712186: step 1686, loss 0.0943189, acc 0.96875, learning_rate 0.000104979
2017-10-10T14:58:07.798342: step 1687, loss 0.0879425, acc 1, learning_rate 0.000104958
2017-10-10T14:58:08.021166: step 1688, loss 0.218897, acc 0.90625, learning_rate 0.000104938
2017-10-10T14:58:08.119142: step 1689, loss 0.131847, acc 0.984375, learning_rate 0.000104918
2017-10-10T14:58:08.341077: step 1690, loss 0.263057, acc 0.875, learning_rate 0.000104898
2017-10-10T14:58:08.624860: step 1691, loss 0.0773096, acc 0.984375, learning_rate 0.000104878
2017-10-10T14:58:08.958497: step 1692, loss 0.212116, acc 0.921875, learning_rate 0.000104858
2017-10-10T14:58:09.135190: step 1693, loss 0.247139, acc 0.9375, learning_rate 0.000104838
2017-10-10T14:58:09.301542: step 1694, loss 0.10886, acc 0.953125, learning_rate 0.000104818
2017-10-10T14:58:09.523238: step 1695, loss 0.17538, acc 0.9375, learning_rate 0.000104799
2017-10-10T14:58:09.648932: step 1696, loss 0.18549, acc 0.9375, learning_rate 0.000104779
2017-10-10T14:58:09.761323: step 1697, loss 0.130091, acc 0.953125, learning_rate 0.00010476
2017-10-10T14:58:10.071678: step 1698, loss 0.220794, acc 0.921875, learning_rate 0.00010474
2017-10-10T14:58:10.328015: step 1699, loss 0.0682131, acc 0.984375, learning_rate 0.000104721
2017-10-10T14:58:10.480851: step 1700, loss 0.106489, acc 0.953125, learning_rate 0.000104702
2017-10-10T14:58:10.724938: step 1701, loss 0.294255, acc 0.90625, learning_rate 0.000104682
2017-10-10T14:58:10.905342: step 1702, loss 0.170651, acc 0.9375, learning_rate 0.000104663
2017-10-10T14:58:11.141076: step 1703, loss 0.143258, acc 0.953125, learning_rate 0.000104644
2017-10-10T14:58:11.406648: step 1704, loss 0.150495, acc 0.921875, learning_rate 0.000104625
2017-10-10T14:58:11.527983: step 1705, loss 0.27043, acc 0.90625, learning_rate 0.000104606
2017-10-10T14:58:11.752997: step 1706, loss 0.155563, acc 0.953125, learning_rate 0.000104588
2017-10-10T14:58:11.886975: step 1707, loss 0.205982, acc 0.90625, learning_rate 0.000104569
2017-10-10T14:58:11.999580: step 1708, loss 0.114482, acc 0.96875, learning_rate 0.00010455
2017-10-10T14:58:12.114507: step 1709, loss 0.186187, acc 0.921875, learning_rate 0.000104532
2017-10-10T14:58:12.230071: step 1710, loss 0.169594, acc 0.9375, learning_rate 0.000104513
2017-10-10T14:58:12.347737: step 1711, loss 0.184214, acc 0.9375, learning_rate 0.000104495
2017-10-10T14:58:12.598810: step 1712, loss 0.171889, acc 0.9375, learning_rate 0.000104476
2017-10-10T14:58:12.760261: step 1713, loss 0.178574, acc 0.953125, learning_rate 0.000104458
2017-10-10T14:58:13.017121: step 1714, loss 0.120029, acc 0.96875, learning_rate 0.00010444
2017-10-10T14:58:13.185024: step 1715, loss 0.168517, acc 0.953125, learning_rate 0.000104422
2017-10-10T14:58:13.290734: step 1716, loss 0.074428, acc 0.984375, learning_rate 0.000104404
2017-10-10T14:58:13.409561: step 1717, loss 0.204097, acc 0.9375, learning_rate 0.000104386
2017-10-10T14:58:13.587623: step 1718, loss 0.272862, acc 0.90625, learning_rate 0.000104368
2017-10-10T14:58:13.752952: step 1719, loss 0.259918, acc 0.90625, learning_rate 0.00010435
2017-10-10T14:58:13.956848: step 1720, loss 0.163947, acc 0.953125, learning_rate 0.000104332

Evaluation:
2017-10-10T14:58:14.290684: step 1720, loss 0.236613, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1720

2017-10-10T14:58:15.131887: step 1721, loss 0.287565, acc 0.90625, learning_rate 0.000104315
2017-10-10T14:58:15.216705: step 1722, loss 0.197088, acc 0.921875, learning_rate 0.000104297
2017-10-10T14:58:15.400171: step 1723, loss 0.206359, acc 0.96875, learning_rate 0.000104279
2017-10-10T14:58:15.552220: step 1724, loss 0.0871571, acc 0.984375, learning_rate 0.000104262
2017-10-10T14:58:15.651421: step 1725, loss 0.205483, acc 0.9375, learning_rate 0.000104245
2017-10-10T14:58:16.005747: step 1726, loss 0.109493, acc 0.96875, learning_rate 0.000104227
2017-10-10T14:58:16.242765: step 1727, loss 0.121427, acc 0.984375, learning_rate 0.00010421
2017-10-10T14:58:16.372466: step 1728, loss 0.138627, acc 0.96875, learning_rate 0.000104193
2017-10-10T14:58:16.644835: step 1729, loss 0.203993, acc 0.921875, learning_rate 0.000104176
2017-10-10T14:58:16.841259: step 1730, loss 0.145814, acc 0.953125, learning_rate 0.000104159
2017-10-10T14:58:17.064855: step 1731, loss 0.0959204, acc 0.96875, learning_rate 0.000104142
2017-10-10T14:58:17.282163: step 1732, loss 0.211711, acc 0.9375, learning_rate 0.000104125
2017-10-10T14:58:17.553040: step 1733, loss 0.0783644, acc 0.984375, learning_rate 0.000104108
2017-10-10T14:58:17.953760: step 1734, loss 0.153018, acc 0.953125, learning_rate 0.000104091
2017-10-10T14:58:18.136844: step 1735, loss 0.156712, acc 0.9375, learning_rate 0.000104074
2017-10-10T14:58:18.370552: step 1736, loss 0.0850714, acc 1, learning_rate 0.000104058
2017-10-10T14:58:18.562107: step 1737, loss 0.0859537, acc 0.96875, learning_rate 0.000104041
2017-10-10T14:58:18.868820: step 1738, loss 0.20119, acc 0.9375, learning_rate 0.000104025
2017-10-10T14:58:19.058439: step 1739, loss 0.205965, acc 0.9375, learning_rate 0.000104008
2017-10-10T14:58:19.318317: step 1740, loss 0.283304, acc 0.90625, learning_rate 0.000103992
2017-10-10T14:58:19.473258: step 1741, loss 0.0443386, acc 1, learning_rate 0.000103976
2017-10-10T14:58:19.745020: step 1742, loss 0.104785, acc 0.953125, learning_rate 0.000103959
2017-10-10T14:58:19.911896: step 1743, loss 0.15127, acc 0.953125, learning_rate 0.000103943
2017-10-10T14:58:20.188136: step 1744, loss 0.239852, acc 0.921875, learning_rate 0.000103927
2017-10-10T14:58:20.404853: step 1745, loss 0.18083, acc 0.921875, learning_rate 0.000103911
2017-10-10T14:58:20.743031: step 1746, loss 0.287735, acc 0.9375, learning_rate 0.000103895
2017-10-10T14:58:21.018069: step 1747, loss 0.138326, acc 0.96875, learning_rate 0.000103879
2017-10-10T14:58:21.237800: step 1748, loss 0.116278, acc 0.953125, learning_rate 0.000103863
2017-10-10T14:58:21.533591: step 1749, loss 0.103057, acc 0.984375, learning_rate 0.000103848
2017-10-10T14:58:21.722161: step 1750, loss 0.21836, acc 0.921875, learning_rate 0.000103832
2017-10-10T14:58:22.020063: step 1751, loss 0.182001, acc 0.953125, learning_rate 0.000103816
2017-10-10T14:58:22.306567: step 1752, loss 0.227043, acc 0.9375, learning_rate 0.000103801
2017-10-10T14:58:22.553147: step 1753, loss 0.135732, acc 0.96875, learning_rate 0.000103785
2017-10-10T14:58:22.890076: step 1754, loss 0.11418, acc 0.96875, learning_rate 0.00010377
2017-10-10T14:58:23.129102: step 1755, loss 0.136851, acc 0.96875, learning_rate 0.000103754
2017-10-10T14:58:23.420731: step 1756, loss 0.157952, acc 0.9375, learning_rate 0.000103739
2017-10-10T14:58:23.697163: step 1757, loss 0.212237, acc 0.921875, learning_rate 0.000103724
2017-10-10T14:58:23.993100: step 1758, loss 0.141043, acc 0.953125, learning_rate 0.000103709
2017-10-10T14:58:24.284334: step 1759, loss 0.125604, acc 0.9375, learning_rate 0.000103694
2017-10-10T14:58:24.497058: step 1760, loss 0.135717, acc 0.9375, learning_rate 0.000103678

Evaluation:
2017-10-10T14:58:24.969002: step 1760, loss 0.233552, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1760

2017-10-10T14:58:25.912976: step 1761, loss 0.15745, acc 0.953125, learning_rate 0.000103663
2017-10-10T14:58:26.218207: step 1762, loss 0.0868354, acc 0.984375, learning_rate 0.000103648
2017-10-10T14:58:26.450148: step 1763, loss 0.149123, acc 0.96875, learning_rate 0.000103634
2017-10-10T14:58:26.615027: step 1764, loss 0.1715, acc 0.941176, learning_rate 0.000103619
2017-10-10T14:58:26.956918: step 1765, loss 0.183132, acc 0.953125, learning_rate 0.000103604
2017-10-10T14:58:27.123349: step 1766, loss 0.181135, acc 0.953125, learning_rate 0.000103589
2017-10-10T14:58:27.264410: step 1767, loss 0.258211, acc 0.90625, learning_rate 0.000103575
2017-10-10T14:58:27.461072: step 1768, loss 0.0980593, acc 0.953125, learning_rate 0.00010356
2017-10-10T14:58:27.674820: step 1769, loss 0.151942, acc 0.953125, learning_rate 0.000103545
2017-10-10T14:58:27.973863: step 1770, loss 0.105444, acc 1, learning_rate 0.000103531
2017-10-10T14:58:28.224394: step 1771, loss 0.247925, acc 0.890625, learning_rate 0.000103517
2017-10-10T14:58:28.474457: step 1772, loss 0.136564, acc 0.953125, learning_rate 0.000103502
2017-10-10T14:58:28.673016: step 1773, loss 0.0705612, acc 1, learning_rate 0.000103488
2017-10-10T14:58:28.978941: step 1774, loss 0.0969902, acc 0.96875, learning_rate 0.000103474
2017-10-10T14:58:29.256797: step 1775, loss 0.125717, acc 0.953125, learning_rate 0.00010346
2017-10-10T14:58:29.448534: step 1776, loss 0.0842845, acc 0.984375, learning_rate 0.000103445
2017-10-10T14:58:29.636062: step 1777, loss 0.0931193, acc 0.96875, learning_rate 0.000103431
2017-10-10T14:58:29.791807: step 1778, loss 0.337553, acc 0.890625, learning_rate 0.000103417
2017-10-10T14:58:29.946178: step 1779, loss 0.127013, acc 0.9375, learning_rate 0.000103403
2017-10-10T14:58:30.171727: step 1780, loss 0.125264, acc 0.984375, learning_rate 0.00010339
2017-10-10T14:58:30.370091: step 1781, loss 0.132151, acc 0.953125, learning_rate 0.000103376
2017-10-10T14:58:30.590776: step 1782, loss 0.109551, acc 0.96875, learning_rate 0.000103362
2017-10-10T14:58:30.776826: step 1783, loss 0.112227, acc 0.96875, learning_rate 0.000103348
2017-10-10T14:58:32.562693: step 1784, loss 0.296764, acc 0.90625, learning_rate 0.000103335
2017-10-10T14:58:32.787455: step 1785, loss 0.139327, acc 0.953125, learning_rate 0.000103321
2017-10-10T14:58:33.055333: step 1786, loss 0.268183, acc 0.921875, learning_rate 0.000103307
2017-10-10T14:58:33.364833: step 1787, loss 0.126896, acc 0.953125, learning_rate 0.000103294
2017-10-10T14:58:33.622692: step 1788, loss 0.1422, acc 0.9375, learning_rate 0.00010328
2017-10-10T14:58:33.857015: step 1789, loss 0.13141, acc 0.9375, learning_rate 0.000103267
2017-10-10T14:58:34.089825: step 1790, loss 0.132905, acc 0.9375, learning_rate 0.000103254
2017-10-10T14:58:34.362489: step 1791, loss 0.175113, acc 0.90625, learning_rate 0.00010324
2017-10-10T14:58:34.673634: step 1792, loss 0.140942, acc 0.953125, learning_rate 0.000103227
2017-10-10T14:58:34.885236: step 1793, loss 0.171985, acc 0.921875, learning_rate 0.000103214
2017-10-10T14:58:35.055828: step 1794, loss 0.202268, acc 0.9375, learning_rate 0.000103201
2017-10-10T14:58:35.314185: step 1795, loss 0.16796, acc 0.953125, learning_rate 0.000103188
2017-10-10T14:58:35.539477: step 1796, loss 0.136988, acc 0.953125, learning_rate 0.000103175
2017-10-10T14:58:35.829205: step 1797, loss 0.185161, acc 0.921875, learning_rate 0.000103162
2017-10-10T14:58:36.064974: step 1798, loss 0.192855, acc 0.9375, learning_rate 0.000103149
2017-10-10T14:58:36.236937: step 1799, loss 0.141168, acc 0.96875, learning_rate 0.000103136
2017-10-10T14:58:36.464958: step 1800, loss 0.118635, acc 0.96875, learning_rate 0.000103123

Evaluation:
2017-10-10T14:58:36.891024: step 1800, loss 0.233883, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1800

2017-10-10T14:58:37.991599: step 1801, loss 0.219856, acc 0.9375, learning_rate 0.000103111
2017-10-10T14:58:38.232950: step 1802, loss 0.23834, acc 0.953125, learning_rate 0.000103098
2017-10-10T14:58:38.488857: step 1803, loss 0.148006, acc 0.9375, learning_rate 0.000103085
2017-10-10T14:58:38.725022: step 1804, loss 0.141862, acc 0.953125, learning_rate 0.000103073
2017-10-10T14:58:38.916994: step 1805, loss 0.125441, acc 0.984375, learning_rate 0.00010306
2017-10-10T14:58:39.137069: step 1806, loss 0.0780197, acc 0.984375, learning_rate 0.000103048
2017-10-10T14:58:39.410827: step 1807, loss 0.169919, acc 0.90625, learning_rate 0.000103035
2017-10-10T14:58:39.632949: step 1808, loss 0.112164, acc 0.953125, learning_rate 0.000103023
2017-10-10T14:58:39.837108: step 1809, loss 0.171043, acc 0.9375, learning_rate 0.00010301
2017-10-10T14:58:40.048192: step 1810, loss 0.180343, acc 0.953125, learning_rate 0.000102998
2017-10-10T14:58:40.296588: step 1811, loss 0.137203, acc 0.953125, learning_rate 0.000102986
2017-10-10T14:58:40.493720: step 1812, loss 0.225049, acc 0.890625, learning_rate 0.000102974
2017-10-10T14:58:40.696739: step 1813, loss 0.157947, acc 0.9375, learning_rate 0.000102962
2017-10-10T14:58:40.875972: step 1814, loss 0.173374, acc 0.9375, learning_rate 0.000102949
2017-10-10T14:58:41.088822: step 1815, loss 0.115341, acc 0.96875, learning_rate 0.000102937
2017-10-10T14:58:41.361024: step 1816, loss 0.0832692, acc 0.96875, learning_rate 0.000102925
2017-10-10T14:58:41.515020: step 1817, loss 0.155373, acc 0.96875, learning_rate 0.000102913
2017-10-10T14:58:41.724170: step 1818, loss 0.23156, acc 0.90625, learning_rate 0.000102902
2017-10-10T14:58:41.972960: step 1819, loss 0.277491, acc 0.921875, learning_rate 0.00010289
2017-10-10T14:58:42.229382: step 1820, loss 0.168064, acc 0.953125, learning_rate 0.000102878
2017-10-10T14:58:42.449108: step 1821, loss 0.127773, acc 0.984375, learning_rate 0.000102866
2017-10-10T14:58:42.675643: step 1822, loss 0.148437, acc 0.953125, learning_rate 0.000102855
2017-10-10T14:58:42.917071: step 1823, loss 0.155315, acc 0.9375, learning_rate 0.000102843
2017-10-10T14:58:43.208676: step 1824, loss 0.241214, acc 0.921875, learning_rate 0.000102831
2017-10-10T14:58:43.563083: step 1825, loss 0.181618, acc 0.9375, learning_rate 0.00010282
2017-10-10T14:58:43.871163: step 1826, loss 0.203954, acc 0.921875, learning_rate 0.000102808
2017-10-10T14:58:44.059026: step 1827, loss 0.133789, acc 0.96875, learning_rate 0.000102797
2017-10-10T14:58:44.253056: step 1828, loss 0.18157, acc 0.921875, learning_rate 0.000102785
2017-10-10T14:58:44.462602: step 1829, loss 0.120209, acc 0.96875, learning_rate 0.000102774
2017-10-10T14:58:44.639297: step 1830, loss 0.108122, acc 0.96875, learning_rate 0.000102763
2017-10-10T14:58:44.863395: step 1831, loss 0.0890486, acc 1, learning_rate 0.000102751
2017-10-10T14:58:45.132944: step 1832, loss 0.114268, acc 0.953125, learning_rate 0.00010274
2017-10-10T14:58:45.407174: step 1833, loss 0.116944, acc 0.953125, learning_rate 0.000102729
2017-10-10T14:58:45.782956: step 1834, loss 0.116693, acc 0.984375, learning_rate 0.000102718
2017-10-10T14:58:46.101038: step 1835, loss 0.086328, acc 0.96875, learning_rate 0.000102707
2017-10-10T14:58:46.339884: step 1836, loss 0.18477, acc 0.984375, learning_rate 0.000102696
2017-10-10T14:58:46.564852: step 1837, loss 0.17295, acc 0.96875, learning_rate 0.000102685
2017-10-10T14:58:46.811628: step 1838, loss 0.186422, acc 0.9375, learning_rate 0.000102674
2017-10-10T14:58:47.088872: step 1839, loss 0.235268, acc 0.921875, learning_rate 0.000102663
2017-10-10T14:58:47.369717: step 1840, loss 0.134382, acc 0.953125, learning_rate 0.000102652

Evaluation:
2017-10-10T14:58:47.822566: step 1840, loss 0.233983, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1840

2017-10-10T14:58:48.724688: step 1841, loss 0.193766, acc 0.9375, learning_rate 0.000102641
2017-10-10T14:58:49.029114: step 1842, loss 0.127208, acc 0.96875, learning_rate 0.00010263
2017-10-10T14:58:49.312867: step 1843, loss 0.102941, acc 0.96875, learning_rate 0.00010262
2017-10-10T14:58:49.596928: step 1844, loss 0.0754508, acc 0.96875, learning_rate 0.000102609
2017-10-10T14:58:49.848338: step 1845, loss 0.128493, acc 0.9375, learning_rate 0.000102598
2017-10-10T14:58:50.001002: step 1846, loss 0.169467, acc 0.953125, learning_rate 0.000102588
2017-10-10T14:58:50.317952: step 1847, loss 0.0903254, acc 0.984375, learning_rate 0.000102577
2017-10-10T14:58:50.593504: step 1848, loss 0.073661, acc 0.96875, learning_rate 0.000102567
2017-10-10T14:58:50.807267: step 1849, loss 0.139061, acc 0.953125, learning_rate 0.000102556
2017-10-10T14:58:51.104281: step 1850, loss 0.103874, acc 0.96875, learning_rate 0.000102546
2017-10-10T14:58:51.321211: step 1851, loss 0.20589, acc 0.953125, learning_rate 0.000102535
2017-10-10T14:58:51.597134: step 1852, loss 0.220171, acc 0.9375, learning_rate 0.000102525
2017-10-10T14:58:51.920342: step 1853, loss 0.13355, acc 0.984375, learning_rate 0.000102515
2017-10-10T14:58:52.197207: step 1854, loss 0.126279, acc 0.953125, learning_rate 0.000102504
2017-10-10T14:58:52.474982: step 1855, loss 0.103405, acc 0.953125, learning_rate 0.000102494
2017-10-10T14:58:52.736393: step 1856, loss 0.163515, acc 0.9375, learning_rate 0.000102484
2017-10-10T14:58:52.993827: step 1857, loss 0.140677, acc 0.9375, learning_rate 0.000102474
2017-10-10T14:58:53.303796: step 1858, loss 0.0948345, acc 0.984375, learning_rate 0.000102464
2017-10-10T14:58:53.578342: step 1859, loss 0.118388, acc 0.96875, learning_rate 0.000102454
2017-10-10T14:58:53.820891: step 1860, loss 0.184859, acc 0.953125, learning_rate 0.000102444
2017-10-10T14:58:54.152673: step 1861, loss 0.109544, acc 0.96875, learning_rate 0.000102434
2017-10-10T14:58:54.346947: step 1862, loss 0.139135, acc 0.980392, learning_rate 0.000102424
2017-10-10T14:58:54.586543: step 1863, loss 0.0989432, acc 0.953125, learning_rate 0.000102414
2017-10-10T14:58:54.882407: step 1864, loss 0.196903, acc 0.9375, learning_rate 0.000102404
2017-10-10T14:58:55.181016: step 1865, loss 0.0625866, acc 0.984375, learning_rate 0.000102394
2017-10-10T14:58:55.430273: step 1866, loss 0.134231, acc 0.984375, learning_rate 0.000102384
2017-10-10T14:58:55.723453: step 1867, loss 0.145703, acc 0.953125, learning_rate 0.000102375
2017-10-10T14:58:56.008483: step 1868, loss 0.091493, acc 0.984375, learning_rate 0.000102365
2017-10-10T14:58:56.296055: step 1869, loss 0.209987, acc 0.96875, learning_rate 0.000102355
2017-10-10T14:58:56.624022: step 1870, loss 0.126379, acc 0.984375, learning_rate 0.000102346
2017-10-10T14:58:56.909341: step 1871, loss 0.201078, acc 0.9375, learning_rate 0.000102336
2017-10-10T14:58:57.196790: step 1872, loss 0.210083, acc 0.875, learning_rate 0.000102327
2017-10-10T14:58:57.511091: step 1873, loss 0.0962394, acc 0.984375, learning_rate 0.000102317
2017-10-10T14:58:57.826317: step 1874, loss 0.148525, acc 0.953125, learning_rate 0.000102308
2017-10-10T14:58:58.088973: step 1875, loss 0.0636721, acc 1, learning_rate 0.000102298
2017-10-10T14:58:58.333166: step 1876, loss 0.168499, acc 0.953125, learning_rate 0.000102289
2017-10-10T14:58:58.642738: step 1877, loss 0.0863983, acc 0.96875, learning_rate 0.000102279
2017-10-10T14:58:58.914161: step 1878, loss 0.197803, acc 0.9375, learning_rate 0.00010227
2017-10-10T14:58:59.176604: step 1879, loss 0.0668941, acc 1, learning_rate 0.000102261
2017-10-10T14:58:59.457152: step 1880, loss 0.184446, acc 0.953125, learning_rate 0.000102252

Evaluation:
2017-10-10T14:58:59.920986: step 1880, loss 0.234214, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1880

2017-10-10T14:59:01.043004: step 1881, loss 0.0816582, acc 0.96875, learning_rate 0.000102242
2017-10-10T14:59:01.304917: step 1882, loss 0.151862, acc 0.9375, learning_rate 0.000102233
2017-10-10T14:59:01.591660: step 1883, loss 0.106579, acc 0.96875, learning_rate 0.000102224
2017-10-10T14:59:01.875604: step 1884, loss 0.189398, acc 0.90625, learning_rate 0.000102215
2017-10-10T14:59:02.127396: step 1885, loss 0.0953462, acc 0.96875, learning_rate 0.000102206
2017-10-10T14:59:02.440878: step 1886, loss 0.139419, acc 0.96875, learning_rate 0.000102197
2017-10-10T14:59:02.702304: step 1887, loss 0.109799, acc 0.953125, learning_rate 0.000102188
2017-10-10T14:59:02.990806: step 1888, loss 0.230322, acc 0.890625, learning_rate 0.000102179
2017-10-10T14:59:03.228627: step 1889, loss 0.140021, acc 0.953125, learning_rate 0.00010217
2017-10-10T14:59:03.464995: step 1890, loss 0.199219, acc 0.953125, learning_rate 0.000102161
2017-10-10T14:59:03.652960: step 1891, loss 0.139759, acc 0.96875, learning_rate 0.000102153
2017-10-10T14:59:03.884886: step 1892, loss 0.157019, acc 0.953125, learning_rate 0.000102144
2017-10-10T14:59:04.196796: step 1893, loss 0.194904, acc 0.96875, learning_rate 0.000102135
2017-10-10T14:59:04.463840: step 1894, loss 0.124905, acc 0.953125, learning_rate 0.000102126
2017-10-10T14:59:04.662343: step 1895, loss 0.192579, acc 0.9375, learning_rate 0.000102118
2017-10-10T14:59:04.931885: step 1896, loss 0.0873521, acc 0.96875, learning_rate 0.000102109
2017-10-10T14:59:05.792807: step 1897, loss 0.220176, acc 0.921875, learning_rate 0.0001021
2017-10-10T14:59:06.120687: step 1898, loss 0.151859, acc 0.9375, learning_rate 0.000102092
2017-10-10T14:59:06.386222: step 1899, loss 0.189172, acc 0.921875, learning_rate 0.000102083
2017-10-10T14:59:06.638435: step 1900, loss 0.266016, acc 0.921875, learning_rate 0.000102075
2017-10-10T14:59:06.860102: step 1901, loss 0.195947, acc 0.90625, learning_rate 0.000102066
2017-10-10T14:59:07.172875: step 1902, loss 0.126456, acc 0.96875, learning_rate 0.000102058
2017-10-10T14:59:07.441238: step 1903, loss 0.127385, acc 0.96875, learning_rate 0.00010205
2017-10-10T14:59:07.699920: step 1904, loss 0.119195, acc 0.96875, learning_rate 0.000102041
2017-10-10T14:59:07.978954: step 1905, loss 0.196119, acc 0.953125, learning_rate 0.000102033
2017-10-10T14:59:08.281088: step 1906, loss 0.125024, acc 0.96875, learning_rate 0.000102025
2017-10-10T14:59:08.556872: step 1907, loss 0.11976, acc 0.921875, learning_rate 0.000102016
2017-10-10T14:59:08.834316: step 1908, loss 0.174983, acc 0.9375, learning_rate 0.000102008
2017-10-10T14:59:09.120458: step 1909, loss 0.104351, acc 0.984375, learning_rate 0.000102
2017-10-10T14:59:09.394639: step 1910, loss 0.108004, acc 0.984375, learning_rate 0.000101992
2017-10-10T14:59:09.726873: step 1911, loss 0.195847, acc 0.9375, learning_rate 0.000101984
2017-10-10T14:59:09.985526: step 1912, loss 0.0904899, acc 0.96875, learning_rate 0.000101975
2017-10-10T14:59:10.199933: step 1913, loss 0.166972, acc 0.953125, learning_rate 0.000101967
2017-10-10T14:59:10.525306: step 1914, loss 0.0984148, acc 0.96875, learning_rate 0.000101959
2017-10-10T14:59:10.804905: step 1915, loss 0.147956, acc 0.96875, learning_rate 0.000101951
2017-10-10T14:59:11.095783: step 1916, loss 0.149798, acc 0.953125, learning_rate 0.000101943
2017-10-10T14:59:11.383772: step 1917, loss 0.0906709, acc 0.984375, learning_rate 0.000101935
2017-10-10T14:59:11.714143: step 1918, loss 0.17433, acc 0.921875, learning_rate 0.000101928
2017-10-10T14:59:11.916900: step 1919, loss 0.112757, acc 0.953125, learning_rate 0.00010192
2017-10-10T14:59:12.180851: step 1920, loss 0.321886, acc 0.875, learning_rate 0.000101912

Evaluation:
2017-10-10T14:59:12.630133: step 1920, loss 0.23346, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1920

2017-10-10T14:59:13.808877: step 1921, loss 0.103264, acc 0.953125, learning_rate 0.000101904
2017-10-10T14:59:14.076988: step 1922, loss 0.179305, acc 0.9375, learning_rate 0.000101896
2017-10-10T14:59:14.354242: step 1923, loss 0.232568, acc 0.90625, learning_rate 0.000101889
2017-10-10T14:59:14.664907: step 1924, loss 0.240598, acc 0.90625, learning_rate 0.000101881
2017-10-10T14:59:15.011576: step 1925, loss 0.131026, acc 0.96875, learning_rate 0.000101873
2017-10-10T14:59:15.320137: step 1926, loss 0.163007, acc 0.96875, learning_rate 0.000101865
2017-10-10T14:59:15.641728: step 1927, loss 0.230957, acc 0.890625, learning_rate 0.000101858
2017-10-10T14:59:15.936830: step 1928, loss 0.0987347, acc 0.984375, learning_rate 0.00010185
2017-10-10T14:59:16.192968: step 1929, loss 0.0830309, acc 0.984375, learning_rate 0.000101843
2017-10-10T14:59:16.498428: step 1930, loss 0.0737372, acc 0.984375, learning_rate 0.000101835
2017-10-10T14:59:16.794423: step 1931, loss 0.0753072, acc 0.984375, learning_rate 0.000101828
2017-10-10T14:59:17.089223: step 1932, loss 0.203941, acc 0.9375, learning_rate 0.00010182
2017-10-10T14:59:17.368840: step 1933, loss 0.183899, acc 0.90625, learning_rate 0.000101813
2017-10-10T14:59:17.676826: step 1934, loss 0.142506, acc 0.953125, learning_rate 0.000101805
2017-10-10T14:59:17.849400: step 1935, loss 0.273549, acc 0.90625, learning_rate 0.000101798
2017-10-10T14:59:18.144838: step 1936, loss 0.110385, acc 0.984375, learning_rate 0.000101791
2017-10-10T14:59:18.299973: step 1937, loss 0.112093, acc 0.984375, learning_rate 0.000101783
2017-10-10T14:59:18.614417: step 1938, loss 0.0742994, acc 0.984375, learning_rate 0.000101776
2017-10-10T14:59:18.912858: step 1939, loss 0.198231, acc 0.9375, learning_rate 0.000101769
2017-10-10T14:59:19.305017: step 1940, loss 0.182901, acc 0.953125, learning_rate 0.000101762
2017-10-10T14:59:19.584863: step 1941, loss 0.0928335, acc 0.953125, learning_rate 0.000101754
2017-10-10T14:59:19.839937: step 1942, loss 0.158118, acc 0.9375, learning_rate 0.000101747
2017-10-10T14:59:20.117168: step 1943, loss 0.126331, acc 0.953125, learning_rate 0.00010174
2017-10-10T14:59:20.484875: step 1944, loss 0.172861, acc 0.921875, learning_rate 0.000101733
2017-10-10T14:59:20.739923: step 1945, loss 0.121023, acc 0.9375, learning_rate 0.000101726
2017-10-10T14:59:21.025982: step 1946, loss 0.210524, acc 0.90625, learning_rate 0.000101719
2017-10-10T14:59:21.192973: step 1947, loss 0.202321, acc 0.90625, learning_rate 0.000101712
2017-10-10T14:59:21.499658: step 1948, loss 0.222082, acc 0.890625, learning_rate 0.000101705
2017-10-10T14:59:21.787070: step 1949, loss 0.241811, acc 0.921875, learning_rate 0.000101698
2017-10-10T14:59:21.996988: step 1950, loss 0.159884, acc 0.96875, learning_rate 0.000101691
2017-10-10T14:59:22.283968: step 1951, loss 0.118519, acc 0.984375, learning_rate 0.000101684
2017-10-10T14:59:22.571201: step 1952, loss 0.181998, acc 0.953125, learning_rate 0.000101677
2017-10-10T14:59:22.858120: step 1953, loss 0.107839, acc 0.9375, learning_rate 0.00010167
2017-10-10T14:59:23.155472: step 1954, loss 0.141118, acc 0.953125, learning_rate 0.000101664
2017-10-10T14:59:23.436925: step 1955, loss 0.183524, acc 0.921875, learning_rate 0.000101657
2017-10-10T14:59:23.703165: step 1956, loss 0.140731, acc 0.9375, learning_rate 0.00010165
2017-10-10T14:59:23.905574: step 1957, loss 0.129572, acc 0.9375, learning_rate 0.000101643
2017-10-10T14:59:24.156069: step 1958, loss 0.129862, acc 0.953125, learning_rate 0.000101637
2017-10-10T14:59:24.466004: step 1959, loss 0.151609, acc 0.96875, learning_rate 0.00010163
2017-10-10T14:59:24.724834: step 1960, loss 0.191518, acc 0.941176, learning_rate 0.000101623

Evaluation:
2017-10-10T14:59:25.152924: step 1960, loss 0.232905, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-1960

2017-10-10T14:59:26.272891: step 1961, loss 0.0353378, acc 1, learning_rate 0.000101617
2017-10-10T14:59:26.540621: step 1962, loss 0.19511, acc 0.953125, learning_rate 0.00010161
2017-10-10T14:59:26.783585: step 1963, loss 0.205313, acc 0.9375, learning_rate 0.000101604
2017-10-10T14:59:27.068860: step 1964, loss 0.192602, acc 0.953125, learning_rate 0.000101597
2017-10-10T14:59:27.337096: step 1965, loss 0.23409, acc 0.921875, learning_rate 0.00010159
2017-10-10T14:59:27.596992: step 1966, loss 0.144331, acc 0.96875, learning_rate 0.000101584
2017-10-10T14:59:27.850122: step 1967, loss 0.186921, acc 0.9375, learning_rate 0.000101577
2017-10-10T14:59:28.101096: step 1968, loss 0.106628, acc 0.96875, learning_rate 0.000101571
2017-10-10T14:59:28.356927: step 1969, loss 0.153759, acc 0.9375, learning_rate 0.000101565
2017-10-10T14:59:28.657713: step 1970, loss 0.113895, acc 0.953125, learning_rate 0.000101558
2017-10-10T14:59:29.029138: step 1971, loss 0.172404, acc 0.9375, learning_rate 0.000101552
2017-10-10T14:59:29.312922: step 1972, loss 0.162723, acc 0.921875, learning_rate 0.000101546
2017-10-10T14:59:29.529567: step 1973, loss 0.246203, acc 0.921875, learning_rate 0.000101539
2017-10-10T14:59:29.820955: step 1974, loss 0.0780844, acc 0.96875, learning_rate 0.000101533
2017-10-10T14:59:30.112939: step 1975, loss 0.230356, acc 0.9375, learning_rate 0.000101527
2017-10-10T14:59:30.351142: step 1976, loss 0.187299, acc 0.9375, learning_rate 0.00010152
2017-10-10T14:59:30.598429: step 1977, loss 0.126012, acc 0.953125, learning_rate 0.000101514
2017-10-10T14:59:30.875210: step 1978, loss 0.0905327, acc 0.96875, learning_rate 0.000101508
2017-10-10T14:59:31.133049: step 1979, loss 0.142378, acc 0.96875, learning_rate 0.000101502
2017-10-10T14:59:31.443301: step 1980, loss 0.190495, acc 0.96875, learning_rate 0.000101496
2017-10-10T14:59:31.739711: step 1981, loss 0.162396, acc 0.921875, learning_rate 0.00010149
2017-10-10T14:59:31.987705: step 1982, loss 0.0755593, acc 0.984375, learning_rate 0.000101484
2017-10-10T14:59:32.223992: step 1983, loss 0.146401, acc 0.953125, learning_rate 0.000101478
2017-10-10T14:59:32.527985: step 1984, loss 0.208618, acc 0.90625, learning_rate 0.000101472
2017-10-10T14:59:32.867952: step 1985, loss 0.140359, acc 0.921875, learning_rate 0.000101466
2017-10-10T14:59:33.076499: step 1986, loss 0.140412, acc 0.96875, learning_rate 0.00010146
2017-10-10T14:59:33.335791: step 1987, loss 0.0819848, acc 0.984375, learning_rate 0.000101454
2017-10-10T14:59:33.526366: step 1988, loss 0.189949, acc 0.9375, learning_rate 0.000101448
2017-10-10T14:59:33.768860: step 1989, loss 0.182856, acc 0.921875, learning_rate 0.000101442
2017-10-10T14:59:34.033127: step 1990, loss 0.0970312, acc 0.96875, learning_rate 0.000101436
2017-10-10T14:59:34.259291: step 1991, loss 0.12607, acc 0.953125, learning_rate 0.00010143
2017-10-10T14:59:34.544980: step 1992, loss 0.1057, acc 0.96875, learning_rate 0.000101424
2017-10-10T14:59:34.916749: step 1993, loss 0.144661, acc 0.96875, learning_rate 0.000101418
2017-10-10T14:59:35.164951: step 1994, loss 0.265595, acc 0.921875, learning_rate 0.000101413
2017-10-10T14:59:35.406494: step 1995, loss 0.10544, acc 0.96875, learning_rate 0.000101407
2017-10-10T14:59:35.661324: step 1996, loss 0.11155, acc 0.96875, learning_rate 0.000101401
2017-10-10T14:59:35.940173: step 1997, loss 0.173993, acc 0.953125, learning_rate 0.000101395
2017-10-10T14:59:36.242381: step 1998, loss 0.14499, acc 0.953125, learning_rate 0.00010139
2017-10-10T14:59:36.508841: step 1999, loss 0.0834018, acc 1, learning_rate 0.000101384
2017-10-10T14:59:36.783117: step 2000, loss 0.189193, acc 0.953125, learning_rate 0.000101378

Evaluation:
2017-10-10T14:59:37.279878: step 2000, loss 0.234025, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2000

2017-10-10T14:59:38.201397: step 2001, loss 0.119773, acc 0.984375, learning_rate 0.000101373
2017-10-10T14:59:38.504098: step 2002, loss 0.140011, acc 0.953125, learning_rate 0.000101367
2017-10-10T14:59:38.775490: step 2003, loss 0.163376, acc 0.9375, learning_rate 0.000101362
2017-10-10T14:59:39.048856: step 2004, loss 0.214444, acc 0.90625, learning_rate 0.000101356
2017-10-10T14:59:39.355496: step 2005, loss 0.244136, acc 0.9375, learning_rate 0.00010135
2017-10-10T14:59:39.589248: step 2006, loss 0.12217, acc 0.953125, learning_rate 0.000101345
2017-10-10T14:59:39.897072: step 2007, loss 0.169128, acc 0.9375, learning_rate 0.000101339
2017-10-10T14:59:40.220840: step 2008, loss 0.231557, acc 0.953125, learning_rate 0.000101334
2017-10-10T14:59:40.527191: step 2009, loss 0.184358, acc 0.9375, learning_rate 0.000101328
2017-10-10T14:59:40.812997: step 2010, loss 0.288533, acc 0.890625, learning_rate 0.000101323
2017-10-10T14:59:41.091509: step 2011, loss 0.200164, acc 0.90625, learning_rate 0.000101318
2017-10-10T14:59:41.332992: step 2012, loss 0.148004, acc 0.9375, learning_rate 0.000101312
2017-10-10T14:59:41.540891: step 2013, loss 0.236522, acc 0.953125, learning_rate 0.000101307
2017-10-10T14:59:41.812042: step 2014, loss 0.0612528, acc 1, learning_rate 0.000101302
2017-10-10T14:59:42.080871: step 2015, loss 0.0857557, acc 0.984375, learning_rate 0.000101296
2017-10-10T14:59:42.312897: step 2016, loss 0.148312, acc 0.953125, learning_rate 0.000101291
2017-10-10T14:59:42.576907: step 2017, loss 0.162963, acc 0.96875, learning_rate 0.000101286
2017-10-10T14:59:42.909797: step 2018, loss 0.175755, acc 0.921875, learning_rate 0.00010128
2017-10-10T14:59:43.152922: step 2019, loss 0.143015, acc 0.953125, learning_rate 0.000101275
2017-10-10T14:59:43.436524: step 2020, loss 0.0936797, acc 0.984375, learning_rate 0.00010127
2017-10-10T14:59:43.670757: step 2021, loss 0.257345, acc 0.9375, learning_rate 0.000101265
2017-10-10T14:59:43.976888: step 2022, loss 0.164319, acc 0.953125, learning_rate 0.00010126
2017-10-10T14:59:44.257614: step 2023, loss 0.0848042, acc 1, learning_rate 0.000101255
2017-10-10T14:59:44.538626: step 2024, loss 0.128151, acc 0.953125, learning_rate 0.000101249
2017-10-10T14:59:44.807764: step 2025, loss 0.121643, acc 0.953125, learning_rate 0.000101244
2017-10-10T14:59:45.113901: step 2026, loss 0.112208, acc 0.96875, learning_rate 0.000101239
2017-10-10T14:59:45.448962: step 2027, loss 0.183833, acc 0.921875, learning_rate 0.000101234
2017-10-10T14:59:45.717305: step 2028, loss 0.219862, acc 0.96875, learning_rate 0.000101229
2017-10-10T14:59:46.024620: step 2029, loss 0.175597, acc 0.9375, learning_rate 0.000101224
2017-10-10T14:59:46.270107: step 2030, loss 0.126814, acc 0.96875, learning_rate 0.000101219
2017-10-10T14:59:46.496897: step 2031, loss 0.120182, acc 0.984375, learning_rate 0.000101214
2017-10-10T14:59:46.768204: step 2032, loss 0.0924308, acc 0.96875, learning_rate 0.000101209
2017-10-10T14:59:47.054749: step 2033, loss 0.089417, acc 0.96875, learning_rate 0.000101204
2017-10-10T14:59:47.321012: step 2034, loss 0.261005, acc 0.921875, learning_rate 0.000101199
2017-10-10T14:59:47.584750: step 2035, loss 0.0978553, acc 0.96875, learning_rate 0.000101194
2017-10-10T14:59:47.892904: step 2036, loss 0.119349, acc 0.953125, learning_rate 0.00010119
2017-10-10T14:59:48.216503: step 2037, loss 0.132945, acc 0.953125, learning_rate 0.000101185
2017-10-10T14:59:48.509637: step 2038, loss 0.176126, acc 0.96875, learning_rate 0.00010118
2017-10-10T14:59:48.781356: step 2039, loss 0.130422, acc 0.96875, learning_rate 0.000101175
2017-10-10T14:59:49.080035: step 2040, loss 0.167424, acc 0.96875, learning_rate 0.00010117

Evaluation:
2017-10-10T14:59:49.512797: step 2040, loss 0.232846, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2040

2017-10-10T14:59:50.572860: step 2041, loss 0.235777, acc 0.9375, learning_rate 0.000101166
2017-10-10T14:59:50.896925: step 2042, loss 0.176925, acc 0.953125, learning_rate 0.000101161
2017-10-10T14:59:51.151478: step 2043, loss 0.14039, acc 0.96875, learning_rate 0.000101156
2017-10-10T14:59:51.367654: step 2044, loss 0.11838, acc 0.96875, learning_rate 0.000101151
2017-10-10T14:59:51.655907: step 2045, loss 0.143534, acc 0.953125, learning_rate 0.000101147
2017-10-10T14:59:51.964788: step 2046, loss 0.139743, acc 0.953125, learning_rate 0.000101142
2017-10-10T14:59:52.231092: step 2047, loss 0.189264, acc 0.921875, learning_rate 0.000101137
2017-10-10T14:59:52.464216: step 2048, loss 0.0883226, acc 0.984375, learning_rate 0.000101133
2017-10-10T14:59:52.681226: step 2049, loss 0.102973, acc 0.984375, learning_rate 0.000101128
2017-10-10T14:59:52.957575: step 2050, loss 0.117463, acc 0.96875, learning_rate 0.000101123
2017-10-10T14:59:53.244625: step 2051, loss 0.118772, acc 0.953125, learning_rate 0.000101119
2017-10-10T14:59:53.500408: step 2052, loss 0.0789336, acc 0.96875, learning_rate 0.000101114
2017-10-10T14:59:53.736862: step 2053, loss 0.11044, acc 0.96875, learning_rate 0.00010111
2017-10-10T14:59:53.976860: step 2054, loss 0.0956704, acc 0.984375, learning_rate 0.000101105
2017-10-10T14:59:54.252913: step 2055, loss 0.216272, acc 0.9375, learning_rate 0.000101101
2017-10-10T14:59:54.548037: step 2056, loss 0.104292, acc 0.984375, learning_rate 0.000101096
2017-10-10T14:59:54.792131: step 2057, loss 0.0921408, acc 0.984375, learning_rate 0.000101092
2017-10-10T14:59:55.041021: step 2058, loss 0.330647, acc 0.882353, learning_rate 0.000101087
2017-10-10T14:59:55.352838: step 2059, loss 0.112432, acc 0.953125, learning_rate 0.000101083
2017-10-10T14:59:55.680850: step 2060, loss 0.0931128, acc 0.96875, learning_rate 0.000101078
2017-10-10T14:59:55.917224: step 2061, loss 0.249901, acc 0.921875, learning_rate 0.000101074
2017-10-10T14:59:56.202436: step 2062, loss 0.241651, acc 0.921875, learning_rate 0.00010107
2017-10-10T14:59:56.515423: step 2063, loss 0.113479, acc 0.953125, learning_rate 0.000101065
2017-10-10T14:59:56.721375: step 2064, loss 0.0579715, acc 1, learning_rate 0.000101061
2017-10-10T14:59:57.008912: step 2065, loss 0.146529, acc 0.953125, learning_rate 0.000101057
2017-10-10T14:59:57.328884: step 2066, loss 0.186624, acc 0.9375, learning_rate 0.000101052
2017-10-10T14:59:57.592859: step 2067, loss 0.173628, acc 0.921875, learning_rate 0.000101048
2017-10-10T14:59:57.877035: step 2068, loss 0.0587478, acc 1, learning_rate 0.000101044
2017-10-10T14:59:58.126300: step 2069, loss 0.0897538, acc 0.984375, learning_rate 0.000101039
2017-10-10T14:59:58.431327: step 2070, loss 0.108698, acc 0.953125, learning_rate 0.000101035
2017-10-10T14:59:58.748897: step 2071, loss 0.25079, acc 0.953125, learning_rate 0.000101031
2017-10-10T14:59:59.101172: step 2072, loss 0.154635, acc 0.953125, learning_rate 0.000101027
2017-10-10T14:59:59.364042: step 2073, loss 0.112863, acc 0.984375, learning_rate 0.000101023
2017-10-10T14:59:59.607680: step 2074, loss 0.242439, acc 0.9375, learning_rate 0.000101018
2017-10-10T14:59:59.832838: step 2075, loss 0.122392, acc 0.96875, learning_rate 0.000101014
2017-10-10T15:00:00.076853: step 2076, loss 0.137438, acc 0.96875, learning_rate 0.00010101
2017-10-10T15:00:00.388930: step 2077, loss 0.10264, acc 0.96875, learning_rate 0.000101006
2017-10-10T15:00:00.621800: step 2078, loss 0.271383, acc 0.921875, learning_rate 0.000101002
2017-10-10T15:00:00.920789: step 2079, loss 0.168143, acc 0.953125, learning_rate 0.000100998
2017-10-10T15:00:01.146013: step 2080, loss 0.262544, acc 0.890625, learning_rate 0.000100994

Evaluation:
2017-10-10T15:00:01.521186: step 2080, loss 0.231596, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2080

2017-10-10T15:00:02.658308: step 2081, loss 0.0979596, acc 0.953125, learning_rate 0.00010099
2017-10-10T15:00:02.907965: step 2082, loss 0.183465, acc 0.90625, learning_rate 0.000100986
2017-10-10T15:00:03.144095: step 2083, loss 0.177945, acc 0.9375, learning_rate 0.000100982
2017-10-10T15:00:03.407154: step 2084, loss 0.118354, acc 0.96875, learning_rate 0.000100978
2017-10-10T15:00:03.695868: step 2085, loss 0.162223, acc 0.953125, learning_rate 0.000100974
2017-10-10T15:00:03.994728: step 2086, loss 0.185658, acc 0.953125, learning_rate 0.00010097
2017-10-10T15:00:04.268539: step 2087, loss 0.165211, acc 0.9375, learning_rate 0.000100966
2017-10-10T15:00:04.582236: step 2088, loss 0.133136, acc 0.9375, learning_rate 0.000100962
2017-10-10T15:00:04.861816: step 2089, loss 0.223849, acc 0.9375, learning_rate 0.000100958
2017-10-10T15:00:05.136549: step 2090, loss 0.0982761, acc 0.984375, learning_rate 0.000100954
2017-10-10T15:00:05.393839: step 2091, loss 0.19198, acc 0.921875, learning_rate 0.00010095
2017-10-10T15:00:05.720166: step 2092, loss 0.170863, acc 0.9375, learning_rate 0.000100946
2017-10-10T15:00:06.032987: step 2093, loss 0.1134, acc 0.96875, learning_rate 0.000100942
2017-10-10T15:00:06.305152: step 2094, loss 0.176301, acc 0.953125, learning_rate 0.000100938
2017-10-10T15:00:06.584912: step 2095, loss 0.187764, acc 0.9375, learning_rate 0.000100935
2017-10-10T15:00:06.877076: step 2096, loss 0.0515547, acc 1, learning_rate 0.000100931
2017-10-10T15:00:07.213097: step 2097, loss 0.156478, acc 0.953125, learning_rate 0.000100927
2017-10-10T15:00:07.512601: step 2098, loss 0.250116, acc 0.90625, learning_rate 0.000100923
2017-10-10T15:00:07.787367: step 2099, loss 0.147953, acc 0.953125, learning_rate 0.000100919
2017-10-10T15:00:08.093072: step 2100, loss 0.211932, acc 0.90625, learning_rate 0.000100916
2017-10-10T15:00:08.402517: step 2101, loss 0.137029, acc 0.96875, learning_rate 0.000100912
2017-10-10T15:00:08.705594: step 2102, loss 0.0602447, acc 0.96875, learning_rate 0.000100908
2017-10-10T15:00:08.960450: step 2103, loss 0.199379, acc 0.90625, learning_rate 0.000100904
2017-10-10T15:00:09.192807: step 2104, loss 0.102742, acc 0.9375, learning_rate 0.000100901
2017-10-10T15:00:09.438145: step 2105, loss 0.171948, acc 0.9375, learning_rate 0.000100897
2017-10-10T15:00:09.680372: step 2106, loss 0.111633, acc 0.984375, learning_rate 0.000100893
2017-10-10T15:00:09.994618: step 2107, loss 0.15844, acc 0.921875, learning_rate 0.00010089
2017-10-10T15:00:10.300054: step 2108, loss 0.193752, acc 0.921875, learning_rate 0.000100886
2017-10-10T15:00:10.500558: step 2109, loss 0.0692548, acc 1, learning_rate 0.000100883
2017-10-10T15:00:11.460988: step 2110, loss 0.112699, acc 0.96875, learning_rate 0.000100879
2017-10-10T15:00:11.733851: step 2111, loss 0.161661, acc 0.96875, learning_rate 0.000100875
2017-10-10T15:00:11.980936: step 2112, loss 0.251077, acc 0.921875, learning_rate 0.000100872
2017-10-10T15:00:12.237062: step 2113, loss 0.204468, acc 0.9375, learning_rate 0.000100868
2017-10-10T15:00:12.493154: step 2114, loss 0.32967, acc 0.875, learning_rate 0.000100865
2017-10-10T15:00:12.760865: step 2115, loss 0.129349, acc 0.953125, learning_rate 0.000100861
2017-10-10T15:00:13.120941: step 2116, loss 0.0796096, acc 0.984375, learning_rate 0.000100858
2017-10-10T15:00:13.376843: step 2117, loss 0.100349, acc 0.96875, learning_rate 0.000100854
2017-10-10T15:00:13.628857: step 2118, loss 0.164569, acc 0.921875, learning_rate 0.000100851
2017-10-10T15:00:13.893398: step 2119, loss 0.187316, acc 0.9375, learning_rate 0.000100847
2017-10-10T15:00:14.173136: step 2120, loss 0.223877, acc 0.921875, learning_rate 0.000100844

Evaluation:
2017-10-10T15:00:14.761261: step 2120, loss 0.233276, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2120

2017-10-10T15:00:15.801785: step 2121, loss 0.0993642, acc 0.953125, learning_rate 0.00010084
2017-10-10T15:00:16.139103: step 2122, loss 0.249872, acc 0.90625, learning_rate 0.000100837
2017-10-10T15:00:16.464865: step 2123, loss 0.0891859, acc 0.96875, learning_rate 0.000100833
2017-10-10T15:00:16.709411: step 2124, loss 0.0984449, acc 0.96875, learning_rate 0.00010083
2017-10-10T15:00:17.014449: step 2125, loss 0.044184, acc 0.984375, learning_rate 0.000100827
2017-10-10T15:00:17.377141: step 2126, loss 0.209788, acc 0.921875, learning_rate 0.000100823
2017-10-10T15:00:17.636838: step 2127, loss 0.152492, acc 0.9375, learning_rate 0.00010082
2017-10-10T15:00:17.920488: step 2128, loss 0.0953884, acc 0.984375, learning_rate 0.000100817
2017-10-10T15:00:18.116998: step 2129, loss 0.137496, acc 0.984375, learning_rate 0.000100813
2017-10-10T15:00:18.354822: step 2130, loss 0.164099, acc 0.9375, learning_rate 0.00010081
2017-10-10T15:00:18.624848: step 2131, loss 0.0912873, acc 0.96875, learning_rate 0.000100807
2017-10-10T15:00:18.910004: step 2132, loss 0.129793, acc 0.953125, learning_rate 0.000100803
2017-10-10T15:00:19.145612: step 2133, loss 0.128449, acc 0.953125, learning_rate 0.0001008
2017-10-10T15:00:19.435768: step 2134, loss 0.113489, acc 0.953125, learning_rate 0.000100797
2017-10-10T15:00:19.716987: step 2135, loss 0.0579741, acc 0.984375, learning_rate 0.000100793
2017-10-10T15:00:19.998180: step 2136, loss 0.104059, acc 0.984375, learning_rate 0.00010079
2017-10-10T15:00:20.317657: step 2137, loss 0.168427, acc 0.9375, learning_rate 0.000100787
2017-10-10T15:00:20.546916: step 2138, loss 0.114164, acc 0.9375, learning_rate 0.000100784
2017-10-10T15:00:20.828887: step 2139, loss 0.0965557, acc 0.953125, learning_rate 0.000100781
2017-10-10T15:00:21.100564: step 2140, loss 0.18276, acc 0.9375, learning_rate 0.000100777
2017-10-10T15:00:21.392333: step 2141, loss 0.120186, acc 0.9375, learning_rate 0.000100774
2017-10-10T15:00:21.660266: step 2142, loss 0.0747519, acc 0.984375, learning_rate 0.000100771
2017-10-10T15:00:21.957774: step 2143, loss 0.129976, acc 0.984375, learning_rate 0.000100768
2017-10-10T15:00:22.277023: step 2144, loss 0.0967602, acc 0.96875, learning_rate 0.000100765
2017-10-10T15:00:22.505153: step 2145, loss 0.144401, acc 0.96875, learning_rate 0.000100762
2017-10-10T15:00:22.792204: step 2146, loss 0.111544, acc 0.96875, learning_rate 0.000100759
2017-10-10T15:00:23.058091: step 2147, loss 0.0811288, acc 0.984375, learning_rate 0.000100755
2017-10-10T15:00:23.296852: step 2148, loss 0.0968001, acc 0.96875, learning_rate 0.000100752
2017-10-10T15:00:23.580833: step 2149, loss 0.142091, acc 0.9375, learning_rate 0.000100749
2017-10-10T15:00:23.891345: step 2150, loss 0.16476, acc 0.9375, learning_rate 0.000100746
2017-10-10T15:00:24.148903: step 2151, loss 0.268431, acc 0.9375, learning_rate 0.000100743
2017-10-10T15:00:24.408864: step 2152, loss 0.26239, acc 0.921875, learning_rate 0.00010074
2017-10-10T15:00:24.710546: step 2153, loss 0.181355, acc 0.921875, learning_rate 0.000100737
2017-10-10T15:00:24.981543: step 2154, loss 0.271068, acc 0.90625, learning_rate 0.000100734
2017-10-10T15:00:25.237464: step 2155, loss 0.162835, acc 0.953125, learning_rate 0.000100731
2017-10-10T15:00:25.504928: step 2156, loss 0.226099, acc 0.941176, learning_rate 0.000100728
2017-10-10T15:00:25.808189: step 2157, loss 0.123911, acc 0.9375, learning_rate 0.000100725
2017-10-10T15:00:26.107098: step 2158, loss 0.0713421, acc 0.984375, learning_rate 0.000100722
2017-10-10T15:00:26.281363: step 2159, loss 0.206047, acc 0.90625, learning_rate 0.000100719
2017-10-10T15:00:26.484117: step 2160, loss 0.134094, acc 0.9375, learning_rate 0.000100716

Evaluation:
2017-10-10T15:00:26.946881: step 2160, loss 0.233128, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2160

2017-10-10T15:00:27.872267: step 2161, loss 0.0987779, acc 1, learning_rate 0.000100713
2017-10-10T15:00:28.122642: step 2162, loss 0.1367, acc 0.953125, learning_rate 0.000100711
2017-10-10T15:00:28.423401: step 2163, loss 0.0799482, acc 1, learning_rate 0.000100708
2017-10-10T15:00:28.725998: step 2164, loss 0.246807, acc 0.921875, learning_rate 0.000100705
2017-10-10T15:00:28.981558: step 2165, loss 0.157705, acc 0.953125, learning_rate 0.000100702
2017-10-10T15:00:29.249273: step 2166, loss 0.195813, acc 0.921875, learning_rate 0.000100699
2017-10-10T15:00:29.517129: step 2167, loss 0.166636, acc 0.9375, learning_rate 0.000100696
2017-10-10T15:00:29.764160: step 2168, loss 0.116831, acc 0.96875, learning_rate 0.000100693
2017-10-10T15:00:30.102971: step 2169, loss 0.141082, acc 0.9375, learning_rate 0.00010069
2017-10-10T15:00:30.388715: step 2170, loss 0.286673, acc 0.921875, learning_rate 0.000100688
2017-10-10T15:00:30.644894: step 2171, loss 0.145573, acc 0.953125, learning_rate 0.000100685
2017-10-10T15:00:30.908291: step 2172, loss 0.218443, acc 0.890625, learning_rate 0.000100682
2017-10-10T15:00:31.236414: step 2173, loss 0.165044, acc 0.9375, learning_rate 0.000100679
2017-10-10T15:00:31.532879: step 2174, loss 0.209029, acc 0.90625, learning_rate 0.000100677
2017-10-10T15:00:31.820865: step 2175, loss 0.0932518, acc 0.96875, learning_rate 0.000100674
2017-10-10T15:00:32.128932: step 2176, loss 0.143984, acc 0.9375, learning_rate 0.000100671
2017-10-10T15:00:32.368998: step 2177, loss 0.0992691, acc 0.96875, learning_rate 0.000100668
2017-10-10T15:00:32.651830: step 2178, loss 0.158535, acc 0.953125, learning_rate 0.000100666
2017-10-10T15:00:32.940825: step 2179, loss 0.22198, acc 0.90625, learning_rate 0.000100663
2017-10-10T15:00:33.188519: step 2180, loss 0.221091, acc 0.953125, learning_rate 0.00010066
2017-10-10T15:00:33.443566: step 2181, loss 0.243694, acc 0.921875, learning_rate 0.000100657
2017-10-10T15:00:33.793366: step 2182, loss 0.157092, acc 0.953125, learning_rate 0.000100655
2017-10-10T15:00:34.065009: step 2183, loss 0.0943491, acc 0.96875, learning_rate 0.000100652
2017-10-10T15:00:34.384631: step 2184, loss 0.121398, acc 0.9375, learning_rate 0.000100649
2017-10-10T15:00:34.673149: step 2185, loss 0.0790333, acc 0.984375, learning_rate 0.000100647
2017-10-10T15:00:34.972946: step 2186, loss 0.105382, acc 0.96875, learning_rate 0.000100644
2017-10-10T15:00:35.313048: step 2187, loss 0.140091, acc 0.9375, learning_rate 0.000100641
2017-10-10T15:00:35.592842: step 2188, loss 0.322213, acc 0.859375, learning_rate 0.000100639
2017-10-10T15:00:35.788886: step 2189, loss 0.0792152, acc 0.96875, learning_rate 0.000100636
2017-10-10T15:00:35.947408: step 2190, loss 0.0691579, acc 0.984375, learning_rate 0.000100634
2017-10-10T15:00:36.131017: step 2191, loss 0.289897, acc 0.859375, learning_rate 0.000100631
2017-10-10T15:00:36.382116: step 2192, loss 0.142561, acc 0.921875, learning_rate 0.000100628
2017-10-10T15:00:36.754990: step 2193, loss 0.282085, acc 0.90625, learning_rate 0.000100626
2017-10-10T15:00:37.022141: step 2194, loss 0.0876117, acc 0.96875, learning_rate 0.000100623
2017-10-10T15:00:37.269687: step 2195, loss 0.0658554, acc 0.984375, learning_rate 0.000100621
2017-10-10T15:00:37.524807: step 2196, loss 0.164651, acc 0.921875, learning_rate 0.000100618
2017-10-10T15:00:37.795814: step 2197, loss 0.102894, acc 0.96875, learning_rate 0.000100616
2017-10-10T15:00:38.053378: step 2198, loss 0.0687235, acc 0.96875, learning_rate 0.000100613
2017-10-10T15:00:38.368389: step 2199, loss 0.0910448, acc 1, learning_rate 0.000100611
2017-10-10T15:00:38.662229: step 2200, loss 0.246263, acc 0.90625, learning_rate 0.000100608

Evaluation:
2017-10-10T15:00:39.170395: step 2200, loss 0.232749, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2200

2017-10-10T15:00:40.180816: step 2201, loss 0.109486, acc 0.984375, learning_rate 0.000100606
2017-10-10T15:00:40.472562: step 2202, loss 0.269043, acc 0.9375, learning_rate 0.000100603
2017-10-10T15:00:40.780831: step 2203, loss 0.129923, acc 0.953125, learning_rate 0.000100601
2017-10-10T15:00:40.977038: step 2204, loss 0.102784, acc 0.96875, learning_rate 0.000100598
2017-10-10T15:00:41.288769: step 2205, loss 0.11046, acc 0.96875, learning_rate 0.000100596
2017-10-10T15:00:41.558637: step 2206, loss 0.0801717, acc 0.984375, learning_rate 0.000100594
2017-10-10T15:00:41.817036: step 2207, loss 0.089797, acc 0.984375, learning_rate 0.000100591
2017-10-10T15:00:42.105985: step 2208, loss 0.13564, acc 0.953125, learning_rate 0.000100589
2017-10-10T15:00:42.360129: step 2209, loss 0.102921, acc 0.984375, learning_rate 0.000100586
2017-10-10T15:00:42.665076: step 2210, loss 0.134399, acc 0.9375, learning_rate 0.000100584
2017-10-10T15:00:42.921050: step 2211, loss 0.0856599, acc 0.96875, learning_rate 0.000100581
2017-10-10T15:00:43.164384: step 2212, loss 0.11329, acc 0.96875, learning_rate 0.000100579
2017-10-10T15:00:43.461205: step 2213, loss 0.0975275, acc 0.96875, learning_rate 0.000100577
2017-10-10T15:00:43.703639: step 2214, loss 0.0816371, acc 0.96875, learning_rate 0.000100574
2017-10-10T15:00:43.911129: step 2215, loss 0.124634, acc 0.96875, learning_rate 0.000100572
2017-10-10T15:00:44.227109: step 2216, loss 0.169972, acc 0.9375, learning_rate 0.00010057
2017-10-10T15:00:44.535219: step 2217, loss 0.0843738, acc 0.96875, learning_rate 0.000100567
2017-10-10T15:00:44.808838: step 2218, loss 0.214805, acc 0.921875, learning_rate 0.000100565
2017-10-10T15:00:45.088763: step 2219, loss 0.163363, acc 0.9375, learning_rate 0.000100563
2017-10-10T15:00:45.404717: step 2220, loss 0.0539598, acc 1, learning_rate 0.00010056
2017-10-10T15:00:45.693445: step 2221, loss 0.0734969, acc 0.984375, learning_rate 0.000100558
2017-10-10T15:00:45.980950: step 2222, loss 0.156516, acc 0.953125, learning_rate 0.000100556
2017-10-10T15:00:46.292906: step 2223, loss 0.2375, acc 0.921875, learning_rate 0.000100554
2017-10-10T15:00:46.572544: step 2224, loss 0.22778, acc 0.90625, learning_rate 0.000100551
2017-10-10T15:00:46.845297: step 2225, loss 0.186852, acc 0.9375, learning_rate 0.000100549
2017-10-10T15:00:47.180948: step 2226, loss 0.126851, acc 0.9375, learning_rate 0.000100547
2017-10-10T15:00:47.537030: step 2227, loss 0.0926298, acc 0.984375, learning_rate 0.000100545
2017-10-10T15:00:47.767260: step 2228, loss 0.156574, acc 0.921875, learning_rate 0.000100542
2017-10-10T15:00:48.044976: step 2229, loss 0.0864459, acc 0.984375, learning_rate 0.00010054
2017-10-10T15:00:48.304905: step 2230, loss 0.121529, acc 0.953125, learning_rate 0.000100538
2017-10-10T15:00:48.588934: step 2231, loss 0.0965222, acc 0.984375, learning_rate 0.000100536
2017-10-10T15:00:48.853262: step 2232, loss 0.084988, acc 0.96875, learning_rate 0.000100534
2017-10-10T15:00:49.142396: step 2233, loss 0.149272, acc 0.953125, learning_rate 0.000100531
2017-10-10T15:00:49.421130: step 2234, loss 0.180265, acc 0.953125, learning_rate 0.000100529
2017-10-10T15:00:49.699152: step 2235, loss 0.187518, acc 0.9375, learning_rate 0.000100527
2017-10-10T15:00:49.989136: step 2236, loss 0.157258, acc 0.9375, learning_rate 0.000100525
2017-10-10T15:00:50.214686: step 2237, loss 0.174323, acc 0.921875, learning_rate 0.000100523
2017-10-10T15:00:50.483867: step 2238, loss 0.120832, acc 0.984375, learning_rate 0.000100521
2017-10-10T15:00:50.797175: step 2239, loss 0.147778, acc 0.9375, learning_rate 0.000100519
2017-10-10T15:00:51.064346: step 2240, loss 0.109936, acc 0.96875, learning_rate 0.000100516

Evaluation:
2017-10-10T15:00:51.534974: step 2240, loss 0.232753, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2240

2017-10-10T15:00:52.714125: step 2241, loss 0.209752, acc 0.921875, learning_rate 0.000100514
2017-10-10T15:00:53.024853: step 2242, loss 0.103785, acc 0.96875, learning_rate 0.000100512
2017-10-10T15:00:53.267807: step 2243, loss 0.14821, acc 0.953125, learning_rate 0.00010051
2017-10-10T15:00:53.524854: step 2244, loss 0.0681359, acc 1, learning_rate 0.000100508
2017-10-10T15:00:53.805128: step 2245, loss 0.140467, acc 0.984375, learning_rate 0.000100506
2017-10-10T15:00:54.069993: step 2246, loss 0.144351, acc 0.953125, learning_rate 0.000100504
2017-10-10T15:00:54.335404: step 2247, loss 0.110444, acc 0.953125, learning_rate 0.000100502
2017-10-10T15:00:54.592818: step 2248, loss 0.218522, acc 0.921875, learning_rate 0.0001005
2017-10-10T15:00:54.808489: step 2249, loss 0.139785, acc 0.96875, learning_rate 0.000100498
2017-10-10T15:00:55.076840: step 2250, loss 0.0879963, acc 0.984375, learning_rate 0.000100496
2017-10-10T15:00:55.359566: step 2251, loss 0.185321, acc 0.90625, learning_rate 0.000100494
2017-10-10T15:00:55.629703: step 2252, loss 0.122381, acc 0.9375, learning_rate 0.000100492
2017-10-10T15:00:55.937142: step 2253, loss 0.105385, acc 0.96875, learning_rate 0.00010049
2017-10-10T15:00:56.221169: step 2254, loss 0.187635, acc 0.941176, learning_rate 0.000100488
2017-10-10T15:00:56.489840: step 2255, loss 0.123364, acc 0.96875, learning_rate 0.000100486
2017-10-10T15:00:56.752545: step 2256, loss 0.254201, acc 0.921875, learning_rate 0.000100484
2017-10-10T15:00:57.034782: step 2257, loss 0.155691, acc 0.9375, learning_rate 0.000100482
2017-10-10T15:00:57.379297: step 2258, loss 0.127424, acc 0.953125, learning_rate 0.00010048
2017-10-10T15:00:57.661506: step 2259, loss 0.124483, acc 0.953125, learning_rate 0.000100478
2017-10-10T15:00:57.925133: step 2260, loss 0.0914182, acc 0.96875, learning_rate 0.000100476
2017-10-10T15:00:58.183543: step 2261, loss 0.0489612, acc 0.984375, learning_rate 0.000100474
2017-10-10T15:00:58.473975: step 2262, loss 0.191412, acc 0.921875, learning_rate 0.000100472
2017-10-10T15:00:58.764883: step 2263, loss 0.0738447, acc 0.984375, learning_rate 0.00010047
2017-10-10T15:00:59.020286: step 2264, loss 0.184077, acc 0.921875, learning_rate 0.000100468
2017-10-10T15:00:59.312857: step 2265, loss 0.202334, acc 0.90625, learning_rate 0.000100466
2017-10-10T15:00:59.660088: step 2266, loss 0.19008, acc 0.921875, learning_rate 0.000100464
2017-10-10T15:00:59.918063: step 2267, loss 0.108325, acc 0.984375, learning_rate 0.000100462
2017-10-10T15:01:00.156856: step 2268, loss 0.0984032, acc 0.96875, learning_rate 0.000100461
2017-10-10T15:01:00.428409: step 2269, loss 0.125815, acc 0.96875, learning_rate 0.000100459
2017-10-10T15:01:00.721203: step 2270, loss 0.20049, acc 0.953125, learning_rate 0.000100457
2017-10-10T15:01:00.995861: step 2271, loss 0.184135, acc 0.953125, learning_rate 0.000100455
2017-10-10T15:01:01.312841: step 2272, loss 0.0997402, acc 0.953125, learning_rate 0.000100453
2017-10-10T15:01:01.607840: step 2273, loss 0.19308, acc 0.9375, learning_rate 0.000100451
2017-10-10T15:01:01.884810: step 2274, loss 0.197893, acc 0.953125, learning_rate 0.000100449
2017-10-10T15:01:02.224909: step 2275, loss 0.13952, acc 0.9375, learning_rate 0.000100448
2017-10-10T15:01:02.528845: step 2276, loss 0.182047, acc 0.9375, learning_rate 0.000100446
2017-10-10T15:01:02.798696: step 2277, loss 0.231207, acc 0.921875, learning_rate 0.000100444
2017-10-10T15:01:03.052683: step 2278, loss 0.160958, acc 0.953125, learning_rate 0.000100442
2017-10-10T15:01:03.391930: step 2279, loss 0.114542, acc 0.96875, learning_rate 0.00010044
2017-10-10T15:01:03.708905: step 2280, loss 0.0940128, acc 0.96875, learning_rate 0.000100439

Evaluation:
2017-10-10T15:01:04.137192: step 2280, loss 0.230975, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2280

2017-10-10T15:01:05.252868: step 2281, loss 0.22017, acc 0.9375, learning_rate 0.000100437
2017-10-10T15:01:05.536246: step 2282, loss 0.196178, acc 0.9375, learning_rate 0.000100435
2017-10-10T15:01:05.817329: step 2283, loss 0.196629, acc 0.9375, learning_rate 0.000100433
2017-10-10T15:01:06.079689: step 2284, loss 0.0815197, acc 0.984375, learning_rate 0.000100431
2017-10-10T15:01:06.364823: step 2285, loss 0.181799, acc 0.9375, learning_rate 0.00010043
2017-10-10T15:01:06.580499: step 2286, loss 0.19676, acc 0.90625, learning_rate 0.000100428
2017-10-10T15:01:06.811537: step 2287, loss 0.198159, acc 0.953125, learning_rate 0.000100426
2017-10-10T15:01:07.127250: step 2288, loss 0.112665, acc 0.96875, learning_rate 0.000100424
2017-10-10T15:01:07.397571: step 2289, loss 0.150525, acc 0.984375, learning_rate 0.000100423
2017-10-10T15:01:07.676345: step 2290, loss 0.146667, acc 0.953125, learning_rate 0.000100421
2017-10-10T15:01:07.940447: step 2291, loss 0.149592, acc 0.953125, learning_rate 0.000100419
2017-10-10T15:01:08.194807: step 2292, loss 0.0834459, acc 0.984375, learning_rate 0.000100418
2017-10-10T15:01:08.489240: step 2293, loss 0.209601, acc 0.9375, learning_rate 0.000100416
2017-10-10T15:01:08.797024: step 2294, loss 0.160633, acc 0.9375, learning_rate 0.000100414
2017-10-10T15:01:09.066724: step 2295, loss 0.0578771, acc 0.984375, learning_rate 0.000100412
2017-10-10T15:01:09.386339: step 2296, loss 0.158784, acc 0.953125, learning_rate 0.000100411
2017-10-10T15:01:09.683146: step 2297, loss 0.101448, acc 0.984375, learning_rate 0.000100409
2017-10-10T15:01:09.952986: step 2298, loss 0.200108, acc 0.9375, learning_rate 0.000100407
2017-10-10T15:01:10.173814: step 2299, loss 0.192199, acc 0.921875, learning_rate 0.000100406
2017-10-10T15:01:10.410519: step 2300, loss 0.15306, acc 0.9375, learning_rate 0.000100404
2017-10-10T15:01:10.706007: step 2301, loss 0.12537, acc 0.9375, learning_rate 0.000100402
2017-10-10T15:01:10.961757: step 2302, loss 0.143107, acc 0.953125, learning_rate 0.000100401
2017-10-10T15:01:11.213307: step 2303, loss 0.0989465, acc 0.96875, learning_rate 0.000100399
2017-10-10T15:01:11.460096: step 2304, loss 0.0876081, acc 0.984375, learning_rate 0.000100398
2017-10-10T15:01:11.721006: step 2305, loss 0.0622542, acc 1, learning_rate 0.000100396
2017-10-10T15:01:12.031997: step 2306, loss 0.144641, acc 0.9375, learning_rate 0.000100394
2017-10-10T15:01:12.323383: step 2307, loss 0.0783077, acc 0.96875, learning_rate 0.000100393
2017-10-10T15:01:13.562645: step 2308, loss 0.11584, acc 0.96875, learning_rate 0.000100391
2017-10-10T15:01:13.841059: step 2309, loss 0.196723, acc 0.984375, learning_rate 0.000100389
2017-10-10T15:01:14.096844: step 2310, loss 0.155297, acc 0.9375, learning_rate 0.000100388
2017-10-10T15:01:14.441063: step 2311, loss 0.0474496, acc 1, learning_rate 0.000100386
2017-10-10T15:01:14.649213: step 2312, loss 0.0566264, acc 1, learning_rate 0.000100385
2017-10-10T15:01:14.888969: step 2313, loss 0.164461, acc 0.9375, learning_rate 0.000100383
2017-10-10T15:01:15.132577: step 2314, loss 0.136938, acc 0.96875, learning_rate 0.000100382
2017-10-10T15:01:15.374639: step 2315, loss 0.159348, acc 0.9375, learning_rate 0.00010038
2017-10-10T15:01:15.677727: step 2316, loss 0.163481, acc 0.9375, learning_rate 0.000100378
2017-10-10T15:01:15.940984: step 2317, loss 0.0791969, acc 0.96875, learning_rate 0.000100377
2017-10-10T15:01:16.224872: step 2318, loss 0.168861, acc 0.96875, learning_rate 0.000100375
2017-10-10T15:01:16.481156: step 2319, loss 0.174838, acc 0.921875, learning_rate 0.000100374
2017-10-10T15:01:16.740257: step 2320, loss 0.206397, acc 0.9375, learning_rate 0.000100372

Evaluation:
2017-10-10T15:01:17.208868: step 2320, loss 0.231465, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2320

2017-10-10T15:01:18.159537: step 2321, loss 0.160009, acc 0.953125, learning_rate 0.000100371
2017-10-10T15:01:18.346625: step 2322, loss 0.106952, acc 0.984375, learning_rate 0.000100369
2017-10-10T15:01:18.597434: step 2323, loss 0.115945, acc 0.96875, learning_rate 0.000100368
2017-10-10T15:01:18.832584: step 2324, loss 0.144125, acc 0.921875, learning_rate 0.000100366
2017-10-10T15:01:19.108431: step 2325, loss 0.272398, acc 0.90625, learning_rate 0.000100365
2017-10-10T15:01:19.382379: step 2326, loss 0.0584905, acc 1, learning_rate 0.000100363
2017-10-10T15:01:19.666500: step 2327, loss 0.154879, acc 0.953125, learning_rate 0.000100362
2017-10-10T15:01:19.929417: step 2328, loss 0.123361, acc 0.953125, learning_rate 0.00010036
2017-10-10T15:01:20.248929: step 2329, loss 0.139326, acc 0.953125, learning_rate 0.000100359
2017-10-10T15:01:20.509374: step 2330, loss 0.15018, acc 0.9375, learning_rate 0.000100357
2017-10-10T15:01:20.783326: step 2331, loss 0.108153, acc 0.96875, learning_rate 0.000100356
2017-10-10T15:01:21.110995: step 2332, loss 0.0583082, acc 0.96875, learning_rate 0.000100354
2017-10-10T15:01:21.415588: step 2333, loss 0.159645, acc 0.90625, learning_rate 0.000100353
2017-10-10T15:01:21.777019: step 2334, loss 0.279984, acc 0.890625, learning_rate 0.000100352
2017-10-10T15:01:22.028891: step 2335, loss 0.0761393, acc 0.984375, learning_rate 0.00010035
2017-10-10T15:01:22.317989: step 2336, loss 0.133302, acc 0.984375, learning_rate 0.000100349
2017-10-10T15:01:22.564922: step 2337, loss 0.117239, acc 0.96875, learning_rate 0.000100347
2017-10-10T15:01:22.868995: step 2338, loss 0.149725, acc 0.9375, learning_rate 0.000100346
2017-10-10T15:01:23.139615: step 2339, loss 0.172668, acc 0.921875, learning_rate 0.000100344
2017-10-10T15:01:23.440824: step 2340, loss 0.167073, acc 0.953125, learning_rate 0.000100343
2017-10-10T15:01:23.716086: step 2341, loss 0.141335, acc 0.96875, learning_rate 0.000100342
2017-10-10T15:01:23.936475: step 2342, loss 0.0975157, acc 0.953125, learning_rate 0.00010034
2017-10-10T15:01:24.236506: step 2343, loss 0.123645, acc 0.953125, learning_rate 0.000100339
2017-10-10T15:01:24.516825: step 2344, loss 0.129691, acc 0.96875, learning_rate 0.000100338
2017-10-10T15:01:24.724024: step 2345, loss 0.203996, acc 0.9375, learning_rate 0.000100336
2017-10-10T15:01:25.046233: step 2346, loss 0.228745, acc 0.90625, learning_rate 0.000100335
2017-10-10T15:01:25.337571: step 2347, loss 0.116313, acc 0.953125, learning_rate 0.000100333
2017-10-10T15:01:25.652402: step 2348, loss 0.111338, acc 0.96875, learning_rate 0.000100332
2017-10-10T15:01:25.912896: step 2349, loss 0.206085, acc 0.921875, learning_rate 0.000100331
2017-10-10T15:01:26.132892: step 2350, loss 0.141609, acc 0.96875, learning_rate 0.000100329
2017-10-10T15:01:26.447034: step 2351, loss 0.177433, acc 0.9375, learning_rate 0.000100328
2017-10-10T15:01:26.728889: step 2352, loss 0.132266, acc 0.941176, learning_rate 0.000100327
2017-10-10T15:01:26.968053: step 2353, loss 0.100038, acc 0.96875, learning_rate 0.000100325
2017-10-10T15:01:27.264901: step 2354, loss 0.120284, acc 0.953125, learning_rate 0.000100324
2017-10-10T15:01:27.522380: step 2355, loss 0.0817753, acc 0.984375, learning_rate 0.000100323
2017-10-10T15:01:27.816705: step 2356, loss 0.097825, acc 0.984375, learning_rate 0.000100321
2017-10-10T15:01:28.099357: step 2357, loss 0.187763, acc 0.953125, learning_rate 0.00010032
2017-10-10T15:01:28.328947: step 2358, loss 0.292515, acc 0.90625, learning_rate 0.000100319
2017-10-10T15:01:28.628574: step 2359, loss 0.168923, acc 0.96875, learning_rate 0.000100317
2017-10-10T15:01:28.928924: step 2360, loss 0.153688, acc 0.9375, learning_rate 0.000100316

Evaluation:
2017-10-10T15:01:29.428529: step 2360, loss 0.23044, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2360

2017-10-10T15:01:30.592896: step 2361, loss 0.13092, acc 0.953125, learning_rate 0.000100315
2017-10-10T15:01:30.856576: step 2362, loss 0.223087, acc 0.921875, learning_rate 0.000100314
2017-10-10T15:01:31.115456: step 2363, loss 0.295172, acc 0.890625, learning_rate 0.000100312
2017-10-10T15:01:31.360616: step 2364, loss 0.301176, acc 0.9375, learning_rate 0.000100311
2017-10-10T15:01:31.600912: step 2365, loss 0.0959871, acc 0.96875, learning_rate 0.00010031
2017-10-10T15:01:31.908819: step 2366, loss 0.0932025, acc 0.96875, learning_rate 0.000100308
2017-10-10T15:01:32.167089: step 2367, loss 0.159316, acc 0.953125, learning_rate 0.000100307
2017-10-10T15:01:32.444372: step 2368, loss 0.11948, acc 0.96875, learning_rate 0.000100306
2017-10-10T15:01:32.745333: step 2369, loss 0.085242, acc 0.984375, learning_rate 0.000100305
2017-10-10T15:01:33.008880: step 2370, loss 0.123774, acc 0.953125, learning_rate 0.000100303
2017-10-10T15:01:33.317468: step 2371, loss 0.208461, acc 0.953125, learning_rate 0.000100302
2017-10-10T15:01:33.517186: step 2372, loss 0.0705682, acc 0.984375, learning_rate 0.000100301
2017-10-10T15:01:33.831385: step 2373, loss 0.225068, acc 0.90625, learning_rate 0.0001003
2017-10-10T15:01:34.114185: step 2374, loss 0.10381, acc 0.984375, learning_rate 0.000100299
2017-10-10T15:01:34.340973: step 2375, loss 0.19642, acc 0.96875, learning_rate 0.000100297
2017-10-10T15:01:34.580301: step 2376, loss 0.112166, acc 0.96875, learning_rate 0.000100296
2017-10-10T15:01:34.840924: step 2377, loss 0.184402, acc 0.9375, learning_rate 0.000100295
2017-10-10T15:01:35.094221: step 2378, loss 0.182287, acc 0.9375, learning_rate 0.000100294
2017-10-10T15:01:35.385723: step 2379, loss 0.133528, acc 0.9375, learning_rate 0.000100292
2017-10-10T15:01:35.642846: step 2380, loss 0.114794, acc 0.96875, learning_rate 0.000100291
2017-10-10T15:01:35.913432: step 2381, loss 0.168943, acc 0.953125, learning_rate 0.00010029
2017-10-10T15:01:36.192047: step 2382, loss 0.0641873, acc 0.984375, learning_rate 0.000100289
2017-10-10T15:01:36.488852: step 2383, loss 0.111776, acc 0.96875, learning_rate 0.000100288
2017-10-10T15:01:36.776932: step 2384, loss 0.13117, acc 0.953125, learning_rate 0.000100287
2017-10-10T15:01:37.077802: step 2385, loss 0.0725172, acc 0.96875, learning_rate 0.000100285
2017-10-10T15:01:37.408894: step 2386, loss 0.0893166, acc 0.96875, learning_rate 0.000100284
2017-10-10T15:01:37.702922: step 2387, loss 0.23114, acc 0.9375, learning_rate 0.000100283
2017-10-10T15:01:37.989158: step 2388, loss 0.141096, acc 0.953125, learning_rate 0.000100282
2017-10-10T15:01:38.215276: step 2389, loss 0.12531, acc 0.953125, learning_rate 0.000100281
2017-10-10T15:01:38.469136: step 2390, loss 0.203295, acc 0.953125, learning_rate 0.00010028
2017-10-10T15:01:38.807350: step 2391, loss 0.199865, acc 0.9375, learning_rate 0.000100278
2017-10-10T15:01:39.095260: step 2392, loss 0.130645, acc 0.9375, learning_rate 0.000100277
2017-10-10T15:01:39.396964: step 2393, loss 0.12565, acc 0.953125, learning_rate 0.000100276
2017-10-10T15:01:39.604823: step 2394, loss 0.13311, acc 0.96875, learning_rate 0.000100275
2017-10-10T15:01:39.931166: step 2395, loss 0.105158, acc 0.984375, learning_rate 0.000100274
2017-10-10T15:01:40.232912: step 2396, loss 0.121687, acc 0.953125, learning_rate 0.000100273
2017-10-10T15:01:40.443897: step 2397, loss 0.181811, acc 0.921875, learning_rate 0.000100272
2017-10-10T15:01:40.728807: step 2398, loss 0.17817, acc 0.9375, learning_rate 0.000100271
2017-10-10T15:01:41.013745: step 2399, loss 0.115214, acc 0.9375, learning_rate 0.00010027
2017-10-10T15:01:41.245148: step 2400, loss 0.0859067, acc 0.96875, learning_rate 0.000100268

Evaluation:
2017-10-10T15:01:41.693677: step 2400, loss 0.229513, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2400

2017-10-10T15:01:42.812933: step 2401, loss 0.121155, acc 0.96875, learning_rate 0.000100267
2017-10-10T15:01:43.069050: step 2402, loss 0.117209, acc 0.96875, learning_rate 0.000100266
2017-10-10T15:01:43.352940: step 2403, loss 0.0768537, acc 1, learning_rate 0.000100265
2017-10-10T15:01:43.675564: step 2404, loss 0.154686, acc 0.953125, learning_rate 0.000100264
2017-10-10T15:01:43.981071: step 2405, loss 0.0645579, acc 0.96875, learning_rate 0.000100263
2017-10-10T15:01:44.244143: step 2406, loss 0.19011, acc 0.9375, learning_rate 0.000100262
2017-10-10T15:01:44.496972: step 2407, loss 0.109354, acc 0.953125, learning_rate 0.000100261
2017-10-10T15:01:44.763751: step 2408, loss 0.137723, acc 0.953125, learning_rate 0.00010026
2017-10-10T15:01:45.055625: step 2409, loss 0.0964923, acc 0.96875, learning_rate 0.000100259
2017-10-10T15:01:45.298918: step 2410, loss 0.130112, acc 0.96875, learning_rate 0.000100258
2017-10-10T15:01:45.565481: step 2411, loss 0.238172, acc 0.90625, learning_rate 0.000100257
2017-10-10T15:01:45.827812: step 2412, loss 0.131184, acc 0.96875, learning_rate 0.000100256
2017-10-10T15:01:46.069978: step 2413, loss 0.237681, acc 0.9375, learning_rate 0.000100255
2017-10-10T15:01:46.425820: step 2414, loss 0.140154, acc 0.9375, learning_rate 0.000100253
2017-10-10T15:01:46.723110: step 2415, loss 0.0633712, acc 1, learning_rate 0.000100252
2017-10-10T15:01:46.980873: step 2416, loss 0.213074, acc 0.90625, learning_rate 0.000100251
2017-10-10T15:01:47.265796: step 2417, loss 0.168873, acc 0.921875, learning_rate 0.00010025
2017-10-10T15:01:47.584850: step 2418, loss 0.154063, acc 0.96875, learning_rate 0.000100249
2017-10-10T15:01:47.818611: step 2419, loss 0.164058, acc 0.953125, learning_rate 0.000100248
2017-10-10T15:01:48.105057: step 2420, loss 0.145397, acc 0.96875, learning_rate 0.000100247
2017-10-10T15:01:48.384828: step 2421, loss 0.0801839, acc 0.984375, learning_rate 0.000100246
2017-10-10T15:01:48.714711: step 2422, loss 0.106101, acc 0.984375, learning_rate 0.000100245
2017-10-10T15:01:49.000861: step 2423, loss 0.193714, acc 0.9375, learning_rate 0.000100244
2017-10-10T15:01:49.296503: step 2424, loss 0.206461, acc 0.953125, learning_rate 0.000100243
2017-10-10T15:01:49.540869: step 2425, loss 0.20399, acc 0.921875, learning_rate 0.000100242
2017-10-10T15:01:49.820903: step 2426, loss 0.08119, acc 0.984375, learning_rate 0.000100241
2017-10-10T15:01:50.016739: step 2427, loss 0.133082, acc 0.9375, learning_rate 0.00010024
2017-10-10T15:01:50.336349: step 2428, loss 0.174941, acc 0.9375, learning_rate 0.000100239
2017-10-10T15:01:50.643293: step 2429, loss 0.115401, acc 0.953125, learning_rate 0.000100238
2017-10-10T15:01:50.876898: step 2430, loss 0.0704023, acc 0.984375, learning_rate 0.000100237
2017-10-10T15:01:51.085022: step 2431, loss 0.169446, acc 0.953125, learning_rate 0.000100236
2017-10-10T15:01:51.348025: step 2432, loss 0.0700216, acc 0.984375, learning_rate 0.000100235
2017-10-10T15:01:51.610951: step 2433, loss 0.103631, acc 0.953125, learning_rate 0.000100235
2017-10-10T15:01:51.933071: step 2434, loss 0.0750742, acc 0.984375, learning_rate 0.000100234
2017-10-10T15:01:52.205136: step 2435, loss 0.074659, acc 1, learning_rate 0.000100233
2017-10-10T15:01:52.495911: step 2436, loss 0.0744365, acc 0.984375, learning_rate 0.000100232
2017-10-10T15:01:52.769191: step 2437, loss 0.197989, acc 0.921875, learning_rate 0.000100231
2017-10-10T15:01:53.035772: step 2438, loss 0.172719, acc 0.921875, learning_rate 0.00010023
2017-10-10T15:01:53.300908: step 2439, loss 0.151817, acc 0.9375, learning_rate 0.000100229
2017-10-10T15:01:53.594423: step 2440, loss 0.209343, acc 0.953125, learning_rate 0.000100228

Evaluation:
2017-10-10T15:01:54.062020: step 2440, loss 0.231468, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2440

2017-10-10T15:01:55.244876: step 2441, loss 0.0880228, acc 0.984375, learning_rate 0.000100227
2017-10-10T15:01:55.468838: step 2442, loss 0.28643, acc 0.875, learning_rate 0.000100226
2017-10-10T15:01:55.780858: step 2443, loss 0.107821, acc 0.984375, learning_rate 0.000100225
2017-10-10T15:01:56.028317: step 2444, loss 0.194686, acc 0.953125, learning_rate 0.000100224
2017-10-10T15:01:56.292558: step 2445, loss 0.165807, acc 0.953125, learning_rate 0.000100223
2017-10-10T15:01:56.541165: step 2446, loss 0.183959, acc 0.90625, learning_rate 0.000100222
2017-10-10T15:01:56.770039: step 2447, loss 0.131863, acc 0.96875, learning_rate 0.000100221
2017-10-10T15:01:57.056856: step 2448, loss 0.127626, acc 0.984375, learning_rate 0.000100221
2017-10-10T15:01:57.333430: step 2449, loss 0.165039, acc 0.9375, learning_rate 0.00010022
2017-10-10T15:01:57.558011: step 2450, loss 0.239643, acc 0.901961, learning_rate 0.000100219
2017-10-10T15:01:57.833330: step 2451, loss 0.156789, acc 0.96875, learning_rate 0.000100218
2017-10-10T15:01:58.046641: step 2452, loss 0.065812, acc 1, learning_rate 0.000100217
2017-10-10T15:01:58.296258: step 2453, loss 0.181466, acc 0.953125, learning_rate 0.000100216
2017-10-10T15:01:58.600849: step 2454, loss 0.127323, acc 0.9375, learning_rate 0.000100215
2017-10-10T15:01:58.884912: step 2455, loss 0.191848, acc 0.953125, learning_rate 0.000100214
2017-10-10T15:01:59.157123: step 2456, loss 0.0940984, acc 0.984375, learning_rate 0.000100213
2017-10-10T15:01:59.417592: step 2457, loss 0.179857, acc 0.953125, learning_rate 0.000100213
2017-10-10T15:01:59.742573: step 2458, loss 0.0889302, acc 0.984375, learning_rate 0.000100212
2017-10-10T15:02:00.056850: step 2459, loss 0.103252, acc 0.96875, learning_rate 0.000100211
2017-10-10T15:02:00.266230: step 2460, loss 0.17311, acc 0.984375, learning_rate 0.00010021
2017-10-10T15:02:00.530934: step 2461, loss 0.127674, acc 0.96875, learning_rate 0.000100209
2017-10-10T15:02:00.808877: step 2462, loss 0.155753, acc 0.953125, learning_rate 0.000100208
2017-10-10T15:02:01.079464: step 2463, loss 0.0740312, acc 0.984375, learning_rate 0.000100207
2017-10-10T15:02:01.366925: step 2464, loss 0.135699, acc 0.9375, learning_rate 0.000100207
2017-10-10T15:02:01.645030: step 2465, loss 0.24432, acc 0.90625, learning_rate 0.000100206
2017-10-10T15:02:01.940417: step 2466, loss 0.170054, acc 0.96875, learning_rate 0.000100205
2017-10-10T15:02:02.247424: step 2467, loss 0.15687, acc 0.96875, learning_rate 0.000100204
2017-10-10T15:02:02.489148: step 2468, loss 0.143408, acc 0.984375, learning_rate 0.000100203
2017-10-10T15:02:02.736855: step 2469, loss 0.13917, acc 0.96875, learning_rate 0.000100202
2017-10-10T15:02:03.015387: step 2470, loss 0.202659, acc 0.921875, learning_rate 0.000100202
2017-10-10T15:02:03.332924: step 2471, loss 0.14964, acc 0.953125, learning_rate 0.000100201
2017-10-10T15:02:03.663127: step 2472, loss 0.133867, acc 0.953125, learning_rate 0.0001002
2017-10-10T15:02:03.942622: step 2473, loss 0.205659, acc 0.96875, learning_rate 0.000100199
2017-10-10T15:02:04.213480: step 2474, loss 0.187851, acc 0.9375, learning_rate 0.000100198
2017-10-10T15:02:04.492834: step 2475, loss 0.174814, acc 0.953125, learning_rate 0.000100198
2017-10-10T15:02:04.729140: step 2476, loss 0.125865, acc 0.9375, learning_rate 0.000100197
2017-10-10T15:02:05.006137: step 2477, loss 0.183122, acc 0.953125, learning_rate 0.000100196
2017-10-10T15:02:05.260962: step 2478, loss 0.128986, acc 0.921875, learning_rate 0.000100195
2017-10-10T15:02:05.546859: step 2479, loss 0.12403, acc 0.953125, learning_rate 0.000100194
2017-10-10T15:02:05.820839: step 2480, loss 0.0600565, acc 1, learning_rate 0.000100194

Evaluation:
2017-10-10T15:02:06.220893: step 2480, loss 0.230757, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2480

2017-10-10T15:02:07.221052: step 2481, loss 0.159576, acc 0.953125, learning_rate 0.000100193
2017-10-10T15:02:07.481082: step 2482, loss 0.157705, acc 0.953125, learning_rate 0.000100192
2017-10-10T15:02:07.744976: step 2483, loss 0.206085, acc 0.9375, learning_rate 0.000100191
2017-10-10T15:02:07.942893: step 2484, loss 0.128916, acc 0.953125, learning_rate 0.00010019
2017-10-10T15:02:08.141381: step 2485, loss 0.116015, acc 0.96875, learning_rate 0.00010019
2017-10-10T15:02:08.408944: step 2486, loss 0.11127, acc 0.984375, learning_rate 0.000100189
2017-10-10T15:02:08.693184: step 2487, loss 0.0989573, acc 0.96875, learning_rate 0.000100188
2017-10-10T15:02:08.966178: step 2488, loss 0.0931811, acc 0.984375, learning_rate 0.000100187
2017-10-10T15:02:09.245004: step 2489, loss 0.246266, acc 0.890625, learning_rate 0.000100187
2017-10-10T15:02:09.558593: step 2490, loss 0.174527, acc 0.953125, learning_rate 0.000100186
2017-10-10T15:02:09.908066: step 2491, loss 0.0961313, acc 0.96875, learning_rate 0.000100185
2017-10-10T15:02:10.193082: step 2492, loss 0.152719, acc 0.953125, learning_rate 0.000100184
2017-10-10T15:02:10.444094: step 2493, loss 0.063698, acc 1, learning_rate 0.000100183
2017-10-10T15:02:10.729253: step 2494, loss 0.217747, acc 0.921875, learning_rate 0.000100183
2017-10-10T15:02:11.058652: step 2495, loss 0.230537, acc 0.90625, learning_rate 0.000100182
2017-10-10T15:02:11.260882: step 2496, loss 0.12747, acc 0.953125, learning_rate 0.000100181
2017-10-10T15:02:11.549476: step 2497, loss 0.112462, acc 0.96875, learning_rate 0.000100181
2017-10-10T15:02:11.832937: step 2498, loss 0.217282, acc 0.9375, learning_rate 0.00010018
2017-10-10T15:02:12.068886: step 2499, loss 0.219678, acc 0.9375, learning_rate 0.000100179
2017-10-10T15:02:12.408858: step 2500, loss 0.170799, acc 0.9375, learning_rate 0.000100178
2017-10-10T15:02:12.740843: step 2501, loss 0.174235, acc 0.953125, learning_rate 0.000100178
2017-10-10T15:02:13.040079: step 2502, loss 0.138523, acc 0.953125, learning_rate 0.000100177
2017-10-10T15:02:13.334348: step 2503, loss 0.154948, acc 0.921875, learning_rate 0.000100176
2017-10-10T15:02:13.596293: step 2504, loss 0.120949, acc 0.953125, learning_rate 0.000100175
2017-10-10T15:02:13.842276: step 2505, loss 0.0847178, acc 0.96875, learning_rate 0.000100175
2017-10-10T15:02:14.096200: step 2506, loss 0.0937922, acc 0.96875, learning_rate 0.000100174
2017-10-10T15:02:14.368727: step 2507, loss 0.20812, acc 0.9375, learning_rate 0.000100173
2017-10-10T15:02:14.649158: step 2508, loss 0.142888, acc 0.9375, learning_rate 0.000100173
2017-10-10T15:02:14.966769: step 2509, loss 0.186158, acc 0.953125, learning_rate 0.000100172
2017-10-10T15:02:15.304009: step 2510, loss 0.119956, acc 0.96875, learning_rate 0.000100171
2017-10-10T15:02:15.516128: step 2511, loss 0.157115, acc 0.96875, learning_rate 0.00010017
2017-10-10T15:02:15.816897: step 2512, loss 0.142631, acc 0.9375, learning_rate 0.00010017
2017-10-10T15:02:16.092955: step 2513, loss 0.134475, acc 0.953125, learning_rate 0.000100169
2017-10-10T15:02:16.358739: step 2514, loss 0.142193, acc 0.953125, learning_rate 0.000100168
2017-10-10T15:02:16.584783: step 2515, loss 0.0750238, acc 0.96875, learning_rate 0.000100168
2017-10-10T15:02:16.842575: step 2516, loss 0.0935103, acc 0.984375, learning_rate 0.000100167
2017-10-10T15:02:17.092887: step 2517, loss 0.178983, acc 0.921875, learning_rate 0.000100166
2017-10-10T15:02:17.339568: step 2518, loss 0.093552, acc 0.984375, learning_rate 0.000100166
2017-10-10T15:02:17.608948: step 2519, loss 0.125794, acc 0.96875, learning_rate 0.000100165
2017-10-10T15:02:17.822946: step 2520, loss 0.153848, acc 0.921875, learning_rate 0.000100164

Evaluation:
2017-10-10T15:02:18.276857: step 2520, loss 0.229828, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2520

2017-10-10T15:02:19.271535: step 2521, loss 0.114718, acc 0.96875, learning_rate 0.000100164
2017-10-10T15:02:19.484987: step 2522, loss 0.176083, acc 0.9375, learning_rate 0.000100163
2017-10-10T15:02:19.697020: step 2523, loss 0.0753521, acc 0.984375, learning_rate 0.000100162
2017-10-10T15:02:19.947913: step 2524, loss 0.142139, acc 0.96875, learning_rate 0.000100162
2017-10-10T15:02:20.204827: step 2525, loss 0.171669, acc 0.953125, learning_rate 0.000100161
2017-10-10T15:02:20.532339: step 2526, loss 0.062275, acc 1, learning_rate 0.00010016
2017-10-10T15:02:20.722508: step 2527, loss 0.192692, acc 0.953125, learning_rate 0.00010016
2017-10-10T15:02:20.968259: step 2528, loss 0.156418, acc 0.921875, learning_rate 0.000100159
2017-10-10T15:02:21.198031: step 2529, loss 0.17948, acc 0.921875, learning_rate 0.000100158
2017-10-10T15:02:21.504976: step 2530, loss 0.0977135, acc 0.984375, learning_rate 0.000100158
2017-10-10T15:02:21.872842: step 2531, loss 0.138961, acc 0.96875, learning_rate 0.000100157
2017-10-10T15:02:22.104851: step 2532, loss 0.0929563, acc 0.984375, learning_rate 0.000100156
2017-10-10T15:02:22.400155: step 2533, loss 0.102372, acc 0.953125, learning_rate 0.000100156
2017-10-10T15:02:22.727821: step 2534, loss 0.068706, acc 0.984375, learning_rate 0.000100155
2017-10-10T15:02:23.061225: step 2535, loss 0.175731, acc 0.9375, learning_rate 0.000100155
2017-10-10T15:02:23.372813: step 2536, loss 0.160817, acc 0.953125, learning_rate 0.000100154
2017-10-10T15:02:23.618389: step 2537, loss 0.191422, acc 0.9375, learning_rate 0.000100153
2017-10-10T15:02:23.885782: step 2538, loss 0.130543, acc 0.953125, learning_rate 0.000100153
2017-10-10T15:02:24.160859: step 2539, loss 0.103045, acc 0.96875, learning_rate 0.000100152
2017-10-10T15:02:24.468936: step 2540, loss 0.149973, acc 0.953125, learning_rate 0.000100151
2017-10-10T15:02:24.772933: step 2541, loss 0.167625, acc 0.953125, learning_rate 0.000100151
2017-10-10T15:02:24.994461: step 2542, loss 0.0868257, acc 0.96875, learning_rate 0.00010015
2017-10-10T15:02:25.208864: step 2543, loss 0.215989, acc 0.921875, learning_rate 0.00010015
2017-10-10T15:02:25.491925: step 2544, loss 0.0480207, acc 1, learning_rate 0.000100149
2017-10-10T15:02:25.708873: step 2545, loss 0.133257, acc 0.96875, learning_rate 0.000100148
2017-10-10T15:02:26.035732: step 2546, loss 0.174579, acc 0.921875, learning_rate 0.000100148
2017-10-10T15:02:26.272079: step 2547, loss 0.099568, acc 0.984375, learning_rate 0.000100147
2017-10-10T15:02:26.528375: step 2548, loss 0.157814, acc 0.941176, learning_rate 0.000100147
2017-10-10T15:02:26.823539: step 2549, loss 0.171629, acc 0.921875, learning_rate 0.000100146
2017-10-10T15:02:27.073296: step 2550, loss 0.130905, acc 0.953125, learning_rate 0.000100145
2017-10-10T15:02:27.312817: step 2551, loss 0.182373, acc 0.921875, learning_rate 0.000100145
2017-10-10T15:02:27.580452: step 2552, loss 0.113318, acc 0.96875, learning_rate 0.000100144
2017-10-10T15:02:27.861470: step 2553, loss 0.0965781, acc 0.953125, learning_rate 0.000100144
2017-10-10T15:02:28.069517: step 2554, loss 0.22009, acc 0.90625, learning_rate 0.000100143
2017-10-10T15:02:28.321766: step 2555, loss 0.0832389, acc 0.984375, learning_rate 0.000100142
2017-10-10T15:02:28.684837: step 2556, loss 0.145275, acc 0.9375, learning_rate 0.000100142
2017-10-10T15:02:28.999448: step 2557, loss 0.119226, acc 0.9375, learning_rate 0.000100141
2017-10-10T15:02:29.224672: step 2558, loss 0.106677, acc 0.96875, learning_rate 0.000100141
2017-10-10T15:02:29.496917: step 2559, loss 0.0891262, acc 0.984375, learning_rate 0.00010014
2017-10-10T15:02:29.736897: step 2560, loss 0.132365, acc 0.96875, learning_rate 0.00010014

Evaluation:
2017-10-10T15:02:30.204830: step 2560, loss 0.229258, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2560

2017-10-10T15:02:31.312363: step 2561, loss 0.158957, acc 0.9375, learning_rate 0.000100139
2017-10-10T15:02:31.642240: step 2562, loss 0.142746, acc 0.96875, learning_rate 0.000100138
2017-10-10T15:02:31.890957: step 2563, loss 0.122222, acc 0.9375, learning_rate 0.000100138
2017-10-10T15:02:32.160870: step 2564, loss 0.139358, acc 0.953125, learning_rate 0.000100137
2017-10-10T15:02:32.466823: step 2565, loss 0.117165, acc 0.984375, learning_rate 0.000100137
2017-10-10T15:02:32.715261: step 2566, loss 0.276044, acc 0.90625, learning_rate 0.000100136
2017-10-10T15:02:32.995565: step 2567, loss 0.156994, acc 0.953125, learning_rate 0.000100136
2017-10-10T15:02:33.269687: step 2568, loss 0.0881315, acc 0.984375, learning_rate 0.000100135
2017-10-10T15:02:33.510909: step 2569, loss 0.143362, acc 0.96875, learning_rate 0.000100134
2017-10-10T15:02:33.835333: step 2570, loss 0.128675, acc 0.953125, learning_rate 0.000100134
2017-10-10T15:02:34.108553: step 2571, loss 0.0731092, acc 0.984375, learning_rate 0.000100133
2017-10-10T15:02:34.441791: step 2572, loss 0.133633, acc 0.953125, learning_rate 0.000100133
2017-10-10T15:02:34.709974: step 2573, loss 0.174471, acc 0.921875, learning_rate 0.000100132
2017-10-10T15:02:34.953825: step 2574, loss 0.101457, acc 0.984375, learning_rate 0.000100132
2017-10-10T15:02:35.276983: step 2575, loss 0.134642, acc 0.9375, learning_rate 0.000100131
2017-10-10T15:02:35.560924: step 2576, loss 0.0777174, acc 0.984375, learning_rate 0.000100131
2017-10-10T15:02:35.845213: step 2577, loss 0.148283, acc 0.9375, learning_rate 0.00010013
2017-10-10T15:02:36.040850: step 2578, loss 0.146721, acc 0.96875, learning_rate 0.00010013
2017-10-10T15:02:36.357018: step 2579, loss 0.108868, acc 0.984375, learning_rate 0.000100129
2017-10-10T15:02:36.583083: step 2580, loss 0.0581238, acc 1, learning_rate 0.000100129
2017-10-10T15:02:36.895489: step 2581, loss 0.201854, acc 0.9375, learning_rate 0.000100128
2017-10-10T15:02:37.152429: step 2582, loss 0.0626047, acc 1, learning_rate 0.000100128
2017-10-10T15:02:37.440864: step 2583, loss 0.37582, acc 0.875, learning_rate 0.000100127
2017-10-10T15:02:37.677987: step 2584, loss 0.11344, acc 0.984375, learning_rate 0.000100126
2017-10-10T15:02:37.948883: step 2585, loss 0.128786, acc 0.953125, learning_rate 0.000100126
2017-10-10T15:02:38.235425: step 2586, loss 0.0845566, acc 0.984375, learning_rate 0.000100125
2017-10-10T15:02:38.538368: step 2587, loss 0.174487, acc 0.96875, learning_rate 0.000100125
2017-10-10T15:02:38.832564: step 2588, loss 0.117733, acc 0.96875, learning_rate 0.000100124
2017-10-10T15:02:39.097602: step 2589, loss 0.188475, acc 0.9375, learning_rate 0.000100124
2017-10-10T15:02:39.401504: step 2590, loss 0.134875, acc 0.953125, learning_rate 0.000100123
2017-10-10T15:02:39.680897: step 2591, loss 0.142923, acc 0.921875, learning_rate 0.000100123
2017-10-10T15:02:39.904914: step 2592, loss 0.0721196, acc 0.984375, learning_rate 0.000100122
2017-10-10T15:02:40.156238: step 2593, loss 0.151447, acc 0.953125, learning_rate 0.000100122
2017-10-10T15:02:40.368869: step 2594, loss 0.136715, acc 0.9375, learning_rate 0.000100121
2017-10-10T15:02:40.580862: step 2595, loss 0.163252, acc 0.9375, learning_rate 0.000100121
2017-10-10T15:02:40.828536: step 2596, loss 0.189177, acc 0.921875, learning_rate 0.00010012
2017-10-10T15:02:41.121326: step 2597, loss 0.0782228, acc 0.984375, learning_rate 0.00010012
2017-10-10T15:02:41.316287: step 2598, loss 0.169838, acc 0.9375, learning_rate 0.000100119
2017-10-10T15:02:41.624475: step 2599, loss 0.16916, acc 0.9375, learning_rate 0.000100119
2017-10-10T15:02:41.889048: step 2600, loss 0.156897, acc 0.9375, learning_rate 0.000100118

Evaluation:
2017-10-10T15:02:42.293057: step 2600, loss 0.230657, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2600

2017-10-10T15:02:43.358931: step 2601, loss 0.117635, acc 0.96875, learning_rate 0.000100118
2017-10-10T15:02:43.551657: step 2602, loss 0.0760552, acc 0.96875, learning_rate 0.000100117
2017-10-10T15:02:43.725299: step 2603, loss 0.0848576, acc 0.984375, learning_rate 0.000100117
2017-10-10T15:02:43.809943: step 2604, loss 0.0971429, acc 0.984375, learning_rate 0.000100117
2017-10-10T15:02:44.058547: step 2605, loss 0.0803744, acc 0.96875, learning_rate 0.000100116
2017-10-10T15:02:44.296906: step 2606, loss 0.0910501, acc 0.984375, learning_rate 0.000100116
2017-10-10T15:02:44.579634: step 2607, loss 0.19033, acc 0.921875, learning_rate 0.000100115
2017-10-10T15:02:44.870018: step 2608, loss 0.205806, acc 0.9375, learning_rate 0.000100115
2017-10-10T15:02:45.166073: step 2609, loss 0.210459, acc 0.953125, learning_rate 0.000100114
2017-10-10T15:02:45.421068: step 2610, loss 0.173951, acc 0.9375, learning_rate 0.000100114
2017-10-10T15:02:45.672259: step 2611, loss 0.156013, acc 0.953125, learning_rate 0.000100113
2017-10-10T15:02:45.969873: step 2612, loss 0.094358, acc 0.96875, learning_rate 0.000100113
2017-10-10T15:02:46.260945: step 2613, loss 0.183425, acc 0.921875, learning_rate 0.000100112
2017-10-10T15:02:46.476907: step 2614, loss 0.302951, acc 0.953125, learning_rate 0.000100112
2017-10-10T15:02:46.779414: step 2615, loss 0.194833, acc 0.953125, learning_rate 0.000100111
2017-10-10T15:02:47.072609: step 2616, loss 0.17062, acc 0.953125, learning_rate 0.000100111
2017-10-10T15:02:47.358730: step 2617, loss 0.0971258, acc 0.953125, learning_rate 0.000100111
2017-10-10T15:02:47.608952: step 2618, loss 0.0814047, acc 0.984375, learning_rate 0.00010011
2017-10-10T15:02:47.889414: step 2619, loss 0.24736, acc 0.890625, learning_rate 0.00010011
2017-10-10T15:02:48.128382: step 2620, loss 0.116589, acc 0.9375, learning_rate 0.000100109
2017-10-10T15:02:48.382381: step 2621, loss 0.103745, acc 0.96875, learning_rate 0.000100109
2017-10-10T15:02:48.689227: step 2622, loss 0.14243, acc 0.9375, learning_rate 0.000100108
2017-10-10T15:02:48.924897: step 2623, loss 0.196757, acc 0.9375, learning_rate 0.000100108
2017-10-10T15:02:49.183486: step 2624, loss 0.169755, acc 0.921875, learning_rate 0.000100107
2017-10-10T15:02:49.416062: step 2625, loss 0.0795002, acc 1, learning_rate 0.000100107
2017-10-10T15:02:49.703360: step 2626, loss 0.114567, acc 0.96875, learning_rate 0.000100107
2017-10-10T15:02:50.010862: step 2627, loss 0.12217, acc 0.9375, learning_rate 0.000100106
2017-10-10T15:02:50.273137: step 2628, loss 0.0551428, acc 1, learning_rate 0.000100106
2017-10-10T15:02:50.545535: step 2629, loss 0.0600444, acc 1, learning_rate 0.000100105
2017-10-10T15:02:50.823090: step 2630, loss 0.34253, acc 0.890625, learning_rate 0.000100105
2017-10-10T15:02:51.083861: step 2631, loss 0.104995, acc 0.96875, learning_rate 0.000100104
2017-10-10T15:02:51.387187: step 2632, loss 0.145492, acc 0.96875, learning_rate 0.000100104
2017-10-10T15:02:51.648962: step 2633, loss 0.189936, acc 0.9375, learning_rate 0.000100104
2017-10-10T15:02:51.969003: step 2634, loss 0.201454, acc 0.953125, learning_rate 0.000100103
2017-10-10T15:02:52.295354: step 2635, loss 0.112618, acc 0.953125, learning_rate 0.000100103
2017-10-10T15:02:52.528943: step 2636, loss 0.218161, acc 0.9375, learning_rate 0.000100102
2017-10-10T15:02:52.746201: step 2637, loss 0.0994803, acc 0.96875, learning_rate 0.000100102
2017-10-10T15:02:53.039933: step 2638, loss 0.0622799, acc 0.984375, learning_rate 0.000100101
2017-10-10T15:02:53.258563: step 2639, loss 0.213917, acc 0.921875, learning_rate 0.000100101
2017-10-10T15:02:53.521678: step 2640, loss 0.1665, acc 0.90625, learning_rate 0.000100101

Evaluation:
2017-10-10T15:02:54.070292: step 2640, loss 0.228578, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2640

2017-10-10T15:02:55.087402: step 2641, loss 0.194455, acc 0.96875, learning_rate 0.0001001
2017-10-10T15:02:55.352759: step 2642, loss 0.112742, acc 0.953125, learning_rate 0.0001001
2017-10-10T15:02:55.682880: step 2643, loss 0.089935, acc 0.96875, learning_rate 0.000100099
2017-10-10T15:02:55.950968: step 2644, loss 0.0746788, acc 0.984375, learning_rate 0.000100099
2017-10-10T15:02:56.208210: step 2645, loss 0.110592, acc 0.96875, learning_rate 0.000100099
2017-10-10T15:02:56.524910: step 2646, loss 0.135798, acc 0.921569, learning_rate 0.000100098
2017-10-10T15:02:56.838358: step 2647, loss 0.190775, acc 0.90625, learning_rate 0.000100098
2017-10-10T15:02:57.082022: step 2648, loss 0.0679546, acc 0.984375, learning_rate 0.000100097
2017-10-10T15:02:57.337087: step 2649, loss 0.186848, acc 0.90625, learning_rate 0.000100097
2017-10-10T15:02:57.560987: step 2650, loss 0.126296, acc 0.953125, learning_rate 0.000100097
2017-10-10T15:02:57.807493: step 2651, loss 0.10567, acc 0.953125, learning_rate 0.000100096
2017-10-10T15:02:58.127596: step 2652, loss 0.129937, acc 0.96875, learning_rate 0.000100096
2017-10-10T15:02:58.357809: step 2653, loss 0.167588, acc 0.96875, learning_rate 0.000100095
2017-10-10T15:02:58.652320: step 2654, loss 0.0954113, acc 0.984375, learning_rate 0.000100095
2017-10-10T15:02:58.933448: step 2655, loss 0.156415, acc 0.9375, learning_rate 0.000100095
2017-10-10T15:02:59.147500: step 2656, loss 0.16445, acc 0.9375, learning_rate 0.000100094
2017-10-10T15:02:59.448335: step 2657, loss 0.106725, acc 0.96875, learning_rate 0.000100094
2017-10-10T15:02:59.725193: step 2658, loss 0.315528, acc 0.90625, learning_rate 0.000100093
2017-10-10T15:03:00.004829: step 2659, loss 0.0670582, acc 0.984375, learning_rate 0.000100093
2017-10-10T15:03:00.317084: step 2660, loss 0.0875308, acc 0.984375, learning_rate 0.000100093
2017-10-10T15:03:00.580933: step 2661, loss 0.180544, acc 0.953125, learning_rate 0.000100092
2017-10-10T15:03:00.859287: step 2662, loss 0.182767, acc 0.921875, learning_rate 0.000100092
2017-10-10T15:03:01.129601: step 2663, loss 0.0608706, acc 1, learning_rate 0.000100092
2017-10-10T15:03:01.461013: step 2664, loss 0.177458, acc 0.96875, learning_rate 0.000100091
2017-10-10T15:03:01.774395: step 2665, loss 0.0540587, acc 0.96875, learning_rate 0.000100091
2017-10-10T15:03:02.074038: step 2666, loss 0.155277, acc 0.953125, learning_rate 0.00010009
2017-10-10T15:03:02.245545: step 2667, loss 0.0973041, acc 0.984375, learning_rate 0.00010009
2017-10-10T15:03:02.488834: step 2668, loss 0.262058, acc 0.9375, learning_rate 0.00010009
2017-10-10T15:03:02.789049: step 2669, loss 0.227704, acc 0.90625, learning_rate 0.000100089
2017-10-10T15:03:03.041109: step 2670, loss 0.12143, acc 0.96875, learning_rate 0.000100089
2017-10-10T15:03:03.306866: step 2671, loss 0.214174, acc 0.9375, learning_rate 0.000100089
2017-10-10T15:03:03.565551: step 2672, loss 0.0961861, acc 0.96875, learning_rate 0.000100088
2017-10-10T15:03:03.863770: step 2673, loss 0.134504, acc 0.9375, learning_rate 0.000100088
2017-10-10T15:03:04.208768: step 2674, loss 0.109075, acc 0.9375, learning_rate 0.000100088
2017-10-10T15:03:04.501075: step 2675, loss 0.115556, acc 0.96875, learning_rate 0.000100087
2017-10-10T15:03:04.865245: step 2676, loss 0.202795, acc 0.921875, learning_rate 0.000100087
2017-10-10T15:03:05.148849: step 2677, loss 0.116673, acc 0.96875, learning_rate 0.000100086
2017-10-10T15:03:05.440910: step 2678, loss 0.139003, acc 0.9375, learning_rate 0.000100086
2017-10-10T15:03:05.660859: step 2679, loss 0.0490074, acc 0.984375, learning_rate 0.000100086
2017-10-10T15:03:05.902541: step 2680, loss 0.147838, acc 0.96875, learning_rate 0.000100085

Evaluation:
2017-10-10T15:03:06.380340: step 2680, loss 0.229085, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2680

2017-10-10T15:03:07.664911: step 2681, loss 0.138009, acc 0.9375, learning_rate 0.000100085
2017-10-10T15:03:07.941633: step 2682, loss 0.104518, acc 0.953125, learning_rate 0.000100085
2017-10-10T15:03:08.149165: step 2683, loss 0.105186, acc 0.984375, learning_rate 0.000100084
2017-10-10T15:03:08.467248: step 2684, loss 0.0793609, acc 0.96875, learning_rate 0.000100084
2017-10-10T15:03:08.780923: step 2685, loss 0.213058, acc 0.921875, learning_rate 0.000100084
2017-10-10T15:03:09.010395: step 2686, loss 0.104406, acc 0.984375, learning_rate 0.000100083
2017-10-10T15:03:09.304879: step 2687, loss 0.157216, acc 0.953125, learning_rate 0.000100083
2017-10-10T15:03:09.592409: step 2688, loss 0.110803, acc 0.96875, learning_rate 0.000100083
2017-10-10T15:03:09.812135: step 2689, loss 0.120205, acc 0.96875, learning_rate 0.000100082
2017-10-10T15:03:10.107214: step 2690, loss 0.191153, acc 0.921875, learning_rate 0.000100082
2017-10-10T15:03:10.438650: step 2691, loss 0.0956142, acc 0.984375, learning_rate 0.000100082
2017-10-10T15:03:10.814575: step 2692, loss 0.136993, acc 0.96875, learning_rate 0.000100081
2017-10-10T15:03:11.041217: step 2693, loss 0.16125, acc 0.953125, learning_rate 0.000100081
2017-10-10T15:03:11.320001: step 2694, loss 0.153519, acc 0.9375, learning_rate 0.000100081
2017-10-10T15:03:11.620844: step 2695, loss 0.141473, acc 0.96875, learning_rate 0.00010008
2017-10-10T15:03:11.961024: step 2696, loss 0.0808348, acc 0.984375, learning_rate 0.00010008
2017-10-10T15:03:12.192994: step 2697, loss 0.160426, acc 0.9375, learning_rate 0.00010008
2017-10-10T15:03:12.486801: step 2698, loss 0.210528, acc 0.890625, learning_rate 0.000100079
2017-10-10T15:03:12.760516: step 2699, loss 0.129098, acc 0.953125, learning_rate 0.000100079
2017-10-10T15:03:13.020236: step 2700, loss 0.123134, acc 0.96875, learning_rate 0.000100079
2017-10-10T15:03:13.357351: step 2701, loss 0.166842, acc 0.9375, learning_rate 0.000100078
2017-10-10T15:03:13.681861: step 2702, loss 0.272721, acc 0.890625, learning_rate 0.000100078
2017-10-10T15:03:13.880604: step 2703, loss 0.129998, acc 0.953125, learning_rate 0.000100078
2017-10-10T15:03:14.122739: step 2704, loss 0.0892799, acc 0.96875, learning_rate 0.000100077
2017-10-10T15:03:14.388983: step 2705, loss 0.138034, acc 0.96875, learning_rate 0.000100077
2017-10-10T15:03:14.657083: step 2706, loss 0.199394, acc 0.921875, learning_rate 0.000100077
2017-10-10T15:03:14.911174: step 2707, loss 0.149561, acc 0.9375, learning_rate 0.000100076
2017-10-10T15:03:15.153017: step 2708, loss 0.105454, acc 0.96875, learning_rate 0.000100076
2017-10-10T15:03:15.447438: step 2709, loss 0.0593841, acc 1, learning_rate 0.000100076
2017-10-10T15:03:15.733016: step 2710, loss 0.0674942, acc 0.984375, learning_rate 0.000100076
2017-10-10T15:03:16.027670: step 2711, loss 0.123969, acc 0.953125, learning_rate 0.000100075
2017-10-10T15:03:16.236168: step 2712, loss 0.172984, acc 0.96875, learning_rate 0.000100075
2017-10-10T15:03:16.492879: step 2713, loss 0.100563, acc 0.96875, learning_rate 0.000100075
2017-10-10T15:03:16.781549: step 2714, loss 0.0994116, acc 0.984375, learning_rate 0.000100074
2017-10-10T15:03:17.008966: step 2715, loss 0.064266, acc 0.984375, learning_rate 0.000100074
2017-10-10T15:03:17.279256: step 2716, loss 0.221839, acc 0.953125, learning_rate 0.000100074
2017-10-10T15:03:17.597921: step 2717, loss 0.119619, acc 0.984375, learning_rate 0.000100073
2017-10-10T15:03:17.797844: step 2718, loss 0.122935, acc 0.96875, learning_rate 0.000100073
2017-10-10T15:03:18.096835: step 2719, loss 0.262486, acc 0.90625, learning_rate 0.000100073
2017-10-10T15:03:18.406024: step 2720, loss 0.0954843, acc 0.953125, learning_rate 0.000100073

Evaluation:
2017-10-10T15:03:18.909987: step 2720, loss 0.229348, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2720

2017-10-10T15:03:20.048934: step 2721, loss 0.167421, acc 0.921875, learning_rate 0.000100072
2017-10-10T15:03:20.308909: step 2722, loss 0.121332, acc 0.953125, learning_rate 0.000100072
2017-10-10T15:03:20.548525: step 2723, loss 0.111301, acc 0.96875, learning_rate 0.000100072
2017-10-10T15:03:20.814630: step 2724, loss 0.100617, acc 0.984375, learning_rate 0.000100071
2017-10-10T15:03:21.064894: step 2725, loss 0.0922849, acc 0.953125, learning_rate 0.000100071
2017-10-10T15:03:21.340937: step 2726, loss 0.0889775, acc 0.984375, learning_rate 0.000100071
2017-10-10T15:03:21.585077: step 2727, loss 0.0820699, acc 0.984375, learning_rate 0.00010007
2017-10-10T15:03:21.871609: step 2728, loss 0.0544209, acc 1, learning_rate 0.00010007
2017-10-10T15:03:22.139236: step 2729, loss 0.108606, acc 0.984375, learning_rate 0.00010007
2017-10-10T15:03:22.421212: step 2730, loss 0.285011, acc 0.859375, learning_rate 0.00010007
2017-10-10T15:03:22.699865: step 2731, loss 0.106345, acc 0.984375, learning_rate 0.000100069
2017-10-10T15:03:22.923455: step 2732, loss 0.151005, acc 0.953125, learning_rate 0.000100069
2017-10-10T15:03:23.209640: step 2733, loss 0.0577151, acc 0.984375, learning_rate 0.000100069
2017-10-10T15:03:23.468965: step 2734, loss 0.132742, acc 0.96875, learning_rate 0.000100068
2017-10-10T15:03:23.708831: step 2735, loss 0.187352, acc 0.921875, learning_rate 0.000100068
2017-10-10T15:03:24.016559: step 2736, loss 0.113167, acc 0.96875, learning_rate 0.000100068
2017-10-10T15:03:24.293290: step 2737, loss 0.0996775, acc 0.953125, learning_rate 0.000100068
2017-10-10T15:03:24.554167: step 2738, loss 0.244955, acc 0.875, learning_rate 0.000100067
2017-10-10T15:03:24.849638: step 2739, loss 0.196073, acc 0.953125, learning_rate 0.000100067
2017-10-10T15:03:25.176835: step 2740, loss 0.141149, acc 0.953125, learning_rate 0.000100067
2017-10-10T15:03:25.421120: step 2741, loss 0.176535, acc 0.984375, learning_rate 0.000100067
2017-10-10T15:03:25.694543: step 2742, loss 0.201348, acc 0.90625, learning_rate 0.000100066
2017-10-10T15:03:25.951158: step 2743, loss 0.109558, acc 0.96875, learning_rate 0.000100066
2017-10-10T15:03:26.201029: step 2744, loss 0.14835, acc 0.921569, learning_rate 0.000100066
2017-10-10T15:03:26.500965: step 2745, loss 0.131947, acc 0.953125, learning_rate 0.000100065
2017-10-10T15:03:26.836856: step 2746, loss 0.15075, acc 0.953125, learning_rate 0.000100065
2017-10-10T15:03:27.095797: step 2747, loss 0.134936, acc 0.9375, learning_rate 0.000100065
2017-10-10T15:03:27.300430: step 2748, loss 0.110535, acc 0.96875, learning_rate 0.000100065
2017-10-10T15:03:27.551412: step 2749, loss 0.183047, acc 0.9375, learning_rate 0.000100064
2017-10-10T15:03:27.832848: step 2750, loss 0.0548732, acc 1, learning_rate 0.000100064
2017-10-10T15:03:28.072724: step 2751, loss 0.180304, acc 0.9375, learning_rate 0.000100064
2017-10-10T15:03:28.372486: step 2752, loss 0.218454, acc 0.984375, learning_rate 0.000100064
2017-10-10T15:03:28.652496: step 2753, loss 0.0960557, acc 0.96875, learning_rate 0.000100063
2017-10-10T15:03:28.972857: step 2754, loss 0.0954952, acc 0.984375, learning_rate 0.000100063
2017-10-10T15:03:29.301021: step 2755, loss 0.0832281, acc 0.984375, learning_rate 0.000100063
2017-10-10T15:03:29.584850: step 2756, loss 0.257181, acc 0.921875, learning_rate 0.000100063
2017-10-10T15:03:29.798122: step 2757, loss 0.050848, acc 1, learning_rate 0.000100062
2017-10-10T15:03:30.109012: step 2758, loss 0.150096, acc 0.96875, learning_rate 0.000100062
2017-10-10T15:03:30.442839: step 2759, loss 0.265867, acc 0.921875, learning_rate 0.000100062
2017-10-10T15:03:30.674030: step 2760, loss 0.102663, acc 0.984375, learning_rate 0.000100062

Evaluation:
2017-10-10T15:03:31.052785: step 2760, loss 0.228444, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2760

2017-10-10T15:03:32.019757: step 2761, loss 0.111172, acc 0.9375, learning_rate 0.000100061
2017-10-10T15:03:32.308874: step 2762, loss 0.0933685, acc 0.96875, learning_rate 0.000100061
2017-10-10T15:03:32.536158: step 2763, loss 0.0709066, acc 0.984375, learning_rate 0.000100061
2017-10-10T15:03:32.777269: step 2764, loss 0.200493, acc 0.90625, learning_rate 0.000100061
2017-10-10T15:03:33.012271: step 2765, loss 0.150198, acc 0.9375, learning_rate 0.00010006
2017-10-10T15:03:33.306737: step 2766, loss 0.0779037, acc 0.96875, learning_rate 0.00010006
2017-10-10T15:03:33.518813: step 2767, loss 0.175593, acc 0.921875, learning_rate 0.00010006
2017-10-10T15:03:33.824826: step 2768, loss 0.161051, acc 0.9375, learning_rate 0.00010006
2017-10-10T15:03:34.111822: step 2769, loss 0.214276, acc 0.90625, learning_rate 0.000100059
2017-10-10T15:03:34.369680: step 2770, loss 0.165461, acc 0.953125, learning_rate 0.000100059
2017-10-10T15:03:34.652892: step 2771, loss 0.124284, acc 0.984375, learning_rate 0.000100059
2017-10-10T15:03:34.934532: step 2772, loss 0.0771833, acc 0.96875, learning_rate 0.000100059
2017-10-10T15:03:35.173172: step 2773, loss 0.124989, acc 0.90625, learning_rate 0.000100058
2017-10-10T15:03:35.488846: step 2774, loss 0.101388, acc 0.984375, learning_rate 0.000100058
2017-10-10T15:03:35.713138: step 2775, loss 0.154723, acc 0.96875, learning_rate 0.000100058
2017-10-10T15:03:35.981399: step 2776, loss 0.164666, acc 0.9375, learning_rate 0.000100058
2017-10-10T15:03:36.284835: step 2777, loss 0.0791584, acc 0.96875, learning_rate 0.000100057
2017-10-10T15:03:36.531621: step 2778, loss 0.083632, acc 0.984375, learning_rate 0.000100057
2017-10-10T15:03:36.821764: step 2779, loss 0.148996, acc 0.953125, learning_rate 0.000100057
2017-10-10T15:03:37.109697: step 2780, loss 0.186566, acc 0.953125, learning_rate 0.000100057
2017-10-10T15:03:37.337131: step 2781, loss 0.191835, acc 0.9375, learning_rate 0.000100056
2017-10-10T15:03:37.648985: step 2782, loss 0.226137, acc 0.890625, learning_rate 0.000100056
2017-10-10T15:03:37.940923: step 2783, loss 0.121151, acc 0.984375, learning_rate 0.000100056
2017-10-10T15:03:38.231864: step 2784, loss 0.085217, acc 0.984375, learning_rate 0.000100056
2017-10-10T15:03:38.397021: step 2785, loss 0.121818, acc 0.953125, learning_rate 0.000100056
2017-10-10T15:03:38.626930: step 2786, loss 0.177766, acc 0.9375, learning_rate 0.000100055
2017-10-10T15:03:38.867727: step 2787, loss 0.121497, acc 0.984375, learning_rate 0.000100055
2017-10-10T15:03:39.067049: step 2788, loss 0.166969, acc 0.953125, learning_rate 0.000100055
2017-10-10T15:03:39.265512: step 2789, loss 0.116817, acc 0.96875, learning_rate 0.000100055
2017-10-10T15:03:39.510526: step 2790, loss 0.187982, acc 0.90625, learning_rate 0.000100054
2017-10-10T15:03:39.764415: step 2791, loss 0.0991571, acc 0.984375, learning_rate 0.000100054
2017-10-10T15:03:40.057113: step 2792, loss 0.037057, acc 1, learning_rate 0.000100054
2017-10-10T15:03:40.332922: step 2793, loss 0.110713, acc 0.953125, learning_rate 0.000100054
2017-10-10T15:03:40.617338: step 2794, loss 0.0806578, acc 0.984375, learning_rate 0.000100054
2017-10-10T15:03:40.917045: step 2795, loss 0.0637201, acc 0.984375, learning_rate 0.000100053
2017-10-10T15:03:41.188846: step 2796, loss 0.0504894, acc 1, learning_rate 0.000100053
2017-10-10T15:03:41.422813: step 2797, loss 0.103648, acc 0.984375, learning_rate 0.000100053
2017-10-10T15:03:41.655786: step 2798, loss 0.1241, acc 0.953125, learning_rate 0.000100053
2017-10-10T15:03:41.973379: step 2799, loss 0.140112, acc 0.953125, learning_rate 0.000100052
2017-10-10T15:03:42.223108: step 2800, loss 0.0807207, acc 0.984375, learning_rate 0.000100052

Evaluation:
2017-10-10T15:03:42.724561: step 2800, loss 0.229699, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2800

2017-10-10T15:03:43.827395: step 2801, loss 0.0585606, acc 0.984375, learning_rate 0.000100052
2017-10-10T15:03:44.116216: step 2802, loss 0.123062, acc 0.984375, learning_rate 0.000100052
2017-10-10T15:03:44.393780: step 2803, loss 0.0986473, acc 0.96875, learning_rate 0.000100052
2017-10-10T15:03:44.680928: step 2804, loss 0.0812367, acc 0.984375, learning_rate 0.000100051
2017-10-10T15:03:44.999649: step 2805, loss 0.144762, acc 0.953125, learning_rate 0.000100051
2017-10-10T15:03:45.298439: step 2806, loss 0.181215, acc 0.953125, learning_rate 0.000100051
2017-10-10T15:03:45.539137: step 2807, loss 0.15071, acc 0.96875, learning_rate 0.000100051
2017-10-10T15:03:45.809931: step 2808, loss 0.157461, acc 0.9375, learning_rate 0.000100051
2017-10-10T15:03:46.121049: step 2809, loss 0.15857, acc 0.953125, learning_rate 0.00010005
2017-10-10T15:03:46.469067: step 2810, loss 0.183326, acc 0.9375, learning_rate 0.00010005
2017-10-10T15:03:46.811517: step 2811, loss 0.205012, acc 0.9375, learning_rate 0.00010005
2017-10-10T15:03:47.101264: step 2812, loss 0.144283, acc 0.96875, learning_rate 0.00010005
2017-10-10T15:03:47.343244: step 2813, loss 0.0646948, acc 1, learning_rate 0.00010005
2017-10-10T15:03:47.576961: step 2814, loss 0.257106, acc 0.921875, learning_rate 0.000100049
2017-10-10T15:03:47.788882: step 2815, loss 0.0580531, acc 1, learning_rate 0.000100049
2017-10-10T15:03:48.064916: step 2816, loss 0.12428, acc 0.953125, learning_rate 0.000100049
2017-10-10T15:03:48.340450: step 2817, loss 0.100202, acc 0.984375, learning_rate 0.000100049
2017-10-10T15:03:48.632583: step 2818, loss 0.127564, acc 0.953125, learning_rate 0.000100049
2017-10-10T15:03:48.949616: step 2819, loss 0.0729868, acc 0.984375, learning_rate 0.000100048
2017-10-10T15:03:49.187878: step 2820, loss 0.0800662, acc 0.984375, learning_rate 0.000100048
2017-10-10T15:03:49.463119: step 2821, loss 0.250365, acc 0.9375, learning_rate 0.000100048
2017-10-10T15:03:49.731506: step 2822, loss 0.220482, acc 0.921875, learning_rate 0.000100048
2017-10-10T15:03:49.970143: step 2823, loss 0.0574936, acc 0.984375, learning_rate 0.000100048
2017-10-10T15:03:50.306640: step 2824, loss 0.164757, acc 0.96875, learning_rate 0.000100047
2017-10-10T15:03:50.577497: step 2825, loss 0.134589, acc 0.953125, learning_rate 0.000100047
2017-10-10T15:03:50.832937: step 2826, loss 0.122239, acc 0.96875, learning_rate 0.000100047
2017-10-10T15:03:51.097024: step 2827, loss 0.139338, acc 0.9375, learning_rate 0.000100047
2017-10-10T15:03:51.347974: step 2828, loss 0.129879, acc 0.953125, learning_rate 0.000100047
2017-10-10T15:03:51.619879: step 2829, loss 0.179971, acc 0.9375, learning_rate 0.000100046
2017-10-10T15:03:51.864920: step 2830, loss 0.125779, acc 0.953125, learning_rate 0.000100046
2017-10-10T15:03:52.156480: step 2831, loss 0.135258, acc 0.953125, learning_rate 0.000100046
2017-10-10T15:03:52.484831: step 2832, loss 0.169356, acc 0.921875, learning_rate 0.000100046
2017-10-10T15:03:52.732803: step 2833, loss 0.114832, acc 0.984375, learning_rate 0.000100046
2017-10-10T15:03:53.013063: step 2834, loss 0.137696, acc 0.953125, learning_rate 0.000100045
2017-10-10T15:03:53.310886: step 2835, loss 0.141048, acc 0.96875, learning_rate 0.000100045
2017-10-10T15:03:53.567546: step 2836, loss 0.192129, acc 0.96875, learning_rate 0.000100045
2017-10-10T15:03:53.831679: step 2837, loss 0.187314, acc 0.921875, learning_rate 0.000100045
2017-10-10T15:03:54.081207: step 2838, loss 0.173345, acc 0.921875, learning_rate 0.000100045
2017-10-10T15:03:54.363884: step 2839, loss 0.0991144, acc 0.96875, learning_rate 0.000100045
2017-10-10T15:03:54.664874: step 2840, loss 0.101658, acc 0.96875, learning_rate 0.000100044

Evaluation:
2017-10-10T15:03:55.126965: step 2840, loss 0.229228, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2840

2017-10-10T15:03:56.192842: step 2841, loss 0.126086, acc 0.953125, learning_rate 0.000100044
2017-10-10T15:03:56.473209: step 2842, loss 0.262766, acc 0.921569, learning_rate 0.000100044
2017-10-10T15:03:56.790647: step 2843, loss 0.165539, acc 0.953125, learning_rate 0.000100044
2017-10-10T15:03:57.036817: step 2844, loss 0.0892971, acc 0.984375, learning_rate 0.000100044
2017-10-10T15:03:57.290453: step 2845, loss 0.163419, acc 0.96875, learning_rate 0.000100043
2017-10-10T15:03:57.502515: step 2846, loss 0.0680333, acc 0.984375, learning_rate 0.000100043
2017-10-10T15:03:57.741935: step 2847, loss 0.114466, acc 0.953125, learning_rate 0.000100043
2017-10-10T15:03:58.040953: step 2848, loss 0.126308, acc 0.9375, learning_rate 0.000100043
2017-10-10T15:03:58.385041: step 2849, loss 0.212258, acc 0.90625, learning_rate 0.000100043
2017-10-10T15:03:58.633136: step 2850, loss 0.211766, acc 0.90625, learning_rate 0.000100043
2017-10-10T15:03:58.866144: step 2851, loss 0.125652, acc 0.984375, learning_rate 0.000100042
2017-10-10T15:03:59.128827: step 2852, loss 0.11096, acc 0.96875, learning_rate 0.000100042
2017-10-10T15:03:59.429949: step 2853, loss 0.130472, acc 0.9375, learning_rate 0.000100042
2017-10-10T15:03:59.705013: step 2854, loss 0.096014, acc 0.953125, learning_rate 0.000100042
2017-10-10T15:03:59.920894: step 2855, loss 0.213973, acc 0.921875, learning_rate 0.000100042
2017-10-10T15:04:00.235300: step 2856, loss 0.19965, acc 0.90625, learning_rate 0.000100042
2017-10-10T15:04:00.457724: step 2857, loss 0.0795038, acc 0.984375, learning_rate 0.000100041
2017-10-10T15:04:00.709679: step 2858, loss 0.101789, acc 0.984375, learning_rate 0.000100041
2017-10-10T15:04:00.963074: step 2859, loss 0.133933, acc 0.953125, learning_rate 0.000100041
2017-10-10T15:04:01.222546: step 2860, loss 0.111471, acc 0.984375, learning_rate 0.000100041
2017-10-10T15:04:01.485045: step 2861, loss 0.134944, acc 0.9375, learning_rate 0.000100041
2017-10-10T15:04:01.776046: step 2862, loss 0.123172, acc 0.96875, learning_rate 0.000100041
2017-10-10T15:04:02.073062: step 2863, loss 0.112721, acc 0.96875, learning_rate 0.00010004
2017-10-10T15:04:02.312725: step 2864, loss 0.195127, acc 0.953125, learning_rate 0.00010004
2017-10-10T15:04:02.621771: step 2865, loss 0.0675548, acc 0.984375, learning_rate 0.00010004
2017-10-10T15:04:02.969437: step 2866, loss 0.0808631, acc 0.96875, learning_rate 0.00010004
2017-10-10T15:04:03.284872: step 2867, loss 0.0999275, acc 0.984375, learning_rate 0.00010004
2017-10-10T15:04:03.515680: step 2868, loss 0.0898592, acc 0.96875, learning_rate 0.00010004
2017-10-10T15:04:03.739572: step 2869, loss 0.128227, acc 0.96875, learning_rate 0.000100039
2017-10-10T15:04:03.957202: step 2870, loss 0.0876851, acc 0.953125, learning_rate 0.000100039
2017-10-10T15:04:04.196252: step 2871, loss 0.133636, acc 0.984375, learning_rate 0.000100039
2017-10-10T15:04:04.477043: step 2872, loss 0.132513, acc 0.9375, learning_rate 0.000100039
2017-10-10T15:04:04.792857: step 2873, loss 0.154626, acc 0.9375, learning_rate 0.000100039
2017-10-10T15:04:05.101159: step 2874, loss 0.131442, acc 0.96875, learning_rate 0.000100039
2017-10-10T15:04:05.381349: step 2875, loss 0.161977, acc 0.9375, learning_rate 0.000100038
2017-10-10T15:04:05.678604: step 2876, loss 0.0517246, acc 0.984375, learning_rate 0.000100038
2017-10-10T15:04:05.973375: step 2877, loss 0.134471, acc 0.9375, learning_rate 0.000100038
2017-10-10T15:04:06.282119: step 2878, loss 0.110991, acc 0.953125, learning_rate 0.000100038
2017-10-10T15:04:06.555288: step 2879, loss 0.136993, acc 0.953125, learning_rate 0.000100038
2017-10-10T15:04:06.830971: step 2880, loss 0.100997, acc 0.953125, learning_rate 0.000100038

Evaluation:
2017-10-10T15:04:07.290358: step 2880, loss 0.229403, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2880

2017-10-10T15:04:08.324901: step 2881, loss 0.172822, acc 0.9375, learning_rate 0.000100038
2017-10-10T15:04:08.585147: step 2882, loss 0.0758382, acc 0.96875, learning_rate 0.000100037
2017-10-10T15:04:08.890056: step 2883, loss 0.156255, acc 0.9375, learning_rate 0.000100037
2017-10-10T15:04:09.212918: step 2884, loss 0.130671, acc 0.953125, learning_rate 0.000100037
2017-10-10T15:04:09.463712: step 2885, loss 0.0877302, acc 0.96875, learning_rate 0.000100037
2017-10-10T15:04:09.750673: step 2886, loss 0.144297, acc 0.96875, learning_rate 0.000100037
2017-10-10T15:04:10.056877: step 2887, loss 0.0882696, acc 1, learning_rate 0.000100037
2017-10-10T15:04:10.345469: step 2888, loss 0.185137, acc 0.921875, learning_rate 0.000100036
2017-10-10T15:04:10.589781: step 2889, loss 0.156309, acc 0.953125, learning_rate 0.000100036
2017-10-10T15:04:10.904820: step 2890, loss 0.114104, acc 0.984375, learning_rate 0.000100036
2017-10-10T15:04:11.205004: step 2891, loss 0.139594, acc 0.953125, learning_rate 0.000100036
2017-10-10T15:04:11.499717: step 2892, loss 0.0999143, acc 0.984375, learning_rate 0.000100036
2017-10-10T15:04:11.762007: step 2893, loss 0.143871, acc 0.953125, learning_rate 0.000100036
2017-10-10T15:04:12.029779: step 2894, loss 0.170813, acc 0.9375, learning_rate 0.000100036
2017-10-10T15:04:12.224682: step 2895, loss 0.0948172, acc 0.953125, learning_rate 0.000100035
2017-10-10T15:04:12.524122: step 2896, loss 0.126407, acc 0.984375, learning_rate 0.000100035
2017-10-10T15:04:12.814568: step 2897, loss 0.0891808, acc 0.96875, learning_rate 0.000100035
2017-10-10T15:04:13.092784: step 2898, loss 0.153675, acc 0.953125, learning_rate 0.000100035
2017-10-10T15:04:13.427907: step 2899, loss 0.0843743, acc 0.984375, learning_rate 0.000100035
2017-10-10T15:04:13.677245: step 2900, loss 0.237152, acc 0.875, learning_rate 0.000100035
2017-10-10T15:04:13.985034: step 2901, loss 0.111565, acc 0.984375, learning_rate 0.000100035
2017-10-10T15:04:14.322911: step 2902, loss 0.0585895, acc 0.984375, learning_rate 0.000100034
2017-10-10T15:04:14.628872: step 2903, loss 0.176796, acc 0.953125, learning_rate 0.000100034
2017-10-10T15:04:14.847923: step 2904, loss 0.116107, acc 0.96875, learning_rate 0.000100034
2017-10-10T15:04:15.072828: step 2905, loss 0.122396, acc 0.96875, learning_rate 0.000100034
2017-10-10T15:04:15.319066: step 2906, loss 0.307812, acc 0.890625, learning_rate 0.000100034
2017-10-10T15:04:15.535714: step 2907, loss 0.196604, acc 0.921875, learning_rate 0.000100034
2017-10-10T15:04:15.827086: step 2908, loss 0.0689686, acc 0.984375, learning_rate 0.000100034
2017-10-10T15:04:16.124991: step 2909, loss 0.2238, acc 0.90625, learning_rate 0.000100033
2017-10-10T15:04:16.364285: step 2910, loss 0.0765885, acc 1, learning_rate 0.000100033
2017-10-10T15:04:16.638149: step 2911, loss 0.131404, acc 0.953125, learning_rate 0.000100033
2017-10-10T15:04:16.933734: step 2912, loss 0.163879, acc 0.96875, learning_rate 0.000100033
2017-10-10T15:04:17.238463: step 2913, loss 0.290193, acc 0.890625, learning_rate 0.000100033
2017-10-10T15:04:17.475345: step 2914, loss 0.206365, acc 0.9375, learning_rate 0.000100033
2017-10-10T15:04:17.718951: step 2915, loss 0.156543, acc 0.9375, learning_rate 0.000100033
2017-10-10T15:04:18.092630: step 2916, loss 0.080674, acc 0.984375, learning_rate 0.000100033
2017-10-10T15:04:18.388961: step 2917, loss 0.101954, acc 0.96875, learning_rate 0.000100032
2017-10-10T15:04:18.640849: step 2918, loss 0.154969, acc 0.953125, learning_rate 0.000100032
2017-10-10T15:04:18.902300: step 2919, loss 0.188312, acc 0.9375, learning_rate 0.000100032
2017-10-10T15:04:19.209423: step 2920, loss 0.213051, acc 0.953125, learning_rate 0.000100032

Evaluation:
2017-10-10T15:04:19.674473: step 2920, loss 0.226247, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2920

2017-10-10T15:04:20.722936: step 2921, loss 0.0693122, acc 0.984375, learning_rate 0.000100032
2017-10-10T15:04:20.972664: step 2922, loss 0.13081, acc 0.953125, learning_rate 0.000100032
2017-10-10T15:04:21.161030: step 2923, loss 0.126611, acc 0.953125, learning_rate 0.000100032
2017-10-10T15:04:21.433075: step 2924, loss 0.0306545, acc 1, learning_rate 0.000100031
2017-10-10T15:04:21.704483: step 2925, loss 0.0782246, acc 0.96875, learning_rate 0.000100031
2017-10-10T15:04:22.006160: step 2926, loss 0.204021, acc 0.90625, learning_rate 0.000100031
2017-10-10T15:04:22.288838: step 2927, loss 0.173086, acc 0.953125, learning_rate 0.000100031
2017-10-10T15:04:22.626946: step 2928, loss 0.222292, acc 0.890625, learning_rate 0.000100031
2017-10-10T15:04:22.858502: step 2929, loss 0.110278, acc 0.96875, learning_rate 0.000100031
2017-10-10T15:04:23.060521: step 2930, loss 0.15981, acc 0.9375, learning_rate 0.000100031
2017-10-10T15:04:23.355159: step 2931, loss 0.1337, acc 0.96875, learning_rate 0.000100031
2017-10-10T15:04:23.651129: step 2932, loss 0.192484, acc 0.921875, learning_rate 0.00010003
2017-10-10T15:04:23.928221: step 2933, loss 0.17574, acc 0.9375, learning_rate 0.00010003
2017-10-10T15:04:24.205067: step 2934, loss 0.133714, acc 0.96875, learning_rate 0.00010003
2017-10-10T15:04:24.512824: step 2935, loss 0.122638, acc 0.9375, learning_rate 0.00010003
2017-10-10T15:04:24.803334: step 2936, loss 0.114596, acc 0.984375, learning_rate 0.00010003
2017-10-10T15:04:25.104287: step 2937, loss 0.0885824, acc 0.984375, learning_rate 0.00010003
2017-10-10T15:04:25.334814: step 2938, loss 0.0605759, acc 1, learning_rate 0.00010003
2017-10-10T15:04:25.621570: step 2939, loss 0.183672, acc 0.953125, learning_rate 0.00010003
2017-10-10T15:04:25.837068: step 2940, loss 0.0651145, acc 0.980392, learning_rate 0.000100029
2017-10-10T15:04:26.145076: step 2941, loss 0.130607, acc 0.953125, learning_rate 0.000100029
2017-10-10T15:04:26.461276: step 2942, loss 0.203006, acc 0.9375, learning_rate 0.000100029
2017-10-10T15:04:26.713362: step 2943, loss 0.106261, acc 0.96875, learning_rate 0.000100029
2017-10-10T15:04:26.986221: step 2944, loss 0.0851193, acc 0.984375, learning_rate 0.000100029
2017-10-10T15:04:27.268357: step 2945, loss 0.134762, acc 0.953125, learning_rate 0.000100029
2017-10-10T15:04:27.598179: step 2946, loss 0.110673, acc 0.953125, learning_rate 0.000100029
2017-10-10T15:04:27.810150: step 2947, loss 0.0834575, acc 0.96875, learning_rate 0.000100029
2017-10-10T15:04:28.120874: step 2948, loss 0.145003, acc 0.984375, learning_rate 0.000100029
2017-10-10T15:04:28.393049: step 2949, loss 0.133356, acc 0.953125, learning_rate 0.000100028
2017-10-10T15:04:28.625200: step 2950, loss 0.19832, acc 0.921875, learning_rate 0.000100028
2017-10-10T15:04:28.943556: step 2951, loss 0.152912, acc 0.9375, learning_rate 0.000100028
2017-10-10T15:04:29.250180: step 2952, loss 0.157499, acc 0.96875, learning_rate 0.000100028
2017-10-10T15:04:29.491273: step 2953, loss 0.138111, acc 0.9375, learning_rate 0.000100028
2017-10-10T15:04:29.780171: step 2954, loss 0.146025, acc 0.953125, learning_rate 0.000100028
2017-10-10T15:04:30.096835: step 2955, loss 0.145898, acc 0.9375, learning_rate 0.000100028
2017-10-10T15:04:30.352933: step 2956, loss 0.14834, acc 0.96875, learning_rate 0.000100028
2017-10-10T15:04:30.615833: step 2957, loss 0.107851, acc 0.96875, learning_rate 0.000100028
2017-10-10T15:04:30.822832: step 2958, loss 0.146018, acc 0.921875, learning_rate 0.000100027
2017-10-10T15:04:31.124698: step 2959, loss 0.0845962, acc 0.96875, learning_rate 0.000100027
2017-10-10T15:04:31.376919: step 2960, loss 0.119322, acc 0.953125, learning_rate 0.000100027

Evaluation:
2017-10-10T15:04:31.875570: step 2960, loss 0.226887, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-2960

2017-10-10T15:04:33.065909: step 2961, loss 0.194903, acc 0.953125, learning_rate 0.000100027
2017-10-10T15:04:33.284663: step 2962, loss 0.186621, acc 0.953125, learning_rate 0.000100027
2017-10-10T15:04:33.593657: step 2963, loss 0.0997293, acc 0.96875, learning_rate 0.000100027
2017-10-10T15:04:33.891090: step 2964, loss 0.0616039, acc 0.984375, learning_rate 0.000100027
2017-10-10T15:04:34.168973: step 2965, loss 0.112555, acc 0.953125, learning_rate 0.000100027
2017-10-10T15:04:34.417505: step 2966, loss 0.185342, acc 0.953125, learning_rate 0.000100027
2017-10-10T15:04:34.703766: step 2967, loss 0.227601, acc 0.921875, learning_rate 0.000100026
2017-10-10T15:04:35.007768: step 2968, loss 0.083806, acc 1, learning_rate 0.000100026
2017-10-10T15:04:35.231966: step 2969, loss 0.122346, acc 0.953125, learning_rate 0.000100026
2017-10-10T15:04:35.523539: step 2970, loss 0.0556332, acc 1, learning_rate 0.000100026
2017-10-10T15:04:35.786834: step 2971, loss 0.0956601, acc 0.984375, learning_rate 0.000100026
2017-10-10T15:04:36.100863: step 2972, loss 0.246258, acc 0.875, learning_rate 0.000100026
2017-10-10T15:04:36.417156: step 2973, loss 0.0598369, acc 0.984375, learning_rate 0.000100026
2017-10-10T15:04:36.688972: step 2974, loss 0.132009, acc 0.953125, learning_rate 0.000100026
2017-10-10T15:04:36.916975: step 2975, loss 0.155528, acc 0.9375, learning_rate 0.000100026
2017-10-10T15:04:37.137138: step 2976, loss 0.055398, acc 0.984375, learning_rate 0.000100025
2017-10-10T15:04:37.359427: step 2977, loss 0.133838, acc 0.96875, learning_rate 0.000100025
2017-10-10T15:04:37.616333: step 2978, loss 0.138949, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:04:37.857997: step 2979, loss 0.113957, acc 0.9375, learning_rate 0.000100025
2017-10-10T15:04:38.180955: step 2980, loss 0.0990138, acc 1, learning_rate 0.000100025
2017-10-10T15:04:38.501037: step 2981, loss 0.195395, acc 0.90625, learning_rate 0.000100025
2017-10-10T15:04:38.768923: step 2982, loss 0.12183, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:04:38.999895: step 2983, loss 0.191141, acc 0.953125, learning_rate 0.000100025
2017-10-10T15:04:39.254644: step 2984, loss 0.054427, acc 0.984375, learning_rate 0.000100025
2017-10-10T15:04:39.468985: step 2985, loss 0.0687738, acc 0.984375, learning_rate 0.000100025
2017-10-10T15:04:39.816987: step 2986, loss 0.103555, acc 0.96875, learning_rate 0.000100024
2017-10-10T15:04:40.090050: step 2987, loss 0.126091, acc 0.953125, learning_rate 0.000100024
2017-10-10T15:04:40.339991: step 2988, loss 0.152054, acc 0.953125, learning_rate 0.000100024
2017-10-10T15:04:40.641765: step 2989, loss 0.221515, acc 0.90625, learning_rate 0.000100024
2017-10-10T15:04:40.927663: step 2990, loss 0.0990474, acc 0.96875, learning_rate 0.000100024
2017-10-10T15:04:41.213282: step 2991, loss 0.118528, acc 0.96875, learning_rate 0.000100024
2017-10-10T15:04:41.461447: step 2992, loss 0.22213, acc 0.90625, learning_rate 0.000100024
2017-10-10T15:04:41.781477: step 2993, loss 0.0966297, acc 0.984375, learning_rate 0.000100024
2017-10-10T15:04:42.055889: step 2994, loss 0.122811, acc 0.9375, learning_rate 0.000100024
2017-10-10T15:04:42.285541: step 2995, loss 0.0494432, acc 1, learning_rate 0.000100024
2017-10-10T15:04:42.548869: step 2996, loss 0.121529, acc 0.984375, learning_rate 0.000100023
2017-10-10T15:04:42.822319: step 2997, loss 0.0318808, acc 1, learning_rate 0.000100023
2017-10-10T15:04:43.113071: step 2998, loss 0.13369, acc 0.9375, learning_rate 0.000100023
2017-10-10T15:04:43.404824: step 2999, loss 0.134883, acc 0.9375, learning_rate 0.000100023
2017-10-10T15:04:43.624956: step 3000, loss 0.156897, acc 0.9375, learning_rate 0.000100023

Evaluation:
2017-10-10T15:04:44.121057: step 3000, loss 0.227478, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3000

2017-10-10T15:04:45.192773: step 3001, loss 0.159504, acc 0.921875, learning_rate 0.000100023
2017-10-10T15:04:45.458508: step 3002, loss 0.0968177, acc 0.984375, learning_rate 0.000100023
2017-10-10T15:04:45.742376: step 3003, loss 0.181687, acc 0.953125, learning_rate 0.000100023
2017-10-10T15:04:46.011124: step 3004, loss 0.0862306, acc 0.984375, learning_rate 0.000100023
2017-10-10T15:04:46.304327: step 3005, loss 0.0791217, acc 0.96875, learning_rate 0.000100023
2017-10-10T15:04:46.604542: step 3006, loss 0.187111, acc 0.921875, learning_rate 0.000100023
2017-10-10T15:04:46.886230: step 3007, loss 0.14165, acc 0.9375, learning_rate 0.000100022
2017-10-10T15:04:47.117691: step 3008, loss 0.089376, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:04:47.415116: step 3009, loss 0.111243, acc 0.953125, learning_rate 0.000100022
2017-10-10T15:04:47.688921: step 3010, loss 0.161763, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:04:47.976203: step 3011, loss 0.0744222, acc 0.96875, learning_rate 0.000100022
2017-10-10T15:04:48.246005: step 3012, loss 0.105532, acc 0.984375, learning_rate 0.000100022
2017-10-10T15:04:48.503322: step 3013, loss 0.10059, acc 0.953125, learning_rate 0.000100022
2017-10-10T15:04:48.745354: step 3014, loss 0.119612, acc 0.953125, learning_rate 0.000100022
2017-10-10T15:04:49.001231: step 3015, loss 0.212582, acc 0.9375, learning_rate 0.000100022
2017-10-10T15:04:49.279847: step 3016, loss 0.179778, acc 0.921875, learning_rate 0.000100022
2017-10-10T15:04:49.540874: step 3017, loss 0.141851, acc 0.953125, learning_rate 0.000100022
2017-10-10T15:04:49.824932: step 3018, loss 0.0902128, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:04:50.153163: step 3019, loss 0.117445, acc 0.96875, learning_rate 0.000100021
2017-10-10T15:04:50.489654: step 3020, loss 0.0500093, acc 1, learning_rate 0.000100021
2017-10-10T15:04:50.724757: step 3021, loss 0.0923783, acc 0.984375, learning_rate 0.000100021
2017-10-10T15:04:50.960837: step 3022, loss 0.136176, acc 0.9375, learning_rate 0.000100021
2017-10-10T15:04:51.218918: step 3023, loss 0.130173, acc 0.953125, learning_rate 0.000100021
2017-10-10T15:04:51.561082: step 3024, loss 0.0813022, acc 0.984375, learning_rate 0.000100021
2017-10-10T15:04:51.797693: step 3025, loss 0.183071, acc 0.921875, learning_rate 0.000100021
2017-10-10T15:04:52.100859: step 3026, loss 0.177725, acc 0.9375, learning_rate 0.000100021
2017-10-10T15:04:52.357029: step 3027, loss 0.0782855, acc 0.984375, learning_rate 0.000100021
2017-10-10T15:04:52.612090: step 3028, loss 0.162996, acc 0.96875, learning_rate 0.000100021
2017-10-10T15:04:52.920877: step 3029, loss 0.0935017, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:04:53.225182: step 3030, loss 0.108015, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:04:53.464867: step 3031, loss 0.121393, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:04:53.705114: step 3032, loss 0.165986, acc 0.9375, learning_rate 0.00010002
2017-10-10T15:04:53.948897: step 3033, loss 0.157011, acc 0.9375, learning_rate 0.00010002
2017-10-10T15:04:54.165110: step 3034, loss 0.271943, acc 0.90625, learning_rate 0.00010002
2017-10-10T15:04:54.407212: step 3035, loss 0.0965067, acc 0.96875, learning_rate 0.00010002
2017-10-10T15:04:54.727792: step 3036, loss 0.249073, acc 0.890625, learning_rate 0.00010002
2017-10-10T15:04:55.042267: step 3037, loss 0.0600056, acc 0.984375, learning_rate 0.00010002
2017-10-10T15:04:55.261406: step 3038, loss 0.196122, acc 0.921569, learning_rate 0.00010002
2017-10-10T15:04:55.625805: step 3039, loss 0.163048, acc 0.9375, learning_rate 0.00010002
2017-10-10T15:04:55.912844: step 3040, loss 0.10619, acc 0.96875, learning_rate 0.00010002

Evaluation:
2017-10-10T15:04:56.332057: step 3040, loss 0.226098, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3040

2017-10-10T15:04:57.336876: step 3041, loss 0.144142, acc 0.953125, learning_rate 0.00010002
2017-10-10T15:04:57.644862: step 3042, loss 0.135854, acc 0.953125, learning_rate 0.000100019
2017-10-10T15:04:57.901726: step 3043, loss 0.0929721, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:04:58.184914: step 3044, loss 0.116497, acc 0.953125, learning_rate 0.000100019
2017-10-10T15:04:58.444513: step 3045, loss 0.0623733, acc 1, learning_rate 0.000100019
2017-10-10T15:04:58.714538: step 3046, loss 0.0741266, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:04:59.009295: step 3047, loss 0.220736, acc 0.90625, learning_rate 0.000100019
2017-10-10T15:04:59.297247: step 3048, loss 0.12609, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:04:59.560880: step 3049, loss 0.198861, acc 0.90625, learning_rate 0.000100019
2017-10-10T15:04:59.824877: step 3050, loss 0.0616467, acc 1, learning_rate 0.000100019
2017-10-10T15:05:00.130557: step 3051, loss 0.118728, acc 0.953125, learning_rate 0.000100019
2017-10-10T15:05:00.432855: step 3052, loss 0.0920788, acc 0.96875, learning_rate 0.000100019
2017-10-10T15:05:00.687037: step 3053, loss 0.127938, acc 0.921875, learning_rate 0.000100019
2017-10-10T15:05:00.953239: step 3054, loss 0.163482, acc 0.9375, learning_rate 0.000100018
2017-10-10T15:05:01.305062: step 3055, loss 0.0568244, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:05:01.600819: step 3056, loss 0.0677656, acc 0.984375, learning_rate 0.000100018
2017-10-10T15:05:01.920862: step 3057, loss 0.0770783, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:05:02.241392: step 3058, loss 0.0889015, acc 0.984375, learning_rate 0.000100018
2017-10-10T15:05:02.527894: step 3059, loss 0.104571, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:05:02.815487: step 3060, loss 0.161252, acc 0.953125, learning_rate 0.000100018
2017-10-10T15:05:03.074253: step 3061, loss 0.132436, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:05:03.376813: step 3062, loss 0.171262, acc 0.921875, learning_rate 0.000100018
2017-10-10T15:05:03.622334: step 3063, loss 0.156238, acc 0.921875, learning_rate 0.000100018
2017-10-10T15:05:03.904499: step 3064, loss 0.105361, acc 0.96875, learning_rate 0.000100018
2017-10-10T15:05:04.221408: step 3065, loss 0.226721, acc 0.90625, learning_rate 0.000100018
2017-10-10T15:05:04.481997: step 3066, loss 0.206518, acc 0.9375, learning_rate 0.000100018
2017-10-10T15:05:04.751027: step 3067, loss 0.130012, acc 0.984375, learning_rate 0.000100018
2017-10-10T15:05:04.992854: step 3068, loss 0.0715969, acc 0.984375, learning_rate 0.000100017
2017-10-10T15:05:05.296860: step 3069, loss 0.124456, acc 0.984375, learning_rate 0.000100017
2017-10-10T15:05:05.542884: step 3070, loss 0.116761, acc 0.921875, learning_rate 0.000100017
2017-10-10T15:05:05.750460: step 3071, loss 0.132016, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:05:06.021733: step 3072, loss 0.089222, acc 0.984375, learning_rate 0.000100017
2017-10-10T15:05:06.322302: step 3073, loss 0.0802925, acc 0.984375, learning_rate 0.000100017
2017-10-10T15:05:06.676949: step 3074, loss 0.144888, acc 0.953125, learning_rate 0.000100017
2017-10-10T15:05:06.893957: step 3075, loss 0.0686706, acc 0.984375, learning_rate 0.000100017
2017-10-10T15:05:07.142432: step 3076, loss 0.118017, acc 0.96875, learning_rate 0.000100017
2017-10-10T15:05:07.388828: step 3077, loss 0.232033, acc 0.921875, learning_rate 0.000100017
2017-10-10T15:05:07.607342: step 3078, loss 0.0834711, acc 0.984375, learning_rate 0.000100017
2017-10-10T15:05:07.898375: step 3079, loss 0.172554, acc 0.9375, learning_rate 0.000100017
2017-10-10T15:05:08.216844: step 3080, loss 0.0671778, acc 1, learning_rate 0.000100017

Evaluation:
2017-10-10T15:05:08.715713: step 3080, loss 0.227853, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3080

2017-10-10T15:05:09.771133: step 3081, loss 0.234547, acc 0.890625, learning_rate 0.000100017
2017-10-10T15:05:10.044923: step 3082, loss 0.142525, acc 0.9375, learning_rate 0.000100016
2017-10-10T15:05:10.289088: step 3083, loss 0.0664227, acc 0.984375, learning_rate 0.000100016
2017-10-10T15:05:10.556841: step 3084, loss 0.138582, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:05:10.780950: step 3085, loss 0.109386, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:05:11.034613: step 3086, loss 0.13255, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:05:11.338739: step 3087, loss 0.184294, acc 0.921875, learning_rate 0.000100016
2017-10-10T15:05:11.601229: step 3088, loss 0.243451, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:05:11.900240: step 3089, loss 0.132427, acc 0.953125, learning_rate 0.000100016
2017-10-10T15:05:12.152928: step 3090, loss 0.0923053, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:05:12.436892: step 3091, loss 0.267757, acc 0.890625, learning_rate 0.000100016
2017-10-10T15:05:12.714508: step 3092, loss 0.141037, acc 0.984375, learning_rate 0.000100016
2017-10-10T15:05:13.035359: step 3093, loss 0.0866688, acc 0.984375, learning_rate 0.000100016
2017-10-10T15:05:13.281521: step 3094, loss 0.0732327, acc 0.984375, learning_rate 0.000100016
2017-10-10T15:05:13.557532: step 3095, loss 0.235996, acc 0.921875, learning_rate 0.000100016
2017-10-10T15:05:13.801642: step 3096, loss 0.192473, acc 0.9375, learning_rate 0.000100016
2017-10-10T15:05:14.078992: step 3097, loss 0.101606, acc 0.96875, learning_rate 0.000100016
2017-10-10T15:05:14.374287: step 3098, loss 0.151742, acc 0.921875, learning_rate 0.000100015
2017-10-10T15:05:14.633567: step 3099, loss 0.14917, acc 0.9375, learning_rate 0.000100015
2017-10-10T15:05:14.913110: step 3100, loss 0.13997, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:05:15.200858: step 3101, loss 0.136205, acc 0.9375, learning_rate 0.000100015
2017-10-10T15:05:15.488126: step 3102, loss 0.0516181, acc 1, learning_rate 0.000100015
2017-10-10T15:05:15.745929: step 3103, loss 0.118179, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:05:16.048882: step 3104, loss 0.126884, acc 0.921875, learning_rate 0.000100015
2017-10-10T15:05:16.319327: step 3105, loss 0.110636, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:05:16.593055: step 3106, loss 0.131456, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:05:16.848957: step 3107, loss 0.121316, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:05:17.146101: step 3108, loss 0.0457975, acc 1, learning_rate 0.000100015
2017-10-10T15:05:17.473960: step 3109, loss 0.110813, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:05:17.730435: step 3110, loss 0.117477, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:05:18.049128: step 3111, loss 0.19381, acc 0.921875, learning_rate 0.000100015
2017-10-10T15:05:18.352830: step 3112, loss 0.134055, acc 0.953125, learning_rate 0.000100015
2017-10-10T15:05:18.627539: step 3113, loss 0.131385, acc 0.96875, learning_rate 0.000100015
2017-10-10T15:05:18.846961: step 3114, loss 0.164716, acc 0.921875, learning_rate 0.000100014
2017-10-10T15:05:19.065062: step 3115, loss 0.13139, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:05:19.322135: step 3116, loss 0.117314, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:05:19.539733: step 3117, loss 0.241821, acc 0.9375, learning_rate 0.000100014
2017-10-10T15:05:19.788091: step 3118, loss 0.0714487, acc 0.984375, learning_rate 0.000100014
2017-10-10T15:05:20.096261: step 3119, loss 0.138544, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:05:20.382091: step 3120, loss 0.273521, acc 0.90625, learning_rate 0.000100014

Evaluation:
2017-10-10T15:05:20.792541: step 3120, loss 0.227081, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3120

2017-10-10T15:05:21.932977: step 3121, loss 0.106462, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:05:22.192205: step 3122, loss 0.110456, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:05:22.513059: step 3123, loss 0.232891, acc 0.921875, learning_rate 0.000100014
2017-10-10T15:05:22.840825: step 3124, loss 0.0945485, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:05:23.122847: step 3125, loss 0.128221, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:05:23.362837: step 3126, loss 0.160209, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:05:23.693157: step 3127, loss 0.0971019, acc 0.953125, learning_rate 0.000100014
2017-10-10T15:05:23.991261: step 3128, loss 0.0646593, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:05:24.262625: step 3129, loss 0.130769, acc 0.9375, learning_rate 0.000100014
2017-10-10T15:05:24.612851: step 3130, loss 0.160042, acc 0.96875, learning_rate 0.000100014
2017-10-10T15:05:24.865138: step 3131, loss 0.11108, acc 0.9375, learning_rate 0.000100014
2017-10-10T15:05:25.118146: step 3132, loss 0.084168, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:05:25.419622: step 3133, loss 0.101642, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:05:25.672993: step 3134, loss 0.161973, acc 0.9375, learning_rate 0.000100013
2017-10-10T15:05:25.952945: step 3135, loss 0.15146, acc 0.9375, learning_rate 0.000100013
2017-10-10T15:05:26.272991: step 3136, loss 0.0788195, acc 0.980392, learning_rate 0.000100013
2017-10-10T15:05:26.589150: step 3137, loss 0.150137, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:05:26.871719: step 3138, loss 0.115716, acc 0.9375, learning_rate 0.000100013
2017-10-10T15:05:27.055883: step 3139, loss 0.204211, acc 0.921875, learning_rate 0.000100013
2017-10-10T15:05:27.262646: step 3140, loss 0.149784, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:05:27.449980: step 3141, loss 0.124421, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:05:27.644956: step 3142, loss 0.0839369, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:05:27.983901: step 3143, loss 0.140149, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:05:28.304861: step 3144, loss 0.115314, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:05:28.644397: step 3145, loss 0.0490624, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:05:28.944346: step 3146, loss 0.121698, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:05:29.216821: step 3147, loss 0.0965095, acc 0.984375, learning_rate 0.000100013
2017-10-10T15:05:29.482083: step 3148, loss 0.106297, acc 0.96875, learning_rate 0.000100013
2017-10-10T15:05:29.752135: step 3149, loss 0.111973, acc 0.953125, learning_rate 0.000100013
2017-10-10T15:05:30.005970: step 3150, loss 0.091285, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:05:30.296211: step 3151, loss 0.123738, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:05:30.580940: step 3152, loss 0.166407, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:05:30.805512: step 3153, loss 0.107784, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:05:31.128895: step 3154, loss 0.145199, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:05:31.367355: step 3155, loss 0.157104, acc 0.921875, learning_rate 0.000100012
2017-10-10T15:05:31.636985: step 3156, loss 0.123995, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:05:32.015935: step 3157, loss 0.105425, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:05:32.291678: step 3158, loss 0.100139, acc 0.984375, learning_rate 0.000100012
2017-10-10T15:05:32.552804: step 3159, loss 0.145587, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:05:32.848854: step 3160, loss 0.0732133, acc 0.984375, learning_rate 0.000100012

Evaluation:
2017-10-10T15:05:33.316925: step 3160, loss 0.226673, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3160

2017-10-10T15:05:34.217169: step 3161, loss 0.106608, acc 1, learning_rate 0.000100012
2017-10-10T15:05:34.471354: step 3162, loss 0.182492, acc 0.953125, learning_rate 0.000100012
2017-10-10T15:05:34.762313: step 3163, loss 0.134103, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:05:34.961038: step 3164, loss 0.14153, acc 0.984375, learning_rate 0.000100012
2017-10-10T15:05:35.237634: step 3165, loss 0.143067, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:05:35.565718: step 3166, loss 0.152727, acc 0.921875, learning_rate 0.000100012
2017-10-10T15:05:35.834633: step 3167, loss 0.246032, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:05:36.065170: step 3168, loss 0.183533, acc 0.9375, learning_rate 0.000100012
2017-10-10T15:05:36.360928: step 3169, loss 0.315043, acc 0.90625, learning_rate 0.000100012
2017-10-10T15:05:36.654331: step 3170, loss 0.126049, acc 0.96875, learning_rate 0.000100012
2017-10-10T15:05:36.917142: step 3171, loss 0.140353, acc 0.921875, learning_rate 0.000100011
2017-10-10T15:05:37.216144: step 3172, loss 0.0804929, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:05:37.503421: step 3173, loss 0.0803466, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:05:37.717156: step 3174, loss 0.230737, acc 0.921875, learning_rate 0.000100011
2017-10-10T15:05:37.985600: step 3175, loss 0.124373, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:05:38.288849: step 3176, loss 0.117533, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:05:38.618818: step 3177, loss 0.0609543, acc 1, learning_rate 0.000100011
2017-10-10T15:05:38.909084: step 3178, loss 0.154708, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:05:39.196877: step 3179, loss 0.184794, acc 0.90625, learning_rate 0.000100011
2017-10-10T15:05:39.495492: step 3180, loss 0.172674, acc 0.90625, learning_rate 0.000100011
2017-10-10T15:05:39.837000: step 3181, loss 0.0621912, acc 1, learning_rate 0.000100011
2017-10-10T15:05:40.073681: step 3182, loss 0.161161, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:05:40.328958: step 3183, loss 0.0667724, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:05:40.564814: step 3184, loss 0.135298, acc 0.921875, learning_rate 0.000100011
2017-10-10T15:05:40.856388: step 3185, loss 0.0590992, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:05:41.092086: step 3186, loss 0.20688, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:05:41.400975: step 3187, loss 0.118755, acc 0.953125, learning_rate 0.000100011
2017-10-10T15:05:41.673896: step 3188, loss 0.201679, acc 0.921875, learning_rate 0.000100011
2017-10-10T15:05:41.955953: step 3189, loss 0.150765, acc 0.96875, learning_rate 0.000100011
2017-10-10T15:05:42.241049: step 3190, loss 0.141468, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:05:42.528967: step 3191, loss 0.194881, acc 0.984375, learning_rate 0.000100011
2017-10-10T15:05:42.809540: step 3192, loss 0.231498, acc 0.9375, learning_rate 0.000100011
2017-10-10T15:05:43.141970: step 3193, loss 0.0954556, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:05:43.508845: step 3194, loss 0.124782, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:05:43.705011: step 3195, loss 0.103804, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:05:43.948872: step 3196, loss 0.172272, acc 0.9375, learning_rate 0.00010001
2017-10-10T15:05:44.188893: step 3197, loss 0.0509936, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:05:44.524874: step 3198, loss 0.19916, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:05:44.820831: step 3199, loss 0.0395385, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:05:45.062717: step 3200, loss 0.141369, acc 0.921875, learning_rate 0.00010001

Evaluation:
2017-10-10T15:05:45.558235: step 3200, loss 0.227335, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3200

2017-10-10T15:05:46.595219: step 3201, loss 0.121108, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:05:46.906808: step 3202, loss 0.111625, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:05:47.241308: step 3203, loss 0.0573732, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:05:47.540839: step 3204, loss 0.205528, acc 0.90625, learning_rate 0.00010001
2017-10-10T15:05:47.816992: step 3205, loss 0.0995481, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:05:48.157134: step 3206, loss 0.155027, acc 0.984375, learning_rate 0.00010001
2017-10-10T15:05:48.446012: step 3207, loss 0.197294, acc 0.921875, learning_rate 0.00010001
2017-10-10T15:05:48.730863: step 3208, loss 0.112581, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:05:49.003174: step 3209, loss 0.242011, acc 0.890625, learning_rate 0.00010001
2017-10-10T15:05:49.312843: step 3210, loss 0.135297, acc 0.9375, learning_rate 0.00010001
2017-10-10T15:05:49.596708: step 3211, loss 0.289566, acc 0.90625, learning_rate 0.00010001
2017-10-10T15:05:49.872821: step 3212, loss 0.131812, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:05:50.147833: step 3213, loss 0.097537, acc 0.96875, learning_rate 0.00010001
2017-10-10T15:05:50.405890: step 3214, loss 0.282508, acc 0.921875, learning_rate 0.00010001
2017-10-10T15:05:50.735469: step 3215, loss 0.081636, acc 0.953125, learning_rate 0.00010001
2017-10-10T15:05:51.036621: step 3216, loss 0.061326, acc 1, learning_rate 0.00010001
2017-10-10T15:05:51.284937: step 3217, loss 0.152512, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:05:51.524011: step 3218, loss 0.164902, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:05:51.733076: step 3219, loss 0.0595034, acc 1, learning_rate 0.000100009
2017-10-10T15:05:52.045012: step 3220, loss 0.0918538, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:05:52.383612: step 3221, loss 0.212362, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:05:52.678635: step 3222, loss 0.204775, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:05:52.916940: step 3223, loss 0.0603421, acc 1, learning_rate 0.000100009
2017-10-10T15:05:53.224890: step 3224, loss 0.0522227, acc 1, learning_rate 0.000100009
2017-10-10T15:05:53.467286: step 3225, loss 0.190252, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:05:53.720891: step 3226, loss 0.0299861, acc 1, learning_rate 0.000100009
2017-10-10T15:05:53.992956: step 3227, loss 0.210928, acc 0.921875, learning_rate 0.000100009
2017-10-10T15:05:54.235200: step 3228, loss 0.0624637, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:05:54.535455: step 3229, loss 0.108474, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:05:54.832864: step 3230, loss 0.179009, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:05:55.148914: step 3231, loss 0.204888, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:05:55.394759: step 3232, loss 0.129344, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:05:55.668917: step 3233, loss 0.112653, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:05:55.925052: step 3234, loss 0.191441, acc 0.882353, learning_rate 0.000100009
2017-10-10T15:05:56.238102: step 3235, loss 0.102681, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:05:56.503232: step 3236, loss 0.115184, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:05:56.739927: step 3237, loss 0.108409, acc 0.953125, learning_rate 0.000100009
2017-10-10T15:05:57.087135: step 3238, loss 0.0592795, acc 0.984375, learning_rate 0.000100009
2017-10-10T15:05:57.404897: step 3239, loss 0.0569812, acc 1, learning_rate 0.000100009
2017-10-10T15:05:57.617276: step 3240, loss 0.074544, acc 0.984375, learning_rate 0.000100009

Evaluation:
2017-10-10T15:05:58.059344: step 3240, loss 0.226435, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3240

2017-10-10T15:05:59.133251: step 3241, loss 0.187993, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:05:59.388847: step 3242, loss 0.067621, acc 1, learning_rate 0.000100009
2017-10-10T15:05:59.641277: step 3243, loss 0.194529, acc 0.9375, learning_rate 0.000100009
2017-10-10T15:05:59.933387: step 3244, loss 0.080264, acc 0.96875, learning_rate 0.000100009
2017-10-10T15:06:00.256808: step 3245, loss 0.100638, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:00.465652: step 3246, loss 0.104445, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:00.715687: step 3247, loss 0.147702, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:06:00.926036: step 3248, loss 0.113354, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:06:01.144957: step 3249, loss 0.139822, acc 0.9375, learning_rate 0.000100008
2017-10-10T15:06:01.471811: step 3250, loss 0.0800303, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:06:01.788673: step 3251, loss 0.0997456, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:02.028944: step 3252, loss 0.226466, acc 0.921875, learning_rate 0.000100008
2017-10-10T15:06:02.282647: step 3253, loss 0.127298, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:02.608890: step 3254, loss 0.150517, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:06:02.911802: step 3255, loss 0.174153, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:06:03.141781: step 3256, loss 0.0989323, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:03.393171: step 3257, loss 0.110119, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:03.664303: step 3258, loss 0.145773, acc 0.90625, learning_rate 0.000100008
2017-10-10T15:06:03.952176: step 3259, loss 0.194298, acc 0.921875, learning_rate 0.000100008
2017-10-10T15:06:04.213928: step 3260, loss 0.151032, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:06:04.490525: step 3261, loss 0.0626898, acc 1, learning_rate 0.000100008
2017-10-10T15:06:04.769523: step 3262, loss 0.170878, acc 0.921875, learning_rate 0.000100008
2017-10-10T15:06:05.072835: step 3263, loss 0.0887695, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:06:05.380793: step 3264, loss 0.152773, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:05.649155: step 3265, loss 0.181914, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:05.906173: step 3266, loss 0.0569424, acc 0.984375, learning_rate 0.000100008
2017-10-10T15:06:06.212840: step 3267, loss 0.185985, acc 0.90625, learning_rate 0.000100008
2017-10-10T15:06:06.521153: step 3268, loss 0.0887519, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:06.788903: step 3269, loss 0.131346, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:06:07.086705: step 3270, loss 0.156736, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:06:07.351361: step 3271, loss 0.154017, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:06:07.638777: step 3272, loss 0.122487, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:07.956910: step 3273, loss 0.0830077, acc 0.953125, learning_rate 0.000100008
2017-10-10T15:06:08.212998: step 3274, loss 0.0997322, acc 0.96875, learning_rate 0.000100008
2017-10-10T15:06:08.516197: step 3275, loss 0.181221, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:06:08.799011: step 3276, loss 0.103024, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:06:09.065383: step 3277, loss 0.170582, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:06:09.380152: step 3278, loss 0.173247, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:06:09.644479: step 3279, loss 0.143862, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:06:09.936230: step 3280, loss 0.0917374, acc 0.953125, learning_rate 0.000100007

Evaluation:
2017-10-10T15:06:10.395757: step 3280, loss 0.227898, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3280

2017-10-10T15:06:11.461211: step 3281, loss 0.100231, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:06:11.793715: step 3282, loss 0.136295, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:06:12.096795: step 3283, loss 0.104069, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:06:12.299443: step 3284, loss 0.0734468, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:06:12.559586: step 3285, loss 0.171154, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:06:12.802304: step 3286, loss 0.103319, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:06:12.978340: step 3287, loss 0.116797, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:06:13.199243: step 3288, loss 0.0824296, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:06:13.449382: step 3289, loss 0.128414, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:06:13.757087: step 3290, loss 0.155171, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:06:14.035474: step 3291, loss 0.114571, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:06:14.300523: step 3292, loss 0.201755, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:06:14.569188: step 3293, loss 0.0858011, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:06:14.848866: step 3294, loss 0.180225, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:06:15.141623: step 3295, loss 0.0629085, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:06:15.419684: step 3296, loss 0.200342, acc 0.921875, learning_rate 0.000100007
2017-10-10T15:06:15.721831: step 3297, loss 0.174098, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:06:15.958186: step 3298, loss 0.230655, acc 0.90625, learning_rate 0.000100007
2017-10-10T15:06:16.255969: step 3299, loss 0.12778, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:06:16.592871: step 3300, loss 0.143118, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:06:16.795475: step 3301, loss 0.0857832, acc 0.984375, learning_rate 0.000100007
2017-10-10T15:06:17.050725: step 3302, loss 0.157324, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:06:17.264844: step 3303, loss 0.0667243, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:06:17.464558: step 3304, loss 0.105684, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:06:17.788854: step 3305, loss 0.149236, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:06:17.988070: step 3306, loss 0.111299, acc 0.96875, learning_rate 0.000100007
2017-10-10T15:06:18.304974: step 3307, loss 0.0624694, acc 1, learning_rate 0.000100007
2017-10-10T15:06:18.542366: step 3308, loss 0.125266, acc 0.953125, learning_rate 0.000100007
2017-10-10T15:06:18.806222: step 3309, loss 0.219954, acc 0.9375, learning_rate 0.000100007
2017-10-10T15:06:19.099572: step 3310, loss 0.163498, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:06:19.386151: step 3311, loss 0.142313, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:06:19.627627: step 3312, loss 0.102338, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:06:19.993309: step 3313, loss 0.182773, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:06:20.297512: step 3314, loss 0.107594, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:06:20.560982: step 3315, loss 0.19979, acc 0.921875, learning_rate 0.000100006
2017-10-10T15:06:20.885927: step 3316, loss 0.102567, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:06:21.171206: step 3317, loss 0.154398, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:06:21.462004: step 3318, loss 0.125367, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:06:21.687895: step 3319, loss 0.0388999, acc 1, learning_rate 0.000100006
2017-10-10T15:06:21.943259: step 3320, loss 0.140676, acc 0.96875, learning_rate 0.000100006

Evaluation:
2017-10-10T15:06:22.311763: step 3320, loss 0.225883, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3320

2017-10-10T15:06:23.318190: step 3321, loss 0.113606, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:06:23.563483: step 3322, loss 0.117585, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:06:23.816764: step 3323, loss 0.133433, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:06:24.063966: step 3324, loss 0.171581, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:06:24.359949: step 3325, loss 0.0790713, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:06:24.624302: step 3326, loss 0.100787, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:06:24.918854: step 3327, loss 0.163374, acc 0.921875, learning_rate 0.000100006
2017-10-10T15:06:25.185915: step 3328, loss 0.0290758, acc 1, learning_rate 0.000100006
2017-10-10T15:06:25.424146: step 3329, loss 0.135227, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:06:25.721533: step 3330, loss 0.169293, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:06:26.036962: step 3331, loss 0.157268, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:06:26.249743: step 3332, loss 0.435426, acc 0.843137, learning_rate 0.000100006
2017-10-10T15:06:26.529303: step 3333, loss 0.0722135, acc 1, learning_rate 0.000100006
2017-10-10T15:06:26.773319: step 3334, loss 0.167166, acc 0.90625, learning_rate 0.000100006
2017-10-10T15:06:27.054237: step 3335, loss 0.152965, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:06:27.315714: step 3336, loss 0.0815153, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:06:27.614998: step 3337, loss 0.128398, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:06:27.885680: step 3338, loss 0.046702, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:06:28.154230: step 3339, loss 0.0713228, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:06:28.461372: step 3340, loss 0.174907, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:06:28.709143: step 3341, loss 0.0697087, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:06:28.945146: step 3342, loss 0.0781522, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:06:29.276977: step 3343, loss 0.0976896, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:06:29.522200: step 3344, loss 0.106414, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:06:29.768604: step 3345, loss 0.0562629, acc 0.984375, learning_rate 0.000100006
2017-10-10T15:06:30.050491: step 3346, loss 0.222626, acc 0.9375, learning_rate 0.000100006
2017-10-10T15:06:30.393629: step 3347, loss 0.130207, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:06:30.664813: step 3348, loss 0.0945773, acc 0.96875, learning_rate 0.000100006
2017-10-10T15:06:30.913282: step 3349, loss 0.18251, acc 0.953125, learning_rate 0.000100006
2017-10-10T15:06:31.184911: step 3350, loss 0.0368937, acc 1, learning_rate 0.000100006
2017-10-10T15:06:31.468871: step 3351, loss 0.201143, acc 0.921875, learning_rate 0.000100005
2017-10-10T15:06:31.756957: step 3352, loss 0.140113, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:32.036664: step 3353, loss 0.0567451, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:06:32.255237: step 3354, loss 0.109893, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:32.569801: step 3355, loss 0.0968125, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:32.781098: step 3356, loss 0.121092, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:33.164849: step 3357, loss 0.119622, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:33.499173: step 3358, loss 0.178618, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:33.725064: step 3359, loss 0.0745948, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:06:33.955364: step 3360, loss 0.107695, acc 0.953125, learning_rate 0.000100005

Evaluation:
2017-10-10T15:06:34.484848: step 3360, loss 0.225067, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3360

2017-10-10T15:06:35.535116: step 3361, loss 0.292537, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:06:35.792374: step 3362, loss 0.0698813, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:36.034757: step 3363, loss 0.0915785, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:36.317679: step 3364, loss 0.164727, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:06:36.597318: step 3365, loss 0.130107, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:36.860110: step 3366, loss 0.153345, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:37.161995: step 3367, loss 0.134054, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:37.480540: step 3368, loss 0.112774, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:37.739595: step 3369, loss 0.0931445, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:38.077459: step 3370, loss 0.0892698, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:38.331368: step 3371, loss 0.214956, acc 0.90625, learning_rate 0.000100005
2017-10-10T15:06:38.660534: step 3372, loss 0.143856, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:39.016970: step 3373, loss 0.205065, acc 0.890625, learning_rate 0.000100005
2017-10-10T15:06:39.305769: step 3374, loss 0.122248, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:39.503449: step 3375, loss 0.123374, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:39.740091: step 3376, loss 0.184604, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:06:39.934297: step 3377, loss 0.1317, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:40.181236: step 3378, loss 0.23038, acc 0.890625, learning_rate 0.000100005
2017-10-10T15:06:40.438392: step 3379, loss 0.0633491, acc 1, learning_rate 0.000100005
2017-10-10T15:06:40.692902: step 3380, loss 0.166761, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:40.968844: step 3381, loss 0.131875, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:41.345490: step 3382, loss 0.154772, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:41.629822: step 3383, loss 0.0811158, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:41.899894: step 3384, loss 0.0982872, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:42.172920: step 3385, loss 0.131909, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:06:42.440882: step 3386, loss 0.104343, acc 0.984375, learning_rate 0.000100005
2017-10-10T15:06:42.728953: step 3387, loss 0.0397086, acc 1, learning_rate 0.000100005
2017-10-10T15:06:43.039084: step 3388, loss 0.0766823, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:43.349758: step 3389, loss 0.133757, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:06:43.627103: step 3390, loss 0.101167, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:43.936569: step 3391, loss 0.147224, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:44.216525: step 3392, loss 0.209751, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:44.569333: step 3393, loss 0.159733, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:44.854344: step 3394, loss 0.249155, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:45.164260: step 3395, loss 0.0644454, acc 1, learning_rate 0.000100005
2017-10-10T15:06:45.513050: step 3396, loss 0.100444, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:45.820306: step 3397, loss 0.158342, acc 0.9375, learning_rate 0.000100005
2017-10-10T15:06:46.056844: step 3398, loss 0.169615, acc 0.953125, learning_rate 0.000100005
2017-10-10T15:06:46.341112: step 3399, loss 0.0997466, acc 0.96875, learning_rate 0.000100005
2017-10-10T15:06:46.623534: step 3400, loss 0.176277, acc 0.9375, learning_rate 0.000100004

Evaluation:
2017-10-10T15:06:47.083677: step 3400, loss 0.226168, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3400

2017-10-10T15:06:48.264021: step 3401, loss 0.0826439, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:48.528890: step 3402, loss 0.118604, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:48.791031: step 3403, loss 0.12282, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:49.117166: step 3404, loss 0.118097, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:49.405624: step 3405, loss 0.151008, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:49.636169: step 3406, loss 0.242373, acc 0.90625, learning_rate 0.000100004
2017-10-10T15:06:49.848248: step 3407, loss 0.131866, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:06:50.114933: step 3408, loss 0.123725, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:50.364821: step 3409, loss 0.0830878, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:50.635595: step 3410, loss 0.203223, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:06:50.951362: step 3411, loss 0.147505, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:51.219770: step 3412, loss 0.101508, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:51.491585: step 3413, loss 0.112095, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:51.741855: step 3414, loss 0.0844915, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:51.993016: step 3415, loss 0.0954752, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:52.248385: step 3416, loss 0.158871, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:52.604895: step 3417, loss 0.102871, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:52.897571: step 3418, loss 0.127789, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:06:53.112871: step 3419, loss 0.113374, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:53.412293: step 3420, loss 0.0772167, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:53.655208: step 3421, loss 0.166342, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:53.899339: step 3422, loss 0.116538, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:54.206437: step 3423, loss 0.130548, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:54.564824: step 3424, loss 0.07733, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:54.864971: step 3425, loss 0.210005, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:06:55.085631: step 3426, loss 0.215411, acc 0.90625, learning_rate 0.000100004
2017-10-10T15:06:55.386182: step 3427, loss 0.12532, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:06:55.657080: step 3428, loss 0.0872269, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:55.945318: step 3429, loss 0.0776979, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:56.272083: step 3430, loss 0.20676, acc 0.882353, learning_rate 0.000100004
2017-10-10T15:06:56.542438: step 3431, loss 0.10592, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:06:56.812035: step 3432, loss 0.118686, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:57.141266: step 3433, loss 0.115847, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:06:57.483911: step 3434, loss 0.0918176, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:57.712751: step 3435, loss 0.237359, acc 0.90625, learning_rate 0.000100004
2017-10-10T15:06:57.965641: step 3436, loss 0.0860645, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:58.169018: step 3437, loss 0.0683852, acc 1, learning_rate 0.000100004
2017-10-10T15:06:58.432927: step 3438, loss 0.103881, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:58.736839: step 3439, loss 0.0639903, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:06:59.019511: step 3440, loss 0.0993795, acc 0.984375, learning_rate 0.000100004

Evaluation:
2017-10-10T15:06:59.521020: step 3440, loss 0.225753, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3440

2017-10-10T15:07:00.675738: step 3441, loss 0.218807, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:07:00.945543: step 3442, loss 0.190729, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:07:01.255747: step 3443, loss 0.1137, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:07:01.523506: step 3444, loss 0.193581, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:07:01.738680: step 3445, loss 0.194558, acc 0.90625, learning_rate 0.000100004
2017-10-10T15:07:02.024924: step 3446, loss 0.196608, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:07:02.307437: step 3447, loss 0.205003, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:07:02.617057: step 3448, loss 0.150923, acc 0.921875, learning_rate 0.000100004
2017-10-10T15:07:02.836512: step 3449, loss 0.0800298, acc 1, learning_rate 0.000100004
2017-10-10T15:07:03.148654: step 3450, loss 0.0610977, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:07:03.432892: step 3451, loss 0.0293607, acc 1, learning_rate 0.000100004
2017-10-10T15:07:03.661256: step 3452, loss 0.0659433, acc 1, learning_rate 0.000100004
2017-10-10T15:07:03.992045: step 3453, loss 0.0830485, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:07:04.268864: step 3454, loss 0.125517, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:07:04.568727: step 3455, loss 0.12006, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:07:04.844390: step 3456, loss 0.10696, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:07:05.145966: step 3457, loss 0.123452, acc 0.953125, learning_rate 0.000100004
2017-10-10T15:07:05.448918: step 3458, loss 0.154138, acc 0.9375, learning_rate 0.000100004
2017-10-10T15:07:05.682454: step 3459, loss 0.0700179, acc 0.984375, learning_rate 0.000100004
2017-10-10T15:07:05.983144: step 3460, loss 0.129743, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:07:06.222691: step 3461, loss 0.0999625, acc 0.96875, learning_rate 0.000100004
2017-10-10T15:07:06.452577: step 3462, loss 0.153159, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:06.657006: step 3463, loss 0.19678, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:07:06.915747: step 3464, loss 0.106499, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:07.201016: step 3465, loss 0.177952, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:07:07.498946: step 3466, loss 0.223139, acc 0.875, learning_rate 0.000100003
2017-10-10T15:07:07.765825: step 3467, loss 0.218493, acc 0.890625, learning_rate 0.000100003
2017-10-10T15:07:07.964143: step 3468, loss 0.167409, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:08.231392: step 3469, loss 0.100463, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:08.552922: step 3470, loss 0.0886753, acc 1, learning_rate 0.000100003
2017-10-10T15:07:08.805015: step 3471, loss 0.121551, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:09.165191: step 3472, loss 0.0755939, acc 1, learning_rate 0.000100003
2017-10-10T15:07:09.409543: step 3473, loss 0.109894, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:09.700519: step 3474, loss 0.150726, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:09.987615: step 3475, loss 0.115818, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:10.288852: step 3476, loss 0.0678449, acc 1, learning_rate 0.000100003
2017-10-10T15:07:10.587213: step 3477, loss 0.0993207, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:10.812789: step 3478, loss 0.108329, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:11.116909: step 3479, loss 0.092592, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:11.405165: step 3480, loss 0.182977, acc 0.9375, learning_rate 0.000100003

Evaluation:
2017-10-10T15:07:11.875832: step 3480, loss 0.224613, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3480

2017-10-10T15:07:13.237056: step 3481, loss 0.142656, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:13.469110: step 3482, loss 0.172008, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:07:13.801965: step 3483, loss 0.0854974, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:14.109054: step 3484, loss 0.143521, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:14.360514: step 3485, loss 0.177717, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:14.686584: step 3486, loss 0.0677086, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:14.884931: step 3487, loss 0.0781476, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:15.185067: step 3488, loss 0.0954892, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:15.483555: step 3489, loss 0.0415038, acc 1, learning_rate 0.000100003
2017-10-10T15:07:15.729960: step 3490, loss 0.096817, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:16.083588: step 3491, loss 0.132668, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:16.381279: step 3492, loss 0.0638158, acc 1, learning_rate 0.000100003
2017-10-10T15:07:16.623209: step 3493, loss 0.095646, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:16.856874: step 3494, loss 0.166417, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:17.086405: step 3495, loss 0.127552, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:17.320553: step 3496, loss 0.063722, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:17.541911: step 3497, loss 0.100325, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:17.879228: step 3498, loss 0.277802, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:18.113134: step 3499, loss 0.160586, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:18.361074: step 3500, loss 0.260495, acc 0.890625, learning_rate 0.000100003
2017-10-10T15:07:18.648971: step 3501, loss 0.101174, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:18.977008: step 3502, loss 0.249836, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:07:19.298322: step 3503, loss 0.0700885, acc 1, learning_rate 0.000100003
2017-10-10T15:07:19.528731: step 3504, loss 0.128661, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:19.720748: step 3505, loss 0.11913, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:19.956836: step 3506, loss 0.12803, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:20.256980: step 3507, loss 0.104269, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:20.548982: step 3508, loss 0.0937995, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:20.856602: step 3509, loss 0.160113, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:21.126864: step 3510, loss 0.218324, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:21.403359: step 3511, loss 0.136969, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:21.676920: step 3512, loss 0.137555, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:07:21.972980: step 3513, loss 0.125959, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:22.247225: step 3514, loss 0.115742, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:22.500143: step 3515, loss 0.267546, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:22.792077: step 3516, loss 0.15997, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:23.069002: step 3517, loss 0.0827903, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:23.355510: step 3518, loss 0.0981709, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:23.618514: step 3519, loss 0.163132, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:23.794334: step 3520, loss 0.146417, acc 0.96875, learning_rate 0.000100003

Evaluation:
2017-10-10T15:07:24.245933: step 3520, loss 0.225303, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3520

2017-10-10T15:07:25.280375: step 3521, loss 0.217548, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:25.639164: step 3522, loss 0.175543, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:25.928906: step 3523, loss 0.125831, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:26.188879: step 3524, loss 0.141005, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:26.482268: step 3525, loss 0.11138, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:26.726981: step 3526, loss 0.0731426, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:27.016830: step 3527, loss 0.0949176, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:27.308914: step 3528, loss 0.0483914, acc 0.980392, learning_rate 0.000100003
2017-10-10T15:07:27.598772: step 3529, loss 0.157205, acc 0.9375, learning_rate 0.000100003
2017-10-10T15:07:27.850520: step 3530, loss 0.0535596, acc 0.984375, learning_rate 0.000100003
2017-10-10T15:07:28.104971: step 3531, loss 0.218323, acc 0.90625, learning_rate 0.000100003
2017-10-10T15:07:28.306590: step 3532, loss 0.0555333, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:28.640958: step 3533, loss 0.108762, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:28.891884: step 3534, loss 0.244835, acc 0.90625, learning_rate 0.000100003
2017-10-10T15:07:29.149857: step 3535, loss 0.185513, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:29.403510: step 3536, loss 0.0423435, acc 1, learning_rate 0.000100003
2017-10-10T15:07:29.664427: step 3537, loss 0.161595, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:29.972710: step 3538, loss 0.192729, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:30.237844: step 3539, loss 0.152914, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:30.499571: step 3540, loss 0.198091, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:30.700885: step 3541, loss 0.240776, acc 0.921875, learning_rate 0.000100003
2017-10-10T15:07:30.928123: step 3542, loss 0.149328, acc 0.953125, learning_rate 0.000100003
2017-10-10T15:07:31.247737: step 3543, loss 0.110443, acc 0.96875, learning_rate 0.000100003
2017-10-10T15:07:31.587474: step 3544, loss 0.104155, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:31.913039: step 3545, loss 0.12264, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:32.203210: step 3546, loss 0.272514, acc 0.90625, learning_rate 0.000100002
2017-10-10T15:07:32.498329: step 3547, loss 0.11122, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:32.764948: step 3548, loss 0.0693027, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:32.985841: step 3549, loss 0.0766764, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:33.236884: step 3550, loss 0.105567, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:33.494432: step 3551, loss 0.196817, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:33.764920: step 3552, loss 0.107597, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:34.067193: step 3553, loss 0.17944, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:07:34.380744: step 3554, loss 0.0761399, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:34.651104: step 3555, loss 0.0977843, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:34.888863: step 3556, loss 0.0680659, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:35.131565: step 3557, loss 0.20783, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:35.343409: step 3558, loss 0.114318, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:35.575670: step 3559, loss 0.123429, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:35.859182: step 3560, loss 0.103828, acc 0.96875, learning_rate 0.000100002

Evaluation:
2017-10-10T15:07:36.291848: step 3560, loss 0.225972, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3560

2017-10-10T15:07:37.307067: step 3561, loss 0.0845467, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:37.729979: step 3562, loss 0.160761, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:37.981126: step 3563, loss 0.189823, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:38.223293: step 3564, loss 0.102312, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:38.478827: step 3565, loss 0.076344, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:38.800001: step 3566, loss 0.0964748, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:39.120886: step 3567, loss 0.0540949, acc 1, learning_rate 0.000100002
2017-10-10T15:07:39.472911: step 3568, loss 0.16904, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:39.683285: step 3569, loss 0.174162, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:07:39.920551: step 3570, loss 0.102419, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:40.124596: step 3571, loss 0.121593, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:40.350059: step 3572, loss 0.108161, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:40.641925: step 3573, loss 0.136946, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:40.968582: step 3574, loss 0.195171, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:41.218730: step 3575, loss 0.0909245, acc 1, learning_rate 0.000100002
2017-10-10T15:07:41.541136: step 3576, loss 0.105522, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:41.809043: step 3577, loss 0.147783, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:42.033722: step 3578, loss 0.108637, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:42.313325: step 3579, loss 0.131743, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:42.538010: step 3580, loss 0.0624433, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:42.788502: step 3581, loss 0.160671, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:43.051319: step 3582, loss 0.0454401, acc 1, learning_rate 0.000100002
2017-10-10T15:07:43.295494: step 3583, loss 0.237516, acc 0.90625, learning_rate 0.000100002
2017-10-10T15:07:43.612842: step 3584, loss 0.169023, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:43.919736: step 3585, loss 0.065174, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:44.212864: step 3586, loss 0.231245, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:44.457898: step 3587, loss 0.0515617, acc 1, learning_rate 0.000100002
2017-10-10T15:07:44.796714: step 3588, loss 0.082488, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:45.076877: step 3589, loss 0.12948, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:45.346217: step 3590, loss 0.186427, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:45.673308: step 3591, loss 0.0777977, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:45.945943: step 3592, loss 0.121635, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:46.177540: step 3593, loss 0.211831, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:07:46.408846: step 3594, loss 0.106098, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:46.751943: step 3595, loss 0.0653312, acc 1, learning_rate 0.000100002
2017-10-10T15:07:47.057509: step 3596, loss 0.0836783, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:47.359535: step 3597, loss 0.152585, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:47.624891: step 3598, loss 0.111598, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:47.876920: step 3599, loss 0.113799, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:48.150764: step 3600, loss 0.144033, acc 0.953125, learning_rate 0.000100002

Evaluation:
2017-10-10T15:07:48.632818: step 3600, loss 0.226035, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3600

2017-10-10T15:07:49.701021: step 3601, loss 0.200205, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:49.996903: step 3602, loss 0.115833, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:50.308847: step 3603, loss 0.0682968, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:50.580621: step 3604, loss 0.0318999, acc 1, learning_rate 0.000100002
2017-10-10T15:07:50.852845: step 3605, loss 0.0815331, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:51.160543: step 3606, loss 0.209426, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:51.368988: step 3607, loss 0.253909, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:51.651334: step 3608, loss 0.129969, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:51.968832: step 3609, loss 0.149591, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:52.246043: step 3610, loss 0.0589609, acc 1, learning_rate 0.000100002
2017-10-10T15:07:52.534150: step 3611, loss 0.0912209, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:52.826734: step 3612, loss 0.115764, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:07:53.116851: step 3613, loss 0.127919, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:53.436572: step 3614, loss 0.130672, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:53.649091: step 3615, loss 0.167739, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:53.880699: step 3616, loss 0.172263, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:54.143720: step 3617, loss 0.143794, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:54.432249: step 3618, loss 0.0940892, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:54.679793: step 3619, loss 0.103245, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:07:54.939611: step 3620, loss 0.151858, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:55.269117: step 3621, loss 0.139911, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:55.556937: step 3622, loss 0.202243, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:07:55.917721: step 3623, loss 0.078343, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:56.160898: step 3624, loss 0.137696, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:56.404850: step 3625, loss 0.197618, acc 0.90625, learning_rate 0.000100002
2017-10-10T15:07:56.657739: step 3626, loss 0.295137, acc 0.882353, learning_rate 0.000100002
2017-10-10T15:07:56.911417: step 3627, loss 0.192775, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:07:57.230470: step 3628, loss 0.152681, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:07:57.536332: step 3629, loss 0.0593214, acc 1, learning_rate 0.000100002
2017-10-10T15:07:57.737152: step 3630, loss 0.228719, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:07:58.076956: step 3631, loss 0.113035, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:58.362108: step 3632, loss 0.119099, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:58.679370: step 3633, loss 0.0739625, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:07:58.990943: step 3634, loss 0.20359, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:07:59.237044: step 3635, loss 0.107101, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:59.464108: step 3636, loss 0.213428, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:07:59.725048: step 3637, loss 0.0446631, acc 1, learning_rate 0.000100002
2017-10-10T15:08:00.056845: step 3638, loss 0.117049, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:08:00.264802: step 3639, loss 0.0581239, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:00.556926: step 3640, loss 0.0595075, acc 0.984375, learning_rate 0.000100002

Evaluation:
2017-10-10T15:08:00.994042: step 3640, loss 0.224627, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3640

2017-10-10T15:08:01.969051: step 3641, loss 0.147167, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:08:02.192050: step 3642, loss 0.2467, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:08:02.491469: step 3643, loss 0.165955, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:02.788629: step 3644, loss 0.113486, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:03.049644: step 3645, loss 0.172409, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:08:03.332910: step 3646, loss 0.115733, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:03.644527: step 3647, loss 0.0694908, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:03.834416: step 3648, loss 0.105605, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:08:04.100892: step 3649, loss 0.181869, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:08:04.361793: step 3650, loss 0.0891225, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:04.653363: step 3651, loss 0.106797, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:08:04.964984: step 3652, loss 0.132361, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:05.256059: step 3653, loss 0.0942023, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:05.512328: step 3654, loss 0.145842, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:08:05.800875: step 3655, loss 0.139329, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:08:06.082107: step 3656, loss 0.200142, acc 0.953125, learning_rate 0.000100002
2017-10-10T15:08:06.307169: step 3657, loss 0.0888008, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:06.560116: step 3658, loss 0.157392, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:08:06.882498: step 3659, loss 0.108912, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:07.139461: step 3660, loss 0.104613, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:07.405560: step 3661, loss 0.189901, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:08:07.720840: step 3662, loss 0.128939, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:07.930279: step 3663, loss 0.0937918, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:08.177942: step 3664, loss 0.162003, acc 0.921875, learning_rate 0.000100002
2017-10-10T15:08:08.483796: step 3665, loss 0.145792, acc 0.9375, learning_rate 0.000100002
2017-10-10T15:08:08.701652: step 3666, loss 0.24755, acc 0.90625, learning_rate 0.000100002
2017-10-10T15:08:09.005333: step 3667, loss 0.108685, acc 0.96875, learning_rate 0.000100002
2017-10-10T15:08:09.243922: step 3668, loss 0.0850066, acc 0.984375, learning_rate 0.000100002
2017-10-10T15:08:09.474891: step 3669, loss 0.0557024, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:09.760992: step 3670, loss 0.13976, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:10.044910: step 3671, loss 0.14974, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:10.332832: step 3672, loss 0.114405, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:10.608894: step 3673, loss 0.162636, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:10.876843: step 3674, loss 0.168453, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:08:11.140787: step 3675, loss 0.0910494, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:11.442873: step 3676, loss 0.0636363, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:11.761072: step 3677, loss 0.0884195, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:11.983438: step 3678, loss 0.24992, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:12.252872: step 3679, loss 0.0374787, acc 1, learning_rate 0.000100001
2017-10-10T15:08:12.527258: step 3680, loss 0.147826, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-10T15:08:12.983369: step 3680, loss 0.224019, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3680

2017-10-10T15:08:13.950737: step 3681, loss 0.0534525, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:14.297189: step 3682, loss 0.163445, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:14.577002: step 3683, loss 0.120765, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:14.829611: step 3684, loss 0.0623165, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:15.106460: step 3685, loss 0.171567, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:15.418856: step 3686, loss 0.0663633, acc 1, learning_rate 0.000100001
2017-10-10T15:08:15.641275: step 3687, loss 0.301966, acc 0.875, learning_rate 0.000100001
2017-10-10T15:08:15.933107: step 3688, loss 0.130421, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:16.228923: step 3689, loss 0.141906, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:16.473245: step 3690, loss 0.0749476, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:16.748863: step 3691, loss 0.0973961, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:17.060853: step 3692, loss 0.172565, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:17.333078: step 3693, loss 0.139477, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:17.609174: step 3694, loss 0.229294, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:17.931470: step 3695, loss 0.143838, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:18.200954: step 3696, loss 0.108033, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:18.498831: step 3697, loss 0.195268, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:08:18.808824: step 3698, loss 0.0743754, acc 1, learning_rate 0.000100001
2017-10-10T15:08:19.122571: step 3699, loss 0.10165, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:19.349292: step 3700, loss 0.0763015, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:19.636965: step 3701, loss 0.0855844, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:19.861237: step 3702, loss 0.225478, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:20.130094: step 3703, loss 0.145256, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:20.496762: step 3704, loss 0.099215, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:20.748431: step 3705, loss 0.110177, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:21.020951: step 3706, loss 0.0674858, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:21.296879: step 3707, loss 0.113802, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:21.564906: step 3708, loss 0.0908543, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:21.872208: step 3709, loss 0.141578, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:22.161582: step 3710, loss 0.084066, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:22.412906: step 3711, loss 0.108134, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:22.704192: step 3712, loss 0.118315, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:22.996199: step 3713, loss 0.0478333, acc 1, learning_rate 0.000100001
2017-10-10T15:08:23.305269: step 3714, loss 0.130506, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:23.632853: step 3715, loss 0.144032, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:23.903641: step 3716, loss 0.16901, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:24.173777: step 3717, loss 0.0774138, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:24.444936: step 3718, loss 0.0451796, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:24.749235: step 3719, loss 0.0800316, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:24.985010: step 3720, loss 0.207372, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-10-10T15:08:25.511284: step 3720, loss 0.224428, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3720

2017-10-10T15:08:26.540896: step 3721, loss 0.147792, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:26.759032: step 3722, loss 0.128143, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:27.062076: step 3723, loss 0.0910693, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:27.266309: step 3724, loss 0.079307, acc 0.980392, learning_rate 0.000100001
2017-10-10T15:08:27.540273: step 3725, loss 0.0793019, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:27.820898: step 3726, loss 0.123973, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:28.045185: step 3727, loss 0.174958, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:28.329313: step 3728, loss 0.103678, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:28.598158: step 3729, loss 0.0689603, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:28.920817: step 3730, loss 0.162624, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:29.231399: step 3731, loss 0.127959, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:29.521314: step 3732, loss 0.317189, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:29.779550: step 3733, loss 0.110488, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:29.938336: step 3734, loss 0.091209, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:30.144836: step 3735, loss 0.146884, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:30.302372: step 3736, loss 0.073696, acc 1, learning_rate 0.000100001
2017-10-10T15:08:30.488865: step 3737, loss 0.0672123, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:30.709455: step 3738, loss 0.10543, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:31.007691: step 3739, loss 0.0687626, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:31.281218: step 3740, loss 0.138241, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:31.542875: step 3741, loss 0.231137, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:31.786498: step 3742, loss 0.184537, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:32.007564: step 3743, loss 0.0629489, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:32.303752: step 3744, loss 0.132846, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:32.539732: step 3745, loss 0.148159, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:32.768308: step 3746, loss 0.124333, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:33.024867: step 3747, loss 0.114834, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:33.289111: step 3748, loss 0.0898124, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:33.604911: step 3749, loss 0.150622, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:33.924886: step 3750, loss 0.0770197, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:34.193038: step 3751, loss 0.0757164, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:34.527335: step 3752, loss 0.153047, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:34.855487: step 3753, loss 0.167452, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:35.168178: step 3754, loss 0.138518, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:35.405095: step 3755, loss 0.111048, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:35.700823: step 3756, loss 0.106529, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:35.941111: step 3757, loss 0.129571, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:36.213306: step 3758, loss 0.148535, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:36.559427: step 3759, loss 0.143406, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:36.840859: step 3760, loss 0.110255, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-10T15:08:37.283186: step 3760, loss 0.226307, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3760

2017-10-10T15:08:38.420894: step 3761, loss 0.129781, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:38.731916: step 3762, loss 0.114306, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:39.030727: step 3763, loss 0.118153, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:39.293754: step 3764, loss 0.213334, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:39.518963: step 3765, loss 0.102564, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:39.786158: step 3766, loss 0.109687, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:40.046187: step 3767, loss 0.141417, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:40.349670: step 3768, loss 0.0820717, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:40.634296: step 3769, loss 0.0889614, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:40.909894: step 3770, loss 0.170245, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:41.149142: step 3771, loss 0.122825, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:41.488960: step 3772, loss 0.0823139, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:41.802943: step 3773, loss 0.0539371, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:42.122382: step 3774, loss 0.138379, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:42.416969: step 3775, loss 0.0890428, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:42.621738: step 3776, loss 0.156859, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:42.937092: step 3777, loss 0.0489456, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:43.255082: step 3778, loss 0.0762325, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:43.561246: step 3779, loss 0.0648871, acc 1, learning_rate 0.000100001
2017-10-10T15:08:43.824887: step 3780, loss 0.162537, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:44.138824: step 3781, loss 0.0738049, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:44.441161: step 3782, loss 0.130031, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:44.747094: step 3783, loss 0.125545, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:45.041122: step 3784, loss 0.0753843, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:45.292882: step 3785, loss 0.0753303, acc 1, learning_rate 0.000100001
2017-10-10T15:08:45.556839: step 3786, loss 0.0563922, acc 1, learning_rate 0.000100001
2017-10-10T15:08:45.861037: step 3787, loss 0.119215, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:46.187118: step 3788, loss 0.0851172, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:46.440913: step 3789, loss 0.278309, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:08:46.711923: step 3790, loss 0.121702, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:46.956203: step 3791, loss 0.0945604, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:47.152533: step 3792, loss 0.17447, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:47.519020: step 3793, loss 0.0931627, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:47.813682: step 3794, loss 0.0752577, acc 1, learning_rate 0.000100001
2017-10-10T15:08:48.035754: step 3795, loss 0.136019, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:48.259767: step 3796, loss 0.192823, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:48.439119: step 3797, loss 0.255241, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:08:48.670403: step 3798, loss 0.0330196, acc 1, learning_rate 0.000100001
2017-10-10T15:08:48.900950: step 3799, loss 0.079913, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:49.182352: step 3800, loss 0.0639508, acc 1, learning_rate 0.000100001

Evaluation:
2017-10-10T15:08:49.666544: step 3800, loss 0.22393, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3800

2017-10-10T15:08:50.610178: step 3801, loss 0.0735662, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:50.854169: step 3802, loss 0.176664, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:51.168794: step 3803, loss 0.0553019, acc 1, learning_rate 0.000100001
2017-10-10T15:08:51.453059: step 3804, loss 0.19261, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:08:51.725028: step 3805, loss 0.107719, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:52.153431: step 3806, loss 0.120187, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:52.444888: step 3807, loss 0.115656, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:52.729894: step 3808, loss 0.14785, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:52.923271: step 3809, loss 0.189108, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:53.256888: step 3810, loss 0.173417, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:53.541740: step 3811, loss 0.0666575, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:53.801087: step 3812, loss 0.155358, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:08:54.038008: step 3813, loss 0.148307, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:54.330297: step 3814, loss 0.163572, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:54.665858: step 3815, loss 0.132137, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:54.875556: step 3816, loss 0.0875885, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:55.165784: step 3817, loss 0.119442, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:55.456873: step 3818, loss 0.192954, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:55.720888: step 3819, loss 0.0881244, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:56.004926: step 3820, loss 0.0804771, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:56.297075: step 3821, loss 0.165352, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:56.490178: step 3822, loss 0.108151, acc 0.960784, learning_rate 0.000100001
2017-10-10T15:08:56.770867: step 3823, loss 0.189231, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:57.080905: step 3824, loss 0.134424, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:57.361897: step 3825, loss 0.11989, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:08:57.612913: step 3826, loss 0.0623481, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:57.941657: step 3827, loss 0.122127, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:58.228383: step 3828, loss 0.0754926, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:58.536918: step 3829, loss 0.0500047, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:08:58.803962: step 3830, loss 0.143891, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:59.044919: step 3831, loss 0.0285187, acc 1, learning_rate 0.000100001
2017-10-10T15:08:59.280350: step 3832, loss 0.124552, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:08:59.544811: step 3833, loss 0.0622516, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:08:59.773114: step 3834, loss 0.0731477, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:00.061538: step 3835, loss 0.0931427, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:00.380827: step 3836, loss 0.207741, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:00.595347: step 3837, loss 0.21936, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:09:00.854518: step 3838, loss 0.115274, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:01.165002: step 3839, loss 0.0980757, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:01.461072: step 3840, loss 0.0609334, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T15:09:01.913162: step 3840, loss 0.222436, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3840

2017-10-10T15:09:03.040963: step 3841, loss 0.0392729, acc 1, learning_rate 0.000100001
2017-10-10T15:09:03.341010: step 3842, loss 0.231596, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:04.150098: step 3843, loss 0.0990817, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:04.479197: step 3844, loss 0.080444, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:04.804847: step 3845, loss 0.186667, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:05.045296: step 3846, loss 0.136555, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:05.293335: step 3847, loss 0.200783, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:09:05.565205: step 3848, loss 0.16395, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:05.821100: step 3849, loss 0.131043, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:06.129318: step 3850, loss 0.104408, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:06.453229: step 3851, loss 0.153253, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:06.814600: step 3852, loss 0.088647, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:07.019498: step 3853, loss 0.117757, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:07.220913: step 3854, loss 0.118359, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:07.399220: step 3855, loss 0.149793, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:07.653820: step 3856, loss 0.1248, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:08.000922: step 3857, loss 0.0976088, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:08.278723: step 3858, loss 0.114751, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:08.504881: step 3859, loss 0.0927877, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:08.794098: step 3860, loss 0.0759437, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:09.108822: step 3861, loss 0.0986129, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:09.324879: step 3862, loss 0.15586, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:09.603288: step 3863, loss 0.103749, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:09.912835: step 3864, loss 0.237317, acc 0.890625, learning_rate 0.000100001
2017-10-10T15:09:10.141749: step 3865, loss 0.0820577, acc 1, learning_rate 0.000100001
2017-10-10T15:09:10.411241: step 3866, loss 0.0468224, acc 1, learning_rate 0.000100001
2017-10-10T15:09:10.678097: step 3867, loss 0.212814, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:10.919232: step 3868, loss 0.116596, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:11.164900: step 3869, loss 0.0875325, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:11.387538: step 3870, loss 0.0918761, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:11.630138: step 3871, loss 0.145951, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:11.912840: step 3872, loss 0.0821039, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:12.205137: step 3873, loss 0.0535667, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:12.504588: step 3874, loss 0.197872, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:12.790376: step 3875, loss 0.149161, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:13.061030: step 3876, loss 0.139441, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:13.334668: step 3877, loss 0.0969557, acc 1, learning_rate 0.000100001
2017-10-10T15:09:13.643963: step 3878, loss 0.133701, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:13.920955: step 3879, loss 0.183236, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:14.199431: step 3880, loss 0.0454271, acc 0.984375, learning_rate 0.000100001

Evaluation:
2017-10-10T15:09:14.673094: step 3880, loss 0.225036, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3880

2017-10-10T15:09:15.840381: step 3881, loss 0.108776, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:16.085740: step 3882, loss 0.1871, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:16.359691: step 3883, loss 0.107853, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:16.628849: step 3884, loss 0.0880129, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:16.923763: step 3885, loss 0.0658297, acc 1, learning_rate 0.000100001
2017-10-10T15:09:17.170581: step 3886, loss 0.070886, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:17.429386: step 3887, loss 0.0827716, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:17.720762: step 3888, loss 0.0899463, acc 1, learning_rate 0.000100001
2017-10-10T15:09:17.992976: step 3889, loss 0.0946485, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:18.269646: step 3890, loss 0.106009, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:18.597106: step 3891, loss 0.193306, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:18.819873: step 3892, loss 0.164658, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:19.070482: step 3893, loss 0.0456943, acc 1, learning_rate 0.000100001
2017-10-10T15:09:19.371415: step 3894, loss 0.102131, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:19.669087: step 3895, loss 0.185716, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:19.935028: step 3896, loss 0.201122, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:20.137466: step 3897, loss 0.0984989, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:20.384985: step 3898, loss 0.0941687, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:20.653039: step 3899, loss 0.168842, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:20.938855: step 3900, loss 0.104672, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:21.128892: step 3901, loss 0.119572, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:21.384198: step 3902, loss 0.196547, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:21.713454: step 3903, loss 0.142021, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:21.978932: step 3904, loss 0.105174, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:22.181153: step 3905, loss 0.0594356, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:22.440984: step 3906, loss 0.0968775, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:22.785284: step 3907, loss 0.205413, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:23.037064: step 3908, loss 0.0944303, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:23.304917: step 3909, loss 0.0418593, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:23.612818: step 3910, loss 0.0740653, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:23.918642: step 3911, loss 0.168969, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:24.176224: step 3912, loss 0.100923, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:24.448979: step 3913, loss 0.117682, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:24.822479: step 3914, loss 0.10816, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:25.053890: step 3915, loss 0.203717, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:25.256571: step 3916, loss 0.0643947, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:25.501072: step 3917, loss 0.16408, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:25.764803: step 3918, loss 0.08052, acc 1, learning_rate 0.000100001
2017-10-10T15:09:26.058061: step 3919, loss 0.0906107, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:26.276088: step 3920, loss 0.10374, acc 0.960784, learning_rate 0.000100001

Evaluation:
2017-10-10T15:09:26.768665: step 3920, loss 0.223233, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3920

2017-10-10T15:09:27.893005: step 3921, loss 0.142762, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:28.217502: step 3922, loss 0.0862208, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:28.504790: step 3923, loss 0.125253, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:28.824857: step 3924, loss 0.142802, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:29.125154: step 3925, loss 0.100339, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:29.383296: step 3926, loss 0.152276, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:29.686526: step 3927, loss 0.126324, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:29.961003: step 3928, loss 0.222359, acc 0.921875, learning_rate 0.000100001
2017-10-10T15:09:30.206900: step 3929, loss 0.114358, acc 0.96875, learning_rate 0.000100001
2017-10-10T15:09:30.478963: step 3930, loss 0.109154, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:30.710289: step 3931, loss 0.101138, acc 0.984375, learning_rate 0.000100001
2017-10-10T15:09:30.933087: step 3932, loss 0.169168, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:31.310888: step 3933, loss 0.186378, acc 0.953125, learning_rate 0.000100001
2017-10-10T15:09:31.592294: step 3934, loss 0.144347, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:31.936494: step 3935, loss 0.125091, acc 0.9375, learning_rate 0.000100001
2017-10-10T15:09:32.227952: step 3936, loss 0.191431, acc 0.90625, learning_rate 0.000100001
2017-10-10T15:09:32.507470: step 3937, loss 0.131369, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:32.802418: step 3938, loss 0.163939, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:33.116109: step 3939, loss 0.12244, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:33.312841: step 3940, loss 0.0920506, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:33.601229: step 3941, loss 0.0803134, acc 1, learning_rate 0.0001
2017-10-10T15:09:33.932951: step 3942, loss 0.143386, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:34.160826: step 3943, loss 0.111908, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:34.441970: step 3944, loss 0.0494168, acc 1, learning_rate 0.0001
2017-10-10T15:09:34.738285: step 3945, loss 0.112395, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:35.037362: step 3946, loss 0.128472, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:35.317135: step 3947, loss 0.176195, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:35.627704: step 3948, loss 0.0961764, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:35.897960: step 3949, loss 0.13566, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:36.228880: step 3950, loss 0.106336, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:36.506698: step 3951, loss 0.101198, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:36.741252: step 3952, loss 0.0828766, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:37.027519: step 3953, loss 0.10065, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:37.268330: step 3954, loss 0.0540017, acc 1, learning_rate 0.0001
2017-10-10T15:09:37.591260: step 3955, loss 0.161181, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:37.800300: step 3956, loss 0.0570906, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:38.085250: step 3957, loss 0.0858048, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:38.412509: step 3958, loss 0.13129, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:38.609813: step 3959, loss 0.103629, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:38.887036: step 3960, loss 0.116801, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:09:39.340787: step 3960, loss 0.221581, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-3960

2017-10-10T15:09:40.310233: step 3961, loss 0.254252, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:40.555577: step 3962, loss 0.0611704, acc 1, learning_rate 0.0001
2017-10-10T15:09:40.863405: step 3963, loss 0.0901627, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:41.091634: step 3964, loss 0.172452, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:41.379949: step 3965, loss 0.25097, acc 0.890625, learning_rate 0.0001
2017-10-10T15:09:41.651534: step 3966, loss 0.0704224, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:41.944057: step 3967, loss 0.120362, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:42.200472: step 3968, loss 0.0497977, acc 1, learning_rate 0.0001
2017-10-10T15:09:42.500874: step 3969, loss 0.117357, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:42.889259: step 3970, loss 0.108951, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:43.087586: step 3971, loss 0.232921, acc 0.90625, learning_rate 0.0001
2017-10-10T15:09:43.308850: step 3972, loss 0.110029, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:43.598677: step 3973, loss 0.0781358, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:43.881835: step 3974, loss 0.166674, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:44.160851: step 3975, loss 0.101422, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:44.396941: step 3976, loss 0.111295, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:44.659156: step 3977, loss 0.0711975, acc 1, learning_rate 0.0001
2017-10-10T15:09:44.984634: step 3978, loss 0.0490504, acc 1, learning_rate 0.0001
2017-10-10T15:09:45.280895: step 3979, loss 0.0664692, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:45.485097: step 3980, loss 0.132696, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:45.788327: step 3981, loss 0.0471398, acc 1, learning_rate 0.0001
2017-10-10T15:09:46.060383: step 3982, loss 0.0889099, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:46.337125: step 3983, loss 0.0710831, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:46.673125: step 3984, loss 0.186539, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:47.060994: step 3985, loss 0.109061, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:47.332822: step 3986, loss 0.135349, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:47.596882: step 3987, loss 0.0623988, acc 1, learning_rate 0.0001
2017-10-10T15:09:47.872887: step 3988, loss 0.112471, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:48.177526: step 3989, loss 0.233641, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:48.401078: step 3990, loss 0.123796, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:48.666199: step 3991, loss 0.123725, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:48.932932: step 3992, loss 0.0771025, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:49.183399: step 3993, loss 0.146387, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:49.473033: step 3994, loss 0.214649, acc 0.890625, learning_rate 0.0001
2017-10-10T15:09:49.765408: step 3995, loss 0.0932051, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:50.036970: step 3996, loss 0.124147, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:50.335920: step 3997, loss 0.185206, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:50.640855: step 3998, loss 0.177192, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:50.908291: step 3999, loss 0.154015, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:51.161137: step 4000, loss 0.0555931, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:09:51.626202: step 4000, loss 0.222547, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4000

2017-10-10T15:09:52.736838: step 4001, loss 0.164203, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:53.148845: step 4002, loss 0.155118, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:53.379754: step 4003, loss 0.0716434, acc 1, learning_rate 0.0001
2017-10-10T15:09:53.630190: step 4004, loss 0.0839872, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:53.892594: step 4005, loss 0.108944, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:54.142586: step 4006, loss 0.0743667, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:54.406749: step 4007, loss 0.209861, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:54.762850: step 4008, loss 0.231835, acc 0.90625, learning_rate 0.0001
2017-10-10T15:09:55.039249: step 4009, loss 0.0775348, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:55.278121: step 4010, loss 0.116934, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:55.496944: step 4011, loss 0.15117, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:55.740901: step 4012, loss 0.148517, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:56.044119: step 4013, loss 0.117133, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:56.352942: step 4014, loss 0.102456, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:56.690606: step 4015, loss 0.172816, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:57.006884: step 4016, loss 0.143526, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:57.225325: step 4017, loss 0.125959, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:57.477912: step 4018, loss 0.0838503, acc 0.980392, learning_rate 0.0001
2017-10-10T15:09:57.800823: step 4019, loss 0.0590383, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:58.092014: step 4020, loss 0.0827913, acc 0.984375, learning_rate 0.0001
2017-10-10T15:09:58.379243: step 4021, loss 0.134611, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:58.684918: step 4022, loss 0.284717, acc 0.921875, learning_rate 0.0001
2017-10-10T15:09:58.979904: step 4023, loss 0.076553, acc 0.96875, learning_rate 0.0001
2017-10-10T15:09:59.257088: step 4024, loss 0.112641, acc 0.9375, learning_rate 0.0001
2017-10-10T15:09:59.564974: step 4025, loss 0.1105, acc 0.953125, learning_rate 0.0001
2017-10-10T15:09:59.839930: step 4026, loss 0.132722, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:00.112838: step 4027, loss 0.150886, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:00.441004: step 4028, loss 0.0868373, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:00.777027: step 4029, loss 0.0774272, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:01.015858: step 4030, loss 0.11946, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:01.220921: step 4031, loss 0.0694212, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:01.461854: step 4032, loss 0.165749, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:01.734885: step 4033, loss 0.0338068, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:02.005162: step 4034, loss 0.120929, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:02.311204: step 4035, loss 0.121781, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:02.549651: step 4036, loss 0.14946, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:02.824812: step 4037, loss 0.0737382, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:03.091661: step 4038, loss 0.103744, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:03.325081: step 4039, loss 0.0942201, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:03.602061: step 4040, loss 0.123281, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:10:04.067987: step 4040, loss 0.221996, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4040

2017-10-10T15:10:05.216874: step 4041, loss 0.0704409, acc 1, learning_rate 0.0001
2017-10-10T15:10:05.452964: step 4042, loss 0.0721358, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:05.728498: step 4043, loss 0.108575, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:06.041460: step 4044, loss 0.0784999, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:06.258410: step 4045, loss 0.121084, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:06.553735: step 4046, loss 0.18916, acc 0.90625, learning_rate 0.0001
2017-10-10T15:10:06.821957: step 4047, loss 0.173925, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:07.097760: step 4048, loss 0.100913, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:07.365772: step 4049, loss 0.0349919, acc 1, learning_rate 0.0001
2017-10-10T15:10:07.614390: step 4050, loss 0.185905, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:07.909093: step 4051, loss 0.0971304, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:08.184247: step 4052, loss 0.140132, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:08.432917: step 4053, loss 0.0810416, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:08.721284: step 4054, loss 0.128163, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:09.005056: step 4055, loss 0.174689, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:09.291091: step 4056, loss 0.0486201, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:09.600729: step 4057, loss 0.0889426, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:09.809650: step 4058, loss 0.0806254, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:10.062905: step 4059, loss 0.125285, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:10.328859: step 4060, loss 0.121445, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:10.539776: step 4061, loss 0.150884, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:10.849062: step 4062, loss 0.207162, acc 0.90625, learning_rate 0.0001
2017-10-10T15:10:11.133008: step 4063, loss 0.0631952, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:11.393195: step 4064, loss 0.103308, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:11.669062: step 4065, loss 0.118754, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:11.929094: step 4066, loss 0.123161, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:12.156831: step 4067, loss 0.0728413, acc 1, learning_rate 0.0001
2017-10-10T15:10:12.491894: step 4068, loss 0.108171, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:12.782656: step 4069, loss 0.0975342, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:13.077642: step 4070, loss 0.109571, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:13.392875: step 4071, loss 0.14589, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:13.688867: step 4072, loss 0.137087, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:13.989702: step 4073, loss 0.140076, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:14.225195: step 4074, loss 0.161007, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:14.484488: step 4075, loss 0.245301, acc 0.890625, learning_rate 0.0001
2017-10-10T15:10:14.739689: step 4076, loss 0.153478, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:14.954789: step 4077, loss 0.0540042, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:15.219678: step 4078, loss 0.146267, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:15.469118: step 4079, loss 0.195254, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:15.767638: step 4080, loss 0.145048, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:10:16.192879: step 4080, loss 0.221777, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4080

2017-10-10T15:10:17.324819: step 4081, loss 0.121248, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:17.566963: step 4082, loss 0.144291, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:17.822464: step 4083, loss 0.108488, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:18.154752: step 4084, loss 0.147499, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:18.462506: step 4085, loss 0.168497, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:18.784107: step 4086, loss 0.075525, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:19.050880: step 4087, loss 0.125532, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:19.291765: step 4088, loss 0.225937, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:19.526476: step 4089, loss 0.0963451, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:19.747348: step 4090, loss 0.109957, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:20.002993: step 4091, loss 0.152006, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:20.257740: step 4092, loss 0.0664117, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:20.564992: step 4093, loss 0.162077, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:20.856892: step 4094, loss 0.102582, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:21.176774: step 4095, loss 0.145646, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:21.427979: step 4096, loss 0.136645, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:21.687257: step 4097, loss 0.118407, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:21.983387: step 4098, loss 0.185143, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:22.292810: step 4099, loss 0.0546842, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:22.564216: step 4100, loss 0.0917187, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:22.820409: step 4101, loss 0.132795, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:23.076901: step 4102, loss 0.0886788, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:23.318647: step 4103, loss 0.11873, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:23.609169: step 4104, loss 0.109141, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:23.916008: step 4105, loss 0.129176, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:24.175716: step 4106, loss 0.0897011, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:24.451650: step 4107, loss 0.078941, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:24.757246: step 4108, loss 0.102763, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:25.030262: step 4109, loss 0.151038, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:25.284909: step 4110, loss 0.06356, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:25.634514: step 4111, loss 0.0810031, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:25.912111: step 4112, loss 0.0732479, acc 1, learning_rate 0.0001
2017-10-10T15:10:26.249558: step 4113, loss 0.10214, acc 1, learning_rate 0.0001
2017-10-10T15:10:26.534601: step 4114, loss 0.0948286, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:26.819527: step 4115, loss 0.147027, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:27.038779: step 4116, loss 0.171654, acc 0.960784, learning_rate 0.0001
2017-10-10T15:10:27.349110: step 4117, loss 0.100717, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:27.840992: step 4118, loss 0.0563796, acc 1, learning_rate 0.0001
2017-10-10T15:10:28.032881: step 4119, loss 0.0644266, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:28.280434: step 4120, loss 0.0825879, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:10:28.732969: step 4120, loss 0.219558, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4120

2017-10-10T15:10:29.720948: step 4121, loss 0.122848, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:29.994703: step 4122, loss 0.0758535, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:30.226961: step 4123, loss 0.144838, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:30.483178: step 4124, loss 0.178398, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:30.793815: step 4125, loss 0.12041, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:31.107104: step 4126, loss 0.0887125, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:31.349349: step 4127, loss 0.211867, acc 0.890625, learning_rate 0.0001
2017-10-10T15:10:31.633641: step 4128, loss 0.250221, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:31.960843: step 4129, loss 0.100758, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:32.208873: step 4130, loss 0.0660534, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:32.485130: step 4131, loss 0.0496873, acc 1, learning_rate 0.0001
2017-10-10T15:10:32.765906: step 4132, loss 0.165494, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:33.021103: step 4133, loss 0.132815, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:33.336687: step 4134, loss 0.11687, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:33.664904: step 4135, loss 0.0832749, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:33.895477: step 4136, loss 0.104467, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:34.178243: step 4137, loss 0.10473, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:34.473029: step 4138, loss 0.068786, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:34.673211: step 4139, loss 0.0441277, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:34.983648: step 4140, loss 0.0992714, acc 1, learning_rate 0.0001
2017-10-10T15:10:35.260849: step 4141, loss 0.0826281, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:35.547417: step 4142, loss 0.146728, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:35.811574: step 4143, loss 0.0885149, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:36.065507: step 4144, loss 0.0577902, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:36.362151: step 4145, loss 0.140793, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:36.674874: step 4146, loss 0.0912991, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:36.980895: step 4147, loss 0.0837807, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:37.280055: step 4148, loss 0.090868, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:37.593110: step 4149, loss 0.0807467, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:37.831311: step 4150, loss 0.0912919, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:38.032834: step 4151, loss 0.084695, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:38.277073: step 4152, loss 0.0743618, acc 1, learning_rate 0.0001
2017-10-10T15:10:38.532888: step 4153, loss 0.1371, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:38.888821: step 4154, loss 0.128563, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:39.143681: step 4155, loss 0.0920095, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:39.334692: step 4156, loss 0.044565, acc 1, learning_rate 0.0001
2017-10-10T15:10:39.614210: step 4157, loss 0.102045, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:39.840441: step 4158, loss 0.122537, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:40.093599: step 4159, loss 0.15362, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:40.403334: step 4160, loss 0.140685, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:10:40.971261: step 4160, loss 0.220893, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4160

2017-10-10T15:10:41.991539: step 4161, loss 0.154071, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:42.269739: step 4162, loss 0.117964, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:42.539821: step 4163, loss 0.0803096, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:42.818427: step 4164, loss 0.0717238, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:43.032851: step 4165, loss 0.0533565, acc 1, learning_rate 0.0001
2017-10-10T15:10:43.291773: step 4166, loss 0.117692, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:43.520447: step 4167, loss 0.0579786, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:43.755061: step 4168, loss 0.0541683, acc 1, learning_rate 0.0001
2017-10-10T15:10:44.031744: step 4169, loss 0.140885, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:44.329135: step 4170, loss 0.0926214, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:44.652880: step 4171, loss 0.112305, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:44.903831: step 4172, loss 0.0970152, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:45.182806: step 4173, loss 0.109999, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:45.520675: step 4174, loss 0.073883, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:45.837010: step 4175, loss 0.137222, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:46.037112: step 4176, loss 0.281261, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:46.336869: step 4177, loss 0.0583823, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:46.621018: step 4178, loss 0.0680987, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:46.884275: step 4179, loss 0.119004, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:47.157109: step 4180, loss 0.117353, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:47.453197: step 4181, loss 0.0992998, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:47.732994: step 4182, loss 0.206719, acc 0.921875, learning_rate 0.0001
2017-10-10T15:10:48.044359: step 4183, loss 0.109494, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:48.365700: step 4184, loss 0.106212, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:48.645451: step 4185, loss 0.0971715, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:48.894149: step 4186, loss 0.114565, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:49.176538: step 4187, loss 0.110008, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:49.500844: step 4188, loss 0.23104, acc 0.90625, learning_rate 0.0001
2017-10-10T15:10:49.796951: step 4189, loss 0.0770044, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:50.116284: step 4190, loss 0.170101, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:50.387325: step 4191, loss 0.118673, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:50.686755: step 4192, loss 0.123205, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:50.952861: step 4193, loss 0.122055, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:51.222708: step 4194, loss 0.127658, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:51.478705: step 4195, loss 0.181576, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:51.752977: step 4196, loss 0.0983691, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:52.026711: step 4197, loss 0.109097, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:52.302558: step 4198, loss 0.0761582, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:52.564858: step 4199, loss 0.213588, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:52.888985: step 4200, loss 0.0716265, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:10:53.350247: step 4200, loss 0.218765, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4200

2017-10-10T15:10:54.958324: step 4201, loss 0.0468704, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:55.286351: step 4202, loss 0.150532, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:55.536917: step 4203, loss 0.073774, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:55.753689: step 4204, loss 0.129969, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:56.016125: step 4205, loss 0.247091, acc 0.90625, learning_rate 0.0001
2017-10-10T15:10:56.317814: step 4206, loss 0.0884807, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:56.603252: step 4207, loss 0.100554, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:56.888575: step 4208, loss 0.0983457, acc 0.953125, learning_rate 0.0001
2017-10-10T15:10:57.170109: step 4209, loss 0.0770818, acc 0.96875, learning_rate 0.0001
2017-10-10T15:10:57.456906: step 4210, loss 0.169858, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:57.733036: step 4211, loss 0.0942296, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:57.968122: step 4212, loss 0.0568194, acc 1, learning_rate 0.0001
2017-10-10T15:10:58.232430: step 4213, loss 0.0670504, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:58.476870: step 4214, loss 0.112041, acc 0.980392, learning_rate 0.0001
2017-10-10T15:10:58.816880: step 4215, loss 0.0750899, acc 0.984375, learning_rate 0.0001
2017-10-10T15:10:59.133025: step 4216, loss 0.157092, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:59.428815: step 4217, loss 0.117153, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:59.674856: step 4218, loss 0.144737, acc 0.9375, learning_rate 0.0001
2017-10-10T15:10:59.879818: step 4219, loss 0.179019, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:00.144236: step 4220, loss 0.0803161, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:00.422892: step 4221, loss 0.118998, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:00.720902: step 4222, loss 0.13326, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:00.998470: step 4223, loss 0.0535147, acc 1, learning_rate 0.0001
2017-10-10T15:11:01.234815: step 4224, loss 0.156765, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:01.513424: step 4225, loss 0.147314, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:01.816819: step 4226, loss 0.107545, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:02.077517: step 4227, loss 0.134505, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:02.305170: step 4228, loss 0.172769, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:02.629893: step 4229, loss 0.0919509, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:02.896868: step 4230, loss 0.122876, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:03.228937: step 4231, loss 0.107214, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:03.477093: step 4232, loss 0.140325, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:03.768899: step 4233, loss 0.0444723, acc 1, learning_rate 0.0001
2017-10-10T15:11:04.041176: step 4234, loss 0.0497726, acc 1, learning_rate 0.0001
2017-10-10T15:11:04.309016: step 4235, loss 0.0551661, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:04.550441: step 4236, loss 0.138706, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:04.869260: step 4237, loss 0.131096, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:05.172845: step 4238, loss 0.122541, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:05.449339: step 4239, loss 0.0985017, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:05.701255: step 4240, loss 0.191917, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T15:11:06.190068: step 4240, loss 0.218526, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4240

2017-10-10T15:11:07.129127: step 4241, loss 0.0821996, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:07.462099: step 4242, loss 0.0700783, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:07.685431: step 4243, loss 0.168734, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:07.946323: step 4244, loss 0.15204, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:08.210391: step 4245, loss 0.127798, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:08.510658: step 4246, loss 0.0648792, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:08.723947: step 4247, loss 0.170692, acc 0.90625, learning_rate 0.0001
2017-10-10T15:11:09.012929: step 4248, loss 0.111897, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:09.284927: step 4249, loss 0.156361, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:09.567116: step 4250, loss 0.106088, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:09.815128: step 4251, loss 0.18173, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:10.127746: step 4252, loss 0.0365556, acc 1, learning_rate 0.0001
2017-10-10T15:11:10.389172: step 4253, loss 0.0783789, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:10.648333: step 4254, loss 0.260624, acc 0.890625, learning_rate 0.0001
2017-10-10T15:11:10.911081: step 4255, loss 0.0882258, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:11.152969: step 4256, loss 0.123715, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:11.427089: step 4257, loss 0.0742268, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:11.708867: step 4258, loss 0.123339, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:12.030883: step 4259, loss 0.132226, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:12.277654: step 4260, loss 0.129781, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:12.568492: step 4261, loss 0.0672051, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:12.808468: step 4262, loss 0.148634, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:13.091411: step 4263, loss 0.116072, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:13.298031: step 4264, loss 0.108916, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:13.545006: step 4265, loss 0.0485559, acc 1, learning_rate 0.0001
2017-10-10T15:11:13.780800: step 4266, loss 0.0514524, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:14.038943: step 4267, loss 0.173085, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:14.348853: step 4268, loss 0.0964958, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:14.592218: step 4269, loss 0.140064, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:14.925144: step 4270, loss 0.104837, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:15.216838: step 4271, loss 0.0707382, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:15.525158: step 4272, loss 0.0965025, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:15.836903: step 4273, loss 0.127016, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:16.255939: step 4274, loss 0.111641, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:16.520855: step 4275, loss 0.1346, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:16.764691: step 4276, loss 0.137183, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:16.967174: step 4277, loss 0.169188, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:17.352965: step 4278, loss 0.092323, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:17.615976: step 4279, loss 0.137273, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:17.841819: step 4280, loss 0.202477, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T15:11:18.299166: step 4280, loss 0.219788, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4280

2017-10-10T15:11:19.560862: step 4281, loss 0.12369, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:19.825787: step 4282, loss 0.100771, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:20.112838: step 4283, loss 0.10954, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:20.436898: step 4284, loss 0.090161, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:20.713087: step 4285, loss 0.0682614, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:20.977216: step 4286, loss 0.0739857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:21.258455: step 4287, loss 0.130488, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:21.521061: step 4288, loss 0.118672, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:21.761553: step 4289, loss 0.0805662, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:22.077747: step 4290, loss 0.107529, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:22.329046: step 4291, loss 0.18466, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:22.572889: step 4292, loss 0.0572614, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:22.893072: step 4293, loss 0.0564536, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:23.170222: step 4294, loss 0.069535, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:23.447597: step 4295, loss 0.17413, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:23.688883: step 4296, loss 0.118374, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:23.980947: step 4297, loss 0.0628884, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:24.318442: step 4298, loss 0.0617993, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:24.591216: step 4299, loss 0.0969033, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:24.844821: step 4300, loss 0.129152, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:25.132685: step 4301, loss 0.0739496, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:25.412686: step 4302, loss 0.110488, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:25.703461: step 4303, loss 0.0451138, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:25.950848: step 4304, loss 0.139807, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:26.220874: step 4305, loss 0.0863736, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:26.468686: step 4306, loss 0.0860321, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:26.777641: step 4307, loss 0.143948, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:27.061866: step 4308, loss 0.157078, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:27.308194: step 4309, loss 0.157372, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:27.641111: step 4310, loss 0.0674518, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:27.929998: step 4311, loss 0.0978733, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:28.160868: step 4312, loss 0.0862039, acc 0.941176, learning_rate 0.0001
2017-10-10T15:11:28.422079: step 4313, loss 0.0965552, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:28.716579: step 4314, loss 0.162148, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:29.020261: step 4315, loss 0.158491, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:29.340449: step 4316, loss 0.0663439, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:29.601940: step 4317, loss 0.101851, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:29.928805: step 4318, loss 0.0790494, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:30.213158: step 4319, loss 0.120771, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:30.493052: step 4320, loss 0.120381, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:11:31.041486: step 4320, loss 0.22088, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4320

2017-10-10T15:11:32.121126: step 4321, loss 0.147111, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:32.467103: step 4322, loss 0.0616485, acc 1, learning_rate 0.0001
2017-10-10T15:11:32.744980: step 4323, loss 0.11696, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:32.977393: step 4324, loss 0.12543, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:33.172960: step 4325, loss 0.168506, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:33.398537: step 4326, loss 0.170679, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:33.636908: step 4327, loss 0.0313332, acc 1, learning_rate 0.0001
2017-10-10T15:11:33.921423: step 4328, loss 0.0797119, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:34.156825: step 4329, loss 0.103013, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:34.369867: step 4330, loss 0.116211, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:34.624254: step 4331, loss 0.116541, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:34.953021: step 4332, loss 0.0617503, acc 1, learning_rate 0.0001
2017-10-10T15:11:35.213129: step 4333, loss 0.0973679, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:35.544830: step 4334, loss 0.12965, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:35.841101: step 4335, loss 0.143856, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:36.095314: step 4336, loss 0.172716, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:36.408877: step 4337, loss 0.0464785, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:36.719964: step 4338, loss 0.302365, acc 0.90625, learning_rate 0.0001
2017-10-10T15:11:37.004503: step 4339, loss 0.0515589, acc 1, learning_rate 0.0001
2017-10-10T15:11:37.262027: step 4340, loss 0.0450602, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:37.539530: step 4341, loss 0.0939927, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:37.718099: step 4342, loss 0.0859974, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:38.019938: step 4343, loss 0.0796525, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:38.247288: step 4344, loss 0.16739, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:38.539112: step 4345, loss 0.0442668, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:38.830928: step 4346, loss 0.122563, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:39.110290: step 4347, loss 0.156031, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:39.388947: step 4348, loss 0.219363, acc 0.90625, learning_rate 0.0001
2017-10-10T15:11:39.696952: step 4349, loss 0.0610002, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:39.951032: step 4350, loss 0.111313, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:40.197620: step 4351, loss 0.0940235, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:40.505173: step 4352, loss 0.131318, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:40.800077: step 4353, loss 0.134233, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:41.048916: step 4354, loss 0.115114, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:41.333560: step 4355, loss 0.182802, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:41.611479: step 4356, loss 0.13306, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:41.848478: step 4357, loss 0.181696, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:42.153185: step 4358, loss 0.0983345, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:42.413007: step 4359, loss 0.161261, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:42.700281: step 4360, loss 0.0898275, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:11:43.196220: step 4360, loss 0.218146, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4360

2017-10-10T15:11:44.314325: step 4361, loss 0.127591, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:44.643536: step 4362, loss 0.0932627, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:44.902732: step 4363, loss 0.12455, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:45.177206: step 4364, loss 0.0677567, acc 1, learning_rate 0.0001
2017-10-10T15:11:45.388394: step 4365, loss 0.0716582, acc 1, learning_rate 0.0001
2017-10-10T15:11:45.648955: step 4366, loss 0.13799, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:45.925931: step 4367, loss 0.0798009, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:46.126252: step 4368, loss 0.0966485, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:46.445201: step 4369, loss 0.0490253, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:46.771068: step 4370, loss 0.140046, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:47.076993: step 4371, loss 0.106284, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:47.368629: step 4372, loss 0.128156, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:47.651332: step 4373, loss 0.0959757, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:47.882600: step 4374, loss 0.0628856, acc 1, learning_rate 0.0001
2017-10-10T15:11:48.128631: step 4375, loss 0.154794, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:48.452836: step 4376, loss 0.0620969, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:48.724911: step 4377, loss 0.143231, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:49.053098: step 4378, loss 0.100295, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:49.371754: step 4379, loss 0.160485, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:49.675937: step 4380, loss 0.0776707, acc 1, learning_rate 0.0001
2017-10-10T15:11:49.938854: step 4381, loss 0.140602, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:50.122956: step 4382, loss 0.182372, acc 0.890625, learning_rate 0.0001
2017-10-10T15:11:50.294477: step 4383, loss 0.105174, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:50.477250: step 4384, loss 0.0742989, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:50.735937: step 4385, loss 0.0964067, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:51.040258: step 4386, loss 0.0895393, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:51.284585: step 4387, loss 0.137407, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:51.506615: step 4388, loss 0.122506, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:51.769328: step 4389, loss 0.0750225, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:52.044841: step 4390, loss 0.0567644, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:52.343595: step 4391, loss 0.211078, acc 0.921875, learning_rate 0.0001
2017-10-10T15:11:52.596864: step 4392, loss 0.0866099, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:52.876812: step 4393, loss 0.121298, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:53.125442: step 4394, loss 0.064202, acc 1, learning_rate 0.0001
2017-10-10T15:11:53.396211: step 4395, loss 0.12428, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:53.685547: step 4396, loss 0.111467, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:53.965932: step 4397, loss 0.101358, acc 0.9375, learning_rate 0.0001
2017-10-10T15:11:54.304971: step 4398, loss 0.145024, acc 0.953125, learning_rate 0.0001
2017-10-10T15:11:54.556226: step 4399, loss 0.104082, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:54.843174: step 4400, loss 0.158167, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:11:55.302311: step 4400, loss 0.219031, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4400

2017-10-10T15:11:56.304983: step 4401, loss 0.0772177, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:56.583242: step 4402, loss 0.0779675, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:56.782937: step 4403, loss 0.110241, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:57.054555: step 4404, loss 0.0907852, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:57.359682: step 4405, loss 0.0407091, acc 1, learning_rate 0.0001
2017-10-10T15:11:57.584829: step 4406, loss 0.0482383, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:57.878468: step 4407, loss 0.0826202, acc 0.984375, learning_rate 0.0001
2017-10-10T15:11:58.192951: step 4408, loss 0.0808357, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:58.448807: step 4409, loss 0.112141, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:58.689208: step 4410, loss 0.166023, acc 0.960784, learning_rate 0.0001
2017-10-10T15:11:58.973297: step 4411, loss 0.0780946, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:59.230082: step 4412, loss 0.0551075, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:59.520880: step 4413, loss 0.104124, acc 0.96875, learning_rate 0.0001
2017-10-10T15:11:59.802456: step 4414, loss 0.16378, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:00.092850: step 4415, loss 0.133628, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:00.466979: step 4416, loss 0.176133, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:00.737918: step 4417, loss 0.0771851, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:00.907045: step 4418, loss 0.124725, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:01.196760: step 4419, loss 0.0542581, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:01.488874: step 4420, loss 0.058986, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:01.731035: step 4421, loss 0.146799, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:02.024868: step 4422, loss 0.0745002, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:02.261193: step 4423, loss 0.182072, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:02.577321: step 4424, loss 0.081896, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:02.811409: step 4425, loss 0.0604511, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:03.086140: step 4426, loss 0.0700961, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:03.378957: step 4427, loss 0.0497744, acc 1, learning_rate 0.0001
2017-10-10T15:12:03.611065: step 4428, loss 0.128887, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:03.908936: step 4429, loss 0.120914, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:04.200510: step 4430, loss 0.142726, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:04.520845: step 4431, loss 0.137504, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:04.805016: step 4432, loss 0.187643, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:05.090988: step 4433, loss 0.102745, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:05.400398: step 4434, loss 0.129758, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:05.713913: step 4435, loss 0.100002, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:06.036945: step 4436, loss 0.232648, acc 0.90625, learning_rate 0.0001
2017-10-10T15:12:06.307985: step 4437, loss 0.0945518, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:06.535161: step 4438, loss 0.0797854, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:06.745691: step 4439, loss 0.133359, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:07.034881: step 4440, loss 0.131984, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:07.473655: step 4440, loss 0.218187, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4440

2017-10-10T15:12:08.552851: step 4441, loss 0.0675165, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:08.800064: step 4442, loss 0.0543463, acc 1, learning_rate 0.0001
2017-10-10T15:12:09.032787: step 4443, loss 0.0607819, acc 1, learning_rate 0.0001
2017-10-10T15:12:09.268832: step 4444, loss 0.135569, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:09.583303: step 4445, loss 0.151441, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:09.824947: step 4446, loss 0.148257, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:10.137119: step 4447, loss 0.183377, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:10.416930: step 4448, loss 0.13567, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:10.755633: step 4449, loss 0.0999946, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:10.988873: step 4450, loss 0.0746244, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:11.287846: step 4451, loss 0.0597137, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:11.564333: step 4452, loss 0.094768, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:11.799113: step 4453, loss 0.0373965, acc 1, learning_rate 0.0001
2017-10-10T15:12:12.119958: step 4454, loss 0.151759, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:12.398847: step 4455, loss 0.14044, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:12.638718: step 4456, loss 0.0939016, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:12.919129: step 4457, loss 0.203163, acc 0.890625, learning_rate 0.0001
2017-10-10T15:12:13.161061: step 4458, loss 0.126662, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:13.468859: step 4459, loss 0.178249, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:13.808976: step 4460, loss 0.096328, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:14.068256: step 4461, loss 0.188537, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:14.339040: step 4462, loss 0.0922252, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:14.605012: step 4463, loss 0.0658046, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:14.844932: step 4464, loss 0.127591, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:15.125063: step 4465, loss 0.0830378, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:15.453574: step 4466, loss 0.126157, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:15.743976: step 4467, loss 0.0481206, acc 1, learning_rate 0.0001
2017-10-10T15:12:15.993150: step 4468, loss 0.178347, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:16.273160: step 4469, loss 0.0791893, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:16.554516: step 4470, loss 0.0731547, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:16.812583: step 4471, loss 0.0857477, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:17.096804: step 4472, loss 0.0757181, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:17.442071: step 4473, loss 0.0667983, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:17.701080: step 4474, loss 0.1147, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:18.021842: step 4475, loss 0.109292, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:18.401103: step 4476, loss 0.0915405, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:18.648899: step 4477, loss 0.0960827, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:18.895999: step 4478, loss 0.180552, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:19.108939: step 4479, loss 0.112423, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:19.367091: step 4480, loss 0.124363, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:19.810462: step 4480, loss 0.217374, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4480

2017-10-10T15:12:20.855976: step 4481, loss 0.106206, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:21.173217: step 4482, loss 0.140373, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:21.446252: step 4483, loss 0.0733692, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:21.771162: step 4484, loss 0.118877, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:22.035521: step 4485, loss 0.145487, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:22.341053: step 4486, loss 0.0827969, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:22.629830: step 4487, loss 0.152012, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:22.886300: step 4488, loss 0.128475, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:23.168744: step 4489, loss 0.139174, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:23.438519: step 4490, loss 0.100479, acc 1, learning_rate 0.0001
2017-10-10T15:12:23.660057: step 4491, loss 0.127529, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:23.937744: step 4492, loss 0.111734, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:24.240636: step 4493, loss 0.123088, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:24.501214: step 4494, loss 0.0990373, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:24.782228: step 4495, loss 0.11495, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:25.088990: step 4496, loss 0.121137, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:25.374184: step 4497, loss 0.192483, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:25.621659: step 4498, loss 0.139145, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:25.903508: step 4499, loss 0.100134, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:26.251247: step 4500, loss 0.0614083, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:26.487029: step 4501, loss 0.123744, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:26.731860: step 4502, loss 0.16714, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:26.992916: step 4503, loss 0.185815, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:27.266245: step 4504, loss 0.100934, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:27.432131: step 4505, loss 0.0605282, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:27.711952: step 4506, loss 0.159882, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:27.949249: step 4507, loss 0.122131, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:28.158401: step 4508, loss 0.0793855, acc 0.980392, learning_rate 0.0001
2017-10-10T15:12:28.472890: step 4509, loss 0.105707, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:28.789445: step 4510, loss 0.102071, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:29.112860: step 4511, loss 0.0830821, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:29.432949: step 4512, loss 0.0708133, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:29.705786: step 4513, loss 0.108034, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:29.919423: step 4514, loss 0.0898335, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:30.208911: step 4515, loss 0.0773506, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:30.473067: step 4516, loss 0.099044, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:30.764971: step 4517, loss 0.0945685, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:31.088283: step 4518, loss 0.109894, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:31.392881: step 4519, loss 0.0833953, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:31.640925: step 4520, loss 0.148642, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:32.120863: step 4520, loss 0.219266, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4520

2017-10-10T15:12:33.099248: step 4521, loss 0.0776993, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:33.335411: step 4522, loss 0.0675392, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:33.607868: step 4523, loss 0.138226, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:33.901319: step 4524, loss 0.027082, acc 1, learning_rate 0.0001
2017-10-10T15:12:34.223155: step 4525, loss 0.071314, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:34.484212: step 4526, loss 0.106165, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:34.731249: step 4527, loss 0.126914, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:35.004290: step 4528, loss 0.0533072, acc 1, learning_rate 0.0001
2017-10-10T15:12:35.263724: step 4529, loss 0.131465, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:35.523987: step 4530, loss 0.111797, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:35.813089: step 4531, loss 0.0544833, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:36.028795: step 4532, loss 0.118912, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:36.296608: step 4533, loss 0.104873, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:36.599971: step 4534, loss 0.127884, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:36.874478: step 4535, loss 0.0822186, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:37.175429: step 4536, loss 0.143815, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:37.460955: step 4537, loss 0.106996, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:37.724865: step 4538, loss 0.106361, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:38.007892: step 4539, loss 0.096998, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:38.300231: step 4540, loss 0.155484, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:38.510450: step 4541, loss 0.0725985, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:38.812138: step 4542, loss 0.126424, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:39.107865: step 4543, loss 0.0528596, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:39.469760: step 4544, loss 0.12328, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:39.717953: step 4545, loss 0.104065, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:39.967459: step 4546, loss 0.0695791, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:40.215721: step 4547, loss 0.126944, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:40.541193: step 4548, loss 0.0719924, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:40.773063: step 4549, loss 0.0545027, acc 1, learning_rate 0.0001
2017-10-10T15:12:41.044747: step 4550, loss 0.177612, acc 0.90625, learning_rate 0.0001
2017-10-10T15:12:41.268810: step 4551, loss 0.0908549, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:41.492726: step 4552, loss 0.176, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:41.766664: step 4553, loss 0.110607, acc 1, learning_rate 0.0001
2017-10-10T15:12:42.059742: step 4554, loss 0.0815461, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:42.335213: step 4555, loss 0.138036, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:42.663416: step 4556, loss 0.109993, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:42.964365: step 4557, loss 0.125804, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:43.244315: step 4558, loss 0.0723028, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:43.447043: step 4559, loss 0.144687, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:43.728441: step 4560, loss 0.1876, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:44.209336: step 4560, loss 0.219047, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4560

2017-10-10T15:12:45.400985: step 4561, loss 0.0783198, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:45.675361: step 4562, loss 0.112702, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:45.964950: step 4563, loss 0.111416, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:46.295208: step 4564, loss 0.114967, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:46.636203: step 4565, loss 0.133457, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:46.922925: step 4566, loss 0.0880491, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:47.210807: step 4567, loss 0.0800645, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:47.514584: step 4568, loss 0.0994162, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:47.756947: step 4569, loss 0.0674391, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:48.031532: step 4570, loss 0.0591698, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:48.325011: step 4571, loss 0.118226, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:48.532377: step 4572, loss 0.0764749, acc 1, learning_rate 0.0001
2017-10-10T15:12:48.836428: step 4573, loss 0.0971521, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:49.052882: step 4574, loss 0.0726291, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:49.317855: step 4575, loss 0.109948, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:49.598498: step 4576, loss 0.10266, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:49.866005: step 4577, loss 0.161201, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:50.153017: step 4578, loss 0.196118, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:50.455114: step 4579, loss 0.104, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:50.744895: step 4580, loss 0.0576439, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:51.001004: step 4581, loss 0.10235, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:51.312872: step 4582, loss 0.0472664, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:51.632971: step 4583, loss 0.171653, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:51.929036: step 4584, loss 0.0681852, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:52.122700: step 4585, loss 0.135327, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:52.428842: step 4586, loss 0.0835117, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:52.713749: step 4587, loss 0.0870491, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:52.971715: step 4588, loss 0.118565, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:53.300791: step 4589, loss 0.159112, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:53.619888: step 4590, loss 0.0612257, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:53.848863: step 4591, loss 0.0912723, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:54.164847: step 4592, loss 0.0732926, acc 1, learning_rate 0.0001
2017-10-10T15:12:54.389044: step 4593, loss 0.139897, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:54.633364: step 4594, loss 0.0706304, acc 1, learning_rate 0.0001
2017-10-10T15:12:54.910554: step 4595, loss 0.139308, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:55.158413: step 4596, loss 0.240543, acc 0.90625, learning_rate 0.0001
2017-10-10T15:12:55.442506: step 4597, loss 0.136081, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:55.768963: step 4598, loss 0.104097, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:56.021404: step 4599, loss 0.110551, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:56.311025: step 4600, loss 0.121822, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:12:56.749736: step 4600, loss 0.220116, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4600

2017-10-10T15:12:57.878704: step 4601, loss 0.181146, acc 0.921875, learning_rate 0.0001
2017-10-10T15:12:58.168888: step 4602, loss 0.114443, acc 0.953125, learning_rate 0.0001
2017-10-10T15:12:58.511228: step 4603, loss 0.118533, acc 0.9375, learning_rate 0.0001
2017-10-10T15:12:58.802832: step 4604, loss 0.0539103, acc 0.984375, learning_rate 0.0001
2017-10-10T15:12:59.051226: step 4605, loss 0.0473837, acc 1, learning_rate 0.0001
2017-10-10T15:12:59.285447: step 4606, loss 0.0469821, acc 1, learning_rate 0.0001
2017-10-10T15:12:59.524575: step 4607, loss 0.0882686, acc 0.96875, learning_rate 0.0001
2017-10-10T15:12:59.792234: step 4608, loss 0.122377, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:00.076914: step 4609, loss 0.139706, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:00.374343: step 4610, loss 0.0924762, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:00.637705: step 4611, loss 0.215251, acc 0.90625, learning_rate 0.0001
2017-10-10T15:13:00.935099: step 4612, loss 0.0995858, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:01.251782: step 4613, loss 0.0385355, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:01.587702: step 4614, loss 0.138664, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:01.841152: step 4615, loss 0.126903, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:02.108868: step 4616, loss 0.145098, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:02.434171: step 4617, loss 0.0954811, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:02.709010: step 4618, loss 0.136732, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:03.023794: step 4619, loss 0.0805467, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:03.211880: step 4620, loss 0.147193, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:03.354897: step 4621, loss 0.0945413, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:03.528889: step 4622, loss 0.143735, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:03.749240: step 4623, loss 0.156488, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:03.954794: step 4624, loss 0.0811811, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:04.185008: step 4625, loss 0.122303, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:04.436978: step 4626, loss 0.240596, acc 0.90625, learning_rate 0.0001
2017-10-10T15:13:04.691579: step 4627, loss 0.0768978, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:04.957715: step 4628, loss 0.100214, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:05.256955: step 4629, loss 0.124176, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:05.533113: step 4630, loss 0.115217, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:05.821110: step 4631, loss 0.136043, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:06.133783: step 4632, loss 0.11059, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:06.464923: step 4633, loss 0.154555, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:06.764903: step 4634, loss 0.121389, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:07.012900: step 4635, loss 0.0316731, acc 1, learning_rate 0.0001
2017-10-10T15:13:07.232472: step 4636, loss 0.134839, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:07.444518: step 4637, loss 0.0814717, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:07.710161: step 4638, loss 0.171346, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:07.947058: step 4639, loss 0.0995744, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:08.183948: step 4640, loss 0.114056, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:08.651446: step 4640, loss 0.217796, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4640

2017-10-10T15:13:09.834042: step 4641, loss 0.122727, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:10.104939: step 4642, loss 0.121308, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:10.348964: step 4643, loss 0.162569, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:10.637066: step 4644, loss 0.0815832, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:10.895741: step 4645, loss 0.0493845, acc 1, learning_rate 0.0001
2017-10-10T15:13:11.192605: step 4646, loss 0.10373, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:11.532214: step 4647, loss 0.0702305, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:11.820914: step 4648, loss 0.118259, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:12.120833: step 4649, loss 0.0448598, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:12.483871: step 4650, loss 0.115291, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:12.753380: step 4651, loss 0.195814, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:12.991857: step 4652, loss 0.0928549, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:13.236846: step 4653, loss 0.0626581, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:13.525157: step 4654, loss 0.107262, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:13.798646: step 4655, loss 0.0920923, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:14.117076: step 4656, loss 0.0599993, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:14.411560: step 4657, loss 0.137801, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:14.629656: step 4658, loss 0.09261, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:14.847623: step 4659, loss 0.159276, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:15.136380: step 4660, loss 0.0787385, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:15.432909: step 4661, loss 0.13876, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:15.703945: step 4662, loss 0.0949326, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:15.967177: step 4663, loss 0.152949, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:16.264842: step 4664, loss 0.202983, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:16.500103: step 4665, loss 0.0835275, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:16.804541: step 4666, loss 0.0716107, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:17.090707: step 4667, loss 0.194208, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:17.371877: step 4668, loss 0.20873, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:17.680851: step 4669, loss 0.123503, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:17.960562: step 4670, loss 0.0668461, acc 1, learning_rate 0.0001
2017-10-10T15:13:18.229038: step 4671, loss 0.0951158, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:18.508908: step 4672, loss 0.0701974, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:18.788980: step 4673, loss 0.137368, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:19.005626: step 4674, loss 0.05351, acc 1, learning_rate 0.0001
2017-10-10T15:13:19.263568: step 4675, loss 0.115531, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:19.566835: step 4676, loss 0.0716825, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:19.816838: step 4677, loss 0.0831835, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:20.114602: step 4678, loss 0.0979243, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:20.388986: step 4679, loss 0.103541, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:20.570649: step 4680, loss 0.0522872, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:21.037738: step 4680, loss 0.220151, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4680

2017-10-10T15:13:21.974585: step 4681, loss 0.0731398, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:22.236814: step 4682, loss 0.152708, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:22.495059: step 4683, loss 0.0863441, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:22.743528: step 4684, loss 0.0453193, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:23.049036: step 4685, loss 0.148785, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:23.345991: step 4686, loss 0.0688575, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:23.593137: step 4687, loss 0.0797976, acc 1, learning_rate 0.0001
2017-10-10T15:13:23.868574: step 4688, loss 0.0742534, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:24.189189: step 4689, loss 0.0781158, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:24.489156: step 4690, loss 0.0678316, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:24.773414: step 4691, loss 0.093541, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:25.078996: step 4692, loss 0.0837128, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:25.396922: step 4693, loss 0.110783, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:25.619797: step 4694, loss 0.0777521, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:25.904944: step 4695, loss 0.0709692, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:26.137927: step 4696, loss 0.0584934, acc 1, learning_rate 0.0001
2017-10-10T15:13:26.447171: step 4697, loss 0.171578, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:26.705377: step 4698, loss 0.119154, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:26.997112: step 4699, loss 0.0616234, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:27.241153: step 4700, loss 0.0636571, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:27.508861: step 4701, loss 0.0513218, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:27.791293: step 4702, loss 0.0434573, acc 1, learning_rate 0.0001
2017-10-10T15:13:28.120948: step 4703, loss 0.0614518, acc 1, learning_rate 0.0001
2017-10-10T15:13:28.385955: step 4704, loss 0.211004, acc 0.901961, learning_rate 0.0001
2017-10-10T15:13:28.720962: step 4705, loss 0.149614, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:29.013899: step 4706, loss 0.152058, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:29.269326: step 4707, loss 0.0611915, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:29.509571: step 4708, loss 0.0534486, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:29.752859: step 4709, loss 0.149498, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:30.015405: step 4710, loss 0.0636864, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:30.277503: step 4711, loss 0.132306, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:30.533555: step 4712, loss 0.0764778, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:30.843870: step 4713, loss 0.0467629, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:31.191474: step 4714, loss 0.12229, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:31.519187: step 4715, loss 0.13471, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:31.838716: step 4716, loss 0.109619, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:32.076846: step 4717, loss 0.111884, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:32.405226: step 4718, loss 0.0909783, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:32.731493: step 4719, loss 0.147625, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:32.975350: step 4720, loss 0.156023, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:33.427992: step 4720, loss 0.219445, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4720

2017-10-10T15:13:34.544968: step 4721, loss 0.106023, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:34.831510: step 4722, loss 0.0769884, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:35.104678: step 4723, loss 0.0985202, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:35.440045: step 4724, loss 0.146921, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:35.699948: step 4725, loss 0.0803167, acc 1, learning_rate 0.0001
2017-10-10T15:13:35.989268: step 4726, loss 0.0704035, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:36.245052: step 4727, loss 0.128436, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:36.512932: step 4728, loss 0.0524061, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:36.845235: step 4729, loss 0.0883303, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:37.112193: step 4730, loss 0.101866, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:37.324976: step 4731, loss 0.116083, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:37.545500: step 4732, loss 0.0635965, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:37.870780: step 4733, loss 0.0930328, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:38.203612: step 4734, loss 0.0622652, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:38.416294: step 4735, loss 0.20494, acc 0.90625, learning_rate 0.0001
2017-10-10T15:13:38.760314: step 4736, loss 0.138655, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:39.121154: step 4737, loss 0.112503, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:39.400906: step 4738, loss 0.0761898, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:39.675594: step 4739, loss 0.114642, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:39.889151: step 4740, loss 0.061708, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:40.104824: step 4741, loss 0.0410176, acc 1, learning_rate 0.0001
2017-10-10T15:13:40.416941: step 4742, loss 0.162809, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:40.720469: step 4743, loss 0.0923058, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:40.960297: step 4744, loss 0.146413, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:41.229216: step 4745, loss 0.103667, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:41.501017: step 4746, loss 0.257203, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:41.773346: step 4747, loss 0.0816204, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:42.017660: step 4748, loss 0.0533937, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:42.354813: step 4749, loss 0.17985, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:42.645304: step 4750, loss 0.158827, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:42.873030: step 4751, loss 0.085068, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:43.129008: step 4752, loss 0.146982, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:43.420868: step 4753, loss 0.0713982, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:43.618196: step 4754, loss 0.173405, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:43.943000: step 4755, loss 0.114633, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:44.219151: step 4756, loss 0.153435, acc 0.9375, learning_rate 0.0001
2017-10-10T15:13:44.500938: step 4757, loss 0.0466146, acc 1, learning_rate 0.0001
2017-10-10T15:13:44.764935: step 4758, loss 0.0992542, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:45.064794: step 4759, loss 0.126214, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:45.368949: step 4760, loss 0.179383, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:45.929931: step 4760, loss 0.220019, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4760

2017-10-10T15:13:47.049486: step 4761, loss 0.119837, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:47.347607: step 4762, loss 0.0633122, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:47.685002: step 4763, loss 0.111061, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:47.954924: step 4764, loss 0.0931058, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:48.212659: step 4765, loss 0.093587, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:48.507677: step 4766, loss 0.052553, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:48.718777: step 4767, loss 0.130253, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:48.988196: step 4768, loss 0.087853, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:49.281191: step 4769, loss 0.279316, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:49.528372: step 4770, loss 0.117372, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:49.791298: step 4771, loss 0.11785, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:50.079301: step 4772, loss 0.117945, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:50.310560: step 4773, loss 0.0967075, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:50.569400: step 4774, loss 0.118678, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:50.830297: step 4775, loss 0.071079, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:51.088478: step 4776, loss 0.105261, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:51.397132: step 4777, loss 0.0666229, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:51.683451: step 4778, loss 0.111719, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:51.937265: step 4779, loss 0.109685, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:52.235616: step 4780, loss 0.0966934, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:52.501153: step 4781, loss 0.0970265, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:52.780535: step 4782, loss 0.200288, acc 0.921875, learning_rate 0.0001
2017-10-10T15:13:53.096808: step 4783, loss 0.0947028, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:53.378191: step 4784, loss 0.137932, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:53.598308: step 4785, loss 0.0804184, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:53.923452: step 4786, loss 0.0559586, acc 0.96875, learning_rate 0.0001
2017-10-10T15:13:54.209864: step 4787, loss 0.0364808, acc 1, learning_rate 0.0001
2017-10-10T15:13:54.476813: step 4788, loss 0.128641, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:54.766011: step 4789, loss 0.105851, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:54.992151: step 4790, loss 0.0492688, acc 1, learning_rate 0.0001
2017-10-10T15:13:55.263686: step 4791, loss 0.0500899, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:55.583783: step 4792, loss 0.0551404, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:55.849096: step 4793, loss 0.0807583, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:56.232330: step 4794, loss 0.0735552, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:56.514022: step 4795, loss 0.158511, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:56.881373: step 4796, loss 0.0489506, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:57.204843: step 4797, loss 0.154157, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:57.523317: step 4798, loss 0.083274, acc 0.984375, learning_rate 0.0001
2017-10-10T15:13:57.734043: step 4799, loss 0.0469374, acc 1, learning_rate 0.0001
2017-10-10T15:13:57.953539: step 4800, loss 0.126226, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:13:58.286317: step 4800, loss 0.218204, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4800

2017-10-10T15:13:59.309097: step 4801, loss 0.112773, acc 0.953125, learning_rate 0.0001
2017-10-10T15:13:59.549754: step 4802, loss 0.0782774, acc 0.980392, learning_rate 0.0001
2017-10-10T15:13:59.856984: step 4803, loss 0.0587209, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:00.107608: step 4804, loss 0.212618, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:00.344310: step 4805, loss 0.245693, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:00.494869: step 4806, loss 0.0421881, acc 1, learning_rate 0.0001
2017-10-10T15:14:00.789259: step 4807, loss 0.092819, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:01.081162: step 4808, loss 0.0703818, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:01.434373: step 4809, loss 0.268074, acc 0.90625, learning_rate 0.0001
2017-10-10T15:14:01.685129: step 4810, loss 0.116403, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:01.988101: step 4811, loss 0.0609152, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:02.265122: step 4812, loss 0.110619, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:02.508907: step 4813, loss 0.153075, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:02.764827: step 4814, loss 0.0697892, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:02.947563: step 4815, loss 0.163949, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:03.185094: step 4816, loss 0.170684, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:03.436055: step 4817, loss 0.148446, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:03.680828: step 4818, loss 0.0649088, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:03.960925: step 4819, loss 0.113946, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:04.215934: step 4820, loss 0.142454, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:04.442520: step 4821, loss 0.048432, acc 1, learning_rate 0.0001
2017-10-10T15:14:04.739597: step 4822, loss 0.0502859, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:05.029158: step 4823, loss 0.0998598, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:05.255706: step 4824, loss 0.100838, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:05.546952: step 4825, loss 0.108123, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:05.759838: step 4826, loss 0.0671904, acc 1, learning_rate 0.0001
2017-10-10T15:14:06.035166: step 4827, loss 0.158565, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:06.307522: step 4828, loss 0.0892895, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:06.626170: step 4829, loss 0.0656439, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:06.924902: step 4830, loss 0.0314614, acc 1, learning_rate 0.0001
2017-10-10T15:14:07.256829: step 4831, loss 0.1469, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:07.526493: step 4832, loss 0.0995728, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:07.811528: step 4833, loss 0.159339, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:08.089726: step 4834, loss 0.211708, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:08.381209: step 4835, loss 0.0682392, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:08.605330: step 4836, loss 0.135291, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:08.909985: step 4837, loss 0.0943483, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:09.157198: step 4838, loss 0.0507936, acc 1, learning_rate 0.0001
2017-10-10T15:14:09.433604: step 4839, loss 0.139851, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:09.755419: step 4840, loss 0.0582851, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:10.223365: step 4840, loss 0.219213, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4840

2017-10-10T15:14:11.316829: step 4841, loss 0.1095, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:11.587089: step 4842, loss 0.0517714, acc 1, learning_rate 0.0001
2017-10-10T15:14:11.896949: step 4843, loss 0.122681, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:12.141324: step 4844, loss 0.0413281, acc 1, learning_rate 0.0001
2017-10-10T15:14:12.415463: step 4845, loss 0.0850754, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:12.690992: step 4846, loss 0.163395, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:12.992854: step 4847, loss 0.142546, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:13.221215: step 4848, loss 0.0769152, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:13.541681: step 4849, loss 0.135375, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:13.767595: step 4850, loss 0.0852688, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:13.974722: step 4851, loss 0.0988044, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:14.191478: step 4852, loss 0.0897916, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:14.469386: step 4853, loss 0.157457, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:14.773822: step 4854, loss 0.0967627, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:14.960850: step 4855, loss 0.0519314, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:15.359675: step 4856, loss 0.0779752, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:15.681250: step 4857, loss 0.144665, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:15.966049: step 4858, loss 0.0932905, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:16.228955: step 4859, loss 0.105092, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:16.483445: step 4860, loss 0.0993131, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:16.744963: step 4861, loss 0.0781787, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:17.055415: step 4862, loss 0.106133, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:17.328973: step 4863, loss 0.0690492, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:17.593153: step 4864, loss 0.136797, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:17.880522: step 4865, loss 0.114975, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:18.208815: step 4866, loss 0.176517, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:18.487797: step 4867, loss 0.112782, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:18.825015: step 4868, loss 0.0892965, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:19.132661: step 4869, loss 0.10192, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:19.335062: step 4870, loss 0.101436, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:19.596966: step 4871, loss 0.0857622, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:19.796857: step 4872, loss 0.0571747, acc 1, learning_rate 0.0001
2017-10-10T15:14:20.042648: step 4873, loss 0.0375023, acc 1, learning_rate 0.0001
2017-10-10T15:14:20.304437: step 4874, loss 0.221542, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:20.533710: step 4875, loss 0.13151, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:20.752924: step 4876, loss 0.159604, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:21.035520: step 4877, loss 0.0390171, acc 1, learning_rate 0.0001
2017-10-10T15:14:21.303413: step 4878, loss 0.0891064, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:21.573158: step 4879, loss 0.067105, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:21.912985: step 4880, loss 0.150367, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:22.317829: step 4880, loss 0.217319, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4880

2017-10-10T15:14:23.329413: step 4881, loss 0.0373555, acc 1, learning_rate 0.0001
2017-10-10T15:14:23.623975: step 4882, loss 0.156196, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:23.893452: step 4883, loss 0.113982, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:24.216959: step 4884, loss 0.0572416, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:24.478355: step 4885, loss 0.116079, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:24.742432: step 4886, loss 0.0583795, acc 1, learning_rate 0.0001
2017-10-10T15:14:25.054990: step 4887, loss 0.148028, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:25.275487: step 4888, loss 0.0770494, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:25.545400: step 4889, loss 0.140268, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:25.864253: step 4890, loss 0.0749576, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:26.104404: step 4891, loss 0.059729, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:26.410778: step 4892, loss 0.124141, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:26.719713: step 4893, loss 0.0809674, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:26.970176: step 4894, loss 0.0806018, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:27.216423: step 4895, loss 0.112401, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:27.468836: step 4896, loss 0.079554, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:27.765101: step 4897, loss 0.164002, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:28.046224: step 4898, loss 0.0517467, acc 1, learning_rate 0.0001
2017-10-10T15:14:28.342942: step 4899, loss 0.11658, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:28.544840: step 4900, loss 0.229764, acc 0.921569, learning_rate 0.0001
2017-10-10T15:14:28.848560: step 4901, loss 0.0895407, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:29.077196: step 4902, loss 0.0994053, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:29.347561: step 4903, loss 0.163638, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:29.684154: step 4904, loss 0.0498792, acc 1, learning_rate 0.0001
2017-10-10T15:14:29.885361: step 4905, loss 0.117231, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:30.188845: step 4906, loss 0.109392, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:30.484847: step 4907, loss 0.204481, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:30.788941: step 4908, loss 0.0923222, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:31.042440: step 4909, loss 0.258398, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:31.331869: step 4910, loss 0.0536227, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:31.632597: step 4911, loss 0.102498, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:31.889896: step 4912, loss 0.128845, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:32.200622: step 4913, loss 0.096733, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:32.476851: step 4914, loss 0.0638297, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:32.766150: step 4915, loss 0.161688, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:33.048604: step 4916, loss 0.172655, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:33.340921: step 4917, loss 0.080349, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:33.608810: step 4918, loss 0.035634, acc 1, learning_rate 0.0001
2017-10-10T15:14:33.872837: step 4919, loss 0.0799524, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:34.160922: step 4920, loss 0.0912301, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:34.528614: step 4920, loss 0.216433, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4920

2017-10-10T15:14:35.683925: step 4921, loss 0.0516134, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:35.894086: step 4922, loss 0.0828916, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:36.116871: step 4923, loss 0.116405, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:36.387658: step 4924, loss 0.115033, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:36.653519: step 4925, loss 0.0890604, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:36.968983: step 4926, loss 0.102615, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:37.220768: step 4927, loss 0.0804401, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:37.539189: step 4928, loss 0.111711, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:37.809385: step 4929, loss 0.126757, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:38.101047: step 4930, loss 0.114576, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:38.433419: step 4931, loss 0.167743, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:38.740840: step 4932, loss 0.107395, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:38.998374: step 4933, loss 0.033785, acc 1, learning_rate 0.0001
2017-10-10T15:14:39.289539: step 4934, loss 0.0418212, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:39.580970: step 4935, loss 0.182492, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:39.825276: step 4936, loss 0.0517187, acc 1, learning_rate 0.0001
2017-10-10T15:14:40.058247: step 4937, loss 0.101841, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:40.289048: step 4938, loss 0.103063, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:40.564093: step 4939, loss 0.108998, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:40.821044: step 4940, loss 0.107426, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:41.077226: step 4941, loss 0.146595, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:41.395040: step 4942, loss 0.15188, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:41.657977: step 4943, loss 0.102538, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:41.968938: step 4944, loss 0.127511, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:42.228899: step 4945, loss 0.160289, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:42.556939: step 4946, loss 0.117928, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:42.816847: step 4947, loss 0.107607, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:43.065680: step 4948, loss 0.108721, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:43.329309: step 4949, loss 0.0525235, acc 1, learning_rate 0.0001
2017-10-10T15:14:43.577027: step 4950, loss 0.0664594, acc 1, learning_rate 0.0001
2017-10-10T15:14:43.863692: step 4951, loss 0.161484, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:44.140625: step 4952, loss 0.133601, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:44.438710: step 4953, loss 0.0673495, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:44.670542: step 4954, loss 0.127189, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:44.933398: step 4955, loss 0.0701544, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:45.209917: step 4956, loss 0.0880344, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:45.464356: step 4957, loss 0.0280273, acc 1, learning_rate 0.0001
2017-10-10T15:14:45.784818: step 4958, loss 0.106274, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:46.061936: step 4959, loss 0.114727, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:46.306432: step 4960, loss 0.100783, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:46.791794: step 4960, loss 0.216106, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-4960

2017-10-10T15:14:47.824922: step 4961, loss 0.0798455, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:48.141833: step 4962, loss 0.0802807, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:48.394561: step 4963, loss 0.0741216, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:48.657671: step 4964, loss 0.0612398, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:48.959901: step 4965, loss 0.0643118, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:49.250120: step 4966, loss 0.0899359, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:49.513903: step 4967, loss 0.0608528, acc 1, learning_rate 0.0001
2017-10-10T15:14:49.773097: step 4968, loss 0.0659022, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:50.111532: step 4969, loss 0.0891084, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:50.383357: step 4970, loss 0.164656, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:50.616325: step 4971, loss 0.0513087, acc 1, learning_rate 0.0001
2017-10-10T15:14:50.871726: step 4972, loss 0.086736, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:51.137023: step 4973, loss 0.0903033, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:51.379663: step 4974, loss 0.165531, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:51.696017: step 4975, loss 0.16868, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:52.054516: step 4976, loss 0.0826237, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:52.381054: step 4977, loss 0.125785, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:52.611350: step 4978, loss 0.0543227, acc 1, learning_rate 0.0001
2017-10-10T15:14:52.859819: step 4979, loss 0.163436, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:53.045099: step 4980, loss 0.161488, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:53.188451: step 4981, loss 0.0937554, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:53.368613: step 4982, loss 0.0875163, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:53.575758: step 4983, loss 0.050679, acc 1, learning_rate 0.0001
2017-10-10T15:14:53.900529: step 4984, loss 0.0906643, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:54.182108: step 4985, loss 0.191552, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:54.443254: step 4986, loss 0.102722, acc 0.953125, learning_rate 0.0001
2017-10-10T15:14:54.705966: step 4987, loss 0.0413501, acc 1, learning_rate 0.0001
2017-10-10T15:14:54.975947: step 4988, loss 0.0551281, acc 1, learning_rate 0.0001
2017-10-10T15:14:55.262035: step 4989, loss 0.0789876, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:55.563517: step 4990, loss 0.184382, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:55.824657: step 4991, loss 0.075271, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:56.101791: step 4992, loss 0.227272, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:56.388808: step 4993, loss 0.071192, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:56.622298: step 4994, loss 0.181177, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:56.925644: step 4995, loss 0.160078, acc 0.921875, learning_rate 0.0001
2017-10-10T15:14:57.241437: step 4996, loss 0.143035, acc 0.96875, learning_rate 0.0001
2017-10-10T15:14:57.490824: step 4997, loss 0.0618145, acc 0.984375, learning_rate 0.0001
2017-10-10T15:14:57.766295: step 4998, loss 0.100668, acc 0.960784, learning_rate 0.0001
2017-10-10T15:14:58.045014: step 4999, loss 0.189844, acc 0.9375, learning_rate 0.0001
2017-10-10T15:14:58.318919: step 5000, loss 0.0660485, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:14:58.785819: step 5000, loss 0.216811, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5000

2017-10-10T15:14:59.884572: step 5001, loss 0.12371, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:00.184380: step 5002, loss 0.13431, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:00.488953: step 5003, loss 0.0594036, acc 1, learning_rate 0.0001
2017-10-10T15:15:00.750930: step 5004, loss 0.132558, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:01.063562: step 5005, loss 0.0764895, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:01.356995: step 5006, loss 0.138199, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:01.583220: step 5007, loss 0.0577023, acc 1, learning_rate 0.0001
2017-10-10T15:15:01.878736: step 5008, loss 0.163349, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:02.194639: step 5009, loss 0.100724, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:02.498331: step 5010, loss 0.100411, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:02.777636: step 5011, loss 0.0508497, acc 1, learning_rate 0.0001
2017-10-10T15:15:03.092923: step 5012, loss 0.176954, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:03.351498: step 5013, loss 0.0824573, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:03.644515: step 5014, loss 0.102078, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:03.932811: step 5015, loss 0.0452568, acc 1, learning_rate 0.0001
2017-10-10T15:15:04.194471: step 5016, loss 0.0677078, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:04.442620: step 5017, loss 0.0911911, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:04.759001: step 5018, loss 0.137414, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:05.043095: step 5019, loss 0.09815, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:05.340994: step 5020, loss 0.0937707, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:05.635178: step 5021, loss 0.135214, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:05.958039: step 5022, loss 0.0592328, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:06.276002: step 5023, loss 0.0527129, acc 1, learning_rate 0.0001
2017-10-10T15:15:06.493811: step 5024, loss 0.110779, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:06.805976: step 5025, loss 0.162594, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:07.080966: step 5026, loss 0.099049, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:07.305848: step 5027, loss 0.124311, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:07.593028: step 5028, loss 0.107503, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:07.880711: step 5029, loss 0.125611, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:08.158455: step 5030, loss 0.0601041, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:08.431459: step 5031, loss 0.148376, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:08.748842: step 5032, loss 0.0403643, acc 1, learning_rate 0.0001
2017-10-10T15:15:09.076388: step 5033, loss 0.128778, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:09.313284: step 5034, loss 0.156463, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:09.627312: step 5035, loss 0.122927, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:09.873959: step 5036, loss 0.123588, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:10.128951: step 5037, loss 0.0655945, acc 1, learning_rate 0.0001
2017-10-10T15:15:10.400830: step 5038, loss 0.0824169, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:10.628222: step 5039, loss 0.11238, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:10.860949: step 5040, loss 0.0711751, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:11.308628: step 5040, loss 0.217033, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5040

2017-10-10T15:15:12.440845: step 5041, loss 0.150398, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:12.686335: step 5042, loss 0.136407, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:12.972792: step 5043, loss 0.101738, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:13.177104: step 5044, loss 0.116535, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:13.496494: step 5045, loss 0.063512, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:13.843384: step 5046, loss 0.117797, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:14.121038: step 5047, loss 0.14204, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:14.392274: step 5048, loss 0.0554478, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:14.653109: step 5049, loss 0.0711458, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:14.936865: step 5050, loss 0.0587679, acc 1, learning_rate 0.0001
2017-10-10T15:15:15.228569: step 5051, loss 0.0934869, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:15.529000: step 5052, loss 0.0691793, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:15.865083: step 5053, loss 0.135825, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:16.093912: step 5054, loss 0.0542068, acc 1, learning_rate 0.0001
2017-10-10T15:15:16.367892: step 5055, loss 0.0724761, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:16.718006: step 5056, loss 0.0647891, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:17.000050: step 5057, loss 0.160648, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:17.217074: step 5058, loss 0.0749654, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:17.523102: step 5059, loss 0.116742, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:17.824983: step 5060, loss 0.1342, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:18.076855: step 5061, loss 0.0805451, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:18.355619: step 5062, loss 0.0780474, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:18.611275: step 5063, loss 0.0855856, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:18.917972: step 5064, loss 0.129769, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:19.177004: step 5065, loss 0.0841699, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:19.519919: step 5066, loss 0.144838, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:19.799473: step 5067, loss 0.155328, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:20.038424: step 5068, loss 0.0934, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:20.266232: step 5069, loss 0.0889503, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:20.601157: step 5070, loss 0.141902, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:20.832124: step 5071, loss 0.109696, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:21.120894: step 5072, loss 0.0725906, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:21.440517: step 5073, loss 0.0718417, acc 1, learning_rate 0.0001
2017-10-10T15:15:21.761118: step 5074, loss 0.155118, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:22.044924: step 5075, loss 0.183522, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:22.328539: step 5076, loss 0.0959666, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:22.621224: step 5077, loss 0.170136, acc 0.890625, learning_rate 0.0001
2017-10-10T15:15:22.869255: step 5078, loss 0.0508556, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:23.132836: step 5079, loss 0.0517952, acc 1, learning_rate 0.0001
2017-10-10T15:15:23.423040: step 5080, loss 0.0862245, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:23.939813: step 5080, loss 0.216782, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5080

2017-10-10T15:15:25.072900: step 5081, loss 0.0871323, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:25.348874: step 5082, loss 0.0458292, acc 1, learning_rate 0.0001
2017-10-10T15:15:25.632873: step 5083, loss 0.0881599, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:25.972036: step 5084, loss 0.104385, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:26.220820: step 5085, loss 0.106079, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:26.433211: step 5086, loss 0.101812, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:26.694434: step 5087, loss 0.147964, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:26.925412: step 5088, loss 0.0691458, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:27.163123: step 5089, loss 0.0782301, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:27.481024: step 5090, loss 0.186194, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:27.760812: step 5091, loss 0.139497, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:28.031389: step 5092, loss 0.0880504, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:28.320044: step 5093, loss 0.106903, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:28.528884: step 5094, loss 0.0674844, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:28.756246: step 5095, loss 0.150879, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:28.989131: step 5096, loss 0.108018, acc 0.960784, learning_rate 0.0001
2017-10-10T15:15:29.304889: step 5097, loss 0.0725569, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:29.534817: step 5098, loss 0.145099, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:29.822167: step 5099, loss 0.0811548, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:30.024867: step 5100, loss 0.0778176, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:30.316843: step 5101, loss 0.0667806, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:30.605714: step 5102, loss 0.0699801, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:30.890904: step 5103, loss 0.178878, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:31.198963: step 5104, loss 0.0672903, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:31.461129: step 5105, loss 0.0951875, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:31.696616: step 5106, loss 0.174291, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:32.002768: step 5107, loss 0.124526, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:32.308910: step 5108, loss 0.0733846, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:32.689045: step 5109, loss 0.0762192, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:32.956996: step 5110, loss 0.11057, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:33.223758: step 5111, loss 0.276015, acc 0.90625, learning_rate 0.0001
2017-10-10T15:15:33.461612: step 5112, loss 0.0583824, acc 1, learning_rate 0.0001
2017-10-10T15:15:33.732855: step 5113, loss 0.171025, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:34.014802: step 5114, loss 0.110473, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:34.262629: step 5115, loss 0.0733968, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:34.534020: step 5116, loss 0.069576, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:34.816835: step 5117, loss 0.117464, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:35.069527: step 5118, loss 0.111056, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:35.337759: step 5119, loss 0.128543, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:35.716625: step 5120, loss 0.100287, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:36.143057: step 5120, loss 0.215617, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5120

2017-10-10T15:15:37.140838: step 5121, loss 0.0499802, acc 1, learning_rate 0.0001
2017-10-10T15:15:37.436956: step 5122, loss 0.126111, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:37.686502: step 5123, loss 0.069763, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:37.940166: step 5124, loss 0.0898894, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:38.268915: step 5125, loss 0.12982, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:38.512780: step 5126, loss 0.191609, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:38.804201: step 5127, loss 0.0583815, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:39.116820: step 5128, loss 0.0594459, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:39.383165: step 5129, loss 0.073746, acc 1, learning_rate 0.0001
2017-10-10T15:15:39.661220: step 5130, loss 0.0675639, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:39.964570: step 5131, loss 0.0579988, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:40.261247: step 5132, loss 0.237926, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:40.560071: step 5133, loss 0.0551563, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:40.808824: step 5134, loss 0.133402, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:41.057076: step 5135, loss 0.138829, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:41.356999: step 5136, loss 0.153992, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:41.677130: step 5137, loss 0.0722785, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:41.952983: step 5138, loss 0.235566, acc 0.90625, learning_rate 0.0001
2017-10-10T15:15:42.289276: step 5139, loss 0.116147, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:42.570936: step 5140, loss 0.0240882, acc 1, learning_rate 0.0001
2017-10-10T15:15:42.765568: step 5141, loss 0.0719478, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:42.998776: step 5142, loss 0.0514192, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:43.251981: step 5143, loss 0.112977, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:43.468889: step 5144, loss 0.12774, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:43.779374: step 5145, loss 0.0591334, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:44.069118: step 5146, loss 0.096487, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:44.327163: step 5147, loss 0.115148, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:44.618840: step 5148, loss 0.074967, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:44.870449: step 5149, loss 0.0545666, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:45.130907: step 5150, loss 0.0776111, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:45.389059: step 5151, loss 0.0382753, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:45.737742: step 5152, loss 0.0862437, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:46.105213: step 5153, loss 0.117277, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:46.413036: step 5154, loss 0.0546113, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:46.651979: step 5155, loss 0.118672, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:46.856542: step 5156, loss 0.0732094, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:47.040689: step 5157, loss 0.0836939, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:47.287253: step 5158, loss 0.0680077, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:47.562747: step 5159, loss 0.0373839, acc 1, learning_rate 0.0001
2017-10-10T15:15:47.791192: step 5160, loss 0.105243, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:15:48.279249: step 5160, loss 0.217421, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5160

2017-10-10T15:15:49.360147: step 5161, loss 0.135919, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:49.607356: step 5162, loss 0.119343, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:49.868481: step 5163, loss 0.0263437, acc 1, learning_rate 0.0001
2017-10-10T15:15:50.153080: step 5164, loss 0.0549724, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:50.419984: step 5165, loss 0.0995556, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:50.727258: step 5166, loss 0.0407939, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:51.040945: step 5167, loss 0.0643863, acc 1, learning_rate 0.0001
2017-10-10T15:15:51.291040: step 5168, loss 0.0743249, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:51.502072: step 5169, loss 0.151033, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:51.750870: step 5170, loss 0.194721, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:51.993045: step 5171, loss 0.121217, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:52.256004: step 5172, loss 0.0688488, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:52.568989: step 5173, loss 0.231708, acc 0.890625, learning_rate 0.0001
2017-10-10T15:15:52.828185: step 5174, loss 0.102057, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:53.120539: step 5175, loss 0.130637, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:53.416998: step 5176, loss 0.0576275, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:53.682740: step 5177, loss 0.0946279, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:53.936894: step 5178, loss 0.109674, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:54.221120: step 5179, loss 0.109188, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:54.460114: step 5180, loss 0.109619, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:54.760468: step 5181, loss 0.142239, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:55.025721: step 5182, loss 0.0852811, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:55.298270: step 5183, loss 0.103358, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:55.613141: step 5184, loss 0.0871749, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:55.866571: step 5185, loss 0.0709248, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:56.136946: step 5186, loss 0.104168, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:56.390560: step 5187, loss 0.0979298, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:56.680826: step 5188, loss 0.0851688, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:56.914671: step 5189, loss 0.0804596, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:57.173989: step 5190, loss 0.120722, acc 0.9375, learning_rate 0.0001
2017-10-10T15:15:57.516962: step 5191, loss 0.111701, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:57.777719: step 5192, loss 0.15326, acc 0.921875, learning_rate 0.0001
2017-10-10T15:15:58.096943: step 5193, loss 0.102368, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:58.325309: step 5194, loss 0.122321, acc 0.941176, learning_rate 0.0001
2017-10-10T15:15:58.667847: step 5195, loss 0.114457, acc 0.984375, learning_rate 0.0001
2017-10-10T15:15:58.967092: step 5196, loss 0.139031, acc 0.953125, learning_rate 0.0001
2017-10-10T15:15:59.261551: step 5197, loss 0.0773172, acc 1, learning_rate 0.0001
2017-10-10T15:15:59.450535: step 5198, loss 0.0737711, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:59.674512: step 5199, loss 0.10557, acc 0.96875, learning_rate 0.0001
2017-10-10T15:15:59.904850: step 5200, loss 0.0843718, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:00.406568: step 5200, loss 0.218075, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5200

2017-10-10T15:16:01.496104: step 5201, loss 0.0560703, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:01.683892: step 5202, loss 0.108442, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:01.952857: step 5203, loss 0.0687731, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:02.252961: step 5204, loss 0.161104, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:02.516196: step 5205, loss 0.076052, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:02.760728: step 5206, loss 0.131755, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:03.028933: step 5207, loss 0.0840345, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:03.267213: step 5208, loss 0.0749238, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:03.564907: step 5209, loss 0.134709, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:03.872807: step 5210, loss 0.120906, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:04.276817: step 5211, loss 0.0864296, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:04.584737: step 5212, loss 0.0762855, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:04.827461: step 5213, loss 0.146744, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:05.046510: step 5214, loss 0.170567, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:05.262611: step 5215, loss 0.127056, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:05.528685: step 5216, loss 0.147378, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:05.804919: step 5217, loss 0.0998703, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:06.068903: step 5218, loss 0.168578, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:06.335575: step 5219, loss 0.0916017, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:06.674058: step 5220, loss 0.0928684, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:06.917532: step 5221, loss 0.134891, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:07.164657: step 5222, loss 0.056197, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:07.465796: step 5223, loss 0.0930912, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:07.718053: step 5224, loss 0.125599, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:07.973063: step 5225, loss 0.157152, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:08.216878: step 5226, loss 0.148398, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:08.471893: step 5227, loss 0.117186, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:08.745190: step 5228, loss 0.0334904, acc 1, learning_rate 0.0001
2017-10-10T15:16:09.029083: step 5229, loss 0.142148, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:09.286256: step 5230, loss 0.0500103, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:09.545085: step 5231, loss 0.0633124, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:09.776845: step 5232, loss 0.0721779, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:10.088851: step 5233, loss 0.044499, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:10.338912: step 5234, loss 0.113133, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:10.575496: step 5235, loss 0.13463, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:10.888472: step 5236, loss 0.0977863, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:11.131332: step 5237, loss 0.0515095, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:11.363775: step 5238, loss 0.0854755, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:11.685653: step 5239, loss 0.0498674, acc 1, learning_rate 0.0001
2017-10-10T15:16:11.945174: step 5240, loss 0.0913244, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:12.372879: step 5240, loss 0.216578, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5240

2017-10-10T15:16:13.474831: step 5241, loss 0.0676171, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:13.739544: step 5242, loss 0.117763, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:14.028207: step 5243, loss 0.068684, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:14.284344: step 5244, loss 0.0773462, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:14.560825: step 5245, loss 0.109659, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:14.856334: step 5246, loss 0.109444, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:15.156811: step 5247, loss 0.108545, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:15.464599: step 5248, loss 0.0668697, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:15.705177: step 5249, loss 0.0516211, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:15.937055: step 5250, loss 0.0970784, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:16.177712: step 5251, loss 0.196237, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:16.460814: step 5252, loss 0.0817017, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:16.703905: step 5253, loss 0.0561352, acc 1, learning_rate 0.0001
2017-10-10T15:16:16.979248: step 5254, loss 0.0882226, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:17.236880: step 5255, loss 0.132529, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:17.550676: step 5256, loss 0.176831, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:17.816433: step 5257, loss 0.0602226, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:18.191519: step 5258, loss 0.0234603, acc 1, learning_rate 0.0001
2017-10-10T15:16:18.463648: step 5259, loss 0.073547, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:18.801684: step 5260, loss 0.116608, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:19.096961: step 5261, loss 0.0587059, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:19.347570: step 5262, loss 0.0677665, acc 1, learning_rate 0.0001
2017-10-10T15:16:19.637215: step 5263, loss 0.136735, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:19.903851: step 5264, loss 0.0905297, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:20.153276: step 5265, loss 0.0488485, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:20.439510: step 5266, loss 0.0838987, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:20.704227: step 5267, loss 0.0321746, acc 1, learning_rate 0.0001
2017-10-10T15:16:20.947079: step 5268, loss 0.10458, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:21.208818: step 5269, loss 0.112061, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:21.491694: step 5270, loss 0.227055, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:21.753149: step 5271, loss 0.111994, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:22.048499: step 5272, loss 0.15683, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:22.300092: step 5273, loss 0.109826, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:22.688940: step 5274, loss 0.0864947, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:22.996577: step 5275, loss 0.111241, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:23.269562: step 5276, loss 0.110697, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:23.548878: step 5277, loss 0.150576, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:23.800860: step 5278, loss 0.126757, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:24.096855: step 5279, loss 0.105118, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:24.418298: step 5280, loss 0.120959, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:24.842140: step 5280, loss 0.220012, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5280

2017-10-10T15:16:25.769011: step 5281, loss 0.0748005, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:26.088593: step 5282, loss 0.0738478, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:26.348866: step 5283, loss 0.0591839, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:26.580989: step 5284, loss 0.113972, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:26.872661: step 5285, loss 0.110674, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:27.141521: step 5286, loss 0.0939577, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:27.474379: step 5287, loss 0.122542, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:27.764908: step 5288, loss 0.0634487, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:28.109005: step 5289, loss 0.0735664, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:28.367663: step 5290, loss 0.0803342, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:28.666905: step 5291, loss 0.0621564, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:28.920835: step 5292, loss 0.0895126, acc 0.980392, learning_rate 0.0001
2017-10-10T15:16:29.186416: step 5293, loss 0.253915, acc 0.890625, learning_rate 0.0001
2017-10-10T15:16:29.521671: step 5294, loss 0.0435502, acc 1, learning_rate 0.0001
2017-10-10T15:16:29.783329: step 5295, loss 0.0926622, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:30.001407: step 5296, loss 0.0325522, acc 1, learning_rate 0.0001
2017-10-10T15:16:30.293198: step 5297, loss 0.0556891, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:30.572924: step 5298, loss 0.0962582, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:30.858852: step 5299, loss 0.0572949, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:31.129895: step 5300, loss 0.0831976, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:31.382332: step 5301, loss 0.0460786, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:31.679428: step 5302, loss 0.0902249, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:31.972905: step 5303, loss 0.167179, acc 0.90625, learning_rate 0.0001
2017-10-10T15:16:32.337272: step 5304, loss 0.0899385, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:32.592821: step 5305, loss 0.0901633, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:32.798974: step 5306, loss 0.122487, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:33.048854: step 5307, loss 0.08187, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:33.319269: step 5308, loss 0.0975195, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:33.605889: step 5309, loss 0.0884819, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:33.890891: step 5310, loss 0.108477, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:34.162311: step 5311, loss 0.0598917, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:34.480817: step 5312, loss 0.0651018, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:34.743086: step 5313, loss 0.0949255, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:35.005135: step 5314, loss 0.0593301, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:35.353085: step 5315, loss 0.0258605, acc 1, learning_rate 0.0001
2017-10-10T15:16:35.653780: step 5316, loss 0.0334605, acc 1, learning_rate 0.0001
2017-10-10T15:16:35.908845: step 5317, loss 0.119743, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:36.160592: step 5318, loss 0.0667406, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:36.390416: step 5319, loss 0.152329, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:36.633549: step 5320, loss 0.0610638, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:37.003012: step 5320, loss 0.220014, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5320

2017-10-10T15:16:38.083166: step 5321, loss 0.101464, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:38.357182: step 5322, loss 0.155378, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:38.681154: step 5323, loss 0.150885, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:38.980023: step 5324, loss 0.0521832, acc 1, learning_rate 0.0001
2017-10-10T15:16:39.289077: step 5325, loss 0.111338, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:39.571862: step 5326, loss 0.0595759, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:39.848863: step 5327, loss 0.20847, acc 0.921875, learning_rate 0.0001
2017-10-10T15:16:40.097015: step 5328, loss 0.0420958, acc 1, learning_rate 0.0001
2017-10-10T15:16:40.411061: step 5329, loss 0.142553, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:40.720846: step 5330, loss 0.0750339, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:40.973501: step 5331, loss 0.0433761, acc 1, learning_rate 0.0001
2017-10-10T15:16:41.225891: step 5332, loss 0.0676623, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:41.512141: step 5333, loss 0.087881, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:41.762329: step 5334, loss 0.0759632, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:41.984478: step 5335, loss 0.174474, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:42.386445: step 5336, loss 0.126589, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:42.579457: step 5337, loss 0.135155, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:42.872915: step 5338, loss 0.122074, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:43.116112: step 5339, loss 0.0800109, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:43.366389: step 5340, loss 0.106121, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:43.652903: step 5341, loss 0.0770491, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:43.866908: step 5342, loss 0.139835, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:44.154727: step 5343, loss 0.0908086, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:44.471813: step 5344, loss 0.0967844, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:44.768869: step 5345, loss 0.118502, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:45.128896: step 5346, loss 0.185712, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:45.343084: step 5347, loss 0.0712168, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:45.648904: step 5348, loss 0.0550828, acc 1, learning_rate 0.0001
2017-10-10T15:16:45.972610: step 5349, loss 0.0609864, acc 1, learning_rate 0.0001
2017-10-10T15:16:46.154391: step 5350, loss 0.0875407, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:46.457008: step 5351, loss 0.166625, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:46.763629: step 5352, loss 0.190804, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:47.028905: step 5353, loss 0.0455354, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:47.280904: step 5354, loss 0.126101, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:47.520193: step 5355, loss 0.0896295, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:47.754864: step 5356, loss 0.157391, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:48.048568: step 5357, loss 0.136084, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:48.312906: step 5358, loss 0.115851, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:48.588351: step 5359, loss 0.0831825, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:48.830174: step 5360, loss 0.24516, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T15:16:49.226295: step 5360, loss 0.217404, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5360

2017-10-10T15:16:50.259922: step 5361, loss 0.0462641, acc 1, learning_rate 0.0001
2017-10-10T15:16:50.560872: step 5362, loss 0.0435162, acc 1, learning_rate 0.0001
2017-10-10T15:16:50.879958: step 5363, loss 0.125958, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:51.164461: step 5364, loss 0.0985346, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:51.464679: step 5365, loss 0.0705772, acc 1, learning_rate 0.0001
2017-10-10T15:16:51.702240: step 5366, loss 0.06898, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:52.008862: step 5367, loss 0.0950493, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:52.301078: step 5368, loss 0.139195, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:52.604884: step 5369, loss 0.0850949, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:52.890213: step 5370, loss 0.0720297, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:53.176394: step 5371, loss 0.0209352, acc 1, learning_rate 0.0001
2017-10-10T15:16:53.449316: step 5372, loss 0.103954, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:53.633983: step 5373, loss 0.164449, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:53.976184: step 5374, loss 0.194428, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:54.252934: step 5375, loss 0.0868565, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:54.500879: step 5376, loss 0.0759877, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:54.793818: step 5377, loss 0.104223, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:55.060886: step 5378, loss 0.0653749, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:55.344854: step 5379, loss 0.075035, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:55.640905: step 5380, loss 0.149492, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:55.928853: step 5381, loss 0.148623, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:56.212007: step 5382, loss 0.135875, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:56.518047: step 5383, loss 0.165183, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:56.774203: step 5384, loss 0.109748, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:57.039214: step 5385, loss 0.0683159, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:57.337614: step 5386, loss 0.0768625, acc 1, learning_rate 0.0001
2017-10-10T15:16:57.654717: step 5387, loss 0.11413, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:57.888882: step 5388, loss 0.0583929, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:58.196001: step 5389, loss 0.100677, acc 0.96875, learning_rate 0.0001
2017-10-10T15:16:58.374632: step 5390, loss 0.0856223, acc 0.960784, learning_rate 0.0001
2017-10-10T15:16:58.569391: step 5391, loss 0.0474644, acc 1, learning_rate 0.0001
2017-10-10T15:16:58.836818: step 5392, loss 0.19184, acc 0.9375, learning_rate 0.0001
2017-10-10T15:16:59.144369: step 5393, loss 0.0888117, acc 0.984375, learning_rate 0.0001
2017-10-10T15:16:59.381057: step 5394, loss 0.093407, acc 1, learning_rate 0.0001
2017-10-10T15:16:59.674413: step 5395, loss 0.18359, acc 0.953125, learning_rate 0.0001
2017-10-10T15:16:59.929236: step 5396, loss 0.0692538, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:00.262863: step 5397, loss 0.0576301, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:00.520845: step 5398, loss 0.126772, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:00.759272: step 5399, loss 0.164658, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:01.007064: step 5400, loss 0.115318, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:01.406822: step 5400, loss 0.21843, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5400

2017-10-10T15:17:02.453681: step 5401, loss 0.0245182, acc 1, learning_rate 0.0001
2017-10-10T15:17:02.737407: step 5402, loss 0.0751763, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:02.957481: step 5403, loss 0.125538, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:03.232889: step 5404, loss 0.134922, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:03.489546: step 5405, loss 0.107001, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:03.749024: step 5406, loss 0.0872321, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:04.027104: step 5407, loss 0.0656468, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:04.280962: step 5408, loss 0.105567, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:04.640208: step 5409, loss 0.117102, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:04.885352: step 5410, loss 0.111442, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:05.160388: step 5411, loss 0.225102, acc 0.90625, learning_rate 0.0001
2017-10-10T15:17:05.408924: step 5412, loss 0.0746874, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:05.712218: step 5413, loss 0.170015, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:06.037119: step 5414, loss 0.182561, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:06.234298: step 5415, loss 0.0915796, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:06.480254: step 5416, loss 0.0717308, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:06.729459: step 5417, loss 0.0493775, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:06.877256: step 5418, loss 0.103015, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:07.098731: step 5419, loss 0.0887623, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:07.392260: step 5420, loss 0.0302506, acc 1, learning_rate 0.0001
2017-10-10T15:17:07.608837: step 5421, loss 0.0377294, acc 1, learning_rate 0.0001
2017-10-10T15:17:07.897801: step 5422, loss 0.102969, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:08.155636: step 5423, loss 0.12934, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:08.462734: step 5424, loss 0.092316, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:08.812888: step 5425, loss 0.188023, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:09.116397: step 5426, loss 0.0618242, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:09.338809: step 5427, loss 0.0771006, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:09.620169: step 5428, loss 0.179101, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:09.940228: step 5429, loss 0.0602614, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:10.163532: step 5430, loss 0.0626971, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:10.436955: step 5431, loss 0.09582, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:10.706924: step 5432, loss 0.0797135, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:10.990863: step 5433, loss 0.125234, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:11.262651: step 5434, loss 0.0610366, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:11.565463: step 5435, loss 0.072537, acc 1, learning_rate 0.0001
2017-10-10T15:17:11.799443: step 5436, loss 0.119978, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:12.078989: step 5437, loss 0.0630405, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:12.388515: step 5438, loss 0.0789709, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:12.634082: step 5439, loss 0.0904736, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:12.919554: step 5440, loss 0.165178, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:13.346976: step 5440, loss 0.217324, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5440

2017-10-10T15:17:14.257299: step 5441, loss 0.0475156, acc 1, learning_rate 0.0001
2017-10-10T15:17:14.537722: step 5442, loss 0.0987018, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:14.836618: step 5443, loss 0.125244, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:15.076459: step 5444, loss 0.0645937, acc 1, learning_rate 0.0001
2017-10-10T15:17:15.357827: step 5445, loss 0.105599, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:15.690873: step 5446, loss 0.0770748, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:15.960575: step 5447, loss 0.0441495, acc 1, learning_rate 0.0001
2017-10-10T15:17:16.261411: step 5448, loss 0.0557645, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:16.515648: step 5449, loss 0.0429558, acc 1, learning_rate 0.0001
2017-10-10T15:17:16.833062: step 5450, loss 0.0924935, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:17.124929: step 5451, loss 0.0671063, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:17.423980: step 5452, loss 0.0852692, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:17.717009: step 5453, loss 0.057177, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:17.964720: step 5454, loss 0.095485, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:18.260873: step 5455, loss 0.164806, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:18.593154: step 5456, loss 0.0714218, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:18.828221: step 5457, loss 0.254786, acc 0.859375, learning_rate 0.0001
2017-10-10T15:17:19.051684: step 5458, loss 0.105472, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:19.335723: step 5459, loss 0.0593635, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:19.596327: step 5460, loss 0.127482, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:19.826044: step 5461, loss 0.105603, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:20.089366: step 5462, loss 0.082449, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:20.338839: step 5463, loss 0.0823437, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:20.640820: step 5464, loss 0.0293675, acc 1, learning_rate 0.0001
2017-10-10T15:17:20.935350: step 5465, loss 0.0874285, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:21.223282: step 5466, loss 0.0791778, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:21.507454: step 5467, loss 0.0537154, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:21.788874: step 5468, loss 0.12903, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:22.112844: step 5469, loss 0.0923508, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:22.383719: step 5470, loss 0.12589, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:22.639772: step 5471, loss 0.115737, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:22.865020: step 5472, loss 0.112722, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:23.092968: step 5473, loss 0.150229, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:23.351921: step 5474, loss 0.128278, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:23.628927: step 5475, loss 0.10085, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:23.914631: step 5476, loss 0.075134, acc 1, learning_rate 0.0001
2017-10-10T15:17:24.217245: step 5477, loss 0.0827748, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:24.529415: step 5478, loss 0.044863, acc 1, learning_rate 0.0001
2017-10-10T15:17:24.796591: step 5479, loss 0.104667, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:25.100962: step 5480, loss 0.0799613, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:25.624441: step 5480, loss 0.216074, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5480

2017-10-10T15:17:26.760864: step 5481, loss 0.0675705, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:27.067044: step 5482, loss 0.0588681, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:27.366304: step 5483, loss 0.114727, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:27.630704: step 5484, loss 0.0898578, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:27.924843: step 5485, loss 0.0688629, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:28.200879: step 5486, loss 0.0438373, acc 1, learning_rate 0.0001
2017-10-10T15:17:28.412119: step 5487, loss 0.1475, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:28.684878: step 5488, loss 0.0926393, acc 0.980392, learning_rate 0.0001
2017-10-10T15:17:28.925152: step 5489, loss 0.0436801, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:29.217985: step 5490, loss 0.163577, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:29.477449: step 5491, loss 0.13991, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:29.753363: step 5492, loss 0.0289815, acc 1, learning_rate 0.0001
2017-10-10T15:17:30.005619: step 5493, loss 0.150316, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:30.266541: step 5494, loss 0.158622, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:30.533149: step 5495, loss 0.0895129, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:30.818408: step 5496, loss 0.162652, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:31.109524: step 5497, loss 0.0805413, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:31.331622: step 5498, loss 0.0969418, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:31.663079: step 5499, loss 0.0575093, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:31.889241: step 5500, loss 0.156241, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:32.086659: step 5501, loss 0.170464, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:32.333048: step 5502, loss 0.0281849, acc 1, learning_rate 0.0001
2017-10-10T15:17:32.673135: step 5503, loss 0.0956256, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:32.983008: step 5504, loss 0.232374, acc 0.890625, learning_rate 0.0001
2017-10-10T15:17:33.201142: step 5505, loss 0.0493339, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:33.440813: step 5506, loss 0.192557, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:33.735192: step 5507, loss 0.0800504, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:34.077372: step 5508, loss 0.0641026, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:34.337062: step 5509, loss 0.115987, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:34.556884: step 5510, loss 0.0967764, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:34.845174: step 5511, loss 0.201026, acc 0.90625, learning_rate 0.0001
2017-10-10T15:17:35.102784: step 5512, loss 0.122295, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:35.377580: step 5513, loss 0.0548099, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:35.664733: step 5514, loss 0.0622985, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:35.960869: step 5515, loss 0.133672, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:36.201881: step 5516, loss 0.0762568, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:36.497051: step 5517, loss 0.0763771, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:36.777047: step 5518, loss 0.0208673, acc 1, learning_rate 0.0001
2017-10-10T15:17:37.110730: step 5519, loss 0.138105, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:37.418478: step 5520, loss 0.101677, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:37.821684: step 5520, loss 0.214575, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5520

2017-10-10T15:17:38.976946: step 5521, loss 0.0772242, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:39.216863: step 5522, loss 0.0943884, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:39.480995: step 5523, loss 0.0523402, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:39.704171: step 5524, loss 0.0730725, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:39.924921: step 5525, loss 0.0698265, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:40.188734: step 5526, loss 0.0666086, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:40.440861: step 5527, loss 0.126375, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:40.672049: step 5528, loss 0.0919551, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:40.941634: step 5529, loss 0.119699, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:41.224577: step 5530, loss 0.0495463, acc 1, learning_rate 0.0001
2017-10-10T15:17:41.478569: step 5531, loss 0.0720942, acc 1, learning_rate 0.0001
2017-10-10T15:17:41.784924: step 5532, loss 0.0611738, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:42.065026: step 5533, loss 0.076745, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:42.307038: step 5534, loss 0.0222159, acc 1, learning_rate 0.0001
2017-10-10T15:17:42.590066: step 5535, loss 0.105598, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:42.816813: step 5536, loss 0.0631008, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:43.153119: step 5537, loss 0.0682813, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:43.460833: step 5538, loss 0.0758202, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:43.715663: step 5539, loss 0.103888, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:43.985535: step 5540, loss 0.0836265, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:44.266506: step 5541, loss 0.0396016, acc 1, learning_rate 0.0001
2017-10-10T15:17:44.495505: step 5542, loss 0.0702402, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:44.782603: step 5543, loss 0.0825776, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:45.088962: step 5544, loss 0.0854855, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:45.418023: step 5545, loss 0.092837, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:45.683349: step 5546, loss 0.160075, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:45.960789: step 5547, loss 0.107049, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:46.184024: step 5548, loss 0.0496913, acc 1, learning_rate 0.0001
2017-10-10T15:17:46.414042: step 5549, loss 0.103588, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:46.632946: step 5550, loss 0.0772128, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:46.888285: step 5551, loss 0.143998, acc 0.90625, learning_rate 0.0001
2017-10-10T15:17:47.136944: step 5552, loss 0.17611, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:47.409388: step 5553, loss 0.0848248, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:47.717093: step 5554, loss 0.0543726, acc 1, learning_rate 0.0001
2017-10-10T15:17:48.029818: step 5555, loss 0.0821963, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:48.328947: step 5556, loss 0.0738134, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:48.609024: step 5557, loss 0.0506965, acc 1, learning_rate 0.0001
2017-10-10T15:17:48.913168: step 5558, loss 0.0731944, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:49.264566: step 5559, loss 0.159268, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:49.527312: step 5560, loss 0.0505829, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:17:50.100948: step 5560, loss 0.214901, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5560

2017-10-10T15:17:51.223572: step 5561, loss 0.0710439, acc 1, learning_rate 0.0001
2017-10-10T15:17:51.506467: step 5562, loss 0.172053, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:51.811575: step 5563, loss 0.120529, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:52.106055: step 5564, loss 0.122314, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:52.317017: step 5565, loss 0.0711654, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:52.624985: step 5566, loss 0.0834895, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:52.925013: step 5567, loss 0.144811, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:53.170906: step 5568, loss 0.0769686, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:53.454171: step 5569, loss 0.0717275, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:53.786328: step 5570, loss 0.0832824, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:53.990619: step 5571, loss 0.121972, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:54.256883: step 5572, loss 0.093718, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:54.456861: step 5573, loss 0.0807929, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:54.797531: step 5574, loss 0.100983, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:55.150654: step 5575, loss 0.0775043, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:55.505985: step 5576, loss 0.0776349, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:55.701799: step 5577, loss 0.159007, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:55.908573: step 5578, loss 0.160154, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:56.130098: step 5579, loss 0.135717, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:56.314619: step 5580, loss 0.062548, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:56.544890: step 5581, loss 0.0615597, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:56.780832: step 5582, loss 0.0612027, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:57.118883: step 5583, loss 0.0443021, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:57.301145: step 5584, loss 0.0875535, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:57.563895: step 5585, loss 0.123772, acc 0.953125, learning_rate 0.0001
2017-10-10T15:17:57.793688: step 5586, loss 0.0938793, acc 0.960784, learning_rate 0.0001
2017-10-10T15:17:58.066916: step 5587, loss 0.0875827, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:58.366077: step 5588, loss 0.218051, acc 0.921875, learning_rate 0.0001
2017-10-10T15:17:58.700860: step 5589, loss 0.0562737, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:58.960744: step 5590, loss 0.0573525, acc 1, learning_rate 0.0001
2017-10-10T15:17:59.239597: step 5591, loss 0.104735, acc 0.984375, learning_rate 0.0001
2017-10-10T15:17:59.557256: step 5592, loss 0.0661248, acc 0.96875, learning_rate 0.0001
2017-10-10T15:17:59.740268: step 5593, loss 0.196585, acc 0.9375, learning_rate 0.0001
2017-10-10T15:17:59.996850: step 5594, loss 0.0849511, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:00.272981: step 5595, loss 0.121036, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:00.556255: step 5596, loss 0.119955, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:00.863242: step 5597, loss 0.0372591, acc 1, learning_rate 0.0001
2017-10-10T15:18:01.174201: step 5598, loss 0.131291, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:01.473225: step 5599, loss 0.0830197, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:01.744874: step 5600, loss 0.189459, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:02.272626: step 5600, loss 0.215299, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5600

2017-10-10T15:18:03.257065: step 5601, loss 0.0704715, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:03.604288: step 5602, loss 0.130314, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:03.861015: step 5603, loss 0.0972577, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:04.136205: step 5604, loss 0.0272212, acc 1, learning_rate 0.0001
2017-10-10T15:18:04.425179: step 5605, loss 0.122967, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:04.762907: step 5606, loss 0.153258, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:04.998886: step 5607, loss 0.0694937, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:05.263217: step 5608, loss 0.115253, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:05.524553: step 5609, loss 0.0934601, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:05.836958: step 5610, loss 0.0796146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:06.151865: step 5611, loss 0.147395, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:06.419019: step 5612, loss 0.0519829, acc 1, learning_rate 0.0001
2017-10-10T15:18:06.742232: step 5613, loss 0.178761, acc 0.90625, learning_rate 0.0001
2017-10-10T15:18:06.980874: step 5614, loss 0.0634399, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:07.252968: step 5615, loss 0.0231938, acc 1, learning_rate 0.0001
2017-10-10T15:18:07.497059: step 5616, loss 0.0533061, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:07.759329: step 5617, loss 0.162882, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:07.979309: step 5618, loss 0.0945697, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:08.259054: step 5619, loss 0.200374, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:08.565173: step 5620, loss 0.102302, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:08.884989: step 5621, loss 0.0238905, acc 1, learning_rate 0.0001
2017-10-10T15:18:09.124436: step 5622, loss 0.0568922, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:09.391720: step 5623, loss 0.0950173, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:09.682631: step 5624, loss 0.128126, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:09.860840: step 5625, loss 0.106459, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:10.165085: step 5626, loss 0.171288, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:10.421030: step 5627, loss 0.0484403, acc 1, learning_rate 0.0001
2017-10-10T15:18:10.684543: step 5628, loss 0.114289, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:10.956852: step 5629, loss 0.189093, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:11.262739: step 5630, loss 0.0374931, acc 1, learning_rate 0.0001
2017-10-10T15:18:11.580958: step 5631, loss 0.0962107, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:11.859613: step 5632, loss 0.104027, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:12.201053: step 5633, loss 0.0387052, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:12.540899: step 5634, loss 0.0897894, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:12.867653: step 5635, loss 0.118931, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:13.125077: step 5636, loss 0.0436935, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:13.454709: step 5637, loss 0.0828643, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:13.756967: step 5638, loss 0.0827248, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:13.972841: step 5639, loss 0.038483, acc 1, learning_rate 0.0001
2017-10-10T15:18:14.212939: step 5640, loss 0.209026, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:14.667951: step 5640, loss 0.215936, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5640

2017-10-10T15:18:15.765168: step 5641, loss 0.146222, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:15.985147: step 5642, loss 0.0492932, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:16.259333: step 5643, loss 0.081615, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:16.513175: step 5644, loss 0.119977, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:16.724673: step 5645, loss 0.0977651, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:16.998249: step 5646, loss 0.169122, acc 0.90625, learning_rate 0.0001
2017-10-10T15:18:17.313917: step 5647, loss 0.175763, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:17.574224: step 5648, loss 0.157119, acc 0.90625, learning_rate 0.0001
2017-10-10T15:18:17.840885: step 5649, loss 0.0542985, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:18.091193: step 5650, loss 0.104524, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:18.396800: step 5651, loss 0.0267458, acc 1, learning_rate 0.0001
2017-10-10T15:18:18.721096: step 5652, loss 0.127943, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:18.902655: step 5653, loss 0.164747, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:19.169781: step 5654, loss 0.118682, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:19.440833: step 5655, loss 0.100875, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:19.712866: step 5656, loss 0.145979, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:19.988951: step 5657, loss 0.0613172, acc 1, learning_rate 0.0001
2017-10-10T15:18:20.265009: step 5658, loss 0.0989136, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:20.560816: step 5659, loss 0.138572, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:20.891728: step 5660, loss 0.108101, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:21.169030: step 5661, loss 0.0282968, acc 1, learning_rate 0.0001
2017-10-10T15:18:21.468774: step 5662, loss 0.0504078, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:21.772918: step 5663, loss 0.126052, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:22.033194: step 5664, loss 0.0708994, acc 1, learning_rate 0.0001
2017-10-10T15:18:22.297202: step 5665, loss 0.0843086, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:22.611953: step 5666, loss 0.148373, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:22.895801: step 5667, loss 0.0635505, acc 1, learning_rate 0.0001
2017-10-10T15:18:23.178120: step 5668, loss 0.0772192, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:23.504863: step 5669, loss 0.0828981, acc 1, learning_rate 0.0001
2017-10-10T15:18:23.807789: step 5670, loss 0.114687, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:24.112099: step 5671, loss 0.0961765, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:24.345049: step 5672, loss 0.133865, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:24.622542: step 5673, loss 0.113138, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:24.862853: step 5674, loss 0.0545894, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:25.151469: step 5675, loss 0.129059, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:25.453848: step 5676, loss 0.0941474, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:25.740881: step 5677, loss 0.0671845, acc 1, learning_rate 0.0001
2017-10-10T15:18:26.039358: step 5678, loss 0.0425099, acc 1, learning_rate 0.0001
2017-10-10T15:18:26.293780: step 5679, loss 0.0531189, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:26.472831: step 5680, loss 0.133291, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:26.986589: step 5680, loss 0.217231, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5680

2017-10-10T15:18:28.033069: step 5681, loss 0.0389766, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:28.328149: step 5682, loss 0.0394607, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:28.584879: step 5683, loss 0.0430943, acc 1, learning_rate 0.0001
2017-10-10T15:18:28.824888: step 5684, loss 0.0856043, acc 0.980392, learning_rate 0.0001
2017-10-10T15:18:29.121132: step 5685, loss 0.0882358, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:29.465796: step 5686, loss 0.0900785, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:29.753803: step 5687, loss 0.0666016, acc 1, learning_rate 0.0001
2017-10-10T15:18:30.013069: step 5688, loss 0.0858146, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:30.300994: step 5689, loss 0.112133, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:30.541402: step 5690, loss 0.138031, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:30.771381: step 5691, loss 0.0181309, acc 1, learning_rate 0.0001
2017-10-10T15:18:31.048086: step 5692, loss 0.118615, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:31.268946: step 5693, loss 0.0867698, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:31.530090: step 5694, loss 0.0376917, acc 1, learning_rate 0.0001
2017-10-10T15:18:31.812633: step 5695, loss 0.1581, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:32.123135: step 5696, loss 0.079748, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:32.361069: step 5697, loss 0.0994224, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:32.610563: step 5698, loss 0.184359, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:32.887975: step 5699, loss 0.128412, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:33.133217: step 5700, loss 0.0774362, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:33.441551: step 5701, loss 0.10615, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:33.733638: step 5702, loss 0.0954403, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:33.926247: step 5703, loss 0.17924, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:34.250926: step 5704, loss 0.0360961, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:34.468331: step 5705, loss 0.082293, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:34.755767: step 5706, loss 0.102681, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:35.056936: step 5707, loss 0.184485, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:35.317084: step 5708, loss 0.0789854, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:35.561117: step 5709, loss 0.175884, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:35.836171: step 5710, loss 0.0563145, acc 1, learning_rate 0.0001
2017-10-10T15:18:36.169119: step 5711, loss 0.0761842, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:36.473035: step 5712, loss 0.162799, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:36.761855: step 5713, loss 0.107299, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:37.003147: step 5714, loss 0.154594, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:37.273348: step 5715, loss 0.0500442, acc 1, learning_rate 0.0001
2017-10-10T15:18:37.544778: step 5716, loss 0.10683, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:37.821538: step 5717, loss 0.0698631, acc 1, learning_rate 0.0001
2017-10-10T15:18:38.141659: step 5718, loss 0.0737371, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:38.380048: step 5719, loss 0.122183, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:38.640856: step 5720, loss 0.146598, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:39.216997: step 5720, loss 0.218366, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5720

2017-10-10T15:18:40.288835: step 5721, loss 0.0397761, acc 1, learning_rate 0.0001
2017-10-10T15:18:40.912677: step 5722, loss 0.076682, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:41.216150: step 5723, loss 0.08208, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:41.510045: step 5724, loss 0.0637655, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:41.776718: step 5725, loss 0.179115, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:42.068877: step 5726, loss 0.0949562, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:42.408840: step 5727, loss 0.0592273, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:42.699808: step 5728, loss 0.0870109, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:43.012559: step 5729, loss 0.130699, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:43.328935: step 5730, loss 0.080696, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:43.582378: step 5731, loss 0.144109, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:43.856870: step 5732, loss 0.121526, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:44.181059: step 5733, loss 0.110793, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:44.488630: step 5734, loss 0.0588785, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:44.722008: step 5735, loss 0.11543, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:45.033324: step 5736, loss 0.0792307, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:45.339714: step 5737, loss 0.0538555, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:46.148861: step 5738, loss 0.0981149, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:46.413183: step 5739, loss 0.0703983, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:46.772883: step 5740, loss 0.104607, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:47.076982: step 5741, loss 0.083181, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:47.368841: step 5742, loss 0.0962192, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:47.612950: step 5743, loss 0.150223, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:47.860544: step 5744, loss 0.16207, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:48.108540: step 5745, loss 0.0926709, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:48.365674: step 5746, loss 0.0408628, acc 1, learning_rate 0.0001
2017-10-10T15:18:48.669182: step 5747, loss 0.166985, acc 0.90625, learning_rate 0.0001
2017-10-10T15:18:49.009016: step 5748, loss 0.0991005, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:49.272851: step 5749, loss 0.108106, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:49.558027: step 5750, loss 0.0444229, acc 1, learning_rate 0.0001
2017-10-10T15:18:49.837293: step 5751, loss 0.175675, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:50.052216: step 5752, loss 0.084684, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:50.384832: step 5753, loss 0.0757065, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:50.676866: step 5754, loss 0.0444579, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:50.967631: step 5755, loss 0.0958107, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:51.232000: step 5756, loss 0.0967407, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:51.480820: step 5757, loss 0.0951877, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:51.736058: step 5758, loss 0.0535728, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:51.919299: step 5759, loss 0.115056, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:52.248055: step 5760, loss 0.0705475, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:18:52.786325: step 5760, loss 0.218188, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5760

2017-10-10T15:18:53.734049: step 5761, loss 0.168379, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:54.013577: step 5762, loss 0.0839492, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:54.253188: step 5763, loss 0.0812339, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:54.527699: step 5764, loss 0.125439, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:54.780419: step 5765, loss 0.092542, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:55.048250: step 5766, loss 0.0594184, acc 1, learning_rate 0.0001
2017-10-10T15:18:55.316835: step 5767, loss 0.111911, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:55.571326: step 5768, loss 0.166935, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:55.895051: step 5769, loss 0.0626741, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:56.171685: step 5770, loss 0.188811, acc 0.921875, learning_rate 0.0001
2017-10-10T15:18:56.388929: step 5771, loss 0.0383424, acc 1, learning_rate 0.0001
2017-10-10T15:18:56.636851: step 5772, loss 0.0733739, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:56.956543: step 5773, loss 0.105548, acc 0.96875, learning_rate 0.0001
2017-10-10T15:18:57.221685: step 5774, loss 0.0546177, acc 1, learning_rate 0.0001
2017-10-10T15:18:57.489128: step 5775, loss 0.0514175, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:57.774319: step 5776, loss 0.0581805, acc 1, learning_rate 0.0001
2017-10-10T15:18:58.080032: step 5777, loss 0.0980119, acc 0.9375, learning_rate 0.0001
2017-10-10T15:18:58.319294: step 5778, loss 0.117859, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:58.567811: step 5779, loss 0.034365, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:58.875247: step 5780, loss 0.0580748, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:59.101122: step 5781, loss 0.0877444, acc 0.953125, learning_rate 0.0001
2017-10-10T15:18:59.323099: step 5782, loss 0.0410406, acc 1, learning_rate 0.0001
2017-10-10T15:18:59.620957: step 5783, loss 0.0529547, acc 0.984375, learning_rate 0.0001
2017-10-10T15:18:59.953412: step 5784, loss 0.0835534, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:00.300997: step 5785, loss 0.149111, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:00.577087: step 5786, loss 0.240682, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:00.812879: step 5787, loss 0.086517, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:01.119711: step 5788, loss 0.0524043, acc 1, learning_rate 0.0001
2017-10-10T15:19:01.368880: step 5789, loss 0.149411, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:01.649169: step 5790, loss 0.0893118, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:01.920919: step 5791, loss 0.154054, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:02.221062: step 5792, loss 0.0785269, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:02.492369: step 5793, loss 0.0813406, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:02.752608: step 5794, loss 0.191792, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:03.085005: step 5795, loss 0.0525576, acc 1, learning_rate 0.0001
2017-10-10T15:19:03.416965: step 5796, loss 0.0628391, acc 1, learning_rate 0.0001
2017-10-10T15:19:03.755478: step 5797, loss 0.0823065, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:04.032818: step 5798, loss 0.0978337, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:04.263562: step 5799, loss 0.0816177, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:04.520762: step 5800, loss 0.0795037, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:05.003745: step 5800, loss 0.217734, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5800

2017-10-10T15:19:06.136933: step 5801, loss 0.107363, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:06.390259: step 5802, loss 0.124548, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:06.625246: step 5803, loss 0.1067, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:06.888996: step 5804, loss 0.0582319, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:07.167583: step 5805, loss 0.075364, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:07.513068: step 5806, loss 0.0558682, acc 1, learning_rate 0.0001
2017-10-10T15:19:07.808948: step 5807, loss 0.109415, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:08.103471: step 5808, loss 0.114156, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:08.440578: step 5809, loss 0.130593, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:08.733180: step 5810, loss 0.226591, acc 0.90625, learning_rate 0.0001
2017-10-10T15:19:09.025141: step 5811, loss 0.0568273, acc 1, learning_rate 0.0001
2017-10-10T15:19:09.300020: step 5812, loss 0.0691622, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:09.600885: step 5813, loss 0.0872278, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:09.888031: step 5814, loss 0.0530462, acc 1, learning_rate 0.0001
2017-10-10T15:19:10.120341: step 5815, loss 0.136918, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:10.365913: step 5816, loss 0.0814169, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:10.588856: step 5817, loss 0.0434995, acc 1, learning_rate 0.0001
2017-10-10T15:19:10.829994: step 5818, loss 0.136973, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:11.084239: step 5819, loss 0.101602, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:11.426246: step 5820, loss 0.212827, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:11.680964: step 5821, loss 0.112045, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:11.972889: step 5822, loss 0.0931806, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:12.296885: step 5823, loss 0.167221, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:12.489941: step 5824, loss 0.127902, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:12.764776: step 5825, loss 0.10974, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:13.061728: step 5826, loss 0.13093, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:13.378978: step 5827, loss 0.0586395, acc 1, learning_rate 0.0001
2017-10-10T15:19:13.652423: step 5828, loss 0.118316, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:13.988666: step 5829, loss 0.0773636, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:14.252980: step 5830, loss 0.047534, acc 1, learning_rate 0.0001
2017-10-10T15:19:14.553319: step 5831, loss 0.0647768, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:14.855084: step 5832, loss 0.184606, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:15.124911: step 5833, loss 0.0967045, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:15.374178: step 5834, loss 0.110138, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:15.654623: step 5835, loss 0.0921383, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:15.924478: step 5836, loss 0.162573, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:16.167918: step 5837, loss 0.133095, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:16.468059: step 5838, loss 0.0329017, acc 1, learning_rate 0.0001
2017-10-10T15:19:16.769008: step 5839, loss 0.0470745, acc 1, learning_rate 0.0001
2017-10-10T15:19:17.025275: step 5840, loss 0.05426, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:17.577020: step 5840, loss 0.216645, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5840

2017-10-10T15:19:18.764447: step 5841, loss 0.0784773, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:19.097577: step 5842, loss 0.0398441, acc 1, learning_rate 0.0001
2017-10-10T15:19:19.398957: step 5843, loss 0.0508019, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:19.653021: step 5844, loss 0.14161, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:19.925811: step 5845, loss 0.0948582, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:20.200293: step 5846, loss 0.0591342, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:20.504836: step 5847, loss 0.0715413, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:20.712824: step 5848, loss 0.0851146, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:20.980454: step 5849, loss 0.126478, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:21.256841: step 5850, loss 0.0745622, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:21.488972: step 5851, loss 0.169853, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:21.718642: step 5852, loss 0.198765, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:21.972998: step 5853, loss 0.0888087, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:22.246450: step 5854, loss 0.0859489, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:22.534210: step 5855, loss 0.0934469, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:22.856469: step 5856, loss 0.0940669, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:23.017613: step 5857, loss 0.0893024, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:23.327879: step 5858, loss 0.0995253, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:23.660773: step 5859, loss 0.0566663, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:23.976998: step 5860, loss 0.154803, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:24.180114: step 5861, loss 0.0573075, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:24.479875: step 5862, loss 0.134853, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:24.749899: step 5863, loss 0.15946, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:25.030594: step 5864, loss 0.143241, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:25.305005: step 5865, loss 0.0270613, acc 1, learning_rate 0.0001
2017-10-10T15:19:25.585037: step 5866, loss 0.0545259, acc 1, learning_rate 0.0001
2017-10-10T15:19:25.879366: step 5867, loss 0.100187, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:26.137034: step 5868, loss 0.164947, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:26.385123: step 5869, loss 0.055798, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:26.682372: step 5870, loss 0.04972, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:26.965870: step 5871, loss 0.122803, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:27.242719: step 5872, loss 0.109294, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:27.617538: step 5873, loss 0.102496, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:27.913938: step 5874, loss 0.0993746, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:28.236295: step 5875, loss 0.0925262, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:28.433147: step 5876, loss 0.143541, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:28.665685: step 5877, loss 0.0468857, acc 1, learning_rate 0.0001
2017-10-10T15:19:28.924067: step 5878, loss 0.0864494, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:29.147113: step 5879, loss 0.138236, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:29.373812: step 5880, loss 0.176057, acc 0.921569, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:29.833045: step 5880, loss 0.21512, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5880

2017-10-10T15:19:30.748627: step 5881, loss 0.0430616, acc 1, learning_rate 0.0001
2017-10-10T15:19:31.039835: step 5882, loss 0.0910302, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:31.284363: step 5883, loss 0.0954432, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:31.615658: step 5884, loss 0.0986887, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:31.875580: step 5885, loss 0.0504399, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:32.196827: step 5886, loss 0.0801089, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:32.488915: step 5887, loss 0.0622656, acc 1, learning_rate 0.0001
2017-10-10T15:19:32.705053: step 5888, loss 0.0579696, acc 1, learning_rate 0.0001
2017-10-10T15:19:32.984618: step 5889, loss 0.0493827, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:33.249423: step 5890, loss 0.0596309, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:33.523618: step 5891, loss 0.0513286, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:33.775283: step 5892, loss 0.150759, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:34.277963: step 5893, loss 0.0464138, acc 1, learning_rate 0.0001
2017-10-10T15:19:34.556621: step 5894, loss 0.0982384, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:34.805614: step 5895, loss 0.0873035, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:35.098061: step 5896, loss 0.0974872, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:35.364987: step 5897, loss 0.0689707, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:35.596825: step 5898, loss 0.0880057, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:35.907883: step 5899, loss 0.0623511, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:36.177049: step 5900, loss 0.112279, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:36.416844: step 5901, loss 0.0755646, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:36.691548: step 5902, loss 0.146545, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:37.004915: step 5903, loss 0.168288, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:37.348331: step 5904, loss 0.113778, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:37.626519: step 5905, loss 0.122828, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:37.823748: step 5906, loss 0.10434, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:38.061954: step 5907, loss 0.107455, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:38.302754: step 5908, loss 0.104366, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:38.556753: step 5909, loss 0.122582, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:38.806851: step 5910, loss 0.124896, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:39.156957: step 5911, loss 0.066201, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:39.371424: step 5912, loss 0.158068, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:39.639810: step 5913, loss 0.0492207, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:39.908871: step 5914, loss 0.0943276, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:40.116840: step 5915, loss 0.107883, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:40.421458: step 5916, loss 0.124476, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:40.748946: step 5917, loss 0.0769745, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:40.993736: step 5918, loss 0.118505, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:41.279617: step 5919, loss 0.10968, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:41.577200: step 5920, loss 0.105891, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:42.098843: step 5920, loss 0.216529, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5920

2017-10-10T15:19:43.205758: step 5921, loss 0.0516636, acc 1, learning_rate 0.0001
2017-10-10T15:19:43.456900: step 5922, loss 0.120688, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:43.760833: step 5923, loss 0.0763184, acc 1, learning_rate 0.0001
2017-10-10T15:19:44.070012: step 5924, loss 0.0384247, acc 1, learning_rate 0.0001
2017-10-10T15:19:44.409001: step 5925, loss 0.152629, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:44.676848: step 5926, loss 0.121925, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:44.899760: step 5927, loss 0.0875563, acc 1, learning_rate 0.0001
2017-10-10T15:19:45.199391: step 5928, loss 0.0753376, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:45.436017: step 5929, loss 0.0793388, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:45.652953: step 5930, loss 0.109568, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:45.916259: step 5931, loss 0.0511561, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:46.220235: step 5932, loss 0.201905, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:46.497214: step 5933, loss 0.123701, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:46.809027: step 5934, loss 0.0528401, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:47.063373: step 5935, loss 0.191364, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:47.289732: step 5936, loss 0.0994611, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:47.546971: step 5937, loss 0.0870088, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:47.798412: step 5938, loss 0.17494, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:48.105442: step 5939, loss 0.0775006, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:48.391019: step 5940, loss 0.12266, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:48.644977: step 5941, loss 0.0911431, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:48.924903: step 5942, loss 0.0843494, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:49.183138: step 5943, loss 0.19283, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:49.409286: step 5944, loss 0.0717026, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:49.712078: step 5945, loss 0.092096, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:50.024839: step 5946, loss 0.0959335, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:50.328832: step 5947, loss 0.0928387, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:50.619828: step 5948, loss 0.140542, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:50.898768: step 5949, loss 0.0795426, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:51.139994: step 5950, loss 0.0751073, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:51.460942: step 5951, loss 0.044661, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:51.690937: step 5952, loss 0.0718545, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:51.944101: step 5953, loss 0.142189, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:52.236830: step 5954, loss 0.112971, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:52.449181: step 5955, loss 0.0747811, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:52.745356: step 5956, loss 0.0432441, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:53.019143: step 5957, loss 0.161696, acc 0.953125, learning_rate 0.0001
2017-10-10T15:19:53.306464: step 5958, loss 0.0832656, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:53.572902: step 5959, loss 0.0928397, acc 0.9375, learning_rate 0.0001
2017-10-10T15:19:53.881003: step 5960, loss 0.0309577, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:19:54.482262: step 5960, loss 0.214711, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-5960

2017-10-10T15:19:55.705103: step 5961, loss 0.105369, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:55.939031: step 5962, loss 0.118731, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:56.196619: step 5963, loss 0.214108, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:56.434990: step 5964, loss 0.146648, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:56.644132: step 5965, loss 0.208045, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:56.914893: step 5966, loss 0.0537422, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:57.210412: step 5967, loss 0.123925, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:57.508001: step 5968, loss 0.0601194, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:57.772903: step 5969, loss 0.105433, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:58.080874: step 5970, loss 0.048843, acc 1, learning_rate 0.0001
2017-10-10T15:19:58.408903: step 5971, loss 0.0553452, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:58.720567: step 5972, loss 0.154637, acc 0.96875, learning_rate 0.0001
2017-10-10T15:19:58.889053: step 5973, loss 0.0430254, acc 1, learning_rate 0.0001
2017-10-10T15:19:59.031385: step 5974, loss 0.0795914, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:59.227785: step 5975, loss 0.0670587, acc 1, learning_rate 0.0001
2017-10-10T15:19:59.481150: step 5976, loss 0.144604, acc 0.921875, learning_rate 0.0001
2017-10-10T15:19:59.650720: step 5977, loss 0.0785436, acc 0.984375, learning_rate 0.0001
2017-10-10T15:19:59.885353: step 5978, loss 0.0834665, acc 0.980392, learning_rate 0.0001
2017-10-10T15:20:00.150514: step 5979, loss 0.129822, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:00.458196: step 5980, loss 0.0754163, acc 1, learning_rate 0.0001
2017-10-10T15:20:00.788989: step 5981, loss 0.167667, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:01.111457: step 5982, loss 0.0878251, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:01.397056: step 5983, loss 0.070987, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:01.632986: step 5984, loss 0.147657, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:01.954886: step 5985, loss 0.0497645, acc 1, learning_rate 0.0001
2017-10-10T15:20:02.275162: step 5986, loss 0.133573, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:02.582890: step 5987, loss 0.139382, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:02.945182: step 5988, loss 0.0506385, acc 1, learning_rate 0.0001
2017-10-10T15:20:03.204854: step 5989, loss 0.104428, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:03.481051: step 5990, loss 0.03439, acc 1, learning_rate 0.0001
2017-10-10T15:20:03.797109: step 5991, loss 0.0433915, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:04.135993: step 5992, loss 0.136714, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:04.403611: step 5993, loss 0.0784514, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:04.686717: step 5994, loss 0.0298217, acc 1, learning_rate 0.0001
2017-10-10T15:20:04.921977: step 5995, loss 0.210227, acc 0.90625, learning_rate 0.0001
2017-10-10T15:20:05.227272: step 5996, loss 0.14341, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:05.493772: step 5997, loss 0.0504253, acc 1, learning_rate 0.0001
2017-10-10T15:20:05.766053: step 5998, loss 0.0796515, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:06.008349: step 5999, loss 0.105874, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:06.315405: step 6000, loss 0.0502528, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:06.723414: step 6000, loss 0.216941, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6000

2017-10-10T15:20:07.787507: step 6001, loss 0.070765, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:08.039074: step 6002, loss 0.064596, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:08.261835: step 6003, loss 0.141164, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:08.540859: step 6004, loss 0.231934, acc 0.890625, learning_rate 0.0001
2017-10-10T15:20:08.839467: step 6005, loss 0.0786695, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:09.141955: step 6006, loss 0.0582915, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:09.449628: step 6007, loss 0.0881795, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:09.778795: step 6008, loss 0.140168, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:10.062658: step 6009, loss 0.12835, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:10.382203: step 6010, loss 0.0646738, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:10.625787: step 6011, loss 0.116886, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:10.868967: step 6012, loss 0.130138, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:11.121427: step 6013, loss 0.176661, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:11.326799: step 6014, loss 0.0721283, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:11.608802: step 6015, loss 0.121268, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:11.936825: step 6016, loss 0.100512, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:12.229354: step 6017, loss 0.128828, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:12.501386: step 6018, loss 0.0670437, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:12.767541: step 6019, loss 0.0350964, acc 1, learning_rate 0.0001
2017-10-10T15:20:12.971833: step 6020, loss 0.0677031, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:13.235468: step 6021, loss 0.0567816, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:13.531133: step 6022, loss 0.0534608, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:13.778553: step 6023, loss 0.0653517, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:14.070447: step 6024, loss 0.0410804, acc 1, learning_rate 0.0001
2017-10-10T15:20:14.329083: step 6025, loss 0.0840769, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:14.666573: step 6026, loss 0.0654668, acc 1, learning_rate 0.0001
2017-10-10T15:20:14.948798: step 6027, loss 0.146489, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:15.240028: step 6028, loss 0.121208, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:15.489459: step 6029, loss 0.0506271, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:15.762285: step 6030, loss 0.0681348, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:16.030437: step 6031, loss 0.104329, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:16.281746: step 6032, loss 0.109641, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:16.587893: step 6033, loss 0.149315, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:16.903195: step 6034, loss 0.0610202, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:17.211521: step 6035, loss 0.0873448, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:17.489138: step 6036, loss 0.0696755, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:17.788535: step 6037, loss 0.0312004, acc 1, learning_rate 0.0001
2017-10-10T15:20:18.053539: step 6038, loss 0.100258, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:18.322262: step 6039, loss 0.132958, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:18.600820: step 6040, loss 0.0810794, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:19.145046: step 6040, loss 0.215468, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6040

2017-10-10T15:20:20.130598: step 6041, loss 0.0822626, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:20.411135: step 6042, loss 0.0433325, acc 1, learning_rate 0.0001
2017-10-10T15:20:20.724244: step 6043, loss 0.085911, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:21.004984: step 6044, loss 0.13379, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:21.273045: step 6045, loss 0.106354, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:21.578330: step 6046, loss 0.084279, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:21.858095: step 6047, loss 0.0578312, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:22.118256: step 6048, loss 0.116368, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:22.405283: step 6049, loss 0.0792194, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:22.741842: step 6050, loss 0.106395, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:22.970134: step 6051, loss 0.0341716, acc 1, learning_rate 0.0001
2017-10-10T15:20:23.280905: step 6052, loss 0.0503952, acc 1, learning_rate 0.0001
2017-10-10T15:20:23.554560: step 6053, loss 0.102972, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:23.893086: step 6054, loss 0.0781506, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:24.187535: step 6055, loss 0.059504, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:24.506862: step 6056, loss 0.173606, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:24.758032: step 6057, loss 0.0372578, acc 1, learning_rate 0.0001
2017-10-10T15:20:25.051908: step 6058, loss 0.0743467, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:25.396983: step 6059, loss 0.116018, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:25.611932: step 6060, loss 0.0619084, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:25.874533: step 6061, loss 0.130343, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:26.154092: step 6062, loss 0.0743091, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:26.389107: step 6063, loss 0.10804, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:26.667217: step 6064, loss 0.183585, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:26.956867: step 6065, loss 0.0694587, acc 1, learning_rate 0.0001
2017-10-10T15:20:27.233387: step 6066, loss 0.0342383, acc 1, learning_rate 0.0001
2017-10-10T15:20:27.598817: step 6067, loss 0.0792524, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:27.872409: step 6068, loss 0.0571863, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:28.104935: step 6069, loss 0.0876695, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:28.377572: step 6070, loss 0.0940323, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:28.670538: step 6071, loss 0.0529816, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:28.888241: step 6072, loss 0.0877521, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:29.160851: step 6073, loss 0.0823704, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:29.450235: step 6074, loss 0.0756367, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:29.764841: step 6075, loss 0.0634089, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:29.969699: step 6076, loss 0.0794135, acc 0.980392, learning_rate 0.0001
2017-10-10T15:20:30.243839: step 6077, loss 0.091125, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:30.506701: step 6078, loss 0.0680158, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:30.775149: step 6079, loss 0.0639335, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:31.114201: step 6080, loss 0.0813107, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:31.635321: step 6080, loss 0.215659, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6080

2017-10-10T15:20:32.837044: step 6081, loss 0.13133, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:33.124619: step 6082, loss 0.093914, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:33.392991: step 6083, loss 0.0859388, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:33.676005: step 6084, loss 0.0411185, acc 1, learning_rate 0.0001
2017-10-10T15:20:33.978046: step 6085, loss 0.0920177, acc 1, learning_rate 0.0001
2017-10-10T15:20:34.209045: step 6086, loss 0.0779239, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:34.482943: step 6087, loss 0.18121, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:34.808885: step 6088, loss 0.0582115, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:35.147636: step 6089, loss 0.037815, acc 1, learning_rate 0.0001
2017-10-10T15:20:35.376998: step 6090, loss 0.0758532, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:35.659883: step 6091, loss 0.0367785, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:35.944832: step 6092, loss 0.0906893, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:36.209150: step 6093, loss 0.075173, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:36.486439: step 6094, loss 0.150005, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:36.780866: step 6095, loss 0.171931, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:37.081752: step 6096, loss 0.108982, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:37.388880: step 6097, loss 0.164033, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:37.663257: step 6098, loss 0.166484, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:37.922750: step 6099, loss 0.0887297, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:38.234179: step 6100, loss 0.0316544, acc 1, learning_rate 0.0001
2017-10-10T15:20:38.505041: step 6101, loss 0.102971, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:38.725280: step 6102, loss 0.125525, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:38.983142: step 6103, loss 0.0772794, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:39.273476: step 6104, loss 0.0516338, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:39.608763: step 6105, loss 0.0745298, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:39.909111: step 6106, loss 0.0406104, acc 1, learning_rate 0.0001
2017-10-10T15:20:40.173878: step 6107, loss 0.134951, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:40.455386: step 6108, loss 0.027013, acc 1, learning_rate 0.0001
2017-10-10T15:20:40.726082: step 6109, loss 0.132086, acc 0.921875, learning_rate 0.0001
2017-10-10T15:20:40.996721: step 6110, loss 0.14212, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:41.269799: step 6111, loss 0.0710659, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:41.528840: step 6112, loss 0.0774492, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:41.768964: step 6113, loss 0.0728368, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:42.116260: step 6114, loss 0.103128, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:42.440163: step 6115, loss 0.111685, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:42.628484: step 6116, loss 0.140979, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:42.904989: step 6117, loss 0.0523323, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:43.148396: step 6118, loss 0.0853961, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:43.426684: step 6119, loss 0.144092, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:43.697041: step 6120, loss 0.129222, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:44.369423: step 6120, loss 0.216016, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6120

2017-10-10T15:20:45.368591: step 6121, loss 0.148684, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:45.653681: step 6122, loss 0.0947201, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:45.960855: step 6123, loss 0.064943, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:46.247499: step 6124, loss 0.0633484, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:46.497008: step 6125, loss 0.110846, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:46.804629: step 6126, loss 0.0793989, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:47.084302: step 6127, loss 0.0950042, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:47.403771: step 6128, loss 0.063836, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:47.706870: step 6129, loss 0.056688, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:48.022278: step 6130, loss 0.0922601, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:48.267124: step 6131, loss 0.118652, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:48.581090: step 6132, loss 0.137513, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:48.881985: step 6133, loss 0.133702, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:49.114385: step 6134, loss 0.0430157, acc 1, learning_rate 0.0001
2017-10-10T15:20:49.376365: step 6135, loss 0.150319, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:49.672939: step 6136, loss 0.0470091, acc 1, learning_rate 0.0001
2017-10-10T15:20:49.879733: step 6137, loss 0.203065, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:50.170375: step 6138, loss 0.0782321, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:50.450617: step 6139, loss 0.0965458, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:50.786720: step 6140, loss 0.181193, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:51.016372: step 6141, loss 0.0811396, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:51.269431: step 6142, loss 0.176974, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:51.545209: step 6143, loss 0.0475466, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:51.773266: step 6144, loss 0.129046, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:52.015915: step 6145, loss 0.0554957, acc 1, learning_rate 0.0001
2017-10-10T15:20:52.287306: step 6146, loss 0.149202, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:52.563632: step 6147, loss 0.0850286, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:52.820506: step 6148, loss 0.119319, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:53.100945: step 6149, loss 0.0609933, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:53.428203: step 6150, loss 0.0583642, acc 1, learning_rate 0.0001
2017-10-10T15:20:53.700857: step 6151, loss 0.0372281, acc 1, learning_rate 0.0001
2017-10-10T15:20:53.978248: step 6152, loss 0.111608, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:54.298544: step 6153, loss 0.163573, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:54.585041: step 6154, loss 0.117224, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:54.948865: step 6155, loss 0.11612, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:55.224981: step 6156, loss 0.0891837, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:55.516460: step 6157, loss 0.0631311, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:55.784922: step 6158, loss 0.0693079, acc 1, learning_rate 0.0001
2017-10-10T15:20:56.056491: step 6159, loss 0.111027, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:56.289835: step 6160, loss 0.066599, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:20:56.825992: step 6160, loss 0.214352, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6160

2017-10-10T15:20:57.854480: step 6161, loss 0.0417904, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:58.151913: step 6162, loss 0.0575292, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:58.395413: step 6163, loss 0.0551161, acc 0.984375, learning_rate 0.0001
2017-10-10T15:20:58.666491: step 6164, loss 0.0730452, acc 0.96875, learning_rate 0.0001
2017-10-10T15:20:58.942780: step 6165, loss 0.125942, acc 0.953125, learning_rate 0.0001
2017-10-10T15:20:59.191194: step 6166, loss 0.117761, acc 0.9375, learning_rate 0.0001
2017-10-10T15:20:59.482114: step 6167, loss 0.0507056, acc 1, learning_rate 0.0001
2017-10-10T15:20:59.797548: step 6168, loss 0.0382396, acc 1, learning_rate 0.0001
2017-10-10T15:21:00.173939: step 6169, loss 0.0843232, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:00.465819: step 6170, loss 0.0801431, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:00.788521: step 6171, loss 0.183668, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:01.072322: step 6172, loss 0.0695412, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:01.350828: step 6173, loss 0.0929614, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:01.560213: step 6174, loss 0.0514424, acc 0.980392, learning_rate 0.0001
2017-10-10T15:21:01.813266: step 6175, loss 0.0814797, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:02.053460: step 6176, loss 0.0552272, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:02.277989: step 6177, loss 0.0339004, acc 1, learning_rate 0.0001
2017-10-10T15:21:02.571948: step 6178, loss 0.121281, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:02.848876: step 6179, loss 0.172036, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:03.056029: step 6180, loss 0.13415, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:03.386107: step 6181, loss 0.179111, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:03.680821: step 6182, loss 0.060997, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:03.913201: step 6183, loss 0.142638, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:04.232843: step 6184, loss 0.130274, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:04.519094: step 6185, loss 0.11864, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:04.806339: step 6186, loss 0.0531444, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:05.020936: step 6187, loss 0.0707664, acc 1, learning_rate 0.0001
2017-10-10T15:21:05.266971: step 6188, loss 0.121343, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:05.500828: step 6189, loss 0.113809, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:05.764963: step 6190, loss 0.150918, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:06.095128: step 6191, loss 0.0425171, acc 1, learning_rate 0.0001
2017-10-10T15:21:06.340881: step 6192, loss 0.0402231, acc 1, learning_rate 0.0001
2017-10-10T15:21:06.652341: step 6193, loss 0.163269, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:06.972748: step 6194, loss 0.171036, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:07.176559: step 6195, loss 0.0474488, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:07.467256: step 6196, loss 0.089942, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:07.717005: step 6197, loss 0.0686858, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:08.007853: step 6198, loss 0.124303, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:08.277138: step 6199, loss 0.0731258, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:08.556839: step 6200, loss 0.0472419, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:09.099136: step 6200, loss 0.214053, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6200

2017-10-10T15:21:10.213483: step 6201, loss 0.162144, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:10.493050: step 6202, loss 0.0769931, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:10.780297: step 6203, loss 0.0903736, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:11.063043: step 6204, loss 0.126069, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:11.324839: step 6205, loss 0.0711691, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:11.576198: step 6206, loss 0.0916706, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:11.859160: step 6207, loss 0.145073, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:12.180848: step 6208, loss 0.0885271, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:12.437908: step 6209, loss 0.0755483, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:12.725013: step 6210, loss 0.0657971, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:13.004862: step 6211, loss 0.0999415, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:13.297106: step 6212, loss 0.0423713, acc 1, learning_rate 0.0001
2017-10-10T15:21:13.595700: step 6213, loss 0.0812327, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:13.884849: step 6214, loss 0.0506157, acc 1, learning_rate 0.0001
2017-10-10T15:21:14.107506: step 6215, loss 0.0800922, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:14.398065: step 6216, loss 0.143428, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:14.741212: step 6217, loss 0.0933613, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:15.025021: step 6218, loss 0.0868615, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:15.281125: step 6219, loss 0.0577662, acc 1, learning_rate 0.0001
2017-10-10T15:21:15.561040: step 6220, loss 0.138251, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:15.852900: step 6221, loss 0.0768227, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:16.075414: step 6222, loss 0.0848655, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:16.388308: step 6223, loss 0.125063, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:16.679770: step 6224, loss 0.0968197, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:16.954500: step 6225, loss 0.053964, acc 1, learning_rate 0.0001
2017-10-10T15:21:17.207726: step 6226, loss 0.0540575, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:17.481777: step 6227, loss 0.0627591, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:17.804862: step 6228, loss 0.141566, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:18.086913: step 6229, loss 0.260777, acc 0.90625, learning_rate 0.0001
2017-10-10T15:21:18.286153: step 6230, loss 0.0565208, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:18.404075: step 6231, loss 0.131654, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:18.506164: step 6232, loss 0.0546939, acc 1, learning_rate 0.0001
2017-10-10T15:21:18.745192: step 6233, loss 0.112985, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:18.936842: step 6234, loss 0.0839307, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:19.112991: step 6235, loss 0.052308, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:19.273401: step 6236, loss 0.0780656, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:19.442130: step 6237, loss 0.0618348, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:19.703115: step 6238, loss 0.147645, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:19.992295: step 6239, loss 0.0360599, acc 1, learning_rate 0.0001
2017-10-10T15:21:20.268819: step 6240, loss 0.109968, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:20.746190: step 6240, loss 0.212914, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6240

2017-10-10T15:21:21.889358: step 6241, loss 0.053757, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:22.158346: step 6242, loss 0.148454, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:22.409077: step 6243, loss 0.0337555, acc 1, learning_rate 0.0001
2017-10-10T15:21:22.702566: step 6244, loss 0.103349, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:23.005784: step 6245, loss 0.0648036, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:23.281026: step 6246, loss 0.0519806, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:23.612770: step 6247, loss 0.0872051, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:23.863580: step 6248, loss 0.0615628, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:24.155735: step 6249, loss 0.102317, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:24.428317: step 6250, loss 0.0537551, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:24.728868: step 6251, loss 0.0645351, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:24.977133: step 6252, loss 0.108467, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:25.256044: step 6253, loss 0.11438, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:25.535058: step 6254, loss 0.143004, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:25.748664: step 6255, loss 0.121016, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:26.042225: step 6256, loss 0.0930109, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:26.340985: step 6257, loss 0.0932188, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:26.609155: step 6258, loss 0.0868256, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:26.870953: step 6259, loss 0.0416038, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:27.188468: step 6260, loss 0.0326914, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:27.429114: step 6261, loss 0.100612, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:27.653999: step 6262, loss 0.125571, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:27.933151: step 6263, loss 0.0885388, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:28.215940: step 6264, loss 0.0706612, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:28.473911: step 6265, loss 0.108001, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:28.764511: step 6266, loss 0.0569539, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:29.040189: step 6267, loss 0.0601525, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:29.271092: step 6268, loss 0.0943326, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:29.563673: step 6269, loss 0.138294, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:29.889099: step 6270, loss 0.112024, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:30.194915: step 6271, loss 0.0730902, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:30.411434: step 6272, loss 0.0760393, acc 0.980392, learning_rate 0.0001
2017-10-10T15:21:30.758889: step 6273, loss 0.0559358, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:31.064881: step 6274, loss 0.148753, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:31.357564: step 6275, loss 0.0980622, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:31.646330: step 6276, loss 0.105314, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:31.973792: step 6277, loss 0.0509685, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:32.250975: step 6278, loss 0.148687, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:32.500658: step 6279, loss 0.0476753, acc 1, learning_rate 0.0001
2017-10-10T15:21:32.801281: step 6280, loss 0.0893531, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:33.233138: step 6280, loss 0.21286, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6280

2017-10-10T15:21:34.332505: step 6281, loss 0.0952914, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:34.608212: step 6282, loss 0.0679518, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:34.926343: step 6283, loss 0.12016, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:35.210252: step 6284, loss 0.0616003, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:35.405055: step 6285, loss 0.166685, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:35.696033: step 6286, loss 0.0343944, acc 1, learning_rate 0.0001
2017-10-10T15:21:35.890554: step 6287, loss 0.109689, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:36.219611: step 6288, loss 0.152395, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:36.546911: step 6289, loss 0.0845966, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:36.752608: step 6290, loss 0.0787573, acc 1, learning_rate 0.0001
2017-10-10T15:21:37.002903: step 6291, loss 0.113789, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:37.226187: step 6292, loss 0.019248, acc 1, learning_rate 0.0001
2017-10-10T15:21:37.451161: step 6293, loss 0.037561, acc 1, learning_rate 0.0001
2017-10-10T15:21:37.697151: step 6294, loss 0.323175, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:37.998376: step 6295, loss 0.042987, acc 1, learning_rate 0.0001
2017-10-10T15:21:38.256697: step 6296, loss 0.0900933, acc 1, learning_rate 0.0001
2017-10-10T15:21:38.506348: step 6297, loss 0.168317, acc 0.921875, learning_rate 0.0001
2017-10-10T15:21:38.799341: step 6298, loss 0.0752333, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:39.076864: step 6299, loss 0.061014, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:39.333066: step 6300, loss 0.0776279, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:39.610606: step 6301, loss 0.100865, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:39.906971: step 6302, loss 0.0651651, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:40.210679: step 6303, loss 0.0648647, acc 1, learning_rate 0.0001
2017-10-10T15:21:40.463549: step 6304, loss 0.0315652, acc 1, learning_rate 0.0001
2017-10-10T15:21:40.735418: step 6305, loss 0.140826, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:41.016926: step 6306, loss 0.163782, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:41.260916: step 6307, loss 0.0949021, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:41.548240: step 6308, loss 0.0623342, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:41.836060: step 6309, loss 0.140081, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:42.093827: step 6310, loss 0.117946, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:42.399734: step 6311, loss 0.159256, acc 0.9375, learning_rate 0.0001
2017-10-10T15:21:42.672259: step 6312, loss 0.0382088, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:42.917675: step 6313, loss 0.0411926, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:43.113951: step 6314, loss 0.0666353, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:43.368783: step 6315, loss 0.112716, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:43.693268: step 6316, loss 0.0559832, acc 1, learning_rate 0.0001
2017-10-10T15:21:43.952889: step 6317, loss 0.0943725, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:44.215391: step 6318, loss 0.0968464, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:44.505419: step 6319, loss 0.0371428, acc 1, learning_rate 0.0001
2017-10-10T15:21:44.812912: step 6320, loss 0.0604673, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:45.217965: step 6320, loss 0.212165, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6320

2017-10-10T15:21:46.262030: step 6321, loss 0.103534, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:46.511148: step 6322, loss 0.0559459, acc 1, learning_rate 0.0001
2017-10-10T15:21:46.757110: step 6323, loss 0.147165, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:46.995839: step 6324, loss 0.11381, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:47.299709: step 6325, loss 0.0648824, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:47.536873: step 6326, loss 0.0512554, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:47.812544: step 6327, loss 0.104059, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:48.102135: step 6328, loss 0.0637821, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:48.392927: step 6329, loss 0.0447879, acc 1, learning_rate 0.0001
2017-10-10T15:21:48.667394: step 6330, loss 0.0496197, acc 1, learning_rate 0.0001
2017-10-10T15:21:48.961079: step 6331, loss 0.0637003, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:49.246663: step 6332, loss 0.0954326, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:49.506719: step 6333, loss 0.154063, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:49.778485: step 6334, loss 0.10473, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:50.016975: step 6335, loss 0.0537, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:50.328882: step 6336, loss 0.108507, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:50.638532: step 6337, loss 0.0617859, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:50.893227: step 6338, loss 0.0308965, acc 1, learning_rate 0.0001
2017-10-10T15:21:51.169002: step 6339, loss 0.0866611, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:51.456840: step 6340, loss 0.0505803, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:51.746269: step 6341, loss 0.0845008, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:52.087018: step 6342, loss 0.0626502, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:52.360948: step 6343, loss 0.0980204, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:52.596765: step 6344, loss 0.0781999, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:52.875201: step 6345, loss 0.0807624, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:53.128656: step 6346, loss 0.0700426, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:53.371479: step 6347, loss 0.105036, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:53.636976: step 6348, loss 0.0780927, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:53.985028: step 6349, loss 0.132331, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:54.273712: step 6350, loss 0.112594, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:54.613066: step 6351, loss 0.080185, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:54.892209: step 6352, loss 0.141239, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:55.152890: step 6353, loss 0.0947652, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:55.331181: step 6354, loss 0.112294, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:55.550074: step 6355, loss 0.0958602, acc 0.953125, learning_rate 0.0001
2017-10-10T15:21:55.780902: step 6356, loss 0.204928, acc 0.890625, learning_rate 0.0001
2017-10-10T15:21:56.087807: step 6357, loss 0.0826982, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:56.326146: step 6358, loss 0.113398, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:56.612186: step 6359, loss 0.0854939, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:56.921707: step 6360, loss 0.159504, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:21:57.423451: step 6360, loss 0.214784, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6360

2017-10-10T15:21:58.508154: step 6361, loss 0.0981888, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:58.790893: step 6362, loss 0.0433716, acc 0.984375, learning_rate 0.0001
2017-10-10T15:21:59.060891: step 6363, loss 0.119747, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:59.300924: step 6364, loss 0.0729167, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:59.524991: step 6365, loss 0.113515, acc 0.96875, learning_rate 0.0001
2017-10-10T15:21:59.790702: step 6366, loss 0.0233319, acc 1, learning_rate 0.0001
2017-10-10T15:22:00.001106: step 6367, loss 0.0431472, acc 1, learning_rate 0.0001
2017-10-10T15:22:00.289596: step 6368, loss 0.100996, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:00.600890: step 6369, loss 0.0585571, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:00.805093: step 6370, loss 0.0696155, acc 0.960784, learning_rate 0.0001
2017-10-10T15:22:01.085144: step 6371, loss 0.129672, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:01.380958: step 6372, loss 0.0397192, acc 1, learning_rate 0.0001
2017-10-10T15:22:01.616105: step 6373, loss 0.0249976, acc 1, learning_rate 0.0001
2017-10-10T15:22:01.929583: step 6374, loss 0.0654003, acc 1, learning_rate 0.0001
2017-10-10T15:22:02.188846: step 6375, loss 0.0505253, acc 1, learning_rate 0.0001
2017-10-10T15:22:02.481218: step 6376, loss 0.0893453, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:02.760850: step 6377, loss 0.108922, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:03.025110: step 6378, loss 0.0935736, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:03.327888: step 6379, loss 0.1212, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:03.597798: step 6380, loss 0.0660006, acc 1, learning_rate 0.0001
2017-10-10T15:22:03.880966: step 6381, loss 0.135108, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:04.174382: step 6382, loss 0.0510228, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:04.420367: step 6383, loss 0.0562323, acc 1, learning_rate 0.0001
2017-10-10T15:22:04.715255: step 6384, loss 0.0439108, acc 1, learning_rate 0.0001
2017-10-10T15:22:04.974197: step 6385, loss 0.159539, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:05.228847: step 6386, loss 0.06313, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:05.568526: step 6387, loss 0.127533, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:05.873157: step 6388, loss 0.090457, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:06.137190: step 6389, loss 0.0591302, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:06.388843: step 6390, loss 0.123442, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:06.725228: step 6391, loss 0.0392574, acc 1, learning_rate 0.0001
2017-10-10T15:22:07.032904: step 6392, loss 0.0877485, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:07.345156: step 6393, loss 0.0857511, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:07.657051: step 6394, loss 0.101268, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:07.945254: step 6395, loss 0.0482512, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:08.270512: step 6396, loss 0.179068, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:08.524829: step 6397, loss 0.119453, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:08.796699: step 6398, loss 0.0462881, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:09.010775: step 6399, loss 0.0614193, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:09.260878: step 6400, loss 0.124211, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:09.694482: step 6400, loss 0.21455, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6400

2017-10-10T15:22:10.798225: step 6401, loss 0.0965497, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:11.067935: step 6402, loss 0.100929, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:11.284599: step 6403, loss 0.118454, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:11.537071: step 6404, loss 0.157086, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:11.813305: step 6405, loss 0.0732927, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:12.124840: step 6406, loss 0.0464108, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:12.417224: step 6407, loss 0.149113, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:12.700964: step 6408, loss 0.0793209, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:13.053051: step 6409, loss 0.0589402, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:13.330326: step 6410, loss 0.0892704, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:13.576103: step 6411, loss 0.0496434, acc 1, learning_rate 0.0001
2017-10-10T15:22:13.773570: step 6412, loss 0.0645803, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:14.010347: step 6413, loss 0.0858054, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:14.332889: step 6414, loss 0.0202251, acc 1, learning_rate 0.0001
2017-10-10T15:22:14.646085: step 6415, loss 0.0993257, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:14.913900: step 6416, loss 0.168315, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:15.215960: step 6417, loss 0.0869781, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:15.496220: step 6418, loss 0.20421, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:15.781334: step 6419, loss 0.0876808, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:16.068721: step 6420, loss 0.128313, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:16.317438: step 6421, loss 0.0327483, acc 1, learning_rate 0.0001
2017-10-10T15:22:16.612008: step 6422, loss 0.0889416, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:16.903738: step 6423, loss 0.116794, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:17.132833: step 6424, loss 0.0729973, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:17.402729: step 6425, loss 0.163267, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:17.686498: step 6426, loss 0.111881, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:18.006774: step 6427, loss 0.0600968, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:18.339042: step 6428, loss 0.0452324, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:18.606219: step 6429, loss 0.0321551, acc 1, learning_rate 0.0001
2017-10-10T15:22:18.871055: step 6430, loss 0.109968, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:19.180945: step 6431, loss 0.165648, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:19.419460: step 6432, loss 0.0342726, acc 1, learning_rate 0.0001
2017-10-10T15:22:19.729107: step 6433, loss 0.0407857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:19.942521: step 6434, loss 0.0759116, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:20.241108: step 6435, loss 0.146596, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:20.560865: step 6436, loss 0.0816333, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:20.828535: step 6437, loss 0.064407, acc 1, learning_rate 0.0001
2017-10-10T15:22:21.073205: step 6438, loss 0.102352, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:21.336851: step 6439, loss 0.0416011, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:21.587040: step 6440, loss 0.0644302, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:22.096311: step 6440, loss 0.213754, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6440

2017-10-10T15:22:23.116895: step 6441, loss 0.072282, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:23.407555: step 6442, loss 0.10401, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:23.698434: step 6443, loss 0.162218, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:24.024916: step 6444, loss 0.0723791, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:24.333161: step 6445, loss 0.114742, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:24.577037: step 6446, loss 0.0544471, acc 1, learning_rate 0.0001
2017-10-10T15:22:24.877038: step 6447, loss 0.0308335, acc 1, learning_rate 0.0001
2017-10-10T15:22:25.230036: step 6448, loss 0.0700887, acc 1, learning_rate 0.0001
2017-10-10T15:22:25.499687: step 6449, loss 0.0691694, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:25.736953: step 6450, loss 0.0855936, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:26.065695: step 6451, loss 0.088387, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:26.356834: step 6452, loss 0.0872637, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:26.639773: step 6453, loss 0.0704588, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:26.865107: step 6454, loss 0.121928, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:27.210732: step 6455, loss 0.0678723, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:27.463372: step 6456, loss 0.0408214, acc 1, learning_rate 0.0001
2017-10-10T15:22:27.736254: step 6457, loss 0.0944168, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:28.022061: step 6458, loss 0.0656476, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:28.296436: step 6459, loss 0.0779364, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:28.540961: step 6460, loss 0.127942, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:28.853375: step 6461, loss 0.159616, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:29.113133: step 6462, loss 0.113279, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:29.386853: step 6463, loss 0.117674, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:29.698555: step 6464, loss 0.056299, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:29.995502: step 6465, loss 0.0849919, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:30.289134: step 6466, loss 0.0781884, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:30.554175: step 6467, loss 0.117148, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:30.780939: step 6468, loss 0.125277, acc 0.941176, learning_rate 0.0001
2017-10-10T15:22:31.048651: step 6469, loss 0.0310901, acc 1, learning_rate 0.0001
2017-10-10T15:22:31.311251: step 6470, loss 0.112396, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:31.562793: step 6471, loss 0.0519538, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:31.773110: step 6472, loss 0.115633, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:32.045393: step 6473, loss 0.0944028, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:32.308635: step 6474, loss 0.101033, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:32.649940: step 6475, loss 0.0704622, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:32.900188: step 6476, loss 0.0908507, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:33.175536: step 6477, loss 0.0928857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:33.477978: step 6478, loss 0.0888139, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:33.790613: step 6479, loss 0.0626802, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:34.047365: step 6480, loss 0.134346, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:34.495171: step 6480, loss 0.213883, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6480

2017-10-10T15:22:35.592020: step 6481, loss 0.0734252, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:35.896850: step 6482, loss 0.0558111, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:36.213113: step 6483, loss 0.152912, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:36.455588: step 6484, loss 0.0411449, acc 1, learning_rate 0.0001
2017-10-10T15:22:36.693977: step 6485, loss 0.102448, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:36.985291: step 6486, loss 0.104807, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:37.265975: step 6487, loss 0.0797528, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:37.533396: step 6488, loss 0.0993825, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:37.790235: step 6489, loss 0.124148, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:38.112017: step 6490, loss 0.0839617, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:38.408841: step 6491, loss 0.1157, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:38.711239: step 6492, loss 0.095726, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:38.986750: step 6493, loss 0.10179, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:39.250948: step 6494, loss 0.0631636, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:39.585801: step 6495, loss 0.0797706, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:39.776207: step 6496, loss 0.201384, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:40.083429: step 6497, loss 0.094292, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:40.324825: step 6498, loss 0.052448, acc 1, learning_rate 0.0001
2017-10-10T15:22:40.628889: step 6499, loss 0.0611017, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:40.924868: step 6500, loss 0.0428653, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:41.220851: step 6501, loss 0.0446567, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:41.569099: step 6502, loss 0.0278793, acc 1, learning_rate 0.0001
2017-10-10T15:22:41.824560: step 6503, loss 0.0471599, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:42.015310: step 6504, loss 0.088513, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:42.220388: step 6505, loss 0.109397, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:42.432835: step 6506, loss 0.049611, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:42.661338: step 6507, loss 0.138489, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:42.844938: step 6508, loss 0.170097, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:43.094972: step 6509, loss 0.0640769, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:43.287929: step 6510, loss 0.157882, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:43.521583: step 6511, loss 0.0607867, acc 1, learning_rate 0.0001
2017-10-10T15:22:43.782328: step 6512, loss 0.125155, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:43.964466: step 6513, loss 0.138761, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:44.211133: step 6514, loss 0.184062, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:44.453375: step 6515, loss 0.152484, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:44.659261: step 6516, loss 0.021939, acc 1, learning_rate 0.0001
2017-10-10T15:22:44.895979: step 6517, loss 0.102555, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:45.089090: step 6518, loss 0.109295, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:45.328054: step 6519, loss 0.0954018, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:45.581059: step 6520, loss 0.0877492, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:45.998855: step 6520, loss 0.213762, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6520

2017-10-10T15:22:47.065109: step 6521, loss 0.198648, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:47.327453: step 6522, loss 0.0786506, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:47.563345: step 6523, loss 0.0497397, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:47.812918: step 6524, loss 0.116211, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:48.052906: step 6525, loss 0.0605015, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:48.308982: step 6526, loss 0.127551, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:48.519181: step 6527, loss 0.0995572, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:48.697865: step 6528, loss 0.0650696, acc 1, learning_rate 0.0001
2017-10-10T15:22:48.860819: step 6529, loss 0.146674, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:49.024823: step 6530, loss 0.0717714, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:49.321148: step 6531, loss 0.0723511, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:49.532804: step 6532, loss 0.086481, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:49.766036: step 6533, loss 0.101631, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:49.983810: step 6534, loss 0.0790053, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:50.187692: step 6535, loss 0.0491074, acc 1, learning_rate 0.0001
2017-10-10T15:22:50.412667: step 6536, loss 0.0928559, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:50.667239: step 6537, loss 0.0385732, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:50.933128: step 6538, loss 0.0547049, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:51.175957: step 6539, loss 0.0641, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:51.378877: step 6540, loss 0.110508, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:51.516596: step 6541, loss 0.0286028, acc 1, learning_rate 0.0001
2017-10-10T15:22:51.736845: step 6542, loss 0.0323591, acc 1, learning_rate 0.0001
2017-10-10T15:22:51.979971: step 6543, loss 0.0715642, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:52.204784: step 6544, loss 0.118291, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:52.449424: step 6545, loss 0.116182, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:52.702907: step 6546, loss 0.045524, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:52.913057: step 6547, loss 0.0378373, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:53.093192: step 6548, loss 0.0866323, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:53.329204: step 6549, loss 0.0318404, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:53.552874: step 6550, loss 0.104828, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:53.778289: step 6551, loss 0.0979141, acc 0.953125, learning_rate 0.0001
2017-10-10T15:22:54.028936: step 6552, loss 0.149895, acc 0.921875, learning_rate 0.0001
2017-10-10T15:22:54.272996: step 6553, loss 0.0948047, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:54.513293: step 6554, loss 0.0808531, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:54.796873: step 6555, loss 0.155298, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:55.049520: step 6556, loss 0.0446109, acc 1, learning_rate 0.0001
2017-10-10T15:22:55.220952: step 6557, loss 0.0819841, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:55.527854: step 6558, loss 0.0503254, acc 1, learning_rate 0.0001
2017-10-10T15:22:55.781106: step 6559, loss 0.0281015, acc 1, learning_rate 0.0001
2017-10-10T15:22:55.952867: step 6560, loss 0.0461494, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:22:56.330638: step 6560, loss 0.214204, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6560

2017-10-10T15:22:57.376849: step 6561, loss 0.104525, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:57.620525: step 6562, loss 0.085754, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:57.865047: step 6563, loss 0.055575, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:58.069148: step 6564, loss 0.0889832, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:58.320006: step 6565, loss 0.0554206, acc 0.984375, learning_rate 0.0001
2017-10-10T15:22:58.473124: step 6566, loss 0.0892115, acc 0.980392, learning_rate 0.0001
2017-10-10T15:22:58.715085: step 6567, loss 0.112759, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:58.939793: step 6568, loss 0.144773, acc 0.96875, learning_rate 0.0001
2017-10-10T15:22:59.152083: step 6569, loss 0.0622011, acc 1, learning_rate 0.0001
2017-10-10T15:22:59.385061: step 6570, loss 0.0354167, acc 1, learning_rate 0.0001
2017-10-10T15:22:59.660957: step 6571, loss 0.134954, acc 0.9375, learning_rate 0.0001
2017-10-10T15:22:59.822192: step 6572, loss 0.103045, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:00.092533: step 6573, loss 0.0502828, acc 1, learning_rate 0.0001
2017-10-10T15:23:00.319353: step 6574, loss 0.0848999, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:00.526796: step 6575, loss 0.055223, acc 1, learning_rate 0.0001
2017-10-10T15:23:00.801544: step 6576, loss 0.0931364, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:01.002148: step 6577, loss 0.0561059, acc 1, learning_rate 0.0001
2017-10-10T15:23:01.236888: step 6578, loss 0.109457, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:01.449071: step 6579, loss 0.0371764, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:01.729627: step 6580, loss 0.0937185, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:02.040223: step 6581, loss 0.0695951, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:02.232478: step 6582, loss 0.176478, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:02.421251: step 6583, loss 0.0665372, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:02.648815: step 6584, loss 0.0455797, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:02.949339: step 6585, loss 0.0620949, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:03.212853: step 6586, loss 0.0846865, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:03.408916: step 6587, loss 0.112698, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:03.604450: step 6588, loss 0.0780559, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:03.776873: step 6589, loss 0.0694273, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:03.973634: step 6590, loss 0.0961484, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:04.237522: step 6591, loss 0.201277, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:04.384258: step 6592, loss 0.0481966, acc 1, learning_rate 0.0001
2017-10-10T15:23:04.644502: step 6593, loss 0.0759925, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:04.913472: step 6594, loss 0.10551, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:05.184887: step 6595, loss 0.0785981, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:05.435069: step 6596, loss 0.143377, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:05.624796: step 6597, loss 0.0503082, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:05.903401: step 6598, loss 0.057166, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:06.133097: step 6599, loss 0.0889381, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:06.356853: step 6600, loss 0.105438, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:06.772939: step 6600, loss 0.214301, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6600

2017-10-10T15:23:07.712134: step 6601, loss 0.0515153, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:07.927084: step 6602, loss 0.053687, acc 1, learning_rate 0.0001
2017-10-10T15:23:08.217519: step 6603, loss 0.0180111, acc 1, learning_rate 0.0001
2017-10-10T15:23:08.374912: step 6604, loss 0.110094, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:08.615405: step 6605, loss 0.0382956, acc 1, learning_rate 0.0001
2017-10-10T15:23:08.875648: step 6606, loss 0.104543, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:09.165880: step 6607, loss 0.0716942, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:09.392475: step 6608, loss 0.0671068, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:09.600969: step 6609, loss 0.075737, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:09.804726: step 6610, loss 0.122603, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:09.980132: step 6611, loss 0.0833892, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:10.166192: step 6612, loss 0.0317587, acc 1, learning_rate 0.0001
2017-10-10T15:23:10.445867: step 6613, loss 0.146842, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:10.658603: step 6614, loss 0.0729226, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:10.889328: step 6615, loss 0.0902999, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:11.127079: step 6616, loss 0.0984093, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:11.396807: step 6617, loss 0.156757, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:11.654938: step 6618, loss 0.0708621, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:11.864552: step 6619, loss 0.0442889, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:12.122038: step 6620, loss 0.0585031, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:12.324432: step 6621, loss 0.0679196, acc 1, learning_rate 0.0001
2017-10-10T15:23:12.554510: step 6622, loss 0.087978, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:12.812926: step 6623, loss 0.0941519, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:13.030682: step 6624, loss 0.0498931, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:13.220685: step 6625, loss 0.136596, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:13.422389: step 6626, loss 0.0965035, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:13.600934: step 6627, loss 0.139067, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:13.836155: step 6628, loss 0.0839346, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:14.084831: step 6629, loss 0.027775, acc 1, learning_rate 0.0001
2017-10-10T15:23:14.321226: step 6630, loss 0.0885414, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:14.553703: step 6631, loss 0.122849, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:14.763480: step 6632, loss 0.08185, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:14.989520: step 6633, loss 0.142952, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:15.221368: step 6634, loss 0.106593, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:15.480840: step 6635, loss 0.0396993, acc 1, learning_rate 0.0001
2017-10-10T15:23:15.696937: step 6636, loss 0.142015, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:15.934625: step 6637, loss 0.0702272, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:16.200263: step 6638, loss 0.0829835, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:16.427666: step 6639, loss 0.0627933, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:16.617567: step 6640, loss 0.0193579, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:17.018971: step 6640, loss 0.21314, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6640

2017-10-10T15:23:18.060884: step 6641, loss 0.108933, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:18.228937: step 6642, loss 0.192148, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:18.415454: step 6643, loss 0.152358, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:18.614128: step 6644, loss 0.0388638, acc 1, learning_rate 0.0001
2017-10-10T15:23:18.824557: step 6645, loss 0.0781394, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:19.072834: step 6646, loss 0.0494638, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:19.274475: step 6647, loss 0.172913, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:19.501939: step 6648, loss 0.0372204, acc 1, learning_rate 0.0001
2017-10-10T15:23:19.784838: step 6649, loss 0.180691, acc 0.921875, learning_rate 0.0001
2017-10-10T15:23:19.983585: step 6650, loss 0.0847045, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:20.192954: step 6651, loss 0.0770497, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:20.444631: step 6652, loss 0.0319506, acc 1, learning_rate 0.0001
2017-10-10T15:23:20.655577: step 6653, loss 0.0391254, acc 1, learning_rate 0.0001
2017-10-10T15:23:20.876451: step 6654, loss 0.0952865, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:21.138996: step 6655, loss 0.105911, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:21.388816: step 6656, loss 0.156429, acc 0.921875, learning_rate 0.0001
2017-10-10T15:23:21.609997: step 6657, loss 0.0735764, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:21.845347: step 6658, loss 0.115419, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:22.076356: step 6659, loss 0.0756876, acc 1, learning_rate 0.0001
2017-10-10T15:23:22.270068: step 6660, loss 0.0466938, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:22.540900: step 6661, loss 0.0999764, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:22.862103: step 6662, loss 0.025439, acc 1, learning_rate 0.0001
2017-10-10T15:23:23.092827: step 6663, loss 0.0533025, acc 1, learning_rate 0.0001
2017-10-10T15:23:23.236174: step 6664, loss 0.0995158, acc 0.980392, learning_rate 0.0001
2017-10-10T15:23:23.416893: step 6665, loss 0.207853, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:23.576983: step 6666, loss 0.0952142, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:23.833216: step 6667, loss 0.065492, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:24.083209: step 6668, loss 0.117467, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:24.277064: step 6669, loss 0.068207, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:24.443487: step 6670, loss 0.130675, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:24.671427: step 6671, loss 0.0990354, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:24.948933: step 6672, loss 0.0301857, acc 1, learning_rate 0.0001
2017-10-10T15:23:25.205796: step 6673, loss 0.0592886, acc 1, learning_rate 0.0001
2017-10-10T15:23:25.428025: step 6674, loss 0.108665, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:25.696136: step 6675, loss 0.0739696, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:25.900851: step 6676, loss 0.048927, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:26.122631: step 6677, loss 0.0770517, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:26.402036: step 6678, loss 0.118561, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:26.662920: step 6679, loss 0.0828876, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:26.895686: step 6680, loss 0.12925, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:27.220918: step 6680, loss 0.214091, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6680

2017-10-10T15:23:28.267745: step 6681, loss 0.0517823, acc 1, learning_rate 0.0001
2017-10-10T15:23:28.472075: step 6682, loss 0.0692846, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:28.711978: step 6683, loss 0.153897, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:28.969060: step 6684, loss 0.145261, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:29.224864: step 6685, loss 0.0543916, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:29.428867: step 6686, loss 0.0859694, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:29.667127: step 6687, loss 0.109125, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:29.889738: step 6688, loss 0.0824977, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:30.135082: step 6689, loss 0.0284174, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:30.372827: step 6690, loss 0.0728827, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:30.627957: step 6691, loss 0.0926147, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:30.818028: step 6692, loss 0.134824, acc 0.921875, learning_rate 0.0001
2017-10-10T15:23:31.043537: step 6693, loss 0.0601095, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:31.276939: step 6694, loss 0.0465146, acc 1, learning_rate 0.0001
2017-10-10T15:23:31.443040: step 6695, loss 0.139801, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:31.699248: step 6696, loss 0.0683422, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:31.937036: step 6697, loss 0.104178, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:32.156524: step 6698, loss 0.0892677, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:32.396597: step 6699, loss 0.0688573, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:32.675617: step 6700, loss 0.0690248, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:32.943341: step 6701, loss 0.0934689, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:33.164849: step 6702, loss 0.0504998, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:33.339424: step 6703, loss 0.0664077, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:33.557185: step 6704, loss 0.0979682, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:33.749241: step 6705, loss 0.0687193, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:33.985529: step 6706, loss 0.0923057, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:34.232718: step 6707, loss 0.0695316, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:34.484879: step 6708, loss 0.084439, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:34.784855: step 6709, loss 0.126449, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:35.013118: step 6710, loss 0.0273784, acc 1, learning_rate 0.0001
2017-10-10T15:23:35.211075: step 6711, loss 0.0289203, acc 1, learning_rate 0.0001
2017-10-10T15:23:35.429604: step 6712, loss 0.0508326, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:35.634640: step 6713, loss 0.0544455, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:35.855521: step 6714, loss 0.0789558, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:36.182625: step 6715, loss 0.0926476, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:36.453521: step 6716, loss 0.0713947, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:36.664836: step 6717, loss 0.196046, acc 0.921875, learning_rate 0.0001
2017-10-10T15:23:36.834156: step 6718, loss 0.0448243, acc 1, learning_rate 0.0001
2017-10-10T15:23:37.002358: step 6719, loss 0.0381761, acc 1, learning_rate 0.0001
2017-10-10T15:23:37.221997: step 6720, loss 0.0825792, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:37.604749: step 6720, loss 0.215566, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6720

2017-10-10T15:23:38.610360: step 6721, loss 0.0414702, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:38.886322: step 6722, loss 0.028449, acc 1, learning_rate 0.0001
2017-10-10T15:23:39.116855: step 6723, loss 0.0604479, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:39.340903: step 6724, loss 0.178954, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:39.595168: step 6725, loss 0.0545371, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:39.853508: step 6726, loss 0.0557672, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:40.045228: step 6727, loss 0.0502619, acc 1, learning_rate 0.0001
2017-10-10T15:23:40.272949: step 6728, loss 0.0713762, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:40.526484: step 6729, loss 0.054887, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:40.780016: step 6730, loss 0.0944082, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:41.040966: step 6731, loss 0.0858772, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:41.249155: step 6732, loss 0.0791415, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:41.510001: step 6733, loss 0.10658, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:41.783249: step 6734, loss 0.0611254, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:42.016941: step 6735, loss 0.0849147, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:42.220850: step 6736, loss 0.146928, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:42.474629: step 6737, loss 0.110695, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:42.704859: step 6738, loss 0.108091, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:42.886796: step 6739, loss 0.0569697, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:43.125533: step 6740, loss 0.0854217, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:43.386962: step 6741, loss 0.035629, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:43.572500: step 6742, loss 0.0806361, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:43.836994: step 6743, loss 0.0444249, acc 1, learning_rate 0.0001
2017-10-10T15:23:44.096076: step 6744, loss 0.135214, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:44.324435: step 6745, loss 0.0481818, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:44.499918: step 6746, loss 0.0602799, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:44.741263: step 6747, loss 0.103618, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:45.004833: step 6748, loss 0.0601333, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:45.181001: step 6749, loss 0.122761, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:45.444481: step 6750, loss 0.127426, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:45.663227: step 6751, loss 0.0428652, acc 1, learning_rate 0.0001
2017-10-10T15:23:45.894681: step 6752, loss 0.0963605, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:46.139895: step 6753, loss 0.0418808, acc 1, learning_rate 0.0001
2017-10-10T15:23:46.366159: step 6754, loss 0.0547491, acc 1, learning_rate 0.0001
2017-10-10T15:23:46.552058: step 6755, loss 0.014105, acc 1, learning_rate 0.0001
2017-10-10T15:23:46.750199: step 6756, loss 0.0454104, acc 1, learning_rate 0.0001
2017-10-10T15:23:46.969130: step 6757, loss 0.0992651, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:47.243871: step 6758, loss 0.132472, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:47.445829: step 6759, loss 0.10525, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:47.645973: step 6760, loss 0.117146, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:47.985552: step 6760, loss 0.214967, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6760

2017-10-10T15:23:48.814440: step 6761, loss 0.116879, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:49.018619: step 6762, loss 0.0650546, acc 0.980392, learning_rate 0.0001
2017-10-10T15:23:49.225272: step 6763, loss 0.0875887, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:49.497374: step 6764, loss 0.123779, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:49.709030: step 6765, loss 0.109455, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:49.952913: step 6766, loss 0.0974699, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:50.280815: step 6767, loss 0.062492, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:50.444823: step 6768, loss 0.0670997, acc 1, learning_rate 0.0001
2017-10-10T15:23:50.611622: step 6769, loss 0.0922178, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:50.784816: step 6770, loss 0.0885652, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:50.957914: step 6771, loss 0.0482183, acc 1, learning_rate 0.0001
2017-10-10T15:23:51.151253: step 6772, loss 0.0608999, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:51.440952: step 6773, loss 0.0807649, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:51.680945: step 6774, loss 0.0916971, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:51.879436: step 6775, loss 0.0765732, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:52.110841: step 6776, loss 0.110952, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:52.396536: step 6777, loss 0.0848662, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:52.640898: step 6778, loss 0.0198908, acc 1, learning_rate 0.0001
2017-10-10T15:23:52.828873: step 6779, loss 0.0786246, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:53.087029: step 6780, loss 0.109655, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:53.316685: step 6781, loss 0.072682, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:53.521060: step 6782, loss 0.0873436, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:53.748824: step 6783, loss 0.0764983, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:54.008918: step 6784, loss 0.0707755, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:54.234115: step 6785, loss 0.0368005, acc 1, learning_rate 0.0001
2017-10-10T15:23:54.444066: step 6786, loss 0.0406687, acc 1, learning_rate 0.0001
2017-10-10T15:23:54.678135: step 6787, loss 0.0509518, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:54.914777: step 6788, loss 0.053081, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:55.161668: step 6789, loss 0.0967377, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:55.434155: step 6790, loss 0.0589036, acc 1, learning_rate 0.0001
2017-10-10T15:23:55.650076: step 6791, loss 0.106399, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:55.848876: step 6792, loss 0.115758, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:56.096937: step 6793, loss 0.156913, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:56.262814: step 6794, loss 0.11421, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:56.540823: step 6795, loss 0.139596, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:56.800869: step 6796, loss 0.046787, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:57.085111: step 6797, loss 0.0976377, acc 0.953125, learning_rate 0.0001
2017-10-10T15:23:57.269165: step 6798, loss 0.130348, acc 0.9375, learning_rate 0.0001
2017-10-10T15:23:57.455950: step 6799, loss 0.0702087, acc 0.984375, learning_rate 0.0001
2017-10-10T15:23:57.668498: step 6800, loss 0.0472189, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:23:58.054379: step 6800, loss 0.215464, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6800

2017-10-10T15:23:59.089724: step 6801, loss 0.0382026, acc 1, learning_rate 0.0001
2017-10-10T15:23:59.323114: step 6802, loss 0.0851734, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:59.545168: step 6803, loss 0.0760703, acc 0.96875, learning_rate 0.0001
2017-10-10T15:23:59.821089: step 6804, loss 0.0929995, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:00.076200: step 6805, loss 0.0442357, acc 1, learning_rate 0.0001
2017-10-10T15:24:00.312450: step 6806, loss 0.0634364, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:00.540828: step 6807, loss 0.164771, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:00.772944: step 6808, loss 0.0592729, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:01.032970: step 6809, loss 0.0436343, acc 1, learning_rate 0.0001
2017-10-10T15:24:01.240928: step 6810, loss 0.0849282, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:01.505167: step 6811, loss 0.0926904, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:01.674225: step 6812, loss 0.0850352, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:01.944845: step 6813, loss 0.0501926, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:02.244454: step 6814, loss 0.0425596, acc 1, learning_rate 0.0001
2017-10-10T15:24:02.457650: step 6815, loss 0.0827658, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:02.630954: step 6816, loss 0.0407003, acc 1, learning_rate 0.0001
2017-10-10T15:24:02.835726: step 6817, loss 0.121856, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:03.016837: step 6818, loss 0.0702668, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:03.215939: step 6819, loss 0.0358036, acc 1, learning_rate 0.0001
2017-10-10T15:24:03.508913: step 6820, loss 0.108082, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:03.794812: step 6821, loss 0.101855, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:03.986796: step 6822, loss 0.0450244, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:04.203652: step 6823, loss 0.121335, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:04.356946: step 6824, loss 0.0295481, acc 1, learning_rate 0.0001
2017-10-10T15:24:04.582334: step 6825, loss 0.0547843, acc 1, learning_rate 0.0001
2017-10-10T15:24:04.784874: step 6826, loss 0.0688913, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:05.001583: step 6827, loss 0.145218, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:05.198046: step 6828, loss 0.0629466, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:05.431941: step 6829, loss 0.0849528, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:05.685157: step 6830, loss 0.0510693, acc 1, learning_rate 0.0001
2017-10-10T15:24:05.912265: step 6831, loss 0.0553436, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:06.178215: step 6832, loss 0.113555, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:06.381066: step 6833, loss 0.135141, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:06.602883: step 6834, loss 0.110438, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:06.879435: step 6835, loss 0.0553464, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:07.099779: step 6836, loss 0.0506585, acc 1, learning_rate 0.0001
2017-10-10T15:24:07.319816: step 6837, loss 0.134179, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:07.561020: step 6838, loss 0.0597858, acc 1, learning_rate 0.0001
2017-10-10T15:24:07.828932: step 6839, loss 0.13757, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:08.062962: step 6840, loss 0.0595354, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:08.405450: step 6840, loss 0.214068, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6840

2017-10-10T15:24:09.523297: step 6841, loss 0.085396, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:09.793019: step 6842, loss 0.0385844, acc 1, learning_rate 0.0001
2017-10-10T15:24:09.987231: step 6843, loss 0.122395, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:10.187196: step 6844, loss 0.0892837, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:10.456825: step 6845, loss 0.147801, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:10.729038: step 6846, loss 0.134603, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:10.912948: step 6847, loss 0.0279689, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:11.177029: step 6848, loss 0.119711, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:11.437037: step 6849, loss 0.150911, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:11.680559: step 6850, loss 0.158373, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:11.931300: step 6851, loss 0.165023, acc 0.921875, learning_rate 0.0001
2017-10-10T15:24:12.171913: step 6852, loss 0.0435565, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:12.436877: step 6853, loss 0.0968583, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:12.644931: step 6854, loss 0.18957, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:12.888926: step 6855, loss 0.138306, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:13.145838: step 6856, loss 0.0612883, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:13.372588: step 6857, loss 0.0957891, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:13.549041: step 6858, loss 0.0742884, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:13.838708: step 6859, loss 0.0471477, acc 1, learning_rate 0.0001
2017-10-10T15:24:14.021730: step 6860, loss 0.069161, acc 0.960784, learning_rate 0.0001
2017-10-10T15:24:14.228876: step 6861, loss 0.188708, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:14.489017: step 6862, loss 0.139081, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:14.704851: step 6863, loss 0.176375, acc 0.90625, learning_rate 0.0001
2017-10-10T15:24:14.925760: step 6864, loss 0.114966, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:15.217358: step 6865, loss 0.165496, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:15.442659: step 6866, loss 0.0325745, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:15.601552: step 6867, loss 0.0907398, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:15.852537: step 6868, loss 0.0417471, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:16.033252: step 6869, loss 0.07562, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:16.272892: step 6870, loss 0.0909176, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:16.533319: step 6871, loss 0.0770175, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:16.777018: step 6872, loss 0.0733368, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:17.033048: step 6873, loss 0.119836, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:17.322012: step 6874, loss 0.0969069, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:17.588724: step 6875, loss 0.097332, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:17.736228: step 6876, loss 0.0834039, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:17.872401: step 6877, loss 0.072505, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:18.008966: step 6878, loss 0.0856129, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:18.148818: step 6879, loss 0.105647, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:18.279951: step 6880, loss 0.0681407, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:18.683265: step 6880, loss 0.21393, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6880

2017-10-10T15:24:19.698113: step 6881, loss 0.11962, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:19.864873: step 6882, loss 0.158251, acc 0.921875, learning_rate 0.0001
2017-10-10T15:24:20.112341: step 6883, loss 0.0863457, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:20.345587: step 6884, loss 0.0929938, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:20.553118: step 6885, loss 0.105529, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:20.796833: step 6886, loss 0.116226, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:21.084846: step 6887, loss 0.172467, acc 0.90625, learning_rate 0.0001
2017-10-10T15:24:21.306927: step 6888, loss 0.172803, acc 0.921875, learning_rate 0.0001
2017-10-10T15:24:21.525041: step 6889, loss 0.0821924, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:21.759079: step 6890, loss 0.16783, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:21.931832: step 6891, loss 0.0602311, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:22.212210: step 6892, loss 0.0407819, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:22.496529: step 6893, loss 0.0667471, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:22.704571: step 6894, loss 0.138012, acc 0.921875, learning_rate 0.0001
2017-10-10T15:24:22.924368: step 6895, loss 0.0625833, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:23.183037: step 6896, loss 0.0331866, acc 1, learning_rate 0.0001
2017-10-10T15:24:23.448883: step 6897, loss 0.0927229, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:23.641070: step 6898, loss 0.0651739, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:23.876773: step 6899, loss 0.0413611, acc 1, learning_rate 0.0001
2017-10-10T15:24:24.088148: step 6900, loss 0.0570351, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:24.340899: step 6901, loss 0.0956885, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:24.568981: step 6902, loss 0.0707611, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:24.748777: step 6903, loss 0.129433, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:24.970669: step 6904, loss 0.0289469, acc 1, learning_rate 0.0001
2017-10-10T15:24:25.216004: step 6905, loss 0.0978464, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:25.400958: step 6906, loss 0.0632852, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:25.637670: step 6907, loss 0.143623, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:25.872686: step 6908, loss 0.0712516, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:26.076887: step 6909, loss 0.0537483, acc 1, learning_rate 0.0001
2017-10-10T15:24:26.311171: step 6910, loss 0.0720991, acc 1, learning_rate 0.0001
2017-10-10T15:24:26.481173: step 6911, loss 0.0746376, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:26.718669: step 6912, loss 0.0416572, acc 1, learning_rate 0.0001
2017-10-10T15:24:26.976919: step 6913, loss 0.104776, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:27.256866: step 6914, loss 0.0555463, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:27.480947: step 6915, loss 0.146455, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:27.708508: step 6916, loss 0.0886136, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:27.982909: step 6917, loss 0.052436, acc 1, learning_rate 0.0001
2017-10-10T15:24:28.251621: step 6918, loss 0.0342486, acc 1, learning_rate 0.0001
2017-10-10T15:24:28.426929: step 6919, loss 0.0415323, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:28.697122: step 6920, loss 0.0879022, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:29.112980: step 6920, loss 0.214077, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6920

2017-10-10T15:24:30.054762: step 6921, loss 0.044836, acc 1, learning_rate 0.0001
2017-10-10T15:24:30.273029: step 6922, loss 0.0589263, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:30.456577: step 6923, loss 0.0815311, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:30.709111: step 6924, loss 0.0530223, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:30.996240: step 6925, loss 0.101944, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:31.237066: step 6926, loss 0.122592, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:31.426820: step 6927, loss 0.133231, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:31.648778: step 6928, loss 0.134523, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:31.860947: step 6929, loss 0.0260787, acc 1, learning_rate 0.0001
2017-10-10T15:24:32.069820: step 6930, loss 0.112038, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:32.332603: step 6931, loss 0.0438465, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:32.512889: step 6932, loss 0.0437028, acc 1, learning_rate 0.0001
2017-10-10T15:24:32.675710: step 6933, loss 0.152342, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:32.872813: step 6934, loss 0.0737415, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:33.083633: step 6935, loss 0.0976318, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:33.253110: step 6936, loss 0.0407966, acc 1, learning_rate 0.0001
2017-10-10T15:24:33.513925: step 6937, loss 0.0770496, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:33.732601: step 6938, loss 0.0492482, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:33.996870: step 6939, loss 0.0498007, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:34.239860: step 6940, loss 0.0816472, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:34.480983: step 6941, loss 0.120016, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:34.725116: step 6942, loss 0.0742196, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:34.914275: step 6943, loss 0.100056, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:35.167194: step 6944, loss 0.0987748, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:35.377110: step 6945, loss 0.125677, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:35.617114: step 6946, loss 0.0721077, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:35.880935: step 6947, loss 0.101739, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:36.103056: step 6948, loss 0.0918937, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:36.331367: step 6949, loss 0.1585, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:36.561049: step 6950, loss 0.0491376, acc 1, learning_rate 0.0001
2017-10-10T15:24:36.806920: step 6951, loss 0.0740976, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:37.082402: step 6952, loss 0.0706598, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:37.314732: step 6953, loss 0.0721264, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:37.536959: step 6954, loss 0.0858696, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:37.772886: step 6955, loss 0.152496, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:38.012207: step 6956, loss 0.0215946, acc 1, learning_rate 0.0001
2017-10-10T15:24:38.256861: step 6957, loss 0.0748093, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:38.468930: step 6958, loss 0.0406526, acc 1, learning_rate 0.0001
2017-10-10T15:24:38.684960: step 6959, loss 0.0390211, acc 1, learning_rate 0.0001
2017-10-10T15:24:38.906367: step 6960, loss 0.0961112, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:39.278971: step 6960, loss 0.213932, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-6960

2017-10-10T15:24:40.269772: step 6961, loss 0.0627706, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:40.518186: step 6962, loss 0.074747, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:40.771031: step 6963, loss 0.0504583, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:41.012939: step 6964, loss 0.062367, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:41.276923: step 6965, loss 0.111565, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:41.476284: step 6966, loss 0.0841715, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:41.692831: step 6967, loss 0.0978669, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:41.911417: step 6968, loss 0.0631573, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:42.066879: step 6969, loss 0.162551, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:42.312802: step 6970, loss 0.0375947, acc 1, learning_rate 0.0001
2017-10-10T15:24:42.576287: step 6971, loss 0.0654145, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:42.796872: step 6972, loss 0.159968, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:42.966452: step 6973, loss 0.113446, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:43.184540: step 6974, loss 0.043932, acc 1, learning_rate 0.0001
2017-10-10T15:24:43.418984: step 6975, loss 0.0782854, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:43.690958: step 6976, loss 0.0437501, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:43.865960: step 6977, loss 0.0566754, acc 1, learning_rate 0.0001
2017-10-10T15:24:44.089555: step 6978, loss 0.0549425, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:44.380891: step 6979, loss 0.204498, acc 0.921875, learning_rate 0.0001
2017-10-10T15:24:44.677512: step 6980, loss 0.084878, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:44.868392: step 6981, loss 0.0445404, acc 1, learning_rate 0.0001
2017-10-10T15:24:45.056841: step 6982, loss 0.064727, acc 1, learning_rate 0.0001
2017-10-10T15:24:45.223088: step 6983, loss 0.0544807, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:45.435160: step 6984, loss 0.100868, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:45.651713: step 6985, loss 0.110795, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:45.929612: step 6986, loss 0.0762137, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:46.119662: step 6987, loss 0.0214445, acc 1, learning_rate 0.0001
2017-10-10T15:24:46.373813: step 6988, loss 0.046847, acc 1, learning_rate 0.0001
2017-10-10T15:24:46.622287: step 6989, loss 0.0749504, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:46.865221: step 6990, loss 0.0486839, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:47.152940: step 6991, loss 0.0601941, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:47.449202: step 6992, loss 0.0856209, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:47.653476: step 6993, loss 0.0207601, acc 1, learning_rate 0.0001
2017-10-10T15:24:47.839663: step 6994, loss 0.0927829, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:48.040306: step 6995, loss 0.0787539, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:48.191105: step 6996, loss 0.0409295, acc 1, learning_rate 0.0001
2017-10-10T15:24:48.422219: step 6997, loss 0.168836, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:48.637108: step 6998, loss 0.0284354, acc 1, learning_rate 0.0001
2017-10-10T15:24:48.896912: step 6999, loss 0.107602, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:49.133150: step 7000, loss 0.116516, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:49.500372: step 7000, loss 0.212959, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7000

2017-10-10T15:24:50.550598: step 7001, loss 0.085776, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:50.757794: step 7002, loss 0.0323594, acc 1, learning_rate 0.0001
2017-10-10T15:24:50.993576: step 7003, loss 0.0878416, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:51.200839: step 7004, loss 0.146485, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:51.443169: step 7005, loss 0.0799312, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:51.645999: step 7006, loss 0.125007, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:51.844879: step 7007, loss 0.0328002, acc 1, learning_rate 0.0001
2017-10-10T15:24:52.156838: step 7008, loss 0.0994381, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:52.372916: step 7009, loss 0.0995195, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:52.597193: step 7010, loss 0.0873233, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:52.820995: step 7011, loss 0.0844045, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:52.962691: step 7012, loss 0.0827101, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:53.248192: step 7013, loss 0.0811586, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:53.464887: step 7014, loss 0.104981, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:53.667440: step 7015, loss 0.116872, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:53.908829: step 7016, loss 0.0813117, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:54.147574: step 7017, loss 0.0733918, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:54.377197: step 7018, loss 0.08896, acc 0.9375, learning_rate 0.0001
2017-10-10T15:24:54.629337: step 7019, loss 0.0868161, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:54.880304: step 7020, loss 0.100985, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:55.099655: step 7021, loss 0.0888791, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:55.369036: step 7022, loss 0.0753306, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:55.589033: step 7023, loss 0.0709006, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:55.805784: step 7024, loss 0.102064, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:56.058783: step 7025, loss 0.0749864, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:56.284093: step 7026, loss 0.0927143, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:56.466236: step 7027, loss 0.13572, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:56.710994: step 7028, loss 0.177039, acc 0.953125, learning_rate 0.0001
2017-10-10T15:24:56.953339: step 7029, loss 0.0584155, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:57.208816: step 7030, loss 0.0490842, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:57.481039: step 7031, loss 0.0868187, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:57.736972: step 7032, loss 0.0839611, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:57.937385: step 7033, loss 0.101857, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:58.207596: step 7034, loss 0.192438, acc 0.921875, learning_rate 0.0001
2017-10-10T15:24:58.443352: step 7035, loss 0.0487244, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:58.594699: step 7036, loss 0.033968, acc 1, learning_rate 0.0001
2017-10-10T15:24:58.812479: step 7037, loss 0.0384312, acc 1, learning_rate 0.0001
2017-10-10T15:24:58.983547: step 7038, loss 0.0676419, acc 0.984375, learning_rate 0.0001
2017-10-10T15:24:59.181189: step 7039, loss 0.0774882, acc 0.96875, learning_rate 0.0001
2017-10-10T15:24:59.388843: step 7040, loss 0.0381989, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:24:59.780486: step 7040, loss 0.213491, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7040

2017-10-10T15:25:00.879209: step 7041, loss 0.118159, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:01.127532: step 7042, loss 0.0530678, acc 1, learning_rate 0.0001
2017-10-10T15:25:01.336499: step 7043, loss 0.0817796, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:01.589340: step 7044, loss 0.0915308, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:01.836932: step 7045, loss 0.163205, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:02.107124: step 7046, loss 0.0499148, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:02.400854: step 7047, loss 0.0723771, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:02.625962: step 7048, loss 0.0518364, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:02.824294: step 7049, loss 0.097944, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:02.969728: step 7050, loss 0.0970538, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:03.203070: step 7051, loss 0.143704, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:03.448705: step 7052, loss 0.0487848, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:03.648925: step 7053, loss 0.0761756, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:03.861158: step 7054, loss 0.142861, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:04.060284: step 7055, loss 0.207485, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:04.288847: step 7056, loss 0.0417544, acc 1, learning_rate 0.0001
2017-10-10T15:25:04.535035: step 7057, loss 0.043071, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:04.705843: step 7058, loss 0.0832931, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:04.972814: step 7059, loss 0.0846079, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:05.201202: step 7060, loss 0.0566579, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:05.428001: step 7061, loss 0.0314293, acc 1, learning_rate 0.0001
2017-10-10T15:25:05.698323: step 7062, loss 0.0389917, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:05.940825: step 7063, loss 0.0344562, acc 1, learning_rate 0.0001
2017-10-10T15:25:06.200725: step 7064, loss 0.0579498, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:06.456552: step 7065, loss 0.0265555, acc 1, learning_rate 0.0001
2017-10-10T15:25:06.709143: step 7066, loss 0.0409611, acc 1, learning_rate 0.0001
2017-10-10T15:25:06.885099: step 7067, loss 0.098241, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:07.139932: step 7068, loss 0.0904788, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:07.424814: step 7069, loss 0.0912526, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:07.660883: step 7070, loss 0.155175, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:07.893297: step 7071, loss 0.0718475, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:08.128243: step 7072, loss 0.0700602, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:08.389045: step 7073, loss 0.0497933, acc 1, learning_rate 0.0001
2017-10-10T15:25:08.602110: step 7074, loss 0.0703055, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:08.833849: step 7075, loss 0.0963008, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:09.084813: step 7076, loss 0.0466709, acc 1, learning_rate 0.0001
2017-10-10T15:25:09.291301: step 7077, loss 0.0723982, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:09.512827: step 7078, loss 0.125761, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:09.788404: step 7079, loss 0.111179, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:10.052880: step 7080, loss 0.116401, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:10.418667: step 7080, loss 0.213157, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7080

2017-10-10T15:25:11.289112: step 7081, loss 0.108894, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:11.543995: step 7082, loss 0.0611741, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:11.796979: step 7083, loss 0.0595314, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:12.065012: step 7084, loss 0.0789813, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:12.322559: step 7085, loss 0.0634845, acc 1, learning_rate 0.0001
2017-10-10T15:25:12.542116: step 7086, loss 0.024463, acc 1, learning_rate 0.0001
2017-10-10T15:25:12.771490: step 7087, loss 0.0399977, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:12.988719: step 7088, loss 0.0668497, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:13.172905: step 7089, loss 0.156904, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:13.410930: step 7090, loss 0.0634133, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:13.674279: step 7091, loss 0.0580827, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:13.913375: step 7092, loss 0.105843, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:14.112844: step 7093, loss 0.0843429, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:14.416639: step 7094, loss 0.0691397, acc 1, learning_rate 0.0001
2017-10-10T15:25:14.613345: step 7095, loss 0.0853252, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:14.788490: step 7096, loss 0.0559168, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:15.024847: step 7097, loss 0.0716291, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:15.271546: step 7098, loss 0.0967495, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:15.504541: step 7099, loss 0.0855816, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:15.717573: step 7100, loss 0.0887154, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:15.962686: step 7101, loss 0.0915645, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:16.181966: step 7102, loss 0.0920904, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:16.415220: step 7103, loss 0.126678, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:16.713972: step 7104, loss 0.170349, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:16.990235: step 7105, loss 0.0502825, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:17.187708: step 7106, loss 0.0764398, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:17.396118: step 7107, loss 0.0500554, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:17.608518: step 7108, loss 0.0565132, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:17.796844: step 7109, loss 0.0606799, acc 1, learning_rate 0.0001
2017-10-10T15:25:18.008828: step 7110, loss 0.0814494, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:18.270881: step 7111, loss 0.0940192, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:18.520692: step 7112, loss 0.0694167, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:18.728636: step 7113, loss 0.134665, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:18.993252: step 7114, loss 0.174085, acc 0.921875, learning_rate 0.0001
2017-10-10T15:25:19.149070: step 7115, loss 0.074775, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:19.404822: step 7116, loss 0.0593288, acc 1, learning_rate 0.0001
2017-10-10T15:25:19.684689: step 7117, loss 0.0864429, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:19.879504: step 7118, loss 0.0900116, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:20.079854: step 7119, loss 0.054348, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:20.320908: step 7120, loss 0.0784439, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:20.660598: step 7120, loss 0.210739, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7120

2017-10-10T15:25:21.724545: step 7121, loss 0.0624812, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:22.001342: step 7122, loss 0.12648, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:22.221897: step 7123, loss 0.0591717, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:22.441930: step 7124, loss 0.072564, acc 1, learning_rate 0.0001
2017-10-10T15:25:22.714095: step 7125, loss 0.0409034, acc 1, learning_rate 0.0001
2017-10-10T15:25:22.922066: step 7126, loss 0.0998413, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:23.109770: step 7127, loss 0.0845174, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:23.344250: step 7128, loss 0.0329439, acc 1, learning_rate 0.0001
2017-10-10T15:25:23.568596: step 7129, loss 0.0738548, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:23.850810: step 7130, loss 0.0385772, acc 1, learning_rate 0.0001
2017-10-10T15:25:24.093150: step 7131, loss 0.0689336, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:24.284875: step 7132, loss 0.0994848, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:24.539234: step 7133, loss 0.0933833, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:24.855357: step 7134, loss 0.165909, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:25.126813: step 7135, loss 0.100686, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:25.323586: step 7136, loss 0.152269, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:25.586334: step 7137, loss 0.0572292, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:25.863638: step 7138, loss 0.0851456, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:26.040862: step 7139, loss 0.051091, acc 1, learning_rate 0.0001
2017-10-10T15:25:26.192782: step 7140, loss 0.0697371, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:26.420813: step 7141, loss 0.0861079, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:26.637850: step 7142, loss 0.119245, acc 0.921875, learning_rate 0.0001
2017-10-10T15:25:26.855573: step 7143, loss 0.0746818, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:27.085211: step 7144, loss 0.0469925, acc 1, learning_rate 0.0001
2017-10-10T15:25:27.340228: step 7145, loss 0.0500595, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:27.600819: step 7146, loss 0.10779, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:27.829352: step 7147, loss 0.113308, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:28.045104: step 7148, loss 0.0753424, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:28.307086: step 7149, loss 0.114799, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:28.543350: step 7150, loss 0.115882, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:28.763558: step 7151, loss 0.060155, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:28.984142: step 7152, loss 0.162243, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:29.274701: step 7153, loss 0.0620952, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:29.435214: step 7154, loss 0.121914, acc 0.921569, learning_rate 0.0001
2017-10-10T15:25:29.687776: step 7155, loss 0.103528, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:29.937160: step 7156, loss 0.116262, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:30.149115: step 7157, loss 0.0888882, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:30.357369: step 7158, loss 0.0230719, acc 1, learning_rate 0.0001
2017-10-10T15:25:30.579570: step 7159, loss 0.142824, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:30.744721: step 7160, loss 0.0498763, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:31.155275: step 7160, loss 0.213011, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7160

2017-10-10T15:25:32.243605: step 7161, loss 0.0793979, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:32.447811: step 7162, loss 0.0562506, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:32.624927: step 7163, loss 0.0593466, acc 1, learning_rate 0.0001
2017-10-10T15:25:32.805686: step 7164, loss 0.0861919, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:33.000908: step 7165, loss 0.0788375, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:33.276833: step 7166, loss 0.0918289, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:33.536478: step 7167, loss 0.0602537, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:33.776845: step 7168, loss 0.0731474, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:34.056989: step 7169, loss 0.107017, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:34.320830: step 7170, loss 0.0781452, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:34.558555: step 7171, loss 0.109534, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:34.761129: step 7172, loss 0.0556203, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:35.050342: step 7173, loss 0.0522341, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:35.316819: step 7174, loss 0.0441475, acc 1, learning_rate 0.0001
2017-10-10T15:25:35.534170: step 7175, loss 0.0552328, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:35.760813: step 7176, loss 0.049938, acc 1, learning_rate 0.0001
2017-10-10T15:25:36.028850: step 7177, loss 0.0284889, acc 1, learning_rate 0.0001
2017-10-10T15:25:36.343989: step 7178, loss 0.0865596, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:36.485128: step 7179, loss 0.0692728, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:36.699743: step 7180, loss 0.0480162, acc 1, learning_rate 0.0001
2017-10-10T15:25:36.905455: step 7181, loss 0.0904508, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:37.088746: step 7182, loss 0.0892443, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:37.330433: step 7183, loss 0.0770102, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:37.580882: step 7184, loss 0.115209, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:37.777936: step 7185, loss 0.0800011, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:38.018717: step 7186, loss 0.0896918, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:38.255988: step 7187, loss 0.140774, acc 0.921875, learning_rate 0.0001
2017-10-10T15:25:38.436909: step 7188, loss 0.0945589, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:38.672911: step 7189, loss 0.0294874, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:38.948843: step 7190, loss 0.0619877, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:39.272863: step 7191, loss 0.0414811, acc 1, learning_rate 0.0001
2017-10-10T15:25:39.596807: step 7192, loss 0.115141, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:39.836896: step 7193, loss 0.0711314, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:40.028980: step 7194, loss 0.0124185, acc 1, learning_rate 0.0001
2017-10-10T15:25:40.220981: step 7195, loss 0.0294718, acc 1, learning_rate 0.0001
2017-10-10T15:25:40.477146: step 7196, loss 0.107378, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:40.705189: step 7197, loss 0.0571654, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:40.916151: step 7198, loss 0.110226, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:41.192845: step 7199, loss 0.0437856, acc 1, learning_rate 0.0001
2017-10-10T15:25:41.407121: step 7200, loss 0.134278, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:41.762941: step 7200, loss 0.213444, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7200

2017-10-10T15:25:42.812763: step 7201, loss 0.0634512, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:43.040843: step 7202, loss 0.0745486, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:43.295206: step 7203, loss 0.0526734, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:43.512945: step 7204, loss 0.101635, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:43.749033: step 7205, loss 0.0651034, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:43.973471: step 7206, loss 0.0476178, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:44.220289: step 7207, loss 0.0387495, acc 1, learning_rate 0.0001
2017-10-10T15:25:44.397175: step 7208, loss 0.105777, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:44.666391: step 7209, loss 0.0872648, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:44.862716: step 7210, loss 0.0843512, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:45.086243: step 7211, loss 0.139659, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:45.347096: step 7212, loss 0.0341919, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:45.595864: step 7213, loss 0.0992507, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:45.839656: step 7214, loss 0.164865, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:46.133209: step 7215, loss 0.0680863, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:46.365690: step 7216, loss 0.0532818, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:46.628863: step 7217, loss 0.142131, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:46.901008: step 7218, loss 0.0886944, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:47.176033: step 7219, loss 0.061269, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:47.392821: step 7220, loss 0.0660848, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:47.534768: step 7221, loss 0.0853264, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:47.668835: step 7222, loss 0.0482126, acc 1, learning_rate 0.0001
2017-10-10T15:25:47.837928: step 7223, loss 0.0585318, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:48.010416: step 7224, loss 0.101867, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:48.190126: step 7225, loss 0.0354337, acc 1, learning_rate 0.0001
2017-10-10T15:25:48.396037: step 7226, loss 0.0732601, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:48.609169: step 7227, loss 0.0999728, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:48.838300: step 7228, loss 0.0682929, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:49.069840: step 7229, loss 0.0880542, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:49.306593: step 7230, loss 0.0614362, acc 1, learning_rate 0.0001
2017-10-10T15:25:49.557247: step 7231, loss 0.0482939, acc 1, learning_rate 0.0001
2017-10-10T15:25:49.793293: step 7232, loss 0.048352, acc 1, learning_rate 0.0001
2017-10-10T15:25:50.004850: step 7233, loss 0.0657551, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:50.269051: step 7234, loss 0.144536, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:50.468999: step 7235, loss 0.0605698, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:50.666864: step 7236, loss 0.134324, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:50.916836: step 7237, loss 0.148248, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:51.148073: step 7238, loss 0.0898098, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:51.357747: step 7239, loss 0.10072, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:51.592474: step 7240, loss 0.0625784, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:25:52.020863: step 7240, loss 0.214077, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7240

2017-10-10T15:25:52.919907: step 7241, loss 0.0458454, acc 1, learning_rate 0.0001
2017-10-10T15:25:53.195034: step 7242, loss 0.0692011, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:53.351793: step 7243, loss 0.0702001, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:53.552934: step 7244, loss 0.0858608, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:53.717382: step 7245, loss 0.0717575, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:53.876478: step 7246, loss 0.0499129, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:54.110156: step 7247, loss 0.0412945, acc 1, learning_rate 0.0001
2017-10-10T15:25:54.324864: step 7248, loss 0.0635251, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:54.530442: step 7249, loss 0.040913, acc 1, learning_rate 0.0001
2017-10-10T15:25:54.770926: step 7250, loss 0.0794221, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:55.025147: step 7251, loss 0.0425614, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:55.231860: step 7252, loss 0.0893957, acc 0.980392, learning_rate 0.0001
2017-10-10T15:25:55.496448: step 7253, loss 0.102373, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:55.763544: step 7254, loss 0.096328, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:55.961246: step 7255, loss 0.156932, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:56.208707: step 7256, loss 0.0862419, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:56.465049: step 7257, loss 0.168076, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:56.689555: step 7258, loss 0.0435803, acc 1, learning_rate 0.0001
2017-10-10T15:25:56.904917: step 7259, loss 0.0534537, acc 1, learning_rate 0.0001
2017-10-10T15:25:57.131974: step 7260, loss 0.138599, acc 0.9375, learning_rate 0.0001
2017-10-10T15:25:57.328057: step 7261, loss 0.0828672, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:57.566921: step 7262, loss 0.0601783, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:57.840846: step 7263, loss 0.146248, acc 0.953125, learning_rate 0.0001
2017-10-10T15:25:58.057795: step 7264, loss 0.0177585, acc 1, learning_rate 0.0001
2017-10-10T15:25:58.270508: step 7265, loss 0.0716541, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:58.555232: step 7266, loss 0.0691863, acc 0.96875, learning_rate 0.0001
2017-10-10T15:25:58.763833: step 7267, loss 0.0469336, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:58.918645: step 7268, loss 0.0527958, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:59.156891: step 7269, loss 0.0395926, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:59.330859: step 7270, loss 0.0619259, acc 0.984375, learning_rate 0.0001
2017-10-10T15:25:59.540822: step 7271, loss 0.0357876, acc 1, learning_rate 0.0001
2017-10-10T15:25:59.796025: step 7272, loss 0.0506675, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:00.034490: step 7273, loss 0.0930858, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:00.276851: step 7274, loss 0.0750717, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:00.485035: step 7275, loss 0.0635421, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:00.736831: step 7276, loss 0.0806959, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:00.948908: step 7277, loss 0.0504028, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:01.149017: step 7278, loss 0.0407814, acc 1, learning_rate 0.0001
2017-10-10T15:26:01.412942: step 7279, loss 0.0364726, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:01.665649: step 7280, loss 0.130823, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:02.011538: step 7280, loss 0.213611, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7280

2017-10-10T15:26:02.949963: step 7281, loss 0.07581, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:03.146697: step 7282, loss 0.12729, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:03.383672: step 7283, loss 0.0737421, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:03.626560: step 7284, loss 0.0515825, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:03.826219: step 7285, loss 0.0597275, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:04.070902: step 7286, loss 0.235397, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:04.309576: step 7287, loss 0.0421776, acc 1, learning_rate 0.0001
2017-10-10T15:26:04.529189: step 7288, loss 0.0487706, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:04.750898: step 7289, loss 0.0748277, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:05.025005: step 7290, loss 0.0996646, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:05.207999: step 7291, loss 0.0866842, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:05.446592: step 7292, loss 0.0781685, acc 1, learning_rate 0.0001
2017-10-10T15:26:05.706832: step 7293, loss 0.15725, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:05.881345: step 7294, loss 0.0181903, acc 1, learning_rate 0.0001
2017-10-10T15:26:06.128819: step 7295, loss 0.120917, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:06.435987: step 7296, loss 0.0568284, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:06.705195: step 7297, loss 0.102408, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:06.884812: step 7298, loss 0.0768391, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:07.121136: step 7299, loss 0.0725511, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:07.319356: step 7300, loss 0.105563, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:07.527479: step 7301, loss 0.103699, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:07.740975: step 7302, loss 0.0733135, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:07.960957: step 7303, loss 0.0387442, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:08.232746: step 7304, loss 0.0707531, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:08.450449: step 7305, loss 0.116047, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:08.688611: step 7306, loss 0.156082, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:08.906392: step 7307, loss 0.0984667, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:09.149320: step 7308, loss 0.0542399, acc 1, learning_rate 0.0001
2017-10-10T15:26:09.399084: step 7309, loss 0.0429059, acc 1, learning_rate 0.0001
2017-10-10T15:26:09.636842: step 7310, loss 0.068959, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:09.842554: step 7311, loss 0.0793401, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:10.034707: step 7312, loss 0.049038, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:10.314529: step 7313, loss 0.0672463, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:10.581222: step 7314, loss 0.0938142, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:10.778340: step 7315, loss 0.0648329, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:11.011285: step 7316, loss 0.0954862, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:11.284145: step 7317, loss 0.129686, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:11.508970: step 7318, loss 0.0486856, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:11.784866: step 7319, loss 0.0479923, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:12.017127: step 7320, loss 0.102886, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:12.336480: step 7320, loss 0.21452, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7320

2017-10-10T15:26:13.465033: step 7321, loss 0.0603451, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:13.706278: step 7322, loss 0.0425094, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:13.932938: step 7323, loss 0.0767881, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:14.125092: step 7324, loss 0.0551767, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:14.390343: step 7325, loss 0.0577467, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:14.652251: step 7326, loss 0.037932, acc 1, learning_rate 0.0001
2017-10-10T15:26:14.859692: step 7327, loss 0.107079, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:15.093144: step 7328, loss 0.065828, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:15.336785: step 7329, loss 0.0787835, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:15.582335: step 7330, loss 0.0586294, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:15.807000: step 7331, loss 0.0700485, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:16.007012: step 7332, loss 0.101784, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:16.264741: step 7333, loss 0.0385528, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:16.550758: step 7334, loss 0.0996102, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:16.816704: step 7335, loss 0.04848, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:16.987935: step 7336, loss 0.0392697, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:17.176803: step 7337, loss 0.0928698, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:17.392661: step 7338, loss 0.0976551, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:17.572022: step 7339, loss 0.155091, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:17.789639: step 7340, loss 0.0896369, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:18.003259: step 7341, loss 0.0964444, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:18.286899: step 7342, loss 0.225355, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:18.481105: step 7343, loss 0.0628987, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:18.680844: step 7344, loss 0.0479688, acc 1, learning_rate 0.0001
2017-10-10T15:26:18.950372: step 7345, loss 0.0435581, acc 1, learning_rate 0.0001
2017-10-10T15:26:19.144984: step 7346, loss 0.0518825, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:19.352830: step 7347, loss 0.12292, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:19.596765: step 7348, loss 0.0691476, acc 1, learning_rate 0.0001
2017-10-10T15:26:19.804547: step 7349, loss 0.0361315, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:20.010133: step 7350, loss 0.0458649, acc 0.980392, learning_rate 0.0001
2017-10-10T15:26:20.254859: step 7351, loss 0.0574918, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:20.494893: step 7352, loss 0.196373, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:20.708827: step 7353, loss 0.0443357, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:20.853193: step 7354, loss 0.210324, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:21.012598: step 7355, loss 0.093705, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:21.255760: step 7356, loss 0.0595773, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:21.456855: step 7357, loss 0.177262, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:21.696396: step 7358, loss 0.147602, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:21.965861: step 7359, loss 0.137668, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:22.247747: step 7360, loss 0.0181256, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:22.662591: step 7360, loss 0.214242, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7360

2017-10-10T15:26:23.603859: step 7361, loss 0.109729, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:23.855570: step 7362, loss 0.0614982, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:24.069219: step 7363, loss 0.0628046, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:24.281666: step 7364, loss 0.115085, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:24.529267: step 7365, loss 0.0542088, acc 1, learning_rate 0.0001
2017-10-10T15:26:24.800885: step 7366, loss 0.0549883, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:25.053206: step 7367, loss 0.075932, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:25.281376: step 7368, loss 0.0712435, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:25.483404: step 7369, loss 0.026987, acc 1, learning_rate 0.0001
2017-10-10T15:26:25.744844: step 7370, loss 0.0655783, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:25.972899: step 7371, loss 0.149044, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:26.200762: step 7372, loss 0.085859, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:26.427903: step 7373, loss 0.0726973, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:26.673322: step 7374, loss 0.0754629, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:26.945274: step 7375, loss 0.0332429, acc 1, learning_rate 0.0001
2017-10-10T15:26:27.139414: step 7376, loss 0.147364, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:27.388115: step 7377, loss 0.115906, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:27.606984: step 7378, loss 0.0908276, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:27.816981: step 7379, loss 0.0850966, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:28.049949: step 7380, loss 0.058542, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:28.269373: step 7381, loss 0.0910266, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:28.552887: step 7382, loss 0.0364792, acc 1, learning_rate 0.0001
2017-10-10T15:26:28.812747: step 7383, loss 0.0801206, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:29.018805: step 7384, loss 0.0404537, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:29.248853: step 7385, loss 0.0917425, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:29.511509: step 7386, loss 0.0883394, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:29.764365: step 7387, loss 0.091552, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:29.892147: step 7388, loss 0.0785601, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:30.140501: step 7389, loss 0.0305875, acc 1, learning_rate 0.0001
2017-10-10T15:26:30.370615: step 7390, loss 0.0891028, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:30.585299: step 7391, loss 0.0607831, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:30.830600: step 7392, loss 0.201809, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:31.127339: step 7393, loss 0.105436, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:31.385287: step 7394, loss 0.167186, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:31.623814: step 7395, loss 0.0479311, acc 1, learning_rate 0.0001
2017-10-10T15:26:31.800272: step 7396, loss 0.123239, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:31.920567: step 7397, loss 0.0774655, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:32.044901: step 7398, loss 0.148538, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:32.211510: step 7399, loss 0.142565, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:32.436443: step 7400, loss 0.105941, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:32.805649: step 7400, loss 0.212988, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7400

2017-10-10T15:26:33.712885: step 7401, loss 0.0830162, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:34.037203: step 7402, loss 0.0567332, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:34.303401: step 7403, loss 0.0658934, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:34.467168: step 7404, loss 0.0573479, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:34.667472: step 7405, loss 0.0597525, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:34.863397: step 7406, loss 0.0891776, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:35.032860: step 7407, loss 0.0911932, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:35.234646: step 7408, loss 0.0414887, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:35.469018: step 7409, loss 0.0827057, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:35.692981: step 7410, loss 0.0304932, acc 1, learning_rate 0.0001
2017-10-10T15:26:35.897580: step 7411, loss 0.117992, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:36.180029: step 7412, loss 0.127085, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:36.427269: step 7413, loss 0.055302, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:36.695928: step 7414, loss 0.147245, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:36.896026: step 7415, loss 0.0990667, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:37.149468: step 7416, loss 0.0736414, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:37.392856: step 7417, loss 0.0613872, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:37.635752: step 7418, loss 0.0669454, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:37.892843: step 7419, loss 0.0793055, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:38.139765: step 7420, loss 0.0338901, acc 1, learning_rate 0.0001
2017-10-10T15:26:38.332867: step 7421, loss 0.0957209, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:38.546270: step 7422, loss 0.111487, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:38.758809: step 7423, loss 0.153694, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:38.949749: step 7424, loss 0.0833317, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:39.192558: step 7425, loss 0.067801, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:39.446001: step 7426, loss 0.0767318, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:39.669673: step 7427, loss 0.0577456, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:39.868034: step 7428, loss 0.0445999, acc 1, learning_rate 0.0001
2017-10-10T15:26:40.156812: step 7429, loss 0.159272, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:40.428835: step 7430, loss 0.0288933, acc 1, learning_rate 0.0001
2017-10-10T15:26:40.664850: step 7431, loss 0.123167, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:40.945192: step 7432, loss 0.083309, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:41.199107: step 7433, loss 0.0690337, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:41.372158: step 7434, loss 0.0465529, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:41.602251: step 7435, loss 0.0384035, acc 1, learning_rate 0.0001
2017-10-10T15:26:41.835358: step 7436, loss 0.100663, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:42.006515: step 7437, loss 0.0754543, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:42.268023: step 7438, loss 0.0377795, acc 1, learning_rate 0.0001
2017-10-10T15:26:42.553814: step 7439, loss 0.0652444, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:42.815343: step 7440, loss 0.0810908, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:43.177208: step 7440, loss 0.214045, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7440

2017-10-10T15:26:44.183641: step 7441, loss 0.0251046, acc 1, learning_rate 0.0001
2017-10-10T15:26:44.394954: step 7442, loss 0.065435, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:44.621590: step 7443, loss 0.096722, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:44.879515: step 7444, loss 0.0651384, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:45.107538: step 7445, loss 0.0248972, acc 1, learning_rate 0.0001
2017-10-10T15:26:45.306174: step 7446, loss 0.0732178, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:45.547272: step 7447, loss 0.0503476, acc 1, learning_rate 0.0001
2017-10-10T15:26:45.796843: step 7448, loss 0.0821459, acc 0.960784, learning_rate 0.0001
2017-10-10T15:26:46.085063: step 7449, loss 0.0269348, acc 1, learning_rate 0.0001
2017-10-10T15:26:46.342468: step 7450, loss 0.0783635, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:46.525576: step 7451, loss 0.0694084, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:46.758459: step 7452, loss 0.0525457, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:46.951795: step 7453, loss 0.0654633, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:47.083307: step 7454, loss 0.0597302, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:47.272862: step 7455, loss 0.179161, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:47.544895: step 7456, loss 0.109147, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:47.828996: step 7457, loss 0.0989739, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:48.003771: step 7458, loss 0.112103, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:48.219786: step 7459, loss 0.039959, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:48.384979: step 7460, loss 0.0313841, acc 1, learning_rate 0.0001
2017-10-10T15:26:48.561102: step 7461, loss 0.0765309, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:48.754641: step 7462, loss 0.0571448, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:49.003516: step 7463, loss 0.0805186, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:49.235433: step 7464, loss 0.060033, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:49.459091: step 7465, loss 0.0540518, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:49.701318: step 7466, loss 0.0556688, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:49.941427: step 7467, loss 0.0306974, acc 1, learning_rate 0.0001
2017-10-10T15:26:50.172172: step 7468, loss 0.104963, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:50.359432: step 7469, loss 0.0407699, acc 1, learning_rate 0.0001
2017-10-10T15:26:50.613105: step 7470, loss 0.042329, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:50.841020: step 7471, loss 0.0605749, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:51.047221: step 7472, loss 0.0416683, acc 1, learning_rate 0.0001
2017-10-10T15:26:51.291225: step 7473, loss 0.0624728, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:51.494197: step 7474, loss 0.0554898, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:51.718490: step 7475, loss 0.0693736, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:51.997801: step 7476, loss 0.052679, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:52.223616: step 7477, loss 0.0389207, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:52.448485: step 7478, loss 0.177375, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:52.695867: step 7479, loss 0.0568224, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:52.975665: step 7480, loss 0.0784689, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:26:53.384898: step 7480, loss 0.214478, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7480

2017-10-10T15:26:54.633991: step 7481, loss 0.0531343, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:54.880916: step 7482, loss 0.098853, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:55.120847: step 7483, loss 0.0540876, acc 1, learning_rate 0.0001
2017-10-10T15:26:55.383192: step 7484, loss 0.062783, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:55.632281: step 7485, loss 0.0525647, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:55.876871: step 7486, loss 0.0685656, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:56.144827: step 7487, loss 0.061266, acc 1, learning_rate 0.0001
2017-10-10T15:26:56.427518: step 7488, loss 0.0755756, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:56.637094: step 7489, loss 0.0258929, acc 1, learning_rate 0.0001
2017-10-10T15:26:56.854528: step 7490, loss 0.073018, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:57.121854: step 7491, loss 0.0664412, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:57.323334: step 7492, loss 0.10218, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:57.537039: step 7493, loss 0.0454099, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:57.780091: step 7494, loss 0.0535397, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:58.012169: step 7495, loss 0.0431581, acc 1, learning_rate 0.0001
2017-10-10T15:26:58.208297: step 7496, loss 0.0838522, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:58.436855: step 7497, loss 0.0423664, acc 0.984375, learning_rate 0.0001
2017-10-10T15:26:58.648843: step 7498, loss 0.0922916, acc 0.953125, learning_rate 0.0001
2017-10-10T15:26:58.866637: step 7499, loss 0.0722644, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:59.080093: step 7500, loss 0.196496, acc 0.921875, learning_rate 0.0001
2017-10-10T15:26:59.327164: step 7501, loss 0.100923, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:59.608808: step 7502, loss 0.0688584, acc 0.96875, learning_rate 0.0001
2017-10-10T15:26:59.796289: step 7503, loss 0.174581, acc 0.9375, learning_rate 0.0001
2017-10-10T15:26:59.994792: step 7504, loss 0.115965, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:00.260774: step 7505, loss 0.0654654, acc 1, learning_rate 0.0001
2017-10-10T15:27:00.485318: step 7506, loss 0.0810124, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:00.680924: step 7507, loss 0.095383, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:00.947982: step 7508, loss 0.138884, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:01.222580: step 7509, loss 0.0918916, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:01.520155: step 7510, loss 0.142199, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:01.708423: step 7511, loss 0.0771514, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:01.856374: step 7512, loss 0.0445707, acc 1, learning_rate 0.0001
2017-10-10T15:27:01.999921: step 7513, loss 0.033213, acc 1, learning_rate 0.0001
2017-10-10T15:27:02.144786: step 7514, loss 0.101176, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:02.349816: step 7515, loss 0.0510341, acc 1, learning_rate 0.0001
2017-10-10T15:27:02.502110: step 7516, loss 0.134153, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:02.740989: step 7517, loss 0.0527172, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:03.008950: step 7518, loss 0.0433417, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:03.181098: step 7519, loss 0.0307167, acc 1, learning_rate 0.0001
2017-10-10T15:27:03.412354: step 7520, loss 0.098115, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:03.808478: step 7520, loss 0.213554, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7520

2017-10-10T15:27:04.734313: step 7521, loss 0.0307981, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:04.912854: step 7522, loss 0.11315, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:05.078437: step 7523, loss 0.0804318, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:05.320868: step 7524, loss 0.0807939, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:05.528863: step 7525, loss 0.0399517, acc 1, learning_rate 0.0001
2017-10-10T15:27:05.744913: step 7526, loss 0.0724123, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:06.026645: step 7527, loss 0.0999636, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:06.233041: step 7528, loss 0.0592158, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:06.459660: step 7529, loss 0.181991, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:06.719612: step 7530, loss 0.0615517, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:06.996837: step 7531, loss 0.0878473, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:07.257066: step 7532, loss 0.164502, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:07.513277: step 7533, loss 0.0427061, acc 1, learning_rate 0.0001
2017-10-10T15:27:07.769206: step 7534, loss 0.101021, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:08.026729: step 7535, loss 0.139281, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:08.247468: step 7536, loss 0.0427874, acc 1, learning_rate 0.0001
2017-10-10T15:27:08.456806: step 7537, loss 0.0538444, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:08.712280: step 7538, loss 0.0629488, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:08.899912: step 7539, loss 0.0456868, acc 1, learning_rate 0.0001
2017-10-10T15:27:09.133811: step 7540, loss 0.0929001, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:09.369267: step 7541, loss 0.0759439, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:09.615822: step 7542, loss 0.0604469, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:09.834446: step 7543, loss 0.10245, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:10.080074: step 7544, loss 0.0447711, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:10.349030: step 7545, loss 0.116083, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:10.521650: step 7546, loss 0.0988331, acc 0.941176, learning_rate 0.0001
2017-10-10T15:27:10.748895: step 7547, loss 0.101062, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:10.919897: step 7548, loss 0.0715109, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:11.173960: step 7549, loss 0.068771, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:11.415037: step 7550, loss 0.0357497, acc 1, learning_rate 0.0001
2017-10-10T15:27:11.606802: step 7551, loss 0.0957594, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:11.848046: step 7552, loss 0.0900574, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:12.101010: step 7553, loss 0.140173, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:12.312857: step 7554, loss 0.0333934, acc 1, learning_rate 0.0001
2017-10-10T15:27:12.555408: step 7555, loss 0.164595, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:12.815727: step 7556, loss 0.0601329, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:13.005487: step 7557, loss 0.130204, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:13.227694: step 7558, loss 0.0451562, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:13.431594: step 7559, loss 0.101361, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:13.661192: step 7560, loss 0.0697092, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:14.053272: step 7560, loss 0.213412, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7560

2017-10-10T15:27:15.117196: step 7561, loss 0.0607561, acc 1, learning_rate 0.0001
2017-10-10T15:27:15.388568: step 7562, loss 0.0595611, acc 1, learning_rate 0.0001
2017-10-10T15:27:15.603532: step 7563, loss 0.0307205, acc 1, learning_rate 0.0001
2017-10-10T15:27:15.738240: step 7564, loss 0.110979, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:15.968833: step 7565, loss 0.134297, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:16.180135: step 7566, loss 0.0591705, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:16.407585: step 7567, loss 0.0664724, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:16.577326: step 7568, loss 0.0502157, acc 1, learning_rate 0.0001
2017-10-10T15:27:16.746251: step 7569, loss 0.0525518, acc 1, learning_rate 0.0001
2017-10-10T15:27:16.937958: step 7570, loss 0.183395, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:17.184260: step 7571, loss 0.099028, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:17.369318: step 7572, loss 0.0657863, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:17.601000: step 7573, loss 0.0379624, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:17.808067: step 7574, loss 0.0858345, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:18.024879: step 7575, loss 0.0521574, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:18.262017: step 7576, loss 0.115767, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:18.483371: step 7577, loss 0.0636338, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:18.749040: step 7578, loss 0.0866555, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:18.961610: step 7579, loss 0.0773418, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:19.199469: step 7580, loss 0.0999655, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:19.453305: step 7581, loss 0.0437023, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:19.653022: step 7582, loss 0.02384, acc 1, learning_rate 0.0001
2017-10-10T15:27:19.907268: step 7583, loss 0.0584773, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:20.125466: step 7584, loss 0.0726044, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:20.373301: step 7585, loss 0.108296, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:20.611887: step 7586, loss 0.136985, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:20.792629: step 7587, loss 0.0520158, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:21.029113: step 7588, loss 0.142118, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:21.243778: step 7589, loss 0.0566322, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:21.456848: step 7590, loss 0.0712313, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:21.720944: step 7591, loss 0.0807866, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:21.927804: step 7592, loss 0.0519195, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:22.173245: step 7593, loss 0.0750079, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:22.397338: step 7594, loss 0.0606489, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:22.621119: step 7595, loss 0.162173, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:22.908929: step 7596, loss 0.0701451, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:23.102598: step 7597, loss 0.114648, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:23.310780: step 7598, loss 0.111364, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:23.594921: step 7599, loss 0.13303, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:23.819965: step 7600, loss 0.0212801, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:24.191589: step 7600, loss 0.211847, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7600

2017-10-10T15:27:25.244816: step 7601, loss 0.110224, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:25.479148: step 7602, loss 0.173111, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:25.671439: step 7603, loss 0.077841, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:25.921963: step 7604, loss 0.196195, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:26.199834: step 7605, loss 0.0776302, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:26.365029: step 7606, loss 0.121976, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:26.582363: step 7607, loss 0.082131, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:26.769788: step 7608, loss 0.0457128, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:26.978314: step 7609, loss 0.101983, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:27.232846: step 7610, loss 0.0747179, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:27.420363: step 7611, loss 0.0485218, acc 1, learning_rate 0.0001
2017-10-10T15:27:27.667558: step 7612, loss 0.0332194, acc 1, learning_rate 0.0001
2017-10-10T15:27:27.922693: step 7613, loss 0.0818056, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:28.130362: step 7614, loss 0.0563666, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:28.349327: step 7615, loss 0.108149, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:28.616415: step 7616, loss 0.0371229, acc 1, learning_rate 0.0001
2017-10-10T15:27:28.873434: step 7617, loss 0.0467784, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:29.125772: step 7618, loss 0.0884274, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:29.353360: step 7619, loss 0.0668867, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:29.515368: step 7620, loss 0.0540732, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:29.708669: step 7621, loss 0.0648539, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:29.911281: step 7622, loss 0.158753, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:30.082303: step 7623, loss 0.113748, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:30.300952: step 7624, loss 0.056856, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:30.562038: step 7625, loss 0.072516, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:30.821013: step 7626, loss 0.118567, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:31.060659: step 7627, loss 0.100234, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:31.249620: step 7628, loss 0.0875209, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:31.403325: step 7629, loss 0.0493502, acc 1, learning_rate 0.0001
2017-10-10T15:27:31.615512: step 7630, loss 0.072609, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:31.815632: step 7631, loss 0.10063, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:32.060976: step 7632, loss 0.0811045, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:32.269131: step 7633, loss 0.111444, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:32.502770: step 7634, loss 0.112439, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:32.749208: step 7635, loss 0.112526, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:32.961649: step 7636, loss 0.113445, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:33.176978: step 7637, loss 0.0411019, acc 1, learning_rate 0.0001
2017-10-10T15:27:33.404858: step 7638, loss 0.036667, acc 1, learning_rate 0.0001
2017-10-10T15:27:33.656932: step 7639, loss 0.0599195, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:33.906426: step 7640, loss 0.0860296, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:34.267335: step 7640, loss 0.211925, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7640

2017-10-10T15:27:35.308953: step 7641, loss 0.104618, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:35.573014: step 7642, loss 0.0350092, acc 1, learning_rate 0.0001
2017-10-10T15:27:35.831922: step 7643, loss 0.0952135, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:36.011067: step 7644, loss 0.133474, acc 0.941176, learning_rate 0.0001
2017-10-10T15:27:36.249007: step 7645, loss 0.0532811, acc 1, learning_rate 0.0001
2017-10-10T15:27:36.473200: step 7646, loss 0.104895, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:36.689725: step 7647, loss 0.123188, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:36.938446: step 7648, loss 0.0186011, acc 1, learning_rate 0.0001
2017-10-10T15:27:37.221353: step 7649, loss 0.147166, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:37.468409: step 7650, loss 0.108665, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:37.659879: step 7651, loss 0.0833271, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:37.846074: step 7652, loss 0.0733473, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:38.120860: step 7653, loss 0.0236758, acc 1, learning_rate 0.0001
2017-10-10T15:27:38.302752: step 7654, loss 0.0639365, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:38.534070: step 7655, loss 0.0291696, acc 1, learning_rate 0.0001
2017-10-10T15:27:38.788865: step 7656, loss 0.0974605, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:38.987403: step 7657, loss 0.0781492, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:39.252300: step 7658, loss 0.080216, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:39.503076: step 7659, loss 0.0323761, acc 1, learning_rate 0.0001
2017-10-10T15:27:39.724871: step 7660, loss 0.0324939, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:39.951208: step 7661, loss 0.0643353, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:40.223687: step 7662, loss 0.0349594, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:40.430476: step 7663, loss 0.0851068, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:40.634925: step 7664, loss 0.0418252, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:40.919677: step 7665, loss 0.20286, acc 0.90625, learning_rate 0.0001
2017-10-10T15:27:41.178543: step 7666, loss 0.10007, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:41.357729: step 7667, loss 0.039829, acc 1, learning_rate 0.0001
2017-10-10T15:27:41.581323: step 7668, loss 0.098295, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:41.825071: step 7669, loss 0.0616225, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:42.036437: step 7670, loss 0.060878, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:42.245585: step 7671, loss 0.080853, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:42.502252: step 7672, loss 0.0594142, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:42.780632: step 7673, loss 0.0855929, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:42.987202: step 7674, loss 0.0628203, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:43.171856: step 7675, loss 0.0711594, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:43.384065: step 7676, loss 0.0641704, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:43.577242: step 7677, loss 0.0693066, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:43.790978: step 7678, loss 0.0750004, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:44.020879: step 7679, loss 0.0932335, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:44.205315: step 7680, loss 0.0257635, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:44.615079: step 7680, loss 0.213111, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7680

2017-10-10T15:27:45.490923: step 7681, loss 0.0663638, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:45.802264: step 7682, loss 0.0880768, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:46.037693: step 7683, loss 0.0633523, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:46.262481: step 7684, loss 0.132129, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:46.460883: step 7685, loss 0.0255209, acc 1, learning_rate 0.0001
2017-10-10T15:27:46.659452: step 7686, loss 0.0231881, acc 1, learning_rate 0.0001
2017-10-10T15:27:46.847112: step 7687, loss 0.0880442, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:47.094219: step 7688, loss 0.0578379, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:47.343328: step 7689, loss 0.0735087, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:47.549145: step 7690, loss 0.122369, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:47.779718: step 7691, loss 0.0896976, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:48.060839: step 7692, loss 0.0650158, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:48.301755: step 7693, loss 0.130888, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:48.488859: step 7694, loss 0.194904, acc 0.90625, learning_rate 0.0001
2017-10-10T15:27:48.724825: step 7695, loss 0.0804729, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:48.901818: step 7696, loss 0.108628, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:49.062372: step 7697, loss 0.10744, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:49.313502: step 7698, loss 0.0950157, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:49.543110: step 7699, loss 0.10107, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:49.793488: step 7700, loss 0.181796, acc 0.921875, learning_rate 0.0001
2017-10-10T15:27:49.996974: step 7701, loss 0.0671854, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:50.220865: step 7702, loss 0.0938745, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:50.489452: step 7703, loss 0.0640721, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:50.696862: step 7704, loss 0.0878299, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:50.853122: step 7705, loss 0.263016, acc 0.90625, learning_rate 0.0001
2017-10-10T15:27:51.114887: step 7706, loss 0.0549715, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:51.315025: step 7707, loss 0.0723925, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:51.533778: step 7708, loss 0.0694678, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:51.796867: step 7709, loss 0.113596, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:52.028554: step 7710, loss 0.087365, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:52.276884: step 7711, loss 0.0348455, acc 1, learning_rate 0.0001
2017-10-10T15:27:52.509055: step 7712, loss 0.0824348, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:52.732907: step 7713, loss 0.0508935, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:52.933148: step 7714, loss 0.0898907, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:53.184255: step 7715, loss 0.141325, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:53.399708: step 7716, loss 0.0416586, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:53.583622: step 7717, loss 0.105518, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:53.862807: step 7718, loss 0.0389962, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:54.079614: step 7719, loss 0.149369, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:54.253350: step 7720, loss 0.106788, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T15:27:54.630281: step 7720, loss 0.211671, acc 0.922302

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7720

2017-10-10T15:27:55.761966: step 7721, loss 0.142641, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:56.033331: step 7722, loss 0.0651838, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:56.284354: step 7723, loss 0.0247776, acc 1, learning_rate 0.0001
2017-10-10T15:27:56.492362: step 7724, loss 0.142133, acc 0.9375, learning_rate 0.0001
2017-10-10T15:27:56.695970: step 7725, loss 0.0210971, acc 1, learning_rate 0.0001
2017-10-10T15:27:56.873074: step 7726, loss 0.0379426, acc 1, learning_rate 0.0001
2017-10-10T15:27:57.071241: step 7727, loss 0.0910426, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:57.256925: step 7728, loss 0.0849859, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:57.483686: step 7729, loss 0.0438506, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:57.752854: step 7730, loss 0.0299392, acc 1, learning_rate 0.0001
2017-10-10T15:27:58.017516: step 7731, loss 0.0827968, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:58.208896: step 7732, loss 0.0708583, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:58.448826: step 7733, loss 0.0459774, acc 1, learning_rate 0.0001
2017-10-10T15:27:58.709455: step 7734, loss 0.12749, acc 0.953125, learning_rate 0.0001
2017-10-10T15:27:58.951724: step 7735, loss 0.127939, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:59.164880: step 7736, loss 0.163082, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:59.442524: step 7737, loss 0.0394338, acc 0.984375, learning_rate 0.0001
2017-10-10T15:27:59.650309: step 7738, loss 0.0688573, acc 0.96875, learning_rate 0.0001
2017-10-10T15:27:59.801133: step 7739, loss 0.0288943, acc 1, learning_rate 0.0001
2017-10-10T15:28:00.008840: step 7740, loss 0.0963435, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:00.177037: step 7741, loss 0.0388494, acc 1, learning_rate 0.0001
2017-10-10T15:28:00.448892: step 7742, loss 0.0293515, acc 1, learning_rate 0.0001
2017-10-10T15:28:00.736275: step 7743, loss 0.0488577, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:00.952846: step 7744, loss 0.075288, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:01.158799: step 7745, loss 0.0744112, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:01.377122: step 7746, loss 0.0816237, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:01.528909: step 7747, loss 0.0742678, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:01.726209: step 7748, loss 0.0579899, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:01.951935: step 7749, loss 0.0677344, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:02.205022: step 7750, loss 0.0298464, acc 1, learning_rate 0.0001
2017-10-10T15:28:02.436113: step 7751, loss 0.0503475, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:02.685136: step 7752, loss 0.0916621, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:02.929127: step 7753, loss 0.117748, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:03.148917: step 7754, loss 0.0516053, acc 1, learning_rate 0.0001
2017-10-10T15:28:03.417280: step 7755, loss 0.0719151, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:03.669224: step 7756, loss 0.0956698, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:03.876505: step 7757, loss 0.149029, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:04.113957: step 7758, loss 0.0480887, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:04.389271: step 7759, loss 0.0540576, acc 1, learning_rate 0.0001
2017-10-10T15:28:04.609192: step 7760, loss 0.0805343, acc 1, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:04.998189: step 7760, loss 0.211437, acc 0.917986

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7760

2017-10-10T15:28:06.103918: step 7761, loss 0.0545394, acc 1, learning_rate 0.0001
2017-10-10T15:28:06.316918: step 7762, loss 0.0509577, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:06.521235: step 7763, loss 0.123862, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:06.752604: step 7764, loss 0.0772074, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:06.977940: step 7765, loss 0.0689049, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:07.207002: step 7766, loss 0.0444739, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:07.473267: step 7767, loss 0.107216, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:07.743666: step 7768, loss 0.0309116, acc 1, learning_rate 0.0001
2017-10-10T15:28:07.997041: step 7769, loss 0.0539112, acc 1, learning_rate 0.0001
2017-10-10T15:28:08.236177: step 7770, loss 0.0903858, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:08.468881: step 7771, loss 0.0442694, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:08.692912: step 7772, loss 0.0466486, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:08.932635: step 7773, loss 0.105763, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:09.169442: step 7774, loss 0.0964253, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:09.372871: step 7775, loss 0.0987812, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:09.670852: step 7776, loss 0.204088, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:09.920826: step 7777, loss 0.0908392, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:10.108908: step 7778, loss 0.152144, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:10.344838: step 7779, loss 0.126103, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:10.572335: step 7780, loss 0.0754433, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:10.793148: step 7781, loss 0.0449938, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:11.012908: step 7782, loss 0.0802759, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:11.220562: step 7783, loss 0.0314624, acc 1, learning_rate 0.0001
2017-10-10T15:28:11.448709: step 7784, loss 0.0698, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:11.722850: step 7785, loss 0.038857, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:11.966558: step 7786, loss 0.0312239, acc 1, learning_rate 0.0001
2017-10-10T15:28:12.204855: step 7787, loss 0.090632, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:12.440938: step 7788, loss 0.150048, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:12.674683: step 7789, loss 0.0668001, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:12.924993: step 7790, loss 0.122285, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:13.150310: step 7791, loss 0.104009, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:13.396536: step 7792, loss 0.0665393, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:13.617942: step 7793, loss 0.0744484, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:13.828866: step 7794, loss 0.14756, acc 0.9375, learning_rate 0.0001
2017-10-10T15:28:14.099726: step 7795, loss 0.0628433, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:14.372830: step 7796, loss 0.0535632, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:14.582155: step 7797, loss 0.0824539, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:14.822597: step 7798, loss 0.0706362, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:15.056239: step 7799, loss 0.0585453, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:15.280325: step 7800, loss 0.15959, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:15.711290: step 7800, loss 0.211435, acc 0.919424

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7800

2017-10-10T15:28:16.450714: step 7801, loss 0.0541291, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:16.661682: step 7802, loss 0.0434933, acc 1, learning_rate 0.0001
2017-10-10T15:28:16.968381: step 7803, loss 0.080726, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:17.190725: step 7804, loss 0.0964522, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:17.449165: step 7805, loss 0.0315887, acc 1, learning_rate 0.0001
2017-10-10T15:28:17.631703: step 7806, loss 0.0952876, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:17.899685: step 7807, loss 0.0324881, acc 1, learning_rate 0.0001
2017-10-10T15:28:18.132146: step 7808, loss 0.0961745, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:18.332959: step 7809, loss 0.0630604, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:18.568840: step 7810, loss 0.0332546, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:18.831829: step 7811, loss 0.102749, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:19.066230: step 7812, loss 0.0523027, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:19.321016: step 7813, loss 0.0312387, acc 1, learning_rate 0.0001
2017-10-10T15:28:19.560133: step 7814, loss 0.0786238, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:19.806431: step 7815, loss 0.0652556, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:20.046459: step 7816, loss 0.0895064, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:20.320840: step 7817, loss 0.0632476, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:20.537669: step 7818, loss 0.0800921, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:20.759711: step 7819, loss 0.153944, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:20.940718: step 7820, loss 0.153102, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:21.193889: step 7821, loss 0.0381949, acc 1, learning_rate 0.0001
2017-10-10T15:28:21.515485: step 7822, loss 0.0928743, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:21.704987: step 7823, loss 0.0821168, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:21.896969: step 7824, loss 0.114686, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:22.121382: step 7825, loss 0.0372087, acc 1, learning_rate 0.0001
2017-10-10T15:28:22.340208: step 7826, loss 0.0810516, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:22.603437: step 7827, loss 0.0788077, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:22.860363: step 7828, loss 0.0632359, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:23.113410: step 7829, loss 0.0458563, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:23.376397: step 7830, loss 0.0510596, acc 0.96875, learning_rate 0.0001
2017-10-10T15:28:23.602043: step 7831, loss 0.0682863, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:23.791448: step 7832, loss 0.0576697, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:23.948912: step 7833, loss 0.0451992, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:24.139084: step 7834, loss 0.104797, acc 0.953125, learning_rate 0.0001
2017-10-10T15:28:24.385035: step 7835, loss 0.0888188, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:24.580684: step 7836, loss 0.0776489, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:24.796663: step 7837, loss 0.0521423, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:25.027141: step 7838, loss 0.0890188, acc 0.984375, learning_rate 0.0001
2017-10-10T15:28:25.284845: step 7839, loss 0.0347124, acc 1, learning_rate 0.0001
2017-10-10T15:28:25.468371: step 7840, loss 0.24361, acc 0.901961, learning_rate 0.0001

Evaluation:
2017-10-10T15:28:25.851712: step 7840, loss 0.21066, acc 0.920863

Saved model checkpoint to /home/sheep/bigdata/runs/1507664605/checkpoints/model-7840

