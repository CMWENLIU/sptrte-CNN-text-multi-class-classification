
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.2
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=80

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507738210

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-11T11:10:15.788899: step 1, loss 11.0182, acc 0.28125, learning_rate 0.005
2017-10-11T11:10:15.912951: step 2, loss 9.21363, acc 0.296875, learning_rate 0.00498
2017-10-11T11:10:16.032719: step 3, loss 9.22882, acc 0.296875, learning_rate 0.00496008
2017-10-11T11:10:16.152371: step 4, loss 12.3517, acc 0.234375, learning_rate 0.00494024
2017-10-11T11:10:16.276447: step 5, loss 7.99073, acc 0.328125, learning_rate 0.00492049
2017-10-11T11:10:16.397685: step 6, loss 7.33249, acc 0.328125, learning_rate 0.00490081
2017-10-11T11:10:16.512948: step 7, loss 9.13633, acc 0.21875, learning_rate 0.00488121
2017-10-11T11:10:16.636840: step 8, loss 8.49437, acc 0.25, learning_rate 0.0048617
2017-10-11T11:10:16.748886: step 9, loss 8.02071, acc 0.234375, learning_rate 0.00484226
2017-10-11T11:10:16.909726: step 10, loss 6.95699, acc 0.328125, learning_rate 0.00482291
2017-10-11T11:10:17.035832: step 11, loss 6.70565, acc 0.328125, learning_rate 0.00480363
2017-10-11T11:10:17.149561: step 12, loss 5.198, acc 0.453125, learning_rate 0.00478443
2017-10-11T11:10:17.268368: step 13, loss 6.23052, acc 0.421875, learning_rate 0.00476531
2017-10-11T11:10:17.371941: step 14, loss 5.30427, acc 0.40625, learning_rate 0.00474627
2017-10-11T11:10:17.486457: step 15, loss 7.3122, acc 0.328125, learning_rate 0.0047273
2017-10-11T11:10:17.594408: step 16, loss 4.39866, acc 0.453125, learning_rate 0.00470841
2017-10-11T11:10:17.700849: step 17, loss 5.71595, acc 0.375, learning_rate 0.0046896
2017-10-11T11:10:17.808790: step 18, loss 5.34661, acc 0.40625, learning_rate 0.00467087
2017-10-11T11:10:17.943025: step 19, loss 5.14181, acc 0.328125, learning_rate 0.00465221
2017-10-11T11:10:18.054757: step 20, loss 4.44325, acc 0.390625, learning_rate 0.00463363
2017-10-11T11:10:18.166039: step 21, loss 5.38457, acc 0.3125, learning_rate 0.00461513
2017-10-11T11:10:18.281182: step 22, loss 3.98282, acc 0.46875, learning_rate 0.0045967
2017-10-11T11:10:18.400606: step 23, loss 4.50734, acc 0.3125, learning_rate 0.00457834
2017-10-11T11:10:18.513362: step 24, loss 4.88968, acc 0.375, learning_rate 0.00456006
2017-10-11T11:10:18.628666: step 25, loss 3.75193, acc 0.421875, learning_rate 0.00454186
2017-10-11T11:10:18.746700: step 26, loss 3.84087, acc 0.4375, learning_rate 0.00452373
2017-10-11T11:10:18.866085: step 27, loss 2.56515, acc 0.484375, learning_rate 0.00450567
2017-10-11T11:10:18.989669: step 28, loss 2.89335, acc 0.484375, learning_rate 0.00448769
2017-10-11T11:10:19.088573: step 29, loss 2.71614, acc 0.53125, learning_rate 0.00446978
2017-10-11T11:10:19.252863: step 30, loss 3.12881, acc 0.4375, learning_rate 0.00445194
2017-10-11T11:10:19.393415: step 31, loss 2.85033, acc 0.453125, learning_rate 0.00443418
2017-10-11T11:10:19.478538: step 32, loss 2.56045, acc 0.546875, learning_rate 0.00441649
2017-10-11T11:10:19.577933: step 33, loss 3.63961, acc 0.421875, learning_rate 0.00439887
2017-10-11T11:10:19.666674: step 34, loss 3.18232, acc 0.484375, learning_rate 0.00438132
2017-10-11T11:10:19.754511: step 35, loss 2.20091, acc 0.484375, learning_rate 0.00436385
2017-10-11T11:10:19.842723: step 36, loss 2.20176, acc 0.453125, learning_rate 0.00434644
2017-10-11T11:10:19.928256: step 37, loss 2.78359, acc 0.484375, learning_rate 0.00432911
2017-10-11T11:10:20.016353: step 38, loss 2.50766, acc 0.46875, learning_rate 0.00431185
2017-10-11T11:10:20.105841: step 39, loss 2.38316, acc 0.484375, learning_rate 0.00429465
2017-10-11T11:10:20.218304: step 40, loss 1.92056, acc 0.578125, learning_rate 0.00427753

Evaluation:
2017-10-11T11:10:23.373261: step 40, loss 0.515457, acc 0.81295

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-40

2017-10-11T11:10:28.762755: step 41, loss 2.17564, acc 0.59375, learning_rate 0.00426048
2017-10-11T11:10:28.905663: step 42, loss 1.67476, acc 0.625, learning_rate 0.0042435
2017-10-11T11:10:29.020700: step 43, loss 2.075, acc 0.5625, learning_rate 0.00422659
2017-10-11T11:10:29.136875: step 44, loss 2.29134, acc 0.53125, learning_rate 0.00420974
2017-10-11T11:10:29.245284: step 45, loss 2.28706, acc 0.578125, learning_rate 0.00419297
2017-10-11T11:10:29.355905: step 46, loss 2.65042, acc 0.53125, learning_rate 0.00417626
2017-10-11T11:10:29.459811: step 47, loss 1.83097, acc 0.515625, learning_rate 0.00415962
2017-10-11T11:10:29.544395: step 48, loss 2.12511, acc 0.546875, learning_rate 0.00414305
2017-10-11T11:10:29.626122: step 49, loss 1.1231, acc 0.625, learning_rate 0.00412655
2017-10-11T11:10:29.708416: step 50, loss 2.2605, acc 0.484375, learning_rate 0.00411011
2017-10-11T11:10:29.790000: step 51, loss 2.20762, acc 0.5, learning_rate 0.00409375
2017-10-11T11:10:29.871164: step 52, loss 1.70745, acc 0.5625, learning_rate 0.00407744
2017-10-11T11:10:29.951747: step 53, loss 1.21371, acc 0.671875, learning_rate 0.00406121
2017-10-11T11:10:30.060823: step 54, loss 0.957579, acc 0.6875, learning_rate 0.00404504
2017-10-11T11:10:30.178778: step 55, loss 1.20594, acc 0.59375, learning_rate 0.00402894
2017-10-11T11:10:30.291672: step 56, loss 1.24265, acc 0.671875, learning_rate 0.0040129
2017-10-11T11:10:30.407930: step 57, loss 1.39486, acc 0.65625, learning_rate 0.00399693
2017-10-11T11:10:30.525878: step 58, loss 1.43269, acc 0.59375, learning_rate 0.00398102
2017-10-11T11:10:30.643510: step 59, loss 1.57371, acc 0.5625, learning_rate 0.00396518
2017-10-11T11:10:30.756714: step 60, loss 1.51045, acc 0.59375, learning_rate 0.00394941
2017-10-11T11:10:30.874310: step 61, loss 1.25687, acc 0.65625, learning_rate 0.00393369
2017-10-11T11:10:30.997546: step 62, loss 1.01837, acc 0.6875, learning_rate 0.00391804
2017-10-11T11:10:31.114959: step 63, loss 1.44065, acc 0.609375, learning_rate 0.00390246
2017-10-11T11:10:31.230679: step 64, loss 1.35037, acc 0.625, learning_rate 0.00388694
2017-10-11T11:10:31.349572: step 65, loss 0.888169, acc 0.671875, learning_rate 0.00387148
2017-10-11T11:10:31.456887: step 66, loss 1.09987, acc 0.6875, learning_rate 0.00385609
2017-10-11T11:10:31.563607: step 67, loss 0.864054, acc 0.703125, learning_rate 0.00384076
2017-10-11T11:10:31.683216: step 68, loss 1.24593, acc 0.625, learning_rate 0.00382549
2017-10-11T11:10:31.789954: step 69, loss 1.05909, acc 0.65625, learning_rate 0.00381028
2017-10-11T11:10:31.899430: step 70, loss 1.60789, acc 0.578125, learning_rate 0.00379514
2017-10-11T11:10:32.020877: step 71, loss 1.67805, acc 0.546875, learning_rate 0.00378005
2017-10-11T11:10:32.130354: step 72, loss 0.834581, acc 0.625, learning_rate 0.00376503
2017-10-11T11:10:32.241108: step 73, loss 0.90313, acc 0.6875, learning_rate 0.00375007
2017-10-11T11:10:32.322140: step 74, loss 1.04257, acc 0.6875, learning_rate 0.00373517
2017-10-11T11:10:32.405909: step 75, loss 1.11568, acc 0.640625, learning_rate 0.00372034
2017-10-11T11:10:32.511799: step 76, loss 1.12839, acc 0.65625, learning_rate 0.00370556
2017-10-11T11:10:32.629757: step 77, loss 1.311, acc 0.65625, learning_rate 0.00369084
2017-10-11T11:10:32.754604: step 78, loss 0.954202, acc 0.640625, learning_rate 0.00367619
2017-10-11T11:10:32.871336: step 79, loss 0.692593, acc 0.765625, learning_rate 0.00366159
2017-10-11T11:10:32.988821: step 80, loss 1.23188, acc 0.5625, learning_rate 0.00364705

Evaluation:
2017-10-11T11:10:34.029781: step 80, loss 0.387404, acc 0.854676

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-80

2017-10-11T11:10:37.357862: step 81, loss 1.08984, acc 0.59375, learning_rate 0.00363257
2017-10-11T11:10:37.690639: step 82, loss 1.41882, acc 0.59375, learning_rate 0.00361815
2017-10-11T11:10:37.805375: step 83, loss 0.986614, acc 0.671875, learning_rate 0.00360379
2017-10-11T11:10:37.920355: step 84, loss 0.981906, acc 0.75, learning_rate 0.00358949
2017-10-11T11:10:38.071251: step 85, loss 1.065, acc 0.671875, learning_rate 0.00357525
2017-10-11T11:10:38.194878: step 86, loss 0.608107, acc 0.765625, learning_rate 0.00356106
2017-10-11T11:10:38.277140: step 87, loss 0.965916, acc 0.65625, learning_rate 0.00354694
2017-10-11T11:10:38.362500: step 88, loss 1.30925, acc 0.640625, learning_rate 0.00353287
2017-10-11T11:10:38.442632: step 89, loss 1.21469, acc 0.671875, learning_rate 0.00351885
2017-10-11T11:10:38.523612: step 90, loss 0.95128, acc 0.71875, learning_rate 0.0035049
2017-10-11T11:10:38.605411: step 91, loss 0.583843, acc 0.828125, learning_rate 0.003491
2017-10-11T11:10:38.685610: step 92, loss 0.711207, acc 0.75, learning_rate 0.00347716
2017-10-11T11:10:38.771985: step 93, loss 0.874202, acc 0.75, learning_rate 0.00346338
2017-10-11T11:10:38.860215: step 94, loss 0.703352, acc 0.734375, learning_rate 0.00344965
2017-10-11T11:10:38.947938: step 95, loss 0.801758, acc 0.796875, learning_rate 0.00343597
2017-10-11T11:10:39.034447: step 96, loss 0.713645, acc 0.75, learning_rate 0.00342236
2017-10-11T11:10:39.121908: step 97, loss 1.13289, acc 0.65625, learning_rate 0.0034088
2017-10-11T11:10:39.197000: step 98, loss 0.960289, acc 0.72549, learning_rate 0.00339529
2017-10-11T11:10:39.281824: step 99, loss 0.687025, acc 0.734375, learning_rate 0.00338184
2017-10-11T11:10:39.369916: step 100, loss 1.21587, acc 0.625, learning_rate 0.00336844
2017-10-11T11:10:39.487057: step 101, loss 0.800554, acc 0.71875, learning_rate 0.0033551
2017-10-11T11:10:39.608050: step 102, loss 0.979432, acc 0.65625, learning_rate 0.00334182
2017-10-11T11:10:39.720771: step 103, loss 0.83863, acc 0.75, learning_rate 0.00332858
2017-10-11T11:10:39.828328: step 104, loss 0.702902, acc 0.671875, learning_rate 0.00331541
2017-10-11T11:10:39.943366: step 105, loss 0.950221, acc 0.609375, learning_rate 0.00330228
2017-10-11T11:10:40.064884: step 106, loss 0.631932, acc 0.765625, learning_rate 0.00328921
2017-10-11T11:10:40.172849: step 107, loss 0.798939, acc 0.703125, learning_rate 0.00327619
2017-10-11T11:10:40.284080: step 108, loss 0.894697, acc 0.734375, learning_rate 0.00326323
2017-10-11T11:10:40.396890: step 109, loss 0.613695, acc 0.8125, learning_rate 0.00325032
2017-10-11T11:10:40.504855: step 110, loss 0.691238, acc 0.84375, learning_rate 0.00323746
2017-10-11T11:10:40.622664: step 111, loss 0.642976, acc 0.78125, learning_rate 0.00322465
2017-10-11T11:10:40.748069: step 112, loss 0.588042, acc 0.796875, learning_rate 0.0032119
2017-10-11T11:10:40.862252: step 113, loss 0.816791, acc 0.734375, learning_rate 0.0031992
2017-10-11T11:10:40.973989: step 114, loss 0.692777, acc 0.75, learning_rate 0.00318655
2017-10-11T11:10:41.094172: step 115, loss 0.775989, acc 0.734375, learning_rate 0.00317395
2017-10-11T11:10:41.210082: step 116, loss 0.97532, acc 0.640625, learning_rate 0.0031614
2017-10-11T11:10:41.329924: step 117, loss 0.736657, acc 0.71875, learning_rate 0.0031489
2017-10-11T11:10:41.433463: step 118, loss 0.991479, acc 0.703125, learning_rate 0.00313646
2017-10-11T11:10:41.548811: step 119, loss 0.731362, acc 0.6875, learning_rate 0.00312407
2017-10-11T11:10:41.673383: step 120, loss 0.923151, acc 0.734375, learning_rate 0.00311172

Evaluation:
2017-10-11T11:10:41.940927: step 120, loss 0.378361, acc 0.871942

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-120

2017-10-11T11:10:42.654250: step 121, loss 0.679757, acc 0.734375, learning_rate 0.00309943
2017-10-11T11:10:42.765979: step 122, loss 0.487689, acc 0.859375, learning_rate 0.00308719
2017-10-11T11:10:42.872424: step 123, loss 0.728006, acc 0.75, learning_rate 0.00307499
2017-10-11T11:10:42.988611: step 124, loss 0.498496, acc 0.796875, learning_rate 0.00306285
2017-10-11T11:10:43.100839: step 125, loss 0.786021, acc 0.71875, learning_rate 0.00305076
2017-10-11T11:10:43.215789: step 126, loss 0.477682, acc 0.796875, learning_rate 0.00303871
2017-10-11T11:10:43.307603: step 127, loss 0.814038, acc 0.703125, learning_rate 0.00302672
2017-10-11T11:10:43.423885: step 128, loss 0.79346, acc 0.75, learning_rate 0.00301477
2017-10-11T11:10:43.524872: step 129, loss 0.704604, acc 0.765625, learning_rate 0.00300287
2017-10-11T11:10:43.633204: step 130, loss 0.58911, acc 0.78125, learning_rate 0.00299102
2017-10-11T11:10:43.730765: step 131, loss 0.690942, acc 0.78125, learning_rate 0.00297922
2017-10-11T11:10:43.884588: step 132, loss 0.814933, acc 0.734375, learning_rate 0.00296747
2017-10-11T11:10:43.978096: step 133, loss 0.610468, acc 0.765625, learning_rate 0.00295577
2017-10-11T11:10:44.071540: step 134, loss 0.415702, acc 0.796875, learning_rate 0.00294411
2017-10-11T11:10:44.172244: step 135, loss 0.399661, acc 0.859375, learning_rate 0.0029325
2017-10-11T11:10:44.269547: step 136, loss 0.715996, acc 0.796875, learning_rate 0.00292094
2017-10-11T11:10:44.361867: step 137, loss 0.74812, acc 0.703125, learning_rate 0.00290943
2017-10-11T11:10:44.455497: step 138, loss 0.809116, acc 0.625, learning_rate 0.00289796
2017-10-11T11:10:44.549371: step 139, loss 0.680906, acc 0.75, learning_rate 0.00288654
2017-10-11T11:10:44.634051: step 140, loss 0.602836, acc 0.75, learning_rate 0.00287516
2017-10-11T11:10:44.755023: step 141, loss 0.729224, acc 0.75, learning_rate 0.00286384
2017-10-11T11:10:44.864403: step 142, loss 0.787603, acc 0.765625, learning_rate 0.00285256
2017-10-11T11:10:44.977176: step 143, loss 0.89639, acc 0.625, learning_rate 0.00284132
2017-10-11T11:10:45.099961: step 144, loss 0.577347, acc 0.8125, learning_rate 0.00283013
2017-10-11T11:10:45.220458: step 145, loss 0.478858, acc 0.78125, learning_rate 0.00281899
2017-10-11T11:10:45.334562: step 146, loss 0.593547, acc 0.765625, learning_rate 0.00280789
2017-10-11T11:10:45.446380: step 147, loss 0.691093, acc 0.8125, learning_rate 0.00279684
2017-10-11T11:10:45.556603: step 148, loss 0.537636, acc 0.828125, learning_rate 0.00278583
2017-10-11T11:10:45.673017: step 149, loss 0.65542, acc 0.765625, learning_rate 0.00277486
2017-10-11T11:10:45.780883: step 150, loss 0.837465, acc 0.65625, learning_rate 0.00276395
2017-10-11T11:10:45.892934: step 151, loss 0.930038, acc 0.6875, learning_rate 0.00275307
2017-10-11T11:10:45.996269: step 152, loss 0.596357, acc 0.734375, learning_rate 0.00274224
2017-10-11T11:10:46.112202: step 153, loss 0.669696, acc 0.765625, learning_rate 0.00273146
2017-10-11T11:10:46.225623: step 154, loss 0.606443, acc 0.71875, learning_rate 0.00272072
2017-10-11T11:10:46.334195: step 155, loss 0.490767, acc 0.828125, learning_rate 0.00271002
2017-10-11T11:10:46.448884: step 156, loss 0.697426, acc 0.734375, learning_rate 0.00269937
2017-10-11T11:10:46.578721: step 157, loss 0.440606, acc 0.84375, learning_rate 0.00268876
2017-10-11T11:10:46.698901: step 158, loss 0.495344, acc 0.8125, learning_rate 0.00267819
2017-10-11T11:10:46.804438: step 159, loss 0.798908, acc 0.75, learning_rate 0.00266767
2017-10-11T11:10:46.925311: step 160, loss 0.722969, acc 0.765625, learning_rate 0.00265719

Evaluation:
2017-10-11T11:10:47.180467: step 160, loss 0.353441, acc 0.877698

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-160

2017-10-11T11:10:48.300594: step 161, loss 0.422713, acc 0.875, learning_rate 0.00264675
2017-10-11T11:10:48.407383: step 162, loss 0.617571, acc 0.765625, learning_rate 0.00263635
2017-10-11T11:10:48.525080: step 163, loss 0.818046, acc 0.6875, learning_rate 0.002626
2017-10-11T11:10:48.636759: step 164, loss 0.692008, acc 0.796875, learning_rate 0.00261569
2017-10-11T11:10:48.756153: step 165, loss 0.522694, acc 0.828125, learning_rate 0.00260542
2017-10-11T11:10:48.912996: step 166, loss 0.489366, acc 0.875, learning_rate 0.0025952
2017-10-11T11:10:49.026956: step 167, loss 0.568168, acc 0.84375, learning_rate 0.00258501
2017-10-11T11:10:49.111613: step 168, loss 0.654181, acc 0.765625, learning_rate 0.00257487
2017-10-11T11:10:49.198026: step 169, loss 0.642249, acc 0.8125, learning_rate 0.00256477
2017-10-11T11:10:49.374970: step 170, loss 0.766353, acc 0.78125, learning_rate 0.0025547
2017-10-11T11:10:49.466742: step 171, loss 0.478071, acc 0.8125, learning_rate 0.00254469
2017-10-11T11:10:49.561035: step 172, loss 0.607328, acc 0.765625, learning_rate 0.00253471
2017-10-11T11:10:49.655720: step 173, loss 0.585606, acc 0.765625, learning_rate 0.00252477
2017-10-11T11:10:49.747814: step 174, loss 0.557477, acc 0.8125, learning_rate 0.00251487
2017-10-11T11:10:49.845202: step 175, loss 0.578875, acc 0.796875, learning_rate 0.00250501
2017-10-11T11:10:49.962838: step 176, loss 0.426768, acc 0.84375, learning_rate 0.0024952
2017-10-11T11:10:50.085341: step 177, loss 0.542464, acc 0.84375, learning_rate 0.00248542
2017-10-11T11:10:50.202028: step 178, loss 0.556977, acc 0.8125, learning_rate 0.00247568
2017-10-11T11:10:50.322951: step 179, loss 0.799965, acc 0.78125, learning_rate 0.00246599
2017-10-11T11:10:50.440932: step 180, loss 0.768576, acc 0.734375, learning_rate 0.00245633
2017-10-11T11:10:50.546929: step 181, loss 0.789089, acc 0.78125, learning_rate 0.00244671
2017-10-11T11:10:50.661396: step 182, loss 0.89684, acc 0.765625, learning_rate 0.00243713
2017-10-11T11:10:50.779349: step 183, loss 0.471256, acc 0.859375, learning_rate 0.00242759
2017-10-11T11:10:50.898390: step 184, loss 0.797656, acc 0.671875, learning_rate 0.00241809
2017-10-11T11:10:51.012774: step 185, loss 0.485153, acc 0.859375, learning_rate 0.00240863
2017-10-11T11:10:51.123619: step 186, loss 0.318195, acc 0.921875, learning_rate 0.00239921
2017-10-11T11:10:51.247724: step 187, loss 0.496824, acc 0.875, learning_rate 0.00238982
2017-10-11T11:10:51.363897: step 188, loss 0.494192, acc 0.8125, learning_rate 0.00238048
2017-10-11T11:10:51.480379: step 189, loss 0.514948, acc 0.796875, learning_rate 0.00237117
2017-10-11T11:10:51.612729: step 190, loss 0.526958, acc 0.859375, learning_rate 0.0023619
2017-10-11T11:10:51.734096: step 191, loss 0.650968, acc 0.8125, learning_rate 0.00235267
2017-10-11T11:10:51.845193: step 192, loss 0.782121, acc 0.734375, learning_rate 0.00234347
2017-10-11T11:10:51.954031: step 193, loss 0.418032, acc 0.859375, learning_rate 0.00233431
2017-10-11T11:10:52.068474: step 194, loss 0.594231, acc 0.8125, learning_rate 0.00232519
2017-10-11T11:10:52.191760: step 195, loss 0.53984, acc 0.78125, learning_rate 0.00231611
2017-10-11T11:10:52.301918: step 196, loss 0.415643, acc 0.862745, learning_rate 0.00230707
2017-10-11T11:10:52.420441: step 197, loss 0.406378, acc 0.828125, learning_rate 0.00229806
2017-10-11T11:10:52.534328: step 198, loss 0.560345, acc 0.8125, learning_rate 0.00228908
2017-10-11T11:10:52.648493: step 199, loss 0.51627, acc 0.8125, learning_rate 0.00228015
2017-10-11T11:10:52.755456: step 200, loss 0.728459, acc 0.8125, learning_rate 0.00227125

Evaluation:
2017-10-11T11:10:53.032902: step 200, loss 0.351002, acc 0.876259

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-200

2017-10-11T11:10:53.749966: step 201, loss 0.64474, acc 0.765625, learning_rate 0.00226239
2017-10-11T11:10:53.862315: step 202, loss 0.713439, acc 0.78125, learning_rate 0.00225356
2017-10-11T11:10:53.984714: step 203, loss 0.518427, acc 0.828125, learning_rate 0.00224477
2017-10-11T11:10:54.101169: step 204, loss 0.674603, acc 0.765625, learning_rate 0.00223602
2017-10-11T11:10:54.213902: step 205, loss 0.43857, acc 0.84375, learning_rate 0.0022273
2017-10-11T11:10:54.406463: step 206, loss 0.425729, acc 0.859375, learning_rate 0.00221862
2017-10-11T11:10:54.493377: step 207, loss 0.657397, acc 0.78125, learning_rate 0.00220997
2017-10-11T11:10:54.598312: step 208, loss 0.338584, acc 0.90625, learning_rate 0.00220136
2017-10-11T11:10:54.685292: step 209, loss 0.422311, acc 0.84375, learning_rate 0.00219278
2017-10-11T11:10:54.770653: step 210, loss 0.485235, acc 0.828125, learning_rate 0.00218424
2017-10-11T11:10:54.856139: step 211, loss 0.485183, acc 0.8125, learning_rate 0.00217573
2017-10-11T11:10:54.941944: step 212, loss 0.464222, acc 0.859375, learning_rate 0.00216726
2017-10-11T11:10:55.029762: step 213, loss 0.583726, acc 0.828125, learning_rate 0.00215882
2017-10-11T11:10:55.132945: step 214, loss 0.535453, acc 0.828125, learning_rate 0.00215041
2017-10-11T11:10:55.242293: step 215, loss 0.43464, acc 0.859375, learning_rate 0.00214204
2017-10-11T11:10:55.351288: step 216, loss 0.487368, acc 0.84375, learning_rate 0.00213371
2017-10-11T11:10:55.460738: step 217, loss 0.530169, acc 0.8125, learning_rate 0.00212541
2017-10-11T11:10:55.563538: step 218, loss 0.553821, acc 0.859375, learning_rate 0.00211714
2017-10-11T11:10:55.687395: step 219, loss 0.831258, acc 0.78125, learning_rate 0.00210891
2017-10-11T11:10:55.800491: step 220, loss 0.524969, acc 0.78125, learning_rate 0.00210071
2017-10-11T11:10:55.909111: step 221, loss 0.546773, acc 0.859375, learning_rate 0.00209254
2017-10-11T11:10:56.022926: step 222, loss 0.452823, acc 0.84375, learning_rate 0.00208441
2017-10-11T11:10:56.131335: step 223, loss 0.358721, acc 0.90625, learning_rate 0.00207631
2017-10-11T11:10:56.246834: step 224, loss 0.700245, acc 0.734375, learning_rate 0.00206824
2017-10-11T11:10:56.357825: step 225, loss 0.452079, acc 0.84375, learning_rate 0.00206021
2017-10-11T11:10:56.472322: step 226, loss 0.427134, acc 0.859375, learning_rate 0.00205221
2017-10-11T11:10:56.585022: step 227, loss 0.584977, acc 0.84375, learning_rate 0.00204424
2017-10-11T11:10:56.703579: step 228, loss 0.650156, acc 0.828125, learning_rate 0.0020363
2017-10-11T11:10:56.815529: step 229, loss 0.361652, acc 0.875, learning_rate 0.0020284
2017-10-11T11:10:56.929772: step 230, loss 0.706462, acc 0.84375, learning_rate 0.00202053
2017-10-11T11:10:57.050033: step 231, loss 0.591619, acc 0.796875, learning_rate 0.00201269
2017-10-11T11:10:57.165668: step 232, loss 0.616405, acc 0.796875, learning_rate 0.00200488
2017-10-11T11:10:57.280113: step 233, loss 0.440861, acc 0.859375, learning_rate 0.00199711
2017-10-11T11:10:57.390759: step 234, loss 0.536581, acc 0.765625, learning_rate 0.00198936
2017-10-11T11:10:57.508868: step 235, loss 0.507055, acc 0.78125, learning_rate 0.00198165
2017-10-11T11:10:57.623877: step 236, loss 0.501722, acc 0.796875, learning_rate 0.00197397
2017-10-11T11:10:57.734818: step 237, loss 0.784336, acc 0.765625, learning_rate 0.00196632
2017-10-11T11:10:57.846416: step 238, loss 0.344334, acc 0.90625, learning_rate 0.0019587
2017-10-11T11:10:57.961684: step 239, loss 0.767585, acc 0.703125, learning_rate 0.00195112
2017-10-11T11:10:58.062214: step 240, loss 0.411702, acc 0.859375, learning_rate 0.00194356

Evaluation:
2017-10-11T11:10:58.330486: step 240, loss 0.320756, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-240

2017-10-11T11:10:59.359840: step 241, loss 0.459249, acc 0.84375, learning_rate 0.00193604
2017-10-11T11:10:59.448597: step 242, loss 0.592974, acc 0.78125, learning_rate 0.00192854
2017-10-11T11:10:59.535839: step 243, loss 0.602025, acc 0.828125, learning_rate 0.00192108
2017-10-11T11:10:59.622115: step 244, loss 0.711663, acc 0.8125, learning_rate 0.00191364
2017-10-11T11:10:59.706511: step 245, loss 0.538352, acc 0.71875, learning_rate 0.00190624
2017-10-11T11:10:59.803715: step 246, loss 0.400102, acc 0.859375, learning_rate 0.00189887
2017-10-11T11:10:59.890975: step 247, loss 0.665613, acc 0.84375, learning_rate 0.00189153
2017-10-11T11:10:59.976463: step 248, loss 0.456411, acc 0.8125, learning_rate 0.00188421
2017-10-11T11:11:00.061074: step 249, loss 0.724739, acc 0.78125, learning_rate 0.00187693
2017-10-11T11:11:00.147732: step 250, loss 0.520028, acc 0.828125, learning_rate 0.00186968
2017-10-11T11:11:00.264504: step 251, loss 0.643615, acc 0.78125, learning_rate 0.00186245
2017-10-11T11:11:00.374967: step 252, loss 0.457029, acc 0.8125, learning_rate 0.00185526
2017-10-11T11:11:00.488907: step 253, loss 0.376721, acc 0.875, learning_rate 0.0018481
2017-10-11T11:11:00.605839: step 254, loss 0.475775, acc 0.859375, learning_rate 0.00184096
2017-10-11T11:11:00.720923: step 255, loss 0.422115, acc 0.875, learning_rate 0.00183385
2017-10-11T11:11:00.833213: step 256, loss 0.654166, acc 0.71875, learning_rate 0.00182678
2017-10-11T11:11:01.025409: step 257, loss 0.502357, acc 0.828125, learning_rate 0.00181973
2017-10-11T11:11:01.142425: step 258, loss 0.551746, acc 0.796875, learning_rate 0.00181271
2017-10-11T11:11:01.260138: step 259, loss 0.438256, acc 0.890625, learning_rate 0.00180572
2017-10-11T11:11:01.376205: step 260, loss 0.447442, acc 0.828125, learning_rate 0.00179876
2017-10-11T11:11:01.492485: step 261, loss 0.554773, acc 0.8125, learning_rate 0.00179182
2017-10-11T11:11:01.609289: step 262, loss 0.508551, acc 0.84375, learning_rate 0.00178492
2017-10-11T11:11:01.720216: step 263, loss 0.486745, acc 0.84375, learning_rate 0.00177804
2017-10-11T11:11:01.843675: step 264, loss 0.480028, acc 0.875, learning_rate 0.00177119
2017-10-11T11:11:01.958087: step 265, loss 0.439042, acc 0.875, learning_rate 0.00176437
2017-10-11T11:11:02.071596: step 266, loss 0.546675, acc 0.859375, learning_rate 0.00175758
2017-10-11T11:11:02.188430: step 267, loss 0.561876, acc 0.8125, learning_rate 0.00175081
2017-10-11T11:11:02.311313: step 268, loss 0.422381, acc 0.859375, learning_rate 0.00174407
2017-10-11T11:11:02.431211: step 269, loss 0.592627, acc 0.796875, learning_rate 0.00173736
2017-10-11T11:11:02.548118: step 270, loss 0.329763, acc 0.84375, learning_rate 0.00173068
2017-10-11T11:11:02.655334: step 271, loss 0.423561, acc 0.90625, learning_rate 0.00172402
2017-10-11T11:11:02.778443: step 272, loss 0.626658, acc 0.78125, learning_rate 0.00171739
2017-10-11T11:11:02.887249: step 273, loss 0.410283, acc 0.828125, learning_rate 0.00171079
2017-10-11T11:11:02.994829: step 274, loss 0.525913, acc 0.84375, learning_rate 0.00170422
2017-10-11T11:11:03.119632: step 275, loss 0.569983, acc 0.890625, learning_rate 0.00169767
2017-10-11T11:11:03.239626: step 276, loss 0.422782, acc 0.875, learning_rate 0.00169115
2017-10-11T11:11:03.350628: step 277, loss 0.590398, acc 0.75, learning_rate 0.00168465
2017-10-11T11:11:03.461619: step 278, loss 0.513186, acc 0.859375, learning_rate 0.00167818
2017-10-11T11:11:03.575136: step 279, loss 0.658179, acc 0.8125, learning_rate 0.00167174
2017-10-11T11:11:03.688061: step 280, loss 0.29012, acc 0.9375, learning_rate 0.00166533

Evaluation:
2017-10-11T11:11:34.135665: step 280, loss 0.314143, acc 0.88777

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-280

2017-10-11T11:11:40.404909: step 281, loss 0.516449, acc 0.8125, learning_rate 0.00165894
2017-10-11T11:11:40.628274: step 282, loss 0.730322, acc 0.75, learning_rate 0.00165257
2017-10-11T11:11:40.756824: step 283, loss 0.370022, acc 0.875, learning_rate 0.00164624
2017-10-11T11:11:40.868426: step 284, loss 0.638183, acc 0.75, learning_rate 0.00163993
2017-10-11T11:11:40.987982: step 285, loss 0.59558, acc 0.796875, learning_rate 0.00163364
2017-10-11T11:11:41.112892: step 286, loss 0.352475, acc 0.890625, learning_rate 0.00162738
2017-10-11T11:11:41.220708: step 287, loss 0.484177, acc 0.859375, learning_rate 0.00162115
2017-10-11T11:11:41.327574: step 288, loss 0.516467, acc 0.84375, learning_rate 0.00161494
2017-10-11T11:11:41.440850: step 289, loss 0.63926, acc 0.8125, learning_rate 0.00160875
2017-10-11T11:11:41.560858: step 290, loss 0.508681, acc 0.734375, learning_rate 0.00160259
2017-10-11T11:11:41.680076: step 291, loss 0.418164, acc 0.859375, learning_rate 0.00159646
2017-10-11T11:11:41.802564: step 292, loss 0.468513, acc 0.859375, learning_rate 0.00159035
2017-10-11T11:11:41.920103: step 293, loss 0.465564, acc 0.828125, learning_rate 0.00158427
2017-10-11T11:11:41.988971: step 294, loss 0.30105, acc 0.901961, learning_rate 0.00157821
2017-10-11T11:11:42.574425: step 295, loss 0.447576, acc 0.84375, learning_rate 0.00157218
2017-10-11T11:11:42.683529: step 296, loss 0.518525, acc 0.828125, learning_rate 0.00156617
2017-10-11T11:11:42.805150: step 297, loss 0.507699, acc 0.796875, learning_rate 0.00156018
2017-10-11T11:11:42.919542: step 298, loss 0.325737, acc 0.90625, learning_rate 0.00155422
2017-10-11T11:11:43.032602: step 299, loss 0.438385, acc 0.859375, learning_rate 0.00154829
2017-10-11T11:11:43.150551: step 300, loss 0.44832, acc 0.90625, learning_rate 0.00154238
2017-10-11T11:11:43.260612: step 301, loss 0.7538, acc 0.6875, learning_rate 0.00153649
2017-10-11T11:11:43.382180: step 302, loss 0.421836, acc 0.828125, learning_rate 0.00153063
2017-10-11T11:11:43.499090: step 303, loss 0.471295, acc 0.875, learning_rate 0.00152479
2017-10-11T11:11:43.615575: step 304, loss 0.509095, acc 0.796875, learning_rate 0.00151897
2017-10-11T11:11:43.733106: step 305, loss 0.583468, acc 0.78125, learning_rate 0.00151318
2017-10-11T11:11:43.845575: step 306, loss 0.91633, acc 0.734375, learning_rate 0.00150741
2017-10-11T11:11:43.963340: step 307, loss 0.456871, acc 0.828125, learning_rate 0.00150167
2017-10-11T11:11:44.079766: step 308, loss 0.450444, acc 0.828125, learning_rate 0.00149594
2017-10-11T11:11:44.196841: step 309, loss 0.368432, acc 0.90625, learning_rate 0.00149025
2017-10-11T11:11:44.303726: step 310, loss 0.264856, acc 0.90625, learning_rate 0.00148457
2017-10-11T11:11:44.425895: step 311, loss 0.43941, acc 0.859375, learning_rate 0.00147892
2017-10-11T11:11:44.539353: step 312, loss 0.304369, acc 0.921875, learning_rate 0.00147329
2017-10-11T11:11:44.648163: step 313, loss 0.438922, acc 0.84375, learning_rate 0.00146769
2017-10-11T11:11:44.760879: step 314, loss 0.521819, acc 0.765625, learning_rate 0.0014621
2017-10-11T11:11:44.870184: step 315, loss 0.412972, acc 0.84375, learning_rate 0.00145654
2017-10-11T11:11:44.986176: step 316, loss 0.596509, acc 0.78125, learning_rate 0.00145101
2017-10-11T11:11:45.158689: step 317, loss 0.547172, acc 0.828125, learning_rate 0.00144549
2017-10-11T11:11:45.246976: step 318, loss 0.543997, acc 0.8125, learning_rate 0.00144
2017-10-11T11:11:45.335001: step 319, loss 0.779606, acc 0.75, learning_rate 0.00143453
2017-10-11T11:11:45.419268: step 320, loss 0.595705, acc 0.796875, learning_rate 0.00142908

Evaluation:
2017-10-11T11:11:45.649468: step 320, loss 0.304549, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-320

2017-10-11T11:11:46.202253: step 321, loss 0.586511, acc 0.78125, learning_rate 0.00142366
2017-10-11T11:11:46.326916: step 322, loss 0.442524, acc 0.84375, learning_rate 0.00141826
2017-10-11T11:11:46.448364: step 323, loss 0.409004, acc 0.890625, learning_rate 0.00141288
2017-10-11T11:11:46.569807: step 324, loss 0.421868, acc 0.875, learning_rate 0.00140752
2017-10-11T11:11:46.683158: step 325, loss 0.53507, acc 0.796875, learning_rate 0.00140218
2017-10-11T11:11:46.797946: step 326, loss 0.292359, acc 0.890625, learning_rate 0.00139686
2017-10-11T11:11:46.913339: step 327, loss 0.526451, acc 0.828125, learning_rate 0.00139157
2017-10-11T11:11:47.031562: step 328, loss 0.615547, acc 0.765625, learning_rate 0.0013863
2017-10-11T11:11:47.147556: step 329, loss 0.411848, acc 0.921875, learning_rate 0.00138105
2017-10-11T11:11:47.267525: step 330, loss 0.466394, acc 0.84375, learning_rate 0.00137582
2017-10-11T11:11:47.372559: step 331, loss 0.551053, acc 0.8125, learning_rate 0.00137061
2017-10-11T11:11:47.491972: step 332, loss 0.442703, acc 0.796875, learning_rate 0.00136543
2017-10-11T11:11:47.591791: step 333, loss 0.533835, acc 0.890625, learning_rate 0.00136026
2017-10-11T11:11:47.703147: step 334, loss 0.611597, acc 0.765625, learning_rate 0.00135512
2017-10-11T11:11:47.818444: step 335, loss 0.221969, acc 0.921875, learning_rate 0.00134999
2017-10-11T11:11:47.931316: step 336, loss 0.59429, acc 0.828125, learning_rate 0.00134489
2017-10-11T11:11:48.048387: step 337, loss 0.636766, acc 0.765625, learning_rate 0.00133981
2017-10-11T11:11:48.152797: step 338, loss 0.773069, acc 0.796875, learning_rate 0.00133475
2017-10-11T11:11:48.280905: step 339, loss 0.350864, acc 0.890625, learning_rate 0.00132971
2017-10-11T11:11:48.397952: step 340, loss 0.587011, acc 0.8125, learning_rate 0.00132469
2017-10-11T11:11:48.515519: step 341, loss 0.343047, acc 0.875, learning_rate 0.00131969
2017-10-11T11:11:48.625741: step 342, loss 0.364813, acc 0.890625, learning_rate 0.00131471
2017-10-11T11:11:48.745210: step 343, loss 0.380216, acc 0.828125, learning_rate 0.00130975
2017-10-11T11:11:49.849983: step 344, loss 0.508145, acc 0.859375, learning_rate 0.00130482
2017-10-11T11:11:49.935109: step 345, loss 0.63034, acc 0.71875, learning_rate 0.0012999
2017-10-11T11:11:50.023237: step 346, loss 0.508227, acc 0.78125, learning_rate 0.001295
2017-10-11T11:11:50.110038: step 347, loss 0.584226, acc 0.828125, learning_rate 0.00129012
2017-10-11T11:11:50.200366: step 348, loss 0.463039, acc 0.859375, learning_rate 0.00128527
2017-10-11T11:11:50.283776: step 349, loss 0.471944, acc 0.84375, learning_rate 0.00128043
2017-10-11T11:11:50.380939: step 350, loss 0.276994, acc 0.921875, learning_rate 0.00127561
2017-10-11T11:11:50.464503: step 351, loss 0.477815, acc 0.796875, learning_rate 0.00127081
2017-10-11T11:11:50.563240: step 352, loss 0.354409, acc 0.890625, learning_rate 0.00126603
2017-10-11T11:11:50.682544: step 353, loss 0.582033, acc 0.765625, learning_rate 0.00126127
2017-10-11T11:11:50.788737: step 354, loss 0.493803, acc 0.796875, learning_rate 0.00125653
2017-10-11T11:11:50.908675: step 355, loss 0.446056, acc 0.875, learning_rate 0.00125181
2017-10-11T11:11:51.021210: step 356, loss 0.381285, acc 0.890625, learning_rate 0.00124711
2017-10-11T11:11:51.140577: step 357, loss 0.396816, acc 0.890625, learning_rate 0.00124243
2017-10-11T11:11:51.249225: step 358, loss 0.409385, acc 0.859375, learning_rate 0.00123777
2017-10-11T11:11:51.368180: step 359, loss 0.378708, acc 0.8125, learning_rate 0.00123312
2017-10-11T11:11:51.471453: step 360, loss 0.577891, acc 0.828125, learning_rate 0.0012285

Evaluation:
2017-10-11T11:11:51.729583: step 360, loss 0.301509, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-360

2017-10-11T11:11:52.428914: step 361, loss 0.317963, acc 0.890625, learning_rate 0.00122389
2017-10-11T11:11:52.539925: step 362, loss 0.348908, acc 0.90625, learning_rate 0.0012193
2017-10-11T11:11:52.658732: step 363, loss 0.477691, acc 0.828125, learning_rate 0.00121473
2017-10-11T11:11:52.775874: step 364, loss 0.53893, acc 0.84375, learning_rate 0.00121018
2017-10-11T11:11:52.887153: step 365, loss 0.42119, acc 0.828125, learning_rate 0.00120565
2017-10-11T11:11:53.004708: step 366, loss 0.454239, acc 0.8125, learning_rate 0.00120114
2017-10-11T11:11:53.116044: step 367, loss 0.50436, acc 0.8125, learning_rate 0.00119664
2017-10-11T11:11:53.243428: step 368, loss 0.344495, acc 0.84375, learning_rate 0.00119217
2017-10-11T11:11:53.358643: step 369, loss 0.51902, acc 0.875, learning_rate 0.00118771
2017-10-11T11:11:53.478578: step 370, loss 0.471334, acc 0.8125, learning_rate 0.00118327
2017-10-11T11:11:53.590094: step 371, loss 0.286324, acc 0.90625, learning_rate 0.00117885
2017-10-11T11:11:53.710789: step 372, loss 0.422193, acc 0.859375, learning_rate 0.00117445
2017-10-11T11:11:53.831789: step 373, loss 0.518351, acc 0.84375, learning_rate 0.00117006
2017-10-11T11:11:53.947022: step 374, loss 0.550134, acc 0.875, learning_rate 0.00116569
2017-10-11T11:11:54.056841: step 375, loss 0.371211, acc 0.890625, learning_rate 0.00116134
2017-10-11T11:11:54.171845: step 376, loss 0.33509, acc 0.875, learning_rate 0.00115701
2017-10-11T11:11:54.284876: step 377, loss 0.511557, acc 0.828125, learning_rate 0.0011527
2017-10-11T11:11:54.390405: step 378, loss 0.38037, acc 0.890625, learning_rate 0.0011484
2017-10-11T11:11:54.511987: step 379, loss 0.332426, acc 0.921875, learning_rate 0.00114412
2017-10-11T11:11:54.638622: step 380, loss 0.422385, acc 0.859375, learning_rate 0.00113986
2017-10-11T11:11:54.745695: step 381, loss 0.464175, acc 0.84375, learning_rate 0.00113561
2017-10-11T11:11:54.863368: step 382, loss 0.500006, acc 0.84375, learning_rate 0.00113139
2017-10-11T11:11:55.016963: step 383, loss 0.410807, acc 0.875, learning_rate 0.00112718
2017-10-11T11:11:55.101580: step 384, loss 0.381913, acc 0.875, learning_rate 0.00112298
2017-10-11T11:11:55.189613: step 385, loss 0.447108, acc 0.90625, learning_rate 0.00111881
2017-10-11T11:11:55.275946: step 386, loss 0.356272, acc 0.890625, learning_rate 0.00111465
2017-10-11T11:11:55.361602: step 387, loss 0.425275, acc 0.828125, learning_rate 0.00111051
2017-10-11T11:11:55.448565: step 388, loss 0.482858, acc 0.875, learning_rate 0.00110638
2017-10-11T11:11:55.537130: step 389, loss 0.463702, acc 0.78125, learning_rate 0.00110228
2017-10-11T11:11:55.639113: step 390, loss 0.333004, acc 0.859375, learning_rate 0.00109818
2017-10-11T11:11:55.738766: step 391, loss 0.467566, acc 0.875, learning_rate 0.00109411
2017-10-11T11:11:55.811391: step 392, loss 0.325143, acc 0.901961, learning_rate 0.00109005
2017-10-11T11:11:55.902637: step 393, loss 0.493471, acc 0.84375, learning_rate 0.00108601
2017-10-11T11:11:55.995836: step 394, loss 0.298751, acc 0.90625, learning_rate 0.00108199
2017-10-11T11:11:56.105033: step 395, loss 0.540139, acc 0.796875, learning_rate 0.00107798
2017-10-11T11:11:56.220886: step 396, loss 0.384837, acc 0.84375, learning_rate 0.00107399
2017-10-11T11:11:56.332994: step 397, loss 0.326464, acc 0.90625, learning_rate 0.00107001
2017-10-11T11:11:56.438461: step 398, loss 0.310992, acc 0.921875, learning_rate 0.00106605
2017-10-11T11:11:56.554916: step 399, loss 0.336087, acc 0.84375, learning_rate 0.00106211
2017-10-11T11:11:56.668858: step 400, loss 0.405581, acc 0.890625, learning_rate 0.00105818

Evaluation:
2017-10-11T11:11:56.903682: step 400, loss 0.289509, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-400

2017-10-11T11:11:57.674144: step 401, loss 0.466913, acc 0.796875, learning_rate 0.00105427
2017-10-11T11:11:57.774652: step 402, loss 0.678241, acc 0.828125, learning_rate 0.00105037
2017-10-11T11:11:57.885322: step 403, loss 0.427515, acc 0.859375, learning_rate 0.0010465
2017-10-11T11:11:57.985900: step 404, loss 0.602278, acc 0.828125, learning_rate 0.00104263
2017-10-11T11:11:58.099422: step 405, loss 0.486587, acc 0.84375, learning_rate 0.00103878
2017-10-11T11:11:58.217961: step 406, loss 0.492788, acc 0.859375, learning_rate 0.00103495
2017-10-11T11:11:58.333101: step 407, loss 0.701977, acc 0.765625, learning_rate 0.00103114
2017-10-11T11:11:58.449890: step 408, loss 0.34944, acc 0.890625, learning_rate 0.00102734
2017-10-11T11:11:58.572866: step 409, loss 0.444868, acc 0.875, learning_rate 0.00102355
2017-10-11T11:11:58.698319: step 410, loss 0.652138, acc 0.765625, learning_rate 0.00101978
2017-10-11T11:11:58.808616: step 411, loss 0.424863, acc 0.84375, learning_rate 0.00101603
2017-10-11T11:11:58.922457: step 412, loss 0.338978, acc 0.890625, learning_rate 0.00101229
2017-10-11T11:11:59.026842: step 413, loss 0.595325, acc 0.859375, learning_rate 0.00100856
2017-10-11T11:11:59.141132: step 414, loss 0.424253, acc 0.859375, learning_rate 0.00100486
2017-10-11T11:11:59.256152: step 415, loss 0.220347, acc 0.9375, learning_rate 0.00100116
2017-10-11T11:11:59.372359: step 416, loss 0.275343, acc 0.953125, learning_rate 0.000997483
2017-10-11T11:11:59.484397: step 417, loss 0.38496, acc 0.859375, learning_rate 0.00099382
2017-10-11T11:11:59.608288: step 418, loss 0.600981, acc 0.859375, learning_rate 0.000990172
2017-10-11T11:11:59.725097: step 419, loss 0.377988, acc 0.859375, learning_rate 0.000986538
2017-10-11T11:11:59.844909: step 420, loss 0.49803, acc 0.84375, learning_rate 0.00098292
2017-10-11T11:11:59.961625: step 421, loss 0.344253, acc 0.890625, learning_rate 0.000979316
2017-10-11T11:12:00.078407: step 422, loss 0.511485, acc 0.84375, learning_rate 0.000975727
2017-10-11T11:12:00.188672: step 423, loss 0.35634, acc 0.90625, learning_rate 0.000972152
2017-10-11T11:12:00.299590: step 424, loss 0.361223, acc 0.875, learning_rate 0.000968592
2017-10-11T11:12:00.487514: step 425, loss 0.460784, acc 0.859375, learning_rate 0.000965047
2017-10-11T11:12:00.590628: step 426, loss 0.439726, acc 0.859375, learning_rate 0.000961516
2017-10-11T11:12:00.688718: step 427, loss 0.241799, acc 0.90625, learning_rate 0.000958
2017-10-11T11:12:00.783943: step 428, loss 0.344534, acc 0.875, learning_rate 0.000954497
2017-10-11T11:12:00.869522: step 429, loss 0.378083, acc 0.890625, learning_rate 0.00095101
2017-10-11T11:12:00.958738: step 430, loss 0.360723, acc 0.921875, learning_rate 0.000947536
2017-10-11T11:12:01.043137: step 431, loss 0.56319, acc 0.828125, learning_rate 0.000944076
2017-10-11T11:12:01.130698: step 432, loss 0.337549, acc 0.859375, learning_rate 0.000940631
2017-10-11T11:12:01.229704: step 433, loss 0.415736, acc 0.84375, learning_rate 0.0009372
2017-10-11T11:12:01.350570: step 434, loss 0.30134, acc 0.921875, learning_rate 0.000933783
2017-10-11T11:12:01.459387: step 435, loss 0.363221, acc 0.875, learning_rate 0.000930379
2017-10-11T11:12:01.579131: step 436, loss 0.315619, acc 0.90625, learning_rate 0.00092699
2017-10-11T11:12:01.684964: step 437, loss 0.295531, acc 0.90625, learning_rate 0.000923614
2017-10-11T11:12:01.798546: step 438, loss 0.282759, acc 0.890625, learning_rate 0.000920253
2017-10-11T11:12:01.917310: step 439, loss 0.315601, acc 0.84375, learning_rate 0.000916905
2017-10-11T11:12:02.035364: step 440, loss 0.531488, acc 0.796875, learning_rate 0.00091357

Evaluation:
2017-10-11T11:12:02.285908: step 440, loss 0.284885, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-440

2017-10-11T11:12:02.992911: step 441, loss 0.440427, acc 0.84375, learning_rate 0.000910249
2017-10-11T11:12:03.103378: step 442, loss 0.313018, acc 0.890625, learning_rate 0.000906942
2017-10-11T11:12:03.220894: step 443, loss 0.333149, acc 0.875, learning_rate 0.000903648
2017-10-11T11:12:03.340864: step 444, loss 0.627487, acc 0.765625, learning_rate 0.000900368
2017-10-11T11:12:03.461524: step 445, loss 0.61156, acc 0.796875, learning_rate 0.000897101
2017-10-11T11:12:03.567983: step 446, loss 0.503363, acc 0.84375, learning_rate 0.000893848
2017-10-11T11:12:03.679278: step 447, loss 0.325932, acc 0.890625, learning_rate 0.000890607
2017-10-11T11:12:03.787216: step 448, loss 0.470109, acc 0.84375, learning_rate 0.00088738
2017-10-11T11:12:03.907952: step 449, loss 0.551685, acc 0.828125, learning_rate 0.000884166
2017-10-11T11:12:04.025260: step 450, loss 0.346564, acc 0.875, learning_rate 0.000880966
2017-10-11T11:12:04.142817: step 451, loss 0.24792, acc 0.9375, learning_rate 0.000877778
2017-10-11T11:12:04.247399: step 452, loss 0.47173, acc 0.796875, learning_rate 0.000874603
2017-10-11T11:12:04.371033: step 453, loss 0.574258, acc 0.84375, learning_rate 0.000871441
2017-10-11T11:12:04.477628: step 454, loss 0.383188, acc 0.890625, learning_rate 0.000868293
2017-10-11T11:12:04.599330: step 455, loss 0.644835, acc 0.71875, learning_rate 0.000865157
2017-10-11T11:12:04.707432: step 456, loss 0.491003, acc 0.8125, learning_rate 0.000862033
2017-10-11T11:12:04.815888: step 457, loss 0.374361, acc 0.9375, learning_rate 0.000858923
2017-10-11T11:12:04.930240: step 458, loss 0.468448, acc 0.8125, learning_rate 0.000855825
2017-10-11T11:12:05.050463: step 459, loss 0.386426, acc 0.859375, learning_rate 0.00085274
2017-10-11T11:12:05.171186: step 460, loss 0.298798, acc 0.875, learning_rate 0.000849668
2017-10-11T11:12:05.282430: step 461, loss 0.383753, acc 0.84375, learning_rate 0.000846608
2017-10-11T11:12:05.394938: step 462, loss 0.602831, acc 0.8125, learning_rate 0.00084356
2017-10-11T11:12:05.582229: step 463, loss 0.605601, acc 0.75, learning_rate 0.000840525
2017-10-11T11:12:05.673728: step 464, loss 0.393684, acc 0.859375, learning_rate 0.000837502
2017-10-11T11:12:05.761524: step 465, loss 0.338775, acc 0.890625, learning_rate 0.000834492
2017-10-11T11:12:05.845302: step 466, loss 0.530006, acc 0.765625, learning_rate 0.000831494
2017-10-11T11:12:05.932563: step 467, loss 0.37462, acc 0.84375, learning_rate 0.000828508
2017-10-11T11:12:06.028417: step 468, loss 0.543737, acc 0.78125, learning_rate 0.000825535
2017-10-11T11:12:06.115621: step 469, loss 0.639707, acc 0.78125, learning_rate 0.000822573
2017-10-11T11:12:06.198204: step 470, loss 0.404073, acc 0.90625, learning_rate 0.000819624
2017-10-11T11:12:06.287441: step 471, loss 0.417017, acc 0.828125, learning_rate 0.000816687
2017-10-11T11:12:06.371429: step 472, loss 0.404639, acc 0.84375, learning_rate 0.000813761
2017-10-11T11:12:06.481503: step 473, loss 0.42348, acc 0.84375, learning_rate 0.000810848
2017-10-11T11:12:06.606870: step 474, loss 0.308596, acc 0.921875, learning_rate 0.000807946
2017-10-11T11:12:06.737557: step 475, loss 0.514626, acc 0.890625, learning_rate 0.000805057
2017-10-11T11:12:06.839676: step 476, loss 0.458107, acc 0.84375, learning_rate 0.000802179
2017-10-11T11:12:06.953466: step 477, loss 0.349193, acc 0.859375, learning_rate 0.000799313
2017-10-11T11:12:07.055770: step 478, loss 0.394319, acc 0.875, learning_rate 0.000796458
2017-10-11T11:12:07.160074: step 479, loss 0.560061, acc 0.828125, learning_rate 0.000793616
2017-10-11T11:12:07.281293: step 480, loss 0.409011, acc 0.890625, learning_rate 0.000790784

Evaluation:
2017-10-11T11:12:07.524241: step 480, loss 0.284234, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-480

2017-10-11T11:12:08.264442: step 481, loss 0.391039, acc 0.84375, learning_rate 0.000787965
2017-10-11T11:12:08.391765: step 482, loss 0.611813, acc 0.78125, learning_rate 0.000785157
2017-10-11T11:12:08.497954: step 483, loss 0.370638, acc 0.921875, learning_rate 0.00078236
2017-10-11T11:12:08.605789: step 484, loss 0.441094, acc 0.859375, learning_rate 0.000779575
2017-10-11T11:12:08.715186: step 485, loss 0.235407, acc 0.953125, learning_rate 0.000776801
2017-10-11T11:12:08.836411: step 486, loss 0.447647, acc 0.859375, learning_rate 0.000774038
2017-10-11T11:12:08.958639: step 487, loss 0.351278, acc 0.859375, learning_rate 0.000771287
2017-10-11T11:12:09.072270: step 488, loss 0.519261, acc 0.828125, learning_rate 0.000768547
2017-10-11T11:12:09.194554: step 489, loss 0.350059, acc 0.875, learning_rate 0.000765818
2017-10-11T11:12:09.284877: step 490, loss 0.431188, acc 0.862745, learning_rate 0.000763101
2017-10-11T11:12:09.398131: step 491, loss 0.249316, acc 0.921875, learning_rate 0.000760394
2017-10-11T11:12:09.496492: step 492, loss 0.432637, acc 0.875, learning_rate 0.000757698
2017-10-11T11:12:09.606230: step 493, loss 0.344329, acc 0.890625, learning_rate 0.000755014
2017-10-11T11:12:09.715382: step 494, loss 0.495507, acc 0.859375, learning_rate 0.00075234
2017-10-11T11:12:09.829302: step 495, loss 0.425093, acc 0.859375, learning_rate 0.000749677
2017-10-11T11:12:09.946074: step 496, loss 0.428025, acc 0.890625, learning_rate 0.000747026
2017-10-11T11:12:10.060594: step 497, loss 0.42941, acc 0.859375, learning_rate 0.000744385
2017-10-11T11:12:10.176594: step 498, loss 0.459242, acc 0.859375, learning_rate 0.000741754
2017-10-11T11:12:10.292895: step 499, loss 0.488721, acc 0.84375, learning_rate 0.000739135
2017-10-11T11:12:10.398192: step 500, loss 0.301561, acc 0.90625, learning_rate 0.000736526
2017-10-11T11:12:10.511186: step 501, loss 0.288425, acc 0.875, learning_rate 0.000733928
2017-10-11T11:12:10.627830: step 502, loss 0.476513, acc 0.859375, learning_rate 0.00073134
2017-10-11T11:12:10.736667: step 503, loss 0.400341, acc 0.84375, learning_rate 0.000728763
2017-10-11T11:12:10.927263: step 504, loss 0.377933, acc 0.90625, learning_rate 0.000726197
2017-10-11T11:12:11.012499: step 505, loss 0.389926, acc 0.875, learning_rate 0.000723641
2017-10-11T11:12:11.101132: step 506, loss 0.400519, acc 0.875, learning_rate 0.000721095
2017-10-11T11:12:11.192695: step 507, loss 0.389499, acc 0.859375, learning_rate 0.00071856
2017-10-11T11:12:11.280920: step 508, loss 0.429055, acc 0.875, learning_rate 0.000716036
2017-10-11T11:12:11.369242: step 509, loss 0.480559, acc 0.8125, learning_rate 0.000713521
2017-10-11T11:12:11.453730: step 510, loss 0.397941, acc 0.875, learning_rate 0.000711017
2017-10-11T11:12:11.538830: step 511, loss 0.339318, acc 0.890625, learning_rate 0.000708523
2017-10-11T11:12:11.627202: step 512, loss 0.600872, acc 0.8125, learning_rate 0.000706039
2017-10-11T11:12:11.719324: step 513, loss 0.481721, acc 0.890625, learning_rate 0.000703565
2017-10-11T11:12:11.831980: step 514, loss 0.264797, acc 0.890625, learning_rate 0.000701102
2017-10-11T11:12:11.948921: step 515, loss 0.55186, acc 0.84375, learning_rate 0.000698648
2017-10-11T11:12:12.064899: step 516, loss 0.479432, acc 0.84375, learning_rate 0.000696204
2017-10-11T11:12:12.183588: step 517, loss 0.488069, acc 0.796875, learning_rate 0.000693771
2017-10-11T11:12:12.292595: step 518, loss 0.403162, acc 0.84375, learning_rate 0.000691347
2017-10-11T11:12:12.401628: step 519, loss 0.287508, acc 0.90625, learning_rate 0.000688934
2017-10-11T11:12:12.525686: step 520, loss 0.278201, acc 0.875, learning_rate 0.00068653

Evaluation:
2017-10-11T11:12:12.756758: step 520, loss 0.278854, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-520

2017-10-11T11:12:13.495146: step 521, loss 0.488229, acc 0.8125, learning_rate 0.000684136
2017-10-11T11:12:13.597201: step 522, loss 0.522103, acc 0.8125, learning_rate 0.000681751
2017-10-11T11:12:13.705979: step 523, loss 0.523993, acc 0.828125, learning_rate 0.000679377
2017-10-11T11:12:13.820102: step 524, loss 0.332005, acc 0.875, learning_rate 0.000677012
2017-10-11T11:12:13.944871: step 525, loss 0.27509, acc 0.921875, learning_rate 0.000674657
2017-10-11T11:12:14.056944: step 526, loss 0.405843, acc 0.828125, learning_rate 0.000672311
2017-10-11T11:12:14.171406: step 527, loss 0.278728, acc 0.890625, learning_rate 0.000669975
2017-10-11T11:12:14.284910: step 528, loss 0.445788, acc 0.875, learning_rate 0.000667648
2017-10-11T11:12:14.401890: step 529, loss 0.346924, acc 0.875, learning_rate 0.000665331
2017-10-11T11:12:14.524870: step 530, loss 0.310136, acc 0.90625, learning_rate 0.000663024
2017-10-11T11:12:14.634063: step 531, loss 0.30912, acc 0.875, learning_rate 0.000660726
2017-10-11T11:12:14.745188: step 532, loss 0.745195, acc 0.75, learning_rate 0.000658437
2017-10-11T11:12:14.865553: step 533, loss 0.440841, acc 0.84375, learning_rate 0.000656158
2017-10-11T11:12:14.978141: step 534, loss 0.430202, acc 0.859375, learning_rate 0.000653888
2017-10-11T11:12:15.093075: step 535, loss 0.432761, acc 0.8125, learning_rate 0.000651627
2017-10-11T11:12:15.203640: step 536, loss 0.31181, acc 0.859375, learning_rate 0.000649375
2017-10-11T11:12:15.322997: step 537, loss 0.491138, acc 0.828125, learning_rate 0.000647133
2017-10-11T11:12:15.440481: step 538, loss 0.398405, acc 0.859375, learning_rate 0.000644899
2017-10-11T11:12:15.555457: step 539, loss 0.462664, acc 0.859375, learning_rate 0.000642675
2017-10-11T11:12:15.669932: step 540, loss 0.367217, acc 0.921875, learning_rate 0.00064046
2017-10-11T11:12:15.785968: step 541, loss 0.334787, acc 0.90625, learning_rate 0.000638254
2017-10-11T11:12:15.902094: step 542, loss 0.314262, acc 0.875, learning_rate 0.000636057
2017-10-11T11:12:16.069065: step 543, loss 0.237184, acc 0.9375, learning_rate 0.000633869
2017-10-11T11:12:16.173613: step 544, loss 0.643048, acc 0.734375, learning_rate 0.00063169
2017-10-11T11:12:16.264101: step 545, loss 0.390616, acc 0.890625, learning_rate 0.00062952
2017-10-11T11:12:16.353151: step 546, loss 0.373299, acc 0.859375, learning_rate 0.000627358
2017-10-11T11:12:16.438141: step 547, loss 0.412663, acc 0.84375, learning_rate 0.000625206
2017-10-11T11:12:16.526773: step 548, loss 0.357396, acc 0.84375, learning_rate 0.000623062
2017-10-11T11:12:16.610862: step 549, loss 0.468979, acc 0.8125, learning_rate 0.000620927
2017-10-11T11:12:16.695809: step 550, loss 0.315258, acc 0.890625, learning_rate 0.000618801
2017-10-11T11:12:16.783459: step 551, loss 0.33949, acc 0.875, learning_rate 0.000616683
2017-10-11T11:12:16.872260: step 552, loss 0.303759, acc 0.890625, learning_rate 0.000614574
2017-10-11T11:12:16.960194: step 553, loss 0.414187, acc 0.828125, learning_rate 0.000612474
2017-10-11T11:12:17.048591: step 554, loss 0.322534, acc 0.859375, learning_rate 0.000610382
2017-10-11T11:12:17.169435: step 555, loss 0.203646, acc 0.953125, learning_rate 0.000608299
2017-10-11T11:12:17.284555: step 556, loss 0.384302, acc 0.859375, learning_rate 0.000606224
2017-10-11T11:12:17.392930: step 557, loss 0.362572, acc 0.921875, learning_rate 0.000604158
2017-10-11T11:12:17.514368: step 558, loss 0.373211, acc 0.859375, learning_rate 0.0006021
2017-10-11T11:12:17.631294: step 559, loss 0.489118, acc 0.859375, learning_rate 0.00060005
2017-10-11T11:12:17.751877: step 560, loss 0.307992, acc 0.90625, learning_rate 0.000598009

Evaluation:
2017-10-11T11:12:17.982404: step 560, loss 0.279665, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-560

2017-10-11T11:12:18.727134: step 561, loss 0.406974, acc 0.859375, learning_rate 0.000595977
2017-10-11T11:12:18.844599: step 562, loss 0.557202, acc 0.796875, learning_rate 0.000593952
2017-10-11T11:12:18.957620: step 563, loss 0.29413, acc 0.859375, learning_rate 0.000591936
2017-10-11T11:12:19.080212: step 564, loss 0.491225, acc 0.84375, learning_rate 0.000589928
2017-10-11T11:12:19.193636: step 565, loss 0.326197, acc 0.84375, learning_rate 0.000587928
2017-10-11T11:12:19.309042: step 566, loss 0.534143, acc 0.8125, learning_rate 0.000585937
2017-10-11T11:12:19.428498: step 567, loss 0.666933, acc 0.796875, learning_rate 0.000583953
2017-10-11T11:12:19.542036: step 568, loss 0.389602, acc 0.859375, learning_rate 0.000581978
2017-10-11T11:12:19.664148: step 569, loss 0.465617, acc 0.8125, learning_rate 0.00058001
2017-10-11T11:12:19.783656: step 570, loss 0.509291, acc 0.84375, learning_rate 0.000578051
2017-10-11T11:12:19.901985: step 571, loss 0.343508, acc 0.921875, learning_rate 0.0005761
2017-10-11T11:12:19.999980: step 572, loss 0.20814, acc 0.96875, learning_rate 0.000574157
2017-10-11T11:12:20.120287: step 573, loss 0.384026, acc 0.84375, learning_rate 0.000572221
2017-10-11T11:12:20.243089: step 574, loss 0.323185, acc 0.890625, learning_rate 0.000570294
2017-10-11T11:12:20.361168: step 575, loss 0.295631, acc 0.921875, learning_rate 0.000568374
2017-10-11T11:12:20.472229: step 576, loss 0.407053, acc 0.84375, learning_rate 0.000566462
2017-10-11T11:12:20.586436: step 577, loss 0.408636, acc 0.828125, learning_rate 0.000564558
2017-10-11T11:12:20.699708: step 578, loss 0.413479, acc 0.859375, learning_rate 0.000562662
2017-10-11T11:12:20.816257: step 579, loss 0.438005, acc 0.875, learning_rate 0.000560774
2017-10-11T11:12:20.932840: step 580, loss 0.315923, acc 0.921875, learning_rate 0.000558893
2017-10-11T11:12:21.044199: step 581, loss 0.389848, acc 0.875, learning_rate 0.00055702
2017-10-11T11:12:21.159141: step 582, loss 0.306753, acc 0.90625, learning_rate 0.000555154
2017-10-11T11:12:21.312869: step 583, loss 0.449391, acc 0.875, learning_rate 0.000553296
2017-10-11T11:12:21.432414: step 584, loss 0.390367, acc 0.84375, learning_rate 0.000551446
2017-10-11T11:12:21.518943: step 585, loss 0.357014, acc 0.859375, learning_rate 0.000549604
2017-10-11T11:12:21.603162: step 586, loss 0.290224, acc 0.890625, learning_rate 0.000547768
2017-10-11T11:12:21.688193: step 587, loss 0.302691, acc 0.953125, learning_rate 0.000545941
2017-10-11T11:12:21.769023: step 588, loss 0.596865, acc 0.803922, learning_rate 0.00054412
2017-10-11T11:12:21.858177: step 589, loss 0.28999, acc 0.90625, learning_rate 0.000542308
2017-10-11T11:12:21.944089: step 590, loss 0.460803, acc 0.84375, learning_rate 0.000540502
2017-10-11T11:12:22.033092: step 591, loss 0.44108, acc 0.8125, learning_rate 0.000538704
2017-10-11T11:12:22.142474: step 592, loss 0.366317, acc 0.890625, learning_rate 0.000536914
2017-10-11T11:12:22.257699: step 593, loss 0.435859, acc 0.875, learning_rate 0.00053513
2017-10-11T11:12:22.379537: step 594, loss 0.224944, acc 0.953125, learning_rate 0.000533354
2017-10-11T11:12:22.485881: step 595, loss 0.407011, acc 0.84375, learning_rate 0.000531585
2017-10-11T11:12:22.604770: step 596, loss 0.304786, acc 0.890625, learning_rate 0.000529824
2017-10-11T11:12:22.722234: step 597, loss 0.43894, acc 0.859375, learning_rate 0.000528069
2017-10-11T11:12:22.841523: step 598, loss 0.354942, acc 0.859375, learning_rate 0.000526322
2017-10-11T11:12:22.960862: step 599, loss 0.446581, acc 0.828125, learning_rate 0.000524582
2017-10-11T11:12:23.073299: step 600, loss 0.322822, acc 0.890625, learning_rate 0.000522849

Evaluation:
2017-10-11T11:12:23.299269: step 600, loss 0.274287, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-600

2017-10-11T11:12:24.025640: step 601, loss 0.275459, acc 0.9375, learning_rate 0.000521123
2017-10-11T11:12:24.136030: step 602, loss 0.385501, acc 0.859375, learning_rate 0.000519404
2017-10-11T11:12:24.244400: step 603, loss 0.33328, acc 0.90625, learning_rate 0.000517692
2017-10-11T11:12:24.376829: step 604, loss 0.414042, acc 0.875, learning_rate 0.000515987
2017-10-11T11:12:24.488411: step 605, loss 0.321679, acc 0.921875, learning_rate 0.000514289
2017-10-11T11:12:24.604363: step 606, loss 0.302354, acc 0.875, learning_rate 0.000512598
2017-10-11T11:12:24.728553: step 607, loss 0.387129, acc 0.84375, learning_rate 0.000510914
2017-10-11T11:12:24.824747: step 608, loss 0.422125, acc 0.890625, learning_rate 0.000509237
2017-10-11T11:12:24.945031: step 609, loss 0.366118, acc 0.84375, learning_rate 0.000507566
2017-10-11T11:12:25.054455: step 610, loss 0.446377, acc 0.796875, learning_rate 0.000505903
2017-10-11T11:12:25.169199: step 611, loss 0.308838, acc 0.890625, learning_rate 0.000504246
2017-10-11T11:12:25.293598: step 612, loss 0.561177, acc 0.84375, learning_rate 0.000502596
2017-10-11T11:12:25.402925: step 613, loss 0.46953, acc 0.8125, learning_rate 0.000500953
2017-10-11T11:12:25.518082: step 614, loss 0.592951, acc 0.796875, learning_rate 0.000499316
2017-10-11T11:12:25.631502: step 615, loss 0.426251, acc 0.859375, learning_rate 0.000497686
2017-10-11T11:12:25.739244: step 616, loss 0.299688, acc 0.875, learning_rate 0.000496063
2017-10-11T11:12:25.860055: step 617, loss 0.463894, acc 0.8125, learning_rate 0.000494446
2017-10-11T11:12:25.968121: step 618, loss 0.488212, acc 0.859375, learning_rate 0.000492836
2017-10-11T11:12:26.075640: step 619, loss 0.417859, acc 0.828125, learning_rate 0.000491233
2017-10-11T11:12:26.168941: step 620, loss 0.366525, acc 0.890625, learning_rate 0.000489636
2017-10-11T11:12:26.285105: step 621, loss 0.263456, acc 0.921875, learning_rate 0.000488045
2017-10-11T11:12:26.429143: step 622, loss 0.483875, acc 0.8125, learning_rate 0.000486461
2017-10-11T11:12:26.571639: step 623, loss 0.388443, acc 0.84375, learning_rate 0.000484884
2017-10-11T11:12:26.659373: step 624, loss 0.214178, acc 0.953125, learning_rate 0.000483313
2017-10-11T11:12:26.744620: step 625, loss 0.373695, acc 0.890625, learning_rate 0.000481748
2017-10-11T11:12:26.833774: step 626, loss 0.329716, acc 0.890625, learning_rate 0.00048019
2017-10-11T11:12:26.917748: step 627, loss 0.362839, acc 0.875, learning_rate 0.000478638
2017-10-11T11:12:27.003500: step 628, loss 0.461255, acc 0.859375, learning_rate 0.000477093
2017-10-11T11:12:27.086960: step 629, loss 0.297661, acc 0.890625, learning_rate 0.000475554
2017-10-11T11:12:27.175965: step 630, loss 0.319608, acc 0.890625, learning_rate 0.000474021
2017-10-11T11:12:27.262248: step 631, loss 0.258144, acc 0.921875, learning_rate 0.000472494
2017-10-11T11:12:27.356863: step 632, loss 0.359417, acc 0.875, learning_rate 0.000470974
2017-10-11T11:12:27.462636: step 633, loss 0.384316, acc 0.859375, learning_rate 0.000469459
2017-10-11T11:12:27.579231: step 634, loss 0.402421, acc 0.84375, learning_rate 0.000467951
2017-10-11T11:12:27.692818: step 635, loss 0.520184, acc 0.8125, learning_rate 0.000466449
2017-10-11T11:12:27.800875: step 636, loss 0.245165, acc 0.953125, learning_rate 0.000464954
2017-10-11T11:12:27.916876: step 637, loss 0.592914, acc 0.828125, learning_rate 0.000463464
2017-10-11T11:12:28.036097: step 638, loss 0.251644, acc 0.90625, learning_rate 0.00046198
2017-10-11T11:12:28.144845: step 639, loss 0.55699, acc 0.84375, learning_rate 0.000460503
2017-10-11T11:12:28.253509: step 640, loss 0.342382, acc 0.875, learning_rate 0.000459031

Evaluation:
2017-10-11T11:12:28.515911: step 640, loss 0.272929, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-640

2017-10-11T11:12:29.261496: step 641, loss 0.331331, acc 0.875, learning_rate 0.000457566
2017-10-11T11:12:29.375967: step 642, loss 0.26477, acc 0.875, learning_rate 0.000456106
2017-10-11T11:12:29.492982: step 643, loss 0.370553, acc 0.875, learning_rate 0.000454653
2017-10-11T11:12:29.606179: step 644, loss 0.43044, acc 0.828125, learning_rate 0.000453205
2017-10-11T11:12:29.708476: step 645, loss 0.554839, acc 0.78125, learning_rate 0.000451764
2017-10-11T11:12:29.824667: step 646, loss 0.525856, acc 0.84375, learning_rate 0.000450328
2017-10-11T11:12:29.938978: step 647, loss 0.233981, acc 0.953125, learning_rate 0.000448898
2017-10-11T11:12:30.062024: step 648, loss 0.393377, acc 0.875, learning_rate 0.000447474
2017-10-11T11:12:30.183231: step 649, loss 0.386053, acc 0.875, learning_rate 0.000446055
2017-10-11T11:12:30.299687: step 650, loss 0.603599, acc 0.78125, learning_rate 0.000444643
2017-10-11T11:12:30.419943: step 651, loss 0.597128, acc 0.78125, learning_rate 0.000443236
2017-10-11T11:12:30.533021: step 652, loss 0.378325, acc 0.875, learning_rate 0.000441835
2017-10-11T11:12:30.647469: step 653, loss 0.538054, acc 0.859375, learning_rate 0.00044044
2017-10-11T11:12:30.755805: step 654, loss 0.296263, acc 0.890625, learning_rate 0.00043905
2017-10-11T11:12:30.873482: step 655, loss 0.273278, acc 0.90625, learning_rate 0.000437666
2017-10-11T11:12:30.990024: step 656, loss 0.361966, acc 0.90625, learning_rate 0.000436288
2017-10-11T11:12:31.103185: step 657, loss 0.448201, acc 0.84375, learning_rate 0.000434915
2017-10-11T11:12:31.218000: step 658, loss 0.373561, acc 0.828125, learning_rate 0.000433548
2017-10-11T11:12:31.337925: step 659, loss 0.412584, acc 0.828125, learning_rate 0.000432187
2017-10-11T11:12:31.455091: step 660, loss 0.64357, acc 0.78125, learning_rate 0.000430831
2017-10-11T11:12:31.573392: step 661, loss 0.27657, acc 0.90625, learning_rate 0.000429481
2017-10-11T11:12:31.689816: step 662, loss 0.477201, acc 0.8125, learning_rate 0.000428136
2017-10-11T11:12:31.875109: step 663, loss 0.348114, acc 0.90625, learning_rate 0.000426796
2017-10-11T11:12:31.965844: step 664, loss 0.546394, acc 0.8125, learning_rate 0.000425463
2017-10-11T11:12:32.051330: step 665, loss 0.282779, acc 0.890625, learning_rate 0.000424134
2017-10-11T11:12:32.142013: step 666, loss 0.319284, acc 0.859375, learning_rate 0.000422811
2017-10-11T11:12:32.229468: step 667, loss 0.374658, acc 0.875, learning_rate 0.000421493
2017-10-11T11:12:32.330530: step 668, loss 0.422943, acc 0.828125, learning_rate 0.000420181
2017-10-11T11:12:32.416399: step 669, loss 0.339714, acc 0.890625, learning_rate 0.000418874
2017-10-11T11:12:32.502354: step 670, loss 0.35175, acc 0.875, learning_rate 0.000417573
2017-10-11T11:12:32.589257: step 671, loss 0.450755, acc 0.84375, learning_rate 0.000416276
2017-10-11T11:12:32.675519: step 672, loss 0.465821, acc 0.828125, learning_rate 0.000414985
2017-10-11T11:12:32.759904: step 673, loss 0.422487, acc 0.84375, learning_rate 0.0004137
2017-10-11T11:12:32.864589: step 674, loss 0.359409, acc 0.84375, learning_rate 0.000412419
2017-10-11T11:12:32.975995: step 675, loss 0.437356, acc 0.859375, learning_rate 0.000411144
2017-10-11T11:12:33.089740: step 676, loss 0.305127, acc 0.921875, learning_rate 0.000409874
2017-10-11T11:12:33.206443: step 677, loss 0.511897, acc 0.828125, learning_rate 0.000408609
2017-10-11T11:12:33.324712: step 678, loss 0.374665, acc 0.90625, learning_rate 0.00040735
2017-10-11T11:12:33.434359: step 679, loss 0.675919, acc 0.796875, learning_rate 0.000406095
2017-10-11T11:12:33.559407: step 680, loss 0.29694, acc 0.890625, learning_rate 0.000404846

Evaluation:
2017-10-11T11:12:33.821014: step 680, loss 0.271085, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-680

2017-10-11T11:12:34.521959: step 681, loss 0.400437, acc 0.859375, learning_rate 0.000403601
2017-10-11T11:12:34.634012: step 682, loss 0.45805, acc 0.875, learning_rate 0.000402362
2017-10-11T11:12:34.753826: step 683, loss 0.391561, acc 0.828125, learning_rate 0.000401128
2017-10-11T11:12:34.857100: step 684, loss 0.330485, acc 0.921875, learning_rate 0.000399899
2017-10-11T11:12:34.980854: step 685, loss 0.347182, acc 0.84375, learning_rate 0.000398675
2017-10-11T11:12:35.084868: step 686, loss 0.438846, acc 0.823529, learning_rate 0.000397456
2017-10-11T11:12:35.196908: step 687, loss 0.42919, acc 0.875, learning_rate 0.000396241
2017-10-11T11:12:35.308790: step 688, loss 0.280621, acc 0.890625, learning_rate 0.000395032
2017-10-11T11:12:35.430726: step 689, loss 0.204006, acc 0.921875, learning_rate 0.000393828
2017-10-11T11:12:35.536930: step 690, loss 0.259428, acc 0.90625, learning_rate 0.000392629
2017-10-11T11:12:35.645669: step 691, loss 0.369336, acc 0.90625, learning_rate 0.000391434
2017-10-11T11:12:35.748165: step 692, loss 0.361563, acc 0.875, learning_rate 0.000390245
2017-10-11T11:12:35.864512: step 693, loss 0.291059, acc 0.921875, learning_rate 0.00038906
2017-10-11T11:12:35.980912: step 694, loss 0.350097, acc 0.890625, learning_rate 0.00038788
2017-10-11T11:12:36.080536: step 695, loss 0.441599, acc 0.859375, learning_rate 0.000386705
2017-10-11T11:12:36.192873: step 696, loss 0.567761, acc 0.796875, learning_rate 0.000385535
2017-10-11T11:12:36.300933: step 697, loss 0.286375, acc 0.9375, learning_rate 0.000384369
2017-10-11T11:12:36.416868: step 698, loss 0.364861, acc 0.90625, learning_rate 0.000383209
2017-10-11T11:12:36.533176: step 699, loss 0.22192, acc 0.9375, learning_rate 0.000382053
2017-10-11T11:12:36.648475: step 700, loss 0.624794, acc 0.765625, learning_rate 0.000380901
2017-10-11T11:12:36.764810: step 701, loss 0.285743, acc 0.890625, learning_rate 0.000379755
2017-10-11T11:12:36.879669: step 702, loss 0.352102, acc 0.875, learning_rate 0.000378613
2017-10-11T11:12:36.998379: step 703, loss 0.433044, acc 0.84375, learning_rate 0.000377476
2017-10-11T11:12:37.101869: step 704, loss 0.305266, acc 0.890625, learning_rate 0.000376343
2017-10-11T11:12:37.257342: step 705, loss 0.369037, acc 0.84375, learning_rate 0.000375215
2017-10-11T11:12:37.339517: step 706, loss 0.374105, acc 0.890625, learning_rate 0.000374092
2017-10-11T11:12:37.651586: step 707, loss 0.390034, acc 0.84375, learning_rate 0.000372973
2017-10-11T11:12:37.751200: step 708, loss 0.397963, acc 0.875, learning_rate 0.000371859
2017-10-11T11:12:37.838435: step 709, loss 0.409994, acc 0.84375, learning_rate 0.000370749
2017-10-11T11:12:37.931725: step 710, loss 0.317036, acc 0.875, learning_rate 0.000369644
2017-10-11T11:12:38.014917: step 711, loss 0.295529, acc 0.9375, learning_rate 0.000368543
2017-10-11T11:12:38.106914: step 712, loss 0.327191, acc 0.875, learning_rate 0.000367447
2017-10-11T11:12:38.191578: step 713, loss 0.295185, acc 0.890625, learning_rate 0.000366356
2017-10-11T11:12:38.313167: step 714, loss 0.467933, acc 0.859375, learning_rate 0.000365268
2017-10-11T11:12:38.435237: step 715, loss 0.375219, acc 0.890625, learning_rate 0.000364186
2017-10-11T11:12:38.552921: step 716, loss 0.435889, acc 0.859375, learning_rate 0.000363107
2017-10-11T11:12:38.665946: step 717, loss 0.335882, acc 0.890625, learning_rate 0.000362033
2017-10-11T11:12:38.790522: step 718, loss 0.498363, acc 0.78125, learning_rate 0.000360964
2017-10-11T11:12:38.911363: step 719, loss 0.260458, acc 0.921875, learning_rate 0.000359899
2017-10-11T11:12:39.029207: step 720, loss 0.419601, acc 0.875, learning_rate 0.000358838

Evaluation:
2017-10-11T11:12:39.273491: step 720, loss 0.268704, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-720

2017-10-11T11:12:39.888739: step 721, loss 0.420972, acc 0.828125, learning_rate 0.000357781
2017-10-11T11:12:39.997612: step 722, loss 0.464752, acc 0.859375, learning_rate 0.000356729
2017-10-11T11:12:40.109577: step 723, loss 0.345978, acc 0.921875, learning_rate 0.000355681
2017-10-11T11:12:40.225128: step 724, loss 0.265157, acc 0.890625, learning_rate 0.000354637
2017-10-11T11:12:40.352937: step 725, loss 0.338409, acc 0.90625, learning_rate 0.000353598
2017-10-11T11:12:40.468790: step 726, loss 0.424862, acc 0.890625, learning_rate 0.000352563
2017-10-11T11:12:40.574767: step 727, loss 0.418994, acc 0.859375, learning_rate 0.000351532
2017-10-11T11:12:40.686975: step 728, loss 0.62338, acc 0.796875, learning_rate 0.000350505
2017-10-11T11:12:40.808944: step 729, loss 0.284687, acc 0.921875, learning_rate 0.000349483
2017-10-11T11:12:40.912033: step 730, loss 0.239423, acc 0.9375, learning_rate 0.000348465
2017-10-11T11:12:41.028873: step 731, loss 0.331919, acc 0.921875, learning_rate 0.00034745
2017-10-11T11:12:41.137863: step 732, loss 0.264473, acc 0.859375, learning_rate 0.00034644
2017-10-11T11:12:41.249765: step 733, loss 0.302728, acc 0.875, learning_rate 0.000345434
2017-10-11T11:12:41.358308: step 734, loss 0.279276, acc 0.890625, learning_rate 0.000344433
2017-10-11T11:12:41.474491: step 735, loss 0.282182, acc 0.9375, learning_rate 0.000343435
2017-10-11T11:12:41.588315: step 736, loss 0.443358, acc 0.828125, learning_rate 0.000342441
2017-10-11T11:12:41.694371: step 737, loss 0.306941, acc 0.890625, learning_rate 0.000341452
2017-10-11T11:12:41.803194: step 738, loss 0.315458, acc 0.90625, learning_rate 0.000340466
2017-10-11T11:12:41.933244: step 739, loss 0.328962, acc 0.890625, learning_rate 0.000339485
2017-10-11T11:12:42.036721: step 740, loss 0.419077, acc 0.890625, learning_rate 0.000338507
2017-10-11T11:12:42.152916: step 741, loss 0.41259, acc 0.859375, learning_rate 0.000337534
2017-10-11T11:12:42.269988: step 742, loss 0.263371, acc 0.9375, learning_rate 0.000336564
2017-10-11T11:12:42.383361: step 743, loss 0.329495, acc 0.921875, learning_rate 0.000335598
2017-10-11T11:12:42.493042: step 744, loss 0.326022, acc 0.875, learning_rate 0.000334637
2017-10-11T11:12:42.608491: step 745, loss 0.557957, acc 0.765625, learning_rate 0.000333679
2017-10-11T11:12:42.796350: step 746, loss 0.363833, acc 0.890625, learning_rate 0.000332725
2017-10-11T11:12:42.886278: step 747, loss 0.323489, acc 0.875, learning_rate 0.000331775
2017-10-11T11:12:42.973786: step 748, loss 0.368338, acc 0.859375, learning_rate 0.000330829
2017-10-11T11:12:43.064289: step 749, loss 0.357001, acc 0.890625, learning_rate 0.000329887
2017-10-11T11:12:43.151539: step 750, loss 0.352268, acc 0.890625, learning_rate 0.000328949
2017-10-11T11:12:43.237899: step 751, loss 0.333337, acc 0.875, learning_rate 0.000328014
2017-10-11T11:12:43.327259: step 752, loss 0.269227, acc 0.921875, learning_rate 0.000327083
2017-10-11T11:12:43.414051: step 753, loss 0.476719, acc 0.84375, learning_rate 0.000326157
2017-10-11T11:12:43.499075: step 754, loss 0.689506, acc 0.828125, learning_rate 0.000325233
2017-10-11T11:12:43.593592: step 755, loss 0.362905, acc 0.890625, learning_rate 0.000324314
2017-10-11T11:12:43.703456: step 756, loss 0.312636, acc 0.921875, learning_rate 0.000323399
2017-10-11T11:12:43.816863: step 757, loss 0.406787, acc 0.921875, learning_rate 0.000322487
2017-10-11T11:12:43.919003: step 758, loss 0.367677, acc 0.875, learning_rate 0.000321579
2017-10-11T11:12:44.040863: step 759, loss 0.38825, acc 0.90625, learning_rate 0.000320674
2017-10-11T11:12:44.147983: step 760, loss 0.273574, acc 0.921875, learning_rate 0.000319773

Evaluation:
2017-10-11T11:12:44.409038: step 760, loss 0.268587, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-760

2017-10-11T11:12:45.113065: step 761, loss 0.41658, acc 0.84375, learning_rate 0.000318876
2017-10-11T11:12:45.226923: step 762, loss 0.268647, acc 0.921875, learning_rate 0.000317983
2017-10-11T11:12:45.336068: step 763, loss 0.214581, acc 0.9375, learning_rate 0.000317093
2017-10-11T11:12:45.451016: step 764, loss 0.197044, acc 0.953125, learning_rate 0.000316207
2017-10-11T11:12:45.565495: step 765, loss 0.335239, acc 0.875, learning_rate 0.000315325
2017-10-11T11:12:45.672534: step 766, loss 0.341882, acc 0.90625, learning_rate 0.000314446
2017-10-11T11:12:45.784364: step 767, loss 0.285082, acc 0.875, learning_rate 0.00031357
2017-10-11T11:12:45.896883: step 768, loss 0.453325, acc 0.796875, learning_rate 0.000312699
2017-10-11T11:12:46.021069: step 769, loss 0.420901, acc 0.859375, learning_rate 0.00031183
2017-10-11T11:12:46.127834: step 770, loss 0.396769, acc 0.84375, learning_rate 0.000310966
2017-10-11T11:12:46.241278: step 771, loss 0.399141, acc 0.875, learning_rate 0.000310105
2017-10-11T11:12:46.362521: step 772, loss 0.576682, acc 0.78125, learning_rate 0.000309247
2017-10-11T11:12:46.479278: step 773, loss 0.524354, acc 0.8125, learning_rate 0.000308393
2017-10-11T11:12:46.598486: step 774, loss 0.583934, acc 0.828125, learning_rate 0.000307542
2017-10-11T11:12:46.709716: step 775, loss 0.460765, acc 0.875, learning_rate 0.000306695
2017-10-11T11:12:46.814247: step 776, loss 0.289334, acc 0.890625, learning_rate 0.000305852
2017-10-11T11:12:46.933822: step 777, loss 0.261116, acc 0.921875, learning_rate 0.000305011
2017-10-11T11:12:47.048000: step 778, loss 0.423734, acc 0.796875, learning_rate 0.000304174
2017-10-11T11:12:47.153790: step 779, loss 0.3997, acc 0.890625, learning_rate 0.000303341
2017-10-11T11:12:47.265719: step 780, loss 0.368462, acc 0.890625, learning_rate 0.000302511
2017-10-11T11:12:47.376345: step 781, loss 0.31208, acc 0.890625, learning_rate 0.000301684
2017-10-11T11:12:47.493218: step 782, loss 0.628344, acc 0.828125, learning_rate 0.000300861
2017-10-11T11:12:47.612268: step 783, loss 0.187279, acc 0.921875, learning_rate 0.000300041
2017-10-11T11:12:47.708202: step 784, loss 0.262392, acc 0.882353, learning_rate 0.000299225
2017-10-11T11:12:47.830033: step 785, loss 0.274708, acc 0.90625, learning_rate 0.000298412
2017-10-11T11:12:47.988859: step 786, loss 0.36273, acc 0.875, learning_rate 0.000297602
2017-10-11T11:12:48.115311: step 787, loss 0.217435, acc 0.9375, learning_rate 0.000296795
2017-10-11T11:12:48.202012: step 788, loss 0.627291, acc 0.796875, learning_rate 0.000295992
2017-10-11T11:12:48.289414: step 789, loss 0.231692, acc 0.9375, learning_rate 0.000295192
2017-10-11T11:12:48.376664: step 790, loss 0.29784, acc 0.890625, learning_rate 0.000294395
2017-10-11T11:12:48.465406: step 791, loss 0.261888, acc 0.921875, learning_rate 0.000293602
2017-10-11T11:12:48.556196: step 792, loss 0.324146, acc 0.875, learning_rate 0.000292812
2017-10-11T11:12:48.639217: step 793, loss 0.430762, acc 0.859375, learning_rate 0.000292025
2017-10-11T11:12:48.725765: step 794, loss 0.256778, acc 0.921875, learning_rate 0.000291241
2017-10-11T11:12:48.814994: step 795, loss 0.326968, acc 0.90625, learning_rate 0.00029046
2017-10-11T11:12:48.908137: step 796, loss 0.423716, acc 0.8125, learning_rate 0.000289683
2017-10-11T11:12:49.022735: step 797, loss 0.260987, acc 0.9375, learning_rate 0.000288908
2017-10-11T11:12:49.130962: step 798, loss 0.299332, acc 0.90625, learning_rate 0.000288137
2017-10-11T11:12:49.252334: step 799, loss 0.452959, acc 0.84375, learning_rate 0.000287369
2017-10-11T11:12:49.371119: step 800, loss 0.281535, acc 0.921875, learning_rate 0.000286605

Evaluation:
2017-10-11T11:12:49.600823: step 800, loss 0.267967, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-800

2017-10-11T11:12:50.345757: step 801, loss 0.394824, acc 0.875, learning_rate 0.000285843
2017-10-11T11:12:50.469034: step 802, loss 0.287394, acc 0.921875, learning_rate 0.000285084
2017-10-11T11:12:50.582702: step 803, loss 0.319797, acc 0.90625, learning_rate 0.000284329
2017-10-11T11:12:50.705477: step 804, loss 0.433551, acc 0.859375, learning_rate 0.000283577
2017-10-11T11:12:50.821869: step 805, loss 0.345954, acc 0.890625, learning_rate 0.000282827
2017-10-11T11:12:50.936849: step 806, loss 0.357348, acc 0.859375, learning_rate 0.000282081
2017-10-11T11:12:51.059315: step 807, loss 0.294748, acc 0.875, learning_rate 0.000281338
2017-10-11T11:12:51.164405: step 808, loss 0.321905, acc 0.84375, learning_rate 0.000280598
2017-10-11T11:12:51.276268: step 809, loss 0.468629, acc 0.890625, learning_rate 0.00027986
2017-10-11T11:12:51.393381: step 810, loss 0.461917, acc 0.859375, learning_rate 0.000279126
2017-10-11T11:12:51.508716: step 811, loss 0.363812, acc 0.875, learning_rate 0.000278395
2017-10-11T11:12:51.612265: step 812, loss 0.367718, acc 0.875, learning_rate 0.000277667
2017-10-11T11:12:51.728883: step 813, loss 0.447282, acc 0.828125, learning_rate 0.000276942
2017-10-11T11:12:51.839109: step 814, loss 0.272528, acc 0.921875, learning_rate 0.00027622
2017-10-11T11:12:51.960846: step 815, loss 0.232311, acc 0.90625, learning_rate 0.0002755
2017-10-11T11:12:52.072878: step 816, loss 0.284688, acc 0.921875, learning_rate 0.000274784
2017-10-11T11:12:52.200929: step 817, loss 0.268446, acc 0.921875, learning_rate 0.000274071
2017-10-11T11:12:52.317436: step 818, loss 0.310537, acc 0.875, learning_rate 0.00027336
2017-10-11T11:12:52.432495: step 819, loss 0.461223, acc 0.84375, learning_rate 0.000272652
2017-10-11T11:12:52.544223: step 820, loss 0.373187, acc 0.890625, learning_rate 0.000271948
2017-10-11T11:12:52.657686: step 821, loss 0.31789, acc 0.859375, learning_rate 0.000271246
2017-10-11T11:12:52.777628: step 822, loss 0.326243, acc 0.890625, learning_rate 0.000270547
2017-10-11T11:12:52.899794: step 823, loss 0.402184, acc 0.875, learning_rate 0.000269851
2017-10-11T11:12:53.010623: step 824, loss 0.336758, acc 0.921875, learning_rate 0.000269157
2017-10-11T11:12:53.120456: step 825, loss 0.255411, acc 0.9375, learning_rate 0.000268467
2017-10-11T11:12:53.230673: step 826, loss 0.450081, acc 0.859375, learning_rate 0.000267779
2017-10-11T11:12:53.352883: step 827, loss 0.598275, acc 0.8125, learning_rate 0.000267094
2017-10-11T11:12:53.498481: step 828, loss 0.273743, acc 0.875, learning_rate 0.000266412
2017-10-11T11:12:53.586277: step 829, loss 0.185027, acc 0.96875, learning_rate 0.000265733
2017-10-11T11:12:53.680423: step 830, loss 0.40131, acc 0.921875, learning_rate 0.000265057
2017-10-11T11:12:53.767030: step 831, loss 0.48519, acc 0.828125, learning_rate 0.000264383
2017-10-11T11:12:53.857136: step 832, loss 0.299292, acc 0.890625, learning_rate 0.000263712
2017-10-11T11:12:53.947469: step 833, loss 0.314311, acc 0.890625, learning_rate 0.000263044
2017-10-11T11:12:54.033615: step 834, loss 0.353135, acc 0.828125, learning_rate 0.000262378
2017-10-11T11:12:54.117703: step 835, loss 0.321378, acc 0.890625, learning_rate 0.000261715
2017-10-11T11:12:54.221536: step 836, loss 0.452505, acc 0.859375, learning_rate 0.000261055
2017-10-11T11:12:54.336429: step 837, loss 0.467854, acc 0.8125, learning_rate 0.000260398
2017-10-11T11:12:54.452155: step 838, loss 0.417067, acc 0.84375, learning_rate 0.000259743
2017-10-11T11:12:54.574660: step 839, loss 0.376257, acc 0.921875, learning_rate 0.000259091
2017-10-11T11:12:54.688864: step 840, loss 0.379634, acc 0.890625, learning_rate 0.000258442

Evaluation:
2017-10-11T11:12:54.911059: step 840, loss 0.267047, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-840

2017-10-11T11:12:55.649030: step 841, loss 0.39135, acc 0.828125, learning_rate 0.000257795
2017-10-11T11:12:55.765926: step 842, loss 0.440575, acc 0.84375, learning_rate 0.000257151
2017-10-11T11:12:55.875757: step 843, loss 0.455054, acc 0.875, learning_rate 0.00025651
2017-10-11T11:12:55.990156: step 844, loss 0.321394, acc 0.890625, learning_rate 0.000255871
2017-10-11T11:12:56.111297: step 845, loss 0.357867, acc 0.9375, learning_rate 0.000255235
2017-10-11T11:12:56.223385: step 846, loss 0.300212, acc 0.90625, learning_rate 0.000254601
2017-10-11T11:12:56.336575: step 847, loss 0.242604, acc 0.921875, learning_rate 0.00025397
2017-10-11T11:12:56.449817: step 848, loss 0.442269, acc 0.875, learning_rate 0.000253341
2017-10-11T11:12:56.560808: step 849, loss 0.392628, acc 0.859375, learning_rate 0.000252716
2017-10-11T11:12:56.674030: step 850, loss 0.233372, acc 0.921875, learning_rate 0.000252092
2017-10-11T11:12:56.794251: step 851, loss 0.361142, acc 0.9375, learning_rate 0.000251471
2017-10-11T11:12:56.900208: step 852, loss 0.356312, acc 0.859375, learning_rate 0.000250853
2017-10-11T11:12:57.014812: step 853, loss 0.272438, acc 0.890625, learning_rate 0.000250237
2017-10-11T11:12:57.125627: step 854, loss 0.275842, acc 0.90625, learning_rate 0.000249624
2017-10-11T11:12:57.232972: step 855, loss 0.261387, acc 0.90625, learning_rate 0.000249013
2017-10-11T11:12:57.353079: step 856, loss 0.367598, acc 0.90625, learning_rate 0.000248405
2017-10-11T11:12:57.472943: step 857, loss 0.521455, acc 0.828125, learning_rate 0.000247799
2017-10-11T11:12:57.587943: step 858, loss 0.321635, acc 0.890625, learning_rate 0.000247196
2017-10-11T11:12:57.692857: step 859, loss 0.265637, acc 0.890625, learning_rate 0.000246595
2017-10-11T11:12:57.808447: step 860, loss 0.313079, acc 0.890625, learning_rate 0.000245997
2017-10-11T11:12:57.919655: step 861, loss 0.366758, acc 0.890625, learning_rate 0.000245401
2017-10-11T11:12:58.024869: step 862, loss 0.559681, acc 0.8125, learning_rate 0.000244808
2017-10-11T11:12:58.142924: step 863, loss 0.358321, acc 0.9375, learning_rate 0.000244216
2017-10-11T11:12:58.265523: step 864, loss 0.453526, acc 0.859375, learning_rate 0.000243628
2017-10-11T11:12:58.377766: step 865, loss 0.308674, acc 0.890625, learning_rate 0.000243042
2017-10-11T11:12:58.532987: step 866, loss 0.386824, acc 0.921875, learning_rate 0.000242458
2017-10-11T11:12:58.649445: step 867, loss 0.297149, acc 0.84375, learning_rate 0.000241876
2017-10-11T11:12:58.741531: step 868, loss 0.239058, acc 0.90625, learning_rate 0.000241297
2017-10-11T11:12:58.829506: step 869, loss 0.276623, acc 0.90625, learning_rate 0.00024072
2017-10-11T11:12:58.927614: step 870, loss 0.417693, acc 0.875, learning_rate 0.000240146
2017-10-11T11:12:59.015613: step 871, loss 0.299287, acc 0.921875, learning_rate 0.000239574
2017-10-11T11:12:59.101132: step 872, loss 0.367943, acc 0.859375, learning_rate 0.000239004
2017-10-11T11:12:59.189520: step 873, loss 0.479685, acc 0.828125, learning_rate 0.000238437
2017-10-11T11:12:59.275614: step 874, loss 0.289947, acc 0.890625, learning_rate 0.000237872
2017-10-11T11:12:59.362251: step 875, loss 0.435984, acc 0.8125, learning_rate 0.000237309
2017-10-11T11:12:59.479260: step 876, loss 0.318455, acc 0.890625, learning_rate 0.000236749
2017-10-11T11:12:59.587835: step 877, loss 0.421158, acc 0.859375, learning_rate 0.00023619
2017-10-11T11:12:59.701106: step 878, loss 0.29004, acc 0.890625, learning_rate 0.000235635
2017-10-11T11:12:59.821997: step 879, loss 0.351586, acc 0.859375, learning_rate 0.000235081
2017-10-11T11:12:59.935192: step 880, loss 0.307742, acc 0.875, learning_rate 0.00023453

Evaluation:
2017-10-11T11:13:00.188860: step 880, loss 0.266689, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-880

2017-10-11T11:13:00.885359: step 881, loss 0.400302, acc 0.890625, learning_rate 0.00023398
2017-10-11T11:13:00.982398: step 882, loss 0.267607, acc 0.921569, learning_rate 0.000233434
2017-10-11T11:13:01.103314: step 883, loss 0.389275, acc 0.890625, learning_rate 0.000232889
2017-10-11T11:13:01.214164: step 884, loss 0.483189, acc 0.828125, learning_rate 0.000232346
2017-10-11T11:13:01.335589: step 885, loss 0.381457, acc 0.90625, learning_rate 0.000231806
2017-10-11T11:13:01.441854: step 886, loss 0.218217, acc 0.9375, learning_rate 0.000231268
2017-10-11T11:13:01.559378: step 887, loss 0.237308, acc 0.921875, learning_rate 0.000230732
2017-10-11T11:13:01.659475: step 888, loss 0.575298, acc 0.75, learning_rate 0.000230199
2017-10-11T11:13:01.773489: step 889, loss 0.517216, acc 0.78125, learning_rate 0.000229667
2017-10-11T11:13:01.892794: step 890, loss 0.304497, acc 0.859375, learning_rate 0.000229138
2017-10-11T11:13:02.013665: step 891, loss 0.411311, acc 0.90625, learning_rate 0.000228611
2017-10-11T11:13:02.120879: step 892, loss 0.596964, acc 0.875, learning_rate 0.000228086
2017-10-11T11:13:02.232902: step 893, loss 0.346382, acc 0.890625, learning_rate 0.000227563
2017-10-11T11:13:02.348841: step 894, loss 0.490577, acc 0.875, learning_rate 0.000227043
2017-10-11T11:13:02.451579: step 895, loss 0.334708, acc 0.890625, learning_rate 0.000226524
2017-10-11T11:13:02.569080: step 896, loss 0.317996, acc 0.953125, learning_rate 0.000226008
2017-10-11T11:13:02.674887: step 897, loss 0.450816, acc 0.828125, learning_rate 0.000225493
2017-10-11T11:13:02.785040: step 898, loss 0.333124, acc 0.875, learning_rate 0.000224981
2017-10-11T11:13:02.913144: step 899, loss 0.335702, acc 0.90625, learning_rate 0.000224471
2017-10-11T11:13:03.011614: step 900, loss 0.476561, acc 0.859375, learning_rate 0.000223963
2017-10-11T11:13:03.132352: step 901, loss 0.299509, acc 0.890625, learning_rate 0.000223457
2017-10-11T11:13:03.244927: step 902, loss 0.297974, acc 0.921875, learning_rate 0.000222953
2017-10-11T11:13:03.342552: step 903, loss 0.533983, acc 0.8125, learning_rate 0.000222451
2017-10-11T11:13:03.440873: step 904, loss 0.373937, acc 0.8125, learning_rate 0.000221951
2017-10-11T11:13:03.551166: step 905, loss 0.382359, acc 0.84375, learning_rate 0.000221453
2017-10-11T11:13:03.669055: step 906, loss 0.3046, acc 0.890625, learning_rate 0.000220958
2017-10-11T11:13:03.850083: step 907, loss 0.459433, acc 0.828125, learning_rate 0.000220464
2017-10-11T11:13:03.937209: step 908, loss 0.494141, acc 0.828125, learning_rate 0.000219972
2017-10-11T11:13:04.026667: step 909, loss 0.32432, acc 0.890625, learning_rate 0.000219483
2017-10-11T11:13:04.114298: step 910, loss 0.313665, acc 0.84375, learning_rate 0.000218995
2017-10-11T11:13:04.200751: step 911, loss 0.292656, acc 0.90625, learning_rate 0.000218509
2017-10-11T11:13:04.289138: step 912, loss 0.384106, acc 0.84375, learning_rate 0.000218025
2017-10-11T11:13:04.376009: step 913, loss 0.305138, acc 0.90625, learning_rate 0.000217544
2017-10-11T11:13:04.465313: step 914, loss 0.357158, acc 0.890625, learning_rate 0.000217064
2017-10-11T11:13:04.563805: step 915, loss 0.375753, acc 0.84375, learning_rate 0.000216586
2017-10-11T11:13:04.664929: step 916, loss 0.282034, acc 0.9375, learning_rate 0.00021611
2017-10-11T11:13:04.782388: step 917, loss 0.312514, acc 0.90625, learning_rate 0.000215636
2017-10-11T11:13:04.905151: step 918, loss 0.186248, acc 0.953125, learning_rate 0.000215164
2017-10-11T11:13:05.019968: step 919, loss 0.233206, acc 0.953125, learning_rate 0.000214694
2017-10-11T11:13:05.125820: step 920, loss 0.274564, acc 0.9375, learning_rate 0.000214226

Evaluation:
2017-10-11T11:13:05.381882: step 920, loss 0.264473, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-920

2017-10-11T11:13:06.052952: step 921, loss 0.259887, acc 0.921875, learning_rate 0.00021376
2017-10-11T11:13:06.174513: step 922, loss 0.398865, acc 0.8125, learning_rate 0.000213295
2017-10-11T11:13:06.290411: step 923, loss 0.294203, acc 0.890625, learning_rate 0.000212833
2017-10-11T11:13:06.407499: step 924, loss 0.314618, acc 0.875, learning_rate 0.000212372
2017-10-11T11:13:06.522234: step 925, loss 0.189884, acc 0.9375, learning_rate 0.000211914
2017-10-11T11:13:06.649069: step 926, loss 0.32279, acc 0.921875, learning_rate 0.000211457
2017-10-11T11:13:06.766772: step 927, loss 0.210152, acc 0.90625, learning_rate 0.000211002
2017-10-11T11:13:06.885279: step 928, loss 0.394234, acc 0.921875, learning_rate 0.000210549
2017-10-11T11:13:07.012129: step 929, loss 0.269289, acc 0.90625, learning_rate 0.000210098
2017-10-11T11:13:07.129762: step 930, loss 0.339552, acc 0.875, learning_rate 0.000209648
2017-10-11T11:13:07.240878: step 931, loss 0.558368, acc 0.828125, learning_rate 0.000209201
2017-10-11T11:13:07.354554: step 932, loss 0.374147, acc 0.859375, learning_rate 0.000208755
2017-10-11T11:13:07.471401: step 933, loss 0.206487, acc 0.96875, learning_rate 0.000208311
2017-10-11T11:13:07.582283: step 934, loss 0.451977, acc 0.828125, learning_rate 0.000207869
2017-10-11T11:13:07.699471: step 935, loss 0.456545, acc 0.828125, learning_rate 0.000207429
2017-10-11T11:13:07.816130: step 936, loss 0.571528, acc 0.828125, learning_rate 0.00020699
2017-10-11T11:13:07.933798: step 937, loss 0.150617, acc 0.96875, learning_rate 0.000206554
2017-10-11T11:13:08.049463: step 938, loss 0.301089, acc 0.90625, learning_rate 0.000206119
2017-10-11T11:13:08.165334: step 939, loss 0.393388, acc 0.921875, learning_rate 0.000205685
2017-10-11T11:13:08.276027: step 940, loss 0.38568, acc 0.84375, learning_rate 0.000205254
2017-10-11T11:13:08.394137: step 941, loss 0.307742, acc 0.890625, learning_rate 0.000204824
2017-10-11T11:13:08.510468: step 942, loss 0.410786, acc 0.8125, learning_rate 0.000204397
2017-10-11T11:13:08.624264: step 943, loss 0.224262, acc 0.921875, learning_rate 0.00020397
2017-10-11T11:13:08.736300: step 944, loss 0.357334, acc 0.9375, learning_rate 0.000203546
2017-10-11T11:13:08.859680: step 945, loss 0.331266, acc 0.828125, learning_rate 0.000203123
2017-10-11T11:13:09.041625: step 946, loss 0.509528, acc 0.859375, learning_rate 0.000202702
2017-10-11T11:13:09.147809: step 947, loss 0.360735, acc 0.90625, learning_rate 0.000202283
2017-10-11T11:13:09.236867: step 948, loss 0.227662, acc 0.921875, learning_rate 0.000201866
2017-10-11T11:13:09.324300: step 949, loss 0.243025, acc 0.921875, learning_rate 0.00020145
2017-10-11T11:13:09.420176: step 950, loss 0.362826, acc 0.890625, learning_rate 0.000201036
2017-10-11T11:13:09.506855: step 951, loss 0.196054, acc 0.96875, learning_rate 0.000200623
2017-10-11T11:13:09.593652: step 952, loss 0.269221, acc 0.9375, learning_rate 0.000200213
2017-10-11T11:13:09.680132: step 953, loss 0.482112, acc 0.828125, learning_rate 0.000199804
2017-10-11T11:13:09.765671: step 954, loss 0.493034, acc 0.828125, learning_rate 0.000199396
2017-10-11T11:13:09.854141: step 955, loss 0.388221, acc 0.84375, learning_rate 0.000198991
2017-10-11T11:13:09.943833: step 956, loss 0.344162, acc 0.84375, learning_rate 0.000198587
2017-10-11T11:13:10.041393: step 957, loss 0.400258, acc 0.890625, learning_rate 0.000198184
2017-10-11T11:13:10.155384: step 958, loss 0.213199, acc 0.953125, learning_rate 0.000197783
2017-10-11T11:13:10.263395: step 959, loss 0.381188, acc 0.859375, learning_rate 0.000197384
2017-10-11T11:13:10.381020: step 960, loss 0.660586, acc 0.84375, learning_rate 0.000196987

Evaluation:
2017-10-11T11:13:10.624714: step 960, loss 0.263413, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-960

2017-10-11T11:13:11.364889: step 961, loss 0.387875, acc 0.859375, learning_rate 0.000196591
2017-10-11T11:13:11.484208: step 962, loss 0.408016, acc 0.859375, learning_rate 0.000196197
2017-10-11T11:13:11.604978: step 963, loss 0.361867, acc 0.890625, learning_rate 0.000195804
2017-10-11T11:13:11.717658: step 964, loss 0.348719, acc 0.890625, learning_rate 0.000195413
2017-10-11T11:13:11.840288: step 965, loss 0.300081, acc 0.921875, learning_rate 0.000195023
2017-10-11T11:13:11.965598: step 966, loss 0.329081, acc 0.859375, learning_rate 0.000194636
2017-10-11T11:13:12.082099: step 967, loss 0.294093, acc 0.921875, learning_rate 0.000194249
2017-10-11T11:13:12.200749: step 968, loss 0.386931, acc 0.859375, learning_rate 0.000193865
2017-10-11T11:13:12.312856: step 969, loss 0.457373, acc 0.859375, learning_rate 0.000193482
2017-10-11T11:13:12.431688: step 970, loss 0.478107, acc 0.875, learning_rate 0.0001931
2017-10-11T11:13:12.540895: step 971, loss 0.325782, acc 0.90625, learning_rate 0.00019272
2017-10-11T11:13:12.640286: step 972, loss 0.268714, acc 0.90625, learning_rate 0.000192341
2017-10-11T11:13:12.752744: step 973, loss 0.335413, acc 0.9375, learning_rate 0.000191965
2017-10-11T11:13:12.865316: step 974, loss 0.426236, acc 0.84375, learning_rate 0.000191589
2017-10-11T11:13:12.981644: step 975, loss 0.319352, acc 0.859375, learning_rate 0.000191215
2017-10-11T11:13:13.102490: step 976, loss 0.282071, acc 0.90625, learning_rate 0.000190843
2017-10-11T11:13:13.215658: step 977, loss 0.317971, acc 0.875, learning_rate 0.000190472
2017-10-11T11:13:13.326689: step 978, loss 0.252751, acc 0.90625, learning_rate 0.000190103
2017-10-11T11:13:13.447546: step 979, loss 0.30286, acc 0.890625, learning_rate 0.000189735
2017-10-11T11:13:13.535029: step 980, loss 0.301099, acc 0.901961, learning_rate 0.000189369
2017-10-11T11:13:13.662926: step 981, loss 0.399624, acc 0.859375, learning_rate 0.000189004
2017-10-11T11:13:13.793425: step 982, loss 0.363996, acc 0.875, learning_rate 0.000188641
2017-10-11T11:13:13.917110: step 983, loss 0.237667, acc 0.96875, learning_rate 0.000188279
2017-10-11T11:13:14.023207: step 984, loss 0.276898, acc 0.90625, learning_rate 0.000187919
2017-10-11T11:13:14.134384: step 985, loss 0.326042, acc 0.859375, learning_rate 0.00018756
2017-10-11T11:13:14.304975: step 986, loss 0.390278, acc 0.875, learning_rate 0.000187202
2017-10-11T11:13:14.416158: step 987, loss 0.2612, acc 0.953125, learning_rate 0.000186846
2017-10-11T11:13:14.510309: step 988, loss 0.325537, acc 0.921875, learning_rate 0.000186492
2017-10-11T11:13:14.603257: step 989, loss 0.270548, acc 0.90625, learning_rate 0.000186139
2017-10-11T11:13:14.696782: step 990, loss 0.264773, acc 0.921875, learning_rate 0.000185787
2017-10-11T11:13:14.791929: step 991, loss 0.422633, acc 0.890625, learning_rate 0.000185437
2017-10-11T11:13:14.885952: step 992, loss 0.351972, acc 0.875, learning_rate 0.000185088
2017-10-11T11:13:14.978325: step 993, loss 0.294688, acc 0.9375, learning_rate 0.000184741
2017-10-11T11:13:15.064610: step 994, loss 0.3044, acc 0.90625, learning_rate 0.000184395
2017-10-11T11:13:15.177181: step 995, loss 0.471161, acc 0.84375, learning_rate 0.000184051
2017-10-11T11:13:15.294188: step 996, loss 0.371132, acc 0.859375, learning_rate 0.000183708
2017-10-11T11:13:15.412813: step 997, loss 0.361226, acc 0.84375, learning_rate 0.000183366
2017-10-11T11:13:15.525746: step 998, loss 0.294402, acc 0.9375, learning_rate 0.000183026
2017-10-11T11:13:15.639779: step 999, loss 0.40576, acc 0.8125, learning_rate 0.000182687
2017-10-11T11:13:15.755842: step 1000, loss 0.234281, acc 0.921875, learning_rate 0.000182349

Evaluation:
2017-10-11T11:13:15.983832: step 1000, loss 0.263147, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1000

2017-10-11T11:13:16.749634: step 1001, loss 0.287423, acc 0.875, learning_rate 0.000182013
2017-10-11T11:13:16.865276: step 1002, loss 0.36344, acc 0.890625, learning_rate 0.000181678
2017-10-11T11:13:16.981213: step 1003, loss 0.305243, acc 0.859375, learning_rate 0.000181345
2017-10-11T11:13:17.104902: step 1004, loss 0.417181, acc 0.859375, learning_rate 0.000181013
2017-10-11T11:13:17.228907: step 1005, loss 0.383453, acc 0.859375, learning_rate 0.000180682
2017-10-11T11:13:17.345405: step 1006, loss 0.355531, acc 0.890625, learning_rate 0.000180353
2017-10-11T11:13:17.464570: step 1007, loss 0.568742, acc 0.828125, learning_rate 0.000180025
2017-10-11T11:13:17.576252: step 1008, loss 0.312139, acc 0.875, learning_rate 0.000179698
2017-10-11T11:13:17.699560: step 1009, loss 0.255859, acc 0.921875, learning_rate 0.000179373
2017-10-11T11:13:17.804963: step 1010, loss 0.396696, acc 0.890625, learning_rate 0.000179049
2017-10-11T11:13:17.910823: step 1011, loss 0.290538, acc 0.90625, learning_rate 0.000178726
2017-10-11T11:13:18.026826: step 1012, loss 0.216242, acc 0.921875, learning_rate 0.000178405
2017-10-11T11:13:18.158197: step 1013, loss 0.344188, acc 0.890625, learning_rate 0.000178085
2017-10-11T11:13:18.265565: step 1014, loss 0.308015, acc 0.859375, learning_rate 0.000177766
2017-10-11T11:13:18.375941: step 1015, loss 0.408186, acc 0.84375, learning_rate 0.000177449
2017-10-11T11:13:18.485441: step 1016, loss 0.275417, acc 0.921875, learning_rate 0.000177133
2017-10-11T11:13:18.580835: step 1017, loss 0.409023, acc 0.875, learning_rate 0.000176818
2017-10-11T11:13:18.688132: step 1018, loss 0.414227, acc 0.890625, learning_rate 0.000176504
2017-10-11T11:13:18.804201: step 1019, loss 0.383241, acc 0.875, learning_rate 0.000176192
2017-10-11T11:13:18.930981: step 1020, loss 0.268142, acc 0.9375, learning_rate 0.000175881
2017-10-11T11:13:19.046174: step 1021, loss 0.276079, acc 0.921875, learning_rate 0.000175571
2017-10-11T11:13:19.158460: step 1022, loss 0.369016, acc 0.859375, learning_rate 0.000175263
2017-10-11T11:13:19.282329: step 1023, loss 0.322145, acc 0.90625, learning_rate 0.000174956
2017-10-11T11:13:19.436923: step 1024, loss 0.315011, acc 0.921875, learning_rate 0.00017465
2017-10-11T11:13:19.552188: step 1025, loss 0.29803, acc 0.90625, learning_rate 0.000174345
2017-10-11T11:13:19.642324: step 1026, loss 0.184306, acc 0.96875, learning_rate 0.000174042
2017-10-11T11:13:19.731549: step 1027, loss 0.369849, acc 0.8125, learning_rate 0.000173739
2017-10-11T11:13:19.817913: step 1028, loss 0.320195, acc 0.90625, learning_rate 0.000173438
2017-10-11T11:13:19.905004: step 1029, loss 0.218022, acc 0.953125, learning_rate 0.000173139
2017-10-11T11:13:19.992336: step 1030, loss 0.422853, acc 0.890625, learning_rate 0.00017284
2017-10-11T11:13:20.077167: step 1031, loss 0.398886, acc 0.828125, learning_rate 0.000172543
2017-10-11T11:13:20.167179: step 1032, loss 0.411642, acc 0.859375, learning_rate 0.000172247
2017-10-11T11:13:20.272913: step 1033, loss 0.405221, acc 0.84375, learning_rate 0.000171952
2017-10-11T11:13:20.400847: step 1034, loss 0.263789, acc 0.890625, learning_rate 0.000171658
2017-10-11T11:13:20.507868: step 1035, loss 0.609678, acc 0.796875, learning_rate 0.000171366
2017-10-11T11:13:20.625748: step 1036, loss 0.39125, acc 0.890625, learning_rate 0.000171074
2017-10-11T11:13:20.744289: step 1037, loss 0.398387, acc 0.875, learning_rate 0.000170784
2017-10-11T11:13:20.859840: step 1038, loss 0.441359, acc 0.765625, learning_rate 0.000170495
2017-10-11T11:13:20.971920: step 1039, loss 0.375555, acc 0.875, learning_rate 0.000170208
2017-10-11T11:13:21.089441: step 1040, loss 0.452008, acc 0.890625, learning_rate 0.000169921

Evaluation:
2017-10-11T11:13:21.334096: step 1040, loss 0.261516, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1040

2017-10-11T11:13:21.962256: step 1041, loss 0.295813, acc 0.90625, learning_rate 0.000169636
2017-10-11T11:13:22.068511: step 1042, loss 0.319484, acc 0.921875, learning_rate 0.000169351
2017-10-11T11:13:22.184127: step 1043, loss 0.370268, acc 0.890625, learning_rate 0.000169068
2017-10-11T11:13:22.295649: step 1044, loss 0.422288, acc 0.890625, learning_rate 0.000168786
2017-10-11T11:13:22.407841: step 1045, loss 0.321198, acc 0.875, learning_rate 0.000168506
2017-10-11T11:13:22.528339: step 1046, loss 0.347314, acc 0.890625, learning_rate 0.000168226
2017-10-11T11:13:22.633019: step 1047, loss 0.351521, acc 0.890625, learning_rate 0.000167947
2017-10-11T11:13:22.730362: step 1048, loss 0.328978, acc 0.875, learning_rate 0.00016767
2017-10-11T11:13:22.844706: step 1049, loss 0.380598, acc 0.890625, learning_rate 0.000167394
2017-10-11T11:13:22.964900: step 1050, loss 0.335029, acc 0.90625, learning_rate 0.000167119
2017-10-11T11:13:23.084075: step 1051, loss 0.250949, acc 0.90625, learning_rate 0.000166845
2017-10-11T11:13:23.192840: step 1052, loss 0.283759, acc 0.90625, learning_rate 0.000166572
2017-10-11T11:13:23.308046: step 1053, loss 0.436008, acc 0.859375, learning_rate 0.0001663
2017-10-11T11:13:23.424898: step 1054, loss 0.29351, acc 0.90625, learning_rate 0.00016603
2017-10-11T11:13:23.533727: step 1055, loss 0.449506, acc 0.828125, learning_rate 0.00016576
2017-10-11T11:13:23.656072: step 1056, loss 0.332231, acc 0.859375, learning_rate 0.000165492
2017-10-11T11:13:23.764134: step 1057, loss 0.39036, acc 0.890625, learning_rate 0.000165224
2017-10-11T11:13:23.875586: step 1058, loss 0.435359, acc 0.84375, learning_rate 0.000164958
2017-10-11T11:13:23.988974: step 1059, loss 0.449932, acc 0.859375, learning_rate 0.000164693
2017-10-11T11:13:24.097396: step 1060, loss 0.34646, acc 0.890625, learning_rate 0.000164429
2017-10-11T11:13:24.214653: step 1061, loss 0.482922, acc 0.84375, learning_rate 0.000164166
2017-10-11T11:13:24.333193: step 1062, loss 0.250427, acc 0.953125, learning_rate 0.000163904
2017-10-11T11:13:24.462642: step 1063, loss 0.459965, acc 0.859375, learning_rate 0.000163643
2017-10-11T11:13:24.588947: step 1064, loss 0.281665, acc 0.9375, learning_rate 0.000163383
2017-10-11T11:13:24.752769: step 1065, loss 0.34927, acc 0.890625, learning_rate 0.000163125
2017-10-11T11:13:24.848492: step 1066, loss 0.424199, acc 0.84375, learning_rate 0.000162867
2017-10-11T11:13:24.940957: step 1067, loss 0.223185, acc 0.90625, learning_rate 0.00016261
2017-10-11T11:13:25.039982: step 1068, loss 0.325562, acc 0.890625, learning_rate 0.000162355
2017-10-11T11:13:25.135429: step 1069, loss 0.291672, acc 0.890625, learning_rate 0.0001621
2017-10-11T11:13:25.230209: step 1070, loss 0.414654, acc 0.84375, learning_rate 0.000161847
2017-10-11T11:13:25.325549: step 1071, loss 0.355663, acc 0.890625, learning_rate 0.000161594
2017-10-11T11:13:25.423071: step 1072, loss 0.317746, acc 0.890625, learning_rate 0.000161343
2017-10-11T11:13:25.523508: step 1073, loss 0.337542, acc 0.90625, learning_rate 0.000161093
2017-10-11T11:13:25.622418: step 1074, loss 0.43071, acc 0.859375, learning_rate 0.000160843
2017-10-11T11:13:25.731413: step 1075, loss 0.469276, acc 0.828125, learning_rate 0.000160595
2017-10-11T11:13:25.843151: step 1076, loss 0.388096, acc 0.90625, learning_rate 0.000160348
2017-10-11T11:13:25.956882: step 1077, loss 0.283116, acc 0.9375, learning_rate 0.000160101
2017-10-11T11:13:26.056910: step 1078, loss 0.226811, acc 0.960784, learning_rate 0.000159856
2017-10-11T11:13:26.170599: step 1079, loss 0.349961, acc 0.921875, learning_rate 0.000159612
2017-10-11T11:13:26.273259: step 1080, loss 0.332761, acc 0.890625, learning_rate 0.000159368

Evaluation:
2017-10-11T11:13:26.522481: step 1080, loss 0.260867, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1080

2017-10-11T11:13:27.246015: step 1081, loss 0.412883, acc 0.84375, learning_rate 0.000159126
2017-10-11T11:13:27.354946: step 1082, loss 0.314874, acc 0.875, learning_rate 0.000158885
2017-10-11T11:13:27.476064: step 1083, loss 0.379634, acc 0.890625, learning_rate 0.000158644
2017-10-11T11:13:27.580830: step 1084, loss 0.278788, acc 0.90625, learning_rate 0.000158405
2017-10-11T11:13:27.687088: step 1085, loss 0.367201, acc 0.890625, learning_rate 0.000158167
2017-10-11T11:13:27.797655: step 1086, loss 0.296929, acc 0.890625, learning_rate 0.000157929
2017-10-11T11:13:27.912189: step 1087, loss 0.377306, acc 0.84375, learning_rate 0.000157693
2017-10-11T11:13:28.025251: step 1088, loss 0.333049, acc 0.890625, learning_rate 0.000157457
2017-10-11T11:13:28.137976: step 1089, loss 0.264522, acc 0.921875, learning_rate 0.000157223
2017-10-11T11:13:28.252436: step 1090, loss 0.362269, acc 0.90625, learning_rate 0.000156989
2017-10-11T11:13:28.365233: step 1091, loss 0.293564, acc 0.9375, learning_rate 0.000156757
2017-10-11T11:13:28.486969: step 1092, loss 0.428906, acc 0.875, learning_rate 0.000156525
2017-10-11T11:13:28.606428: step 1093, loss 0.329142, acc 0.890625, learning_rate 0.000156294
2017-10-11T11:13:28.723872: step 1094, loss 0.382246, acc 0.84375, learning_rate 0.000156064
2017-10-11T11:13:28.844125: step 1095, loss 0.324762, acc 0.921875, learning_rate 0.000155836
2017-10-11T11:13:28.958200: step 1096, loss 0.467846, acc 0.859375, learning_rate 0.000155608
2017-10-11T11:13:29.076282: step 1097, loss 0.176203, acc 0.96875, learning_rate 0.000155381
2017-10-11T11:13:29.184698: step 1098, loss 0.340474, acc 0.8125, learning_rate 0.000155155
2017-10-11T11:13:29.300867: step 1099, loss 0.311367, acc 0.890625, learning_rate 0.000154929
2017-10-11T11:13:29.423021: step 1100, loss 0.420702, acc 0.859375, learning_rate 0.000154705
2017-10-11T11:13:29.540978: step 1101, loss 0.456617, acc 0.765625, learning_rate 0.000154482
2017-10-11T11:13:29.660004: step 1102, loss 0.276014, acc 0.875, learning_rate 0.00015426
2017-10-11T11:13:29.778552: step 1103, loss 0.526229, acc 0.828125, learning_rate 0.000154038
2017-10-11T11:13:29.948941: step 1104, loss 0.342919, acc 0.828125, learning_rate 0.000153818
2017-10-11T11:13:30.047676: step 1105, loss 0.271992, acc 0.875, learning_rate 0.000153598
2017-10-11T11:13:30.139888: step 1106, loss 0.395091, acc 0.828125, learning_rate 0.000153379
2017-10-11T11:13:30.226640: step 1107, loss 0.533774, acc 0.84375, learning_rate 0.000153161
2017-10-11T11:13:30.310124: step 1108, loss 0.43363, acc 0.90625, learning_rate 0.000152944
2017-10-11T11:13:30.402383: step 1109, loss 0.251027, acc 0.9375, learning_rate 0.000152728
2017-10-11T11:13:30.489212: step 1110, loss 0.286062, acc 0.90625, learning_rate 0.000152513
2017-10-11T11:13:30.573647: step 1111, loss 0.385926, acc 0.90625, learning_rate 0.000152299
2017-10-11T11:13:30.662505: step 1112, loss 0.389808, acc 0.828125, learning_rate 0.000152085
2017-10-11T11:13:30.777467: step 1113, loss 0.241643, acc 0.890625, learning_rate 0.000151872
2017-10-11T11:13:30.895081: step 1114, loss 0.372071, acc 0.875, learning_rate 0.000151661
2017-10-11T11:13:31.008805: step 1115, loss 0.365955, acc 0.859375, learning_rate 0.00015145
2017-10-11T11:13:31.132920: step 1116, loss 0.339903, acc 0.875, learning_rate 0.00015124
2017-10-11T11:13:31.250968: step 1117, loss 0.399982, acc 0.875, learning_rate 0.000151031
2017-10-11T11:13:31.360882: step 1118, loss 0.320833, acc 0.90625, learning_rate 0.000150822
2017-10-11T11:13:31.470343: step 1119, loss 0.491508, acc 0.84375, learning_rate 0.000150615
2017-10-11T11:13:31.581590: step 1120, loss 0.316805, acc 0.84375, learning_rate 0.000150408

Evaluation:
2017-10-11T11:13:31.833250: step 1120, loss 0.260895, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1120

2017-10-11T11:13:32.544534: step 1121, loss 0.378968, acc 0.875, learning_rate 0.000150203
2017-10-11T11:13:32.652409: step 1122, loss 0.287054, acc 0.890625, learning_rate 0.000149998
2017-10-11T11:13:32.762774: step 1123, loss 0.318846, acc 0.875, learning_rate 0.000149794
2017-10-11T11:13:32.875470: step 1124, loss 0.345353, acc 0.84375, learning_rate 0.00014959
2017-10-11T11:13:32.997939: step 1125, loss 0.360301, acc 0.84375, learning_rate 0.000149388
2017-10-11T11:13:33.114458: step 1126, loss 0.328597, acc 0.90625, learning_rate 0.000149186
2017-10-11T11:13:33.228119: step 1127, loss 0.380492, acc 0.84375, learning_rate 0.000148986
2017-10-11T11:13:33.358112: step 1128, loss 0.434547, acc 0.875, learning_rate 0.000148786
2017-10-11T11:13:33.477925: step 1129, loss 0.405607, acc 0.875, learning_rate 0.000148587
2017-10-11T11:13:33.598142: step 1130, loss 0.326718, acc 0.890625, learning_rate 0.000148388
2017-10-11T11:13:33.712900: step 1131, loss 0.579082, acc 0.8125, learning_rate 0.000148191
2017-10-11T11:13:33.830690: step 1132, loss 0.358966, acc 0.90625, learning_rate 0.000147994
2017-10-11T11:13:33.939776: step 1133, loss 0.245179, acc 0.9375, learning_rate 0.000147798
2017-10-11T11:13:34.041203: step 1134, loss 0.476228, acc 0.828125, learning_rate 0.000147603
2017-10-11T11:13:34.156864: step 1135, loss 0.342691, acc 0.875, learning_rate 0.000147409
2017-10-11T11:13:34.260193: step 1136, loss 0.35832, acc 0.890625, learning_rate 0.000147215
2017-10-11T11:13:34.371716: step 1137, loss 0.342837, acc 0.875, learning_rate 0.000147022
2017-10-11T11:13:34.486566: step 1138, loss 0.256413, acc 0.9375, learning_rate 0.000146831
2017-10-11T11:13:34.599448: step 1139, loss 0.31821, acc 0.890625, learning_rate 0.000146639
2017-10-11T11:13:34.712205: step 1140, loss 0.553472, acc 0.828125, learning_rate 0.000146449
2017-10-11T11:13:34.824428: step 1141, loss 0.397111, acc 0.890625, learning_rate 0.000146259
2017-10-11T11:13:34.938658: step 1142, loss 0.258343, acc 0.921875, learning_rate 0.000146071
2017-10-11T11:13:35.116916: step 1143, loss 0.212605, acc 0.921875, learning_rate 0.000145883
2017-10-11T11:13:35.202758: step 1144, loss 0.312511, acc 0.90625, learning_rate 0.000145695
2017-10-11T11:13:35.285480: step 1145, loss 0.614486, acc 0.8125, learning_rate 0.000145509
2017-10-11T11:13:35.375615: step 1146, loss 0.414982, acc 0.875, learning_rate 0.000145323
2017-10-11T11:13:35.460166: step 1147, loss 0.418876, acc 0.8125, learning_rate 0.000145138
2017-10-11T11:13:35.545569: step 1148, loss 0.251493, acc 0.921875, learning_rate 0.000144954
2017-10-11T11:13:35.641010: step 1149, loss 0.430913, acc 0.84375, learning_rate 0.00014477
2017-10-11T11:13:35.729165: step 1150, loss 0.269914, acc 0.875, learning_rate 0.000144588
2017-10-11T11:13:35.815159: step 1151, loss 0.278489, acc 0.921875, learning_rate 0.000144406
2017-10-11T11:13:35.901272: step 1152, loss 0.265713, acc 0.9375, learning_rate 0.000144224
2017-10-11T11:13:35.985317: step 1153, loss 0.293231, acc 0.921875, learning_rate 0.000144044
2017-10-11T11:13:36.080548: step 1154, loss 0.327044, acc 0.875, learning_rate 0.000143864
2017-10-11T11:13:36.187367: step 1155, loss 0.278766, acc 0.890625, learning_rate 0.000143685
2017-10-11T11:13:36.300879: step 1156, loss 0.240072, acc 0.953125, learning_rate 0.000143507
2017-10-11T11:13:36.414148: step 1157, loss 0.406491, acc 0.875, learning_rate 0.000143329
2017-10-11T11:13:36.523062: step 1158, loss 0.181276, acc 0.953125, learning_rate 0.000143152
2017-10-11T11:13:36.622300: step 1159, loss 0.246715, acc 0.921875, learning_rate 0.000142976
2017-10-11T11:13:36.733934: step 1160, loss 0.281394, acc 0.921875, learning_rate 0.000142801

Evaluation:
2017-10-11T11:13:36.990844: step 1160, loss 0.26054, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1160

2017-10-11T11:13:37.686292: step 1161, loss 0.360539, acc 0.90625, learning_rate 0.000142626
2017-10-11T11:13:37.800946: step 1162, loss 0.261702, acc 0.9375, learning_rate 0.000142452
2017-10-11T11:13:37.906254: step 1163, loss 0.279153, acc 0.890625, learning_rate 0.000142279
2017-10-11T11:13:38.024997: step 1164, loss 0.459616, acc 0.828125, learning_rate 0.000142106
2017-10-11T11:13:38.135864: step 1165, loss 0.300499, acc 0.90625, learning_rate 0.000141934
2017-10-11T11:13:38.250842: step 1166, loss 0.295803, acc 0.9375, learning_rate 0.000141763
2017-10-11T11:13:38.365810: step 1167, loss 0.343062, acc 0.859375, learning_rate 0.000141593
2017-10-11T11:13:38.482605: step 1168, loss 0.427063, acc 0.828125, learning_rate 0.000141423
2017-10-11T11:13:38.595896: step 1169, loss 0.210569, acc 0.9375, learning_rate 0.000141254
2017-10-11T11:13:38.724535: step 1170, loss 0.318153, acc 0.921875, learning_rate 0.000141085
2017-10-11T11:13:38.850211: step 1171, loss 0.252352, acc 0.96875, learning_rate 0.000140918
2017-10-11T11:13:38.961589: step 1172, loss 0.344767, acc 0.828125, learning_rate 0.000140751
2017-10-11T11:13:39.081567: step 1173, loss 0.22837, acc 0.921875, learning_rate 0.000140584
2017-10-11T11:13:39.210582: step 1174, loss 0.260902, acc 0.875, learning_rate 0.000140419
2017-10-11T11:13:39.316849: step 1175, loss 0.274281, acc 0.90625, learning_rate 0.000140254
2017-10-11T11:13:39.409398: step 1176, loss 0.441008, acc 0.843137, learning_rate 0.000140089
2017-10-11T11:13:39.527180: step 1177, loss 0.218739, acc 0.953125, learning_rate 0.000139926
2017-10-11T11:13:39.634973: step 1178, loss 0.411623, acc 0.796875, learning_rate 0.000139763
2017-10-11T11:13:39.747712: step 1179, loss 0.17535, acc 0.953125, learning_rate 0.0001396
2017-10-11T11:13:39.865984: step 1180, loss 0.3383, acc 0.875, learning_rate 0.000139439
2017-10-11T11:13:39.972042: step 1181, loss 0.21491, acc 0.890625, learning_rate 0.000139278
2017-10-11T11:13:40.081785: step 1182, loss 0.344285, acc 0.890625, learning_rate 0.000139118
2017-10-11T11:13:40.183809: step 1183, loss 0.244709, acc 0.921875, learning_rate 0.000138958
2017-10-11T11:13:40.300982: step 1184, loss 0.267414, acc 0.9375, learning_rate 0.000138799
2017-10-11T11:13:40.473737: step 1185, loss 0.342759, acc 0.890625, learning_rate 0.00013864
2017-10-11T11:13:40.554183: step 1186, loss 0.287609, acc 0.890625, learning_rate 0.000138483
2017-10-11T11:13:40.636725: step 1187, loss 0.545888, acc 0.75, learning_rate 0.000138326
2017-10-11T11:13:40.725935: step 1188, loss 0.440801, acc 0.84375, learning_rate 0.000138169
2017-10-11T11:13:40.810773: step 1189, loss 0.36089, acc 0.875, learning_rate 0.000138013
2017-10-11T11:13:40.898971: step 1190, loss 0.265926, acc 0.90625, learning_rate 0.000137858
2017-10-11T11:13:40.984293: step 1191, loss 0.346903, acc 0.921875, learning_rate 0.000137704
2017-10-11T11:13:41.071072: step 1192, loss 0.426979, acc 0.890625, learning_rate 0.00013755
2017-10-11T11:13:41.160612: step 1193, loss 0.392294, acc 0.859375, learning_rate 0.000137397
2017-10-11T11:13:41.248372: step 1194, loss 0.432262, acc 0.890625, learning_rate 0.000137244
2017-10-11T11:13:41.344121: step 1195, loss 0.38482, acc 0.90625, learning_rate 0.000137092
2017-10-11T11:13:41.432621: step 1196, loss 0.294895, acc 0.890625, learning_rate 0.000136941
2017-10-11T11:13:41.516303: step 1197, loss 0.411623, acc 0.828125, learning_rate 0.00013679
2017-10-11T11:13:41.627022: step 1198, loss 0.401473, acc 0.90625, learning_rate 0.00013664
2017-10-11T11:13:41.744439: step 1199, loss 0.361166, acc 0.859375, learning_rate 0.00013649
2017-10-11T11:13:41.869372: step 1200, loss 0.264256, acc 0.953125, learning_rate 0.000136341

Evaluation:
2017-10-11T11:13:42.118681: step 1200, loss 0.259108, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1200

2017-10-11T11:13:42.859257: step 1201, loss 0.273999, acc 0.921875, learning_rate 0.000136193
2017-10-11T11:13:42.979951: step 1202, loss 0.297659, acc 0.9375, learning_rate 0.000136045
2017-10-11T11:13:43.102749: step 1203, loss 0.279666, acc 0.90625, learning_rate 0.000135898
2017-10-11T11:13:43.222792: step 1204, loss 0.294118, acc 0.90625, learning_rate 0.000135751
2017-10-11T11:13:43.344782: step 1205, loss 0.544529, acc 0.796875, learning_rate 0.000135605
2017-10-11T11:13:43.456863: step 1206, loss 0.314423, acc 0.9375, learning_rate 0.00013546
2017-10-11T11:13:43.562481: step 1207, loss 0.462863, acc 0.828125, learning_rate 0.000135315
2017-10-11T11:13:43.676560: step 1208, loss 0.365009, acc 0.828125, learning_rate 0.000135171
2017-10-11T11:13:43.781372: step 1209, loss 0.359439, acc 0.875, learning_rate 0.000135028
2017-10-11T11:13:43.883168: step 1210, loss 0.317579, acc 0.890625, learning_rate 0.000134885
2017-10-11T11:13:44.001774: step 1211, loss 0.399519, acc 0.90625, learning_rate 0.000134742
2017-10-11T11:13:44.104840: step 1212, loss 0.288071, acc 0.875, learning_rate 0.0001346
2017-10-11T11:13:44.217947: step 1213, loss 0.283396, acc 0.9375, learning_rate 0.000134459
2017-10-11T11:13:44.332843: step 1214, loss 0.359736, acc 0.890625, learning_rate 0.000134319
2017-10-11T11:13:44.457048: step 1215, loss 0.376323, acc 0.875, learning_rate 0.000134178
2017-10-11T11:13:44.568099: step 1216, loss 0.354905, acc 0.859375, learning_rate 0.000134039
2017-10-11T11:13:44.675503: step 1217, loss 0.457198, acc 0.8125, learning_rate 0.0001339
2017-10-11T11:13:44.780224: step 1218, loss 0.292411, acc 0.90625, learning_rate 0.000133762
2017-10-11T11:13:44.885778: step 1219, loss 0.265769, acc 0.9375, learning_rate 0.000133624
2017-10-11T11:13:44.998181: step 1220, loss 0.279417, acc 0.921875, learning_rate 0.000133487
2017-10-11T11:13:45.103019: step 1221, loss 0.214463, acc 0.9375, learning_rate 0.00013335
2017-10-11T11:13:45.219441: step 1222, loss 0.450942, acc 0.875, learning_rate 0.000133214
2017-10-11T11:13:45.330759: step 1223, loss 0.265319, acc 0.9375, learning_rate 0.000133078
2017-10-11T11:13:45.447678: step 1224, loss 0.275891, acc 0.890625, learning_rate 0.000132943
2017-10-11T11:13:45.565618: step 1225, loss 0.321141, acc 0.859375, learning_rate 0.000132809
2017-10-11T11:13:45.674729: step 1226, loss 0.307317, acc 0.90625, learning_rate 0.000132675
2017-10-11T11:13:45.787504: step 1227, loss 0.196199, acc 0.921875, learning_rate 0.000132541
2017-10-11T11:13:45.898479: step 1228, loss 0.369406, acc 0.859375, learning_rate 0.000132409
2017-10-11T11:13:46.087886: step 1229, loss 0.203596, acc 0.953125, learning_rate 0.000132276
2017-10-11T11:13:46.176328: step 1230, loss 0.553792, acc 0.8125, learning_rate 0.000132145
2017-10-11T11:13:46.269585: step 1231, loss 0.35918, acc 0.90625, learning_rate 0.000132013
2017-10-11T11:13:46.354448: step 1232, loss 0.407817, acc 0.875, learning_rate 0.000131883
2017-10-11T11:13:46.441390: step 1233, loss 0.557289, acc 0.8125, learning_rate 0.000131753
2017-10-11T11:13:46.529256: step 1234, loss 0.172273, acc 0.953125, learning_rate 0.000131623
2017-10-11T11:13:46.615071: step 1235, loss 0.305938, acc 0.890625, learning_rate 0.000131494
2017-10-11T11:13:46.703783: step 1236, loss 0.33675, acc 0.859375, learning_rate 0.000131365
2017-10-11T11:13:46.812868: step 1237, loss 0.335085, acc 0.90625, learning_rate 0.000131237
2017-10-11T11:13:46.925714: step 1238, loss 0.299865, acc 0.890625, learning_rate 0.00013111
2017-10-11T11:13:47.040657: step 1239, loss 0.450599, acc 0.828125, learning_rate 0.000130983
2017-10-11T11:13:47.160731: step 1240, loss 0.279074, acc 0.875, learning_rate 0.000130856

Evaluation:
2017-10-11T11:13:47.395478: step 1240, loss 0.258143, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1240

2017-10-11T11:13:48.118103: step 1241, loss 0.231, acc 0.90625, learning_rate 0.00013073
2017-10-11T11:13:48.236331: step 1242, loss 0.333692, acc 0.875, learning_rate 0.000130605
2017-10-11T11:13:48.349810: step 1243, loss 0.359765, acc 0.859375, learning_rate 0.00013048
2017-10-11T11:13:48.465333: step 1244, loss 0.290761, acc 0.921875, learning_rate 0.000130356
2017-10-11T11:13:48.576938: step 1245, loss 0.323253, acc 0.859375, learning_rate 0.000130232
2017-10-11T11:13:48.685481: step 1246, loss 0.390333, acc 0.859375, learning_rate 0.000130108
2017-10-11T11:13:48.799692: step 1247, loss 0.308289, acc 0.890625, learning_rate 0.000129985
2017-10-11T11:13:48.912862: step 1248, loss 0.49045, acc 0.875, learning_rate 0.000129863
2017-10-11T11:13:49.044887: step 1249, loss 0.385906, acc 0.875, learning_rate 0.000129741
2017-10-11T11:13:49.151947: step 1250, loss 0.202478, acc 0.9375, learning_rate 0.00012962
2017-10-11T11:13:49.265321: step 1251, loss 0.278387, acc 0.9375, learning_rate 0.000129499
2017-10-11T11:13:49.380851: step 1252, loss 0.395508, acc 0.828125, learning_rate 0.000129378
2017-10-11T11:13:49.487975: step 1253, loss 0.131671, acc 1, learning_rate 0.000129259
2017-10-11T11:13:49.599651: step 1254, loss 0.361225, acc 0.875, learning_rate 0.000129139
2017-10-11T11:13:49.724898: step 1255, loss 0.199163, acc 0.9375, learning_rate 0.00012902
2017-10-11T11:13:49.841865: step 1256, loss 0.288056, acc 0.90625, learning_rate 0.000128902
2017-10-11T11:13:49.981378: step 1257, loss 0.352674, acc 0.859375, learning_rate 0.000128784
2017-10-11T11:13:50.100114: step 1258, loss 0.30415, acc 0.890625, learning_rate 0.000128666
2017-10-11T11:13:50.223132: step 1259, loss 0.433926, acc 0.890625, learning_rate 0.000128549
2017-10-11T11:13:50.340434: step 1260, loss 0.409981, acc 0.890625, learning_rate 0.000128433
2017-10-11T11:13:50.462439: step 1261, loss 0.245002, acc 0.921875, learning_rate 0.000128317
2017-10-11T11:13:50.570913: step 1262, loss 0.362743, acc 0.875, learning_rate 0.000128201
2017-10-11T11:13:50.684141: step 1263, loss 0.341663, acc 0.875, learning_rate 0.000128086
2017-10-11T11:13:50.799947: step 1264, loss 0.38384, acc 0.890625, learning_rate 0.000127971
2017-10-11T11:13:50.912678: step 1265, loss 0.385351, acc 0.90625, learning_rate 0.000127857
2017-10-11T11:13:51.013627: step 1266, loss 0.594236, acc 0.8125, learning_rate 0.000127743
2017-10-11T11:13:51.200953: step 1267, loss 0.417501, acc 0.84375, learning_rate 0.00012763
2017-10-11T11:13:51.295037: step 1268, loss 0.35717, acc 0.875, learning_rate 0.000127517
2017-10-11T11:13:51.390366: step 1269, loss 0.315545, acc 0.921875, learning_rate 0.000127405
2017-10-11T11:13:51.475212: step 1270, loss 0.328305, acc 0.875, learning_rate 0.000127293
2017-10-11T11:13:51.561326: step 1271, loss 0.25209, acc 0.921875, learning_rate 0.000127182
2017-10-11T11:13:51.647625: step 1272, loss 0.263182, acc 0.890625, learning_rate 0.000127071
2017-10-11T11:13:51.734098: step 1273, loss 0.442523, acc 0.921875, learning_rate 0.00012696
2017-10-11T11:13:51.809184: step 1274, loss 0.283733, acc 0.901961, learning_rate 0.00012685
2017-10-11T11:13:51.896745: step 1275, loss 0.324409, acc 0.90625, learning_rate 0.000126741
2017-10-11T11:13:51.980982: step 1276, loss 0.222022, acc 0.96875, learning_rate 0.000126632
2017-10-11T11:13:52.097214: step 1277, loss 0.250114, acc 0.921875, learning_rate 0.000126523
2017-10-11T11:13:52.211349: step 1278, loss 0.440282, acc 0.828125, learning_rate 0.000126415
2017-10-11T11:13:52.328135: step 1279, loss 0.383447, acc 0.859375, learning_rate 0.000126307
2017-10-11T11:13:52.454139: step 1280, loss 0.255855, acc 0.90625, learning_rate 0.000126199

Evaluation:
2017-10-11T11:13:52.708246: step 1280, loss 0.258299, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1280

2017-10-11T11:13:53.827176: step 1281, loss 0.411752, acc 0.890625, learning_rate 0.000126093
2017-10-11T11:13:53.939055: step 1282, loss 0.425969, acc 0.84375, learning_rate 0.000125986
2017-10-11T11:13:54.062705: step 1283, loss 0.339124, acc 0.875, learning_rate 0.00012588
2017-10-11T11:13:54.183458: step 1284, loss 0.239903, acc 0.921875, learning_rate 0.000125774
2017-10-11T11:13:54.298674: step 1285, loss 0.264453, acc 0.921875, learning_rate 0.000125669
2017-10-11T11:13:54.414338: step 1286, loss 0.296221, acc 0.921875, learning_rate 0.000125564
2017-10-11T11:13:54.530058: step 1287, loss 0.24589, acc 0.921875, learning_rate 0.00012546
2017-10-11T11:13:54.649476: step 1288, loss 0.471139, acc 0.796875, learning_rate 0.000125356
2017-10-11T11:13:54.765438: step 1289, loss 0.323197, acc 0.90625, learning_rate 0.000125253
2017-10-11T11:13:54.883242: step 1290, loss 0.347737, acc 0.84375, learning_rate 0.00012515
2017-10-11T11:13:54.998758: step 1291, loss 0.406696, acc 0.84375, learning_rate 0.000125047
2017-10-11T11:13:55.114445: step 1292, loss 0.459473, acc 0.8125, learning_rate 0.000124945
2017-10-11T11:13:55.224914: step 1293, loss 0.406127, acc 0.859375, learning_rate 0.000124843
2017-10-11T11:13:55.339929: step 1294, loss 0.33803, acc 0.90625, learning_rate 0.000124741
2017-10-11T11:13:55.461313: step 1295, loss 0.357467, acc 0.875, learning_rate 0.00012464
2017-10-11T11:13:55.585316: step 1296, loss 0.361528, acc 0.859375, learning_rate 0.00012454
2017-10-11T11:13:55.711080: step 1297, loss 0.325404, acc 0.921875, learning_rate 0.00012444
2017-10-11T11:13:55.829042: step 1298, loss 0.599734, acc 0.796875, learning_rate 0.00012434
2017-10-11T11:13:55.943128: step 1299, loss 0.297603, acc 0.890625, learning_rate 0.000124241
2017-10-11T11:13:56.072492: step 1300, loss 0.182491, acc 0.953125, learning_rate 0.000124142
2017-10-11T11:13:56.179253: step 1301, loss 0.241713, acc 0.90625, learning_rate 0.000124043
2017-10-11T11:13:56.356024: step 1302, loss 0.395794, acc 0.875, learning_rate 0.000123945
2017-10-11T11:13:56.444006: step 1303, loss 0.38059, acc 0.828125, learning_rate 0.000123847
2017-10-11T11:13:56.533289: step 1304, loss 0.412684, acc 0.84375, learning_rate 0.00012375
2017-10-11T11:13:56.625741: step 1305, loss 0.352576, acc 0.890625, learning_rate 0.000123653
2017-10-11T11:13:56.721668: step 1306, loss 0.244427, acc 0.9375, learning_rate 0.000123556
2017-10-11T11:13:56.809299: step 1307, loss 0.406594, acc 0.921875, learning_rate 0.00012346
2017-10-11T11:13:56.896703: step 1308, loss 0.23906, acc 0.90625, learning_rate 0.000123364
2017-10-11T11:13:56.979821: step 1309, loss 0.186148, acc 0.953125, learning_rate 0.000123269
2017-10-11T11:13:57.066547: step 1310, loss 0.286853, acc 0.890625, learning_rate 0.000123174
2017-10-11T11:13:57.153413: step 1311, loss 0.349648, acc 0.875, learning_rate 0.00012308
2017-10-11T11:13:57.242865: step 1312, loss 0.319991, acc 0.875, learning_rate 0.000122985
2017-10-11T11:13:57.356663: step 1313, loss 0.473679, acc 0.765625, learning_rate 0.000122892
2017-10-11T11:13:57.471218: step 1314, loss 0.370182, acc 0.875, learning_rate 0.000122798
2017-10-11T11:13:57.587918: step 1315, loss 0.349001, acc 0.9375, learning_rate 0.000122705
2017-10-11T11:13:57.703841: step 1316, loss 0.291373, acc 0.875, learning_rate 0.000122612
2017-10-11T11:13:57.818492: step 1317, loss 0.178248, acc 0.953125, learning_rate 0.00012252
2017-10-11T11:13:57.931351: step 1318, loss 0.433846, acc 0.84375, learning_rate 0.000122428
2017-10-11T11:13:58.044227: step 1319, loss 0.426621, acc 0.859375, learning_rate 0.000122337
2017-10-11T11:13:58.156483: step 1320, loss 0.230483, acc 0.9375, learning_rate 0.000122245

Evaluation:
2017-10-11T11:13:58.429346: step 1320, loss 0.257454, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1320

2017-10-11T11:13:59.653844: step 1321, loss 0.475583, acc 0.8125, learning_rate 0.000122155
2017-10-11T11:13:59.763661: step 1322, loss 0.322446, acc 0.875, learning_rate 0.000122064
2017-10-11T11:13:59.879992: step 1323, loss 0.44686, acc 0.8125, learning_rate 0.000121974
2017-10-11T11:13:59.999603: step 1324, loss 0.413271, acc 0.859375, learning_rate 0.000121884
2017-10-11T11:14:00.112106: step 1325, loss 0.257524, acc 0.921875, learning_rate 0.000121795
2017-10-11T11:14:00.223269: step 1326, loss 0.473718, acc 0.859375, learning_rate 0.000121706
2017-10-11T11:14:00.338098: step 1327, loss 0.220226, acc 0.9375, learning_rate 0.000121618
2017-10-11T11:14:00.471280: step 1328, loss 0.464334, acc 0.921875, learning_rate 0.000121529
2017-10-11T11:14:00.584451: step 1329, loss 0.433181, acc 0.859375, learning_rate 0.000121441
2017-10-11T11:14:00.690233: step 1330, loss 0.237258, acc 0.953125, learning_rate 0.000121354
2017-10-11T11:14:00.804406: step 1331, loss 0.312484, acc 0.9375, learning_rate 0.000121267
2017-10-11T11:14:00.919570: step 1332, loss 0.308452, acc 0.921875, learning_rate 0.00012118
2017-10-11T11:14:01.044628: step 1333, loss 0.242715, acc 0.921875, learning_rate 0.000121093
2017-10-11T11:14:01.154101: step 1334, loss 0.464344, acc 0.84375, learning_rate 0.000121007
2017-10-11T11:14:01.263593: step 1335, loss 0.446136, acc 0.828125, learning_rate 0.000120922
2017-10-11T11:14:01.384912: step 1336, loss 0.374852, acc 0.859375, learning_rate 0.000120836
2017-10-11T11:14:01.494997: step 1337, loss 0.269374, acc 0.875, learning_rate 0.000120751
2017-10-11T11:14:01.609374: step 1338, loss 0.300726, acc 0.890625, learning_rate 0.000120666
2017-10-11T11:14:01.720944: step 1339, loss 0.335172, acc 0.875, learning_rate 0.000120582
2017-10-11T11:14:01.826770: step 1340, loss 0.368196, acc 0.84375, learning_rate 0.000120498
2017-10-11T11:14:01.931011: step 1341, loss 0.334176, acc 0.921875, learning_rate 0.000120414
2017-10-11T11:14:02.043267: step 1342, loss 0.266286, acc 0.921875, learning_rate 0.000120331
2017-10-11T11:14:02.233560: step 1343, loss 0.344021, acc 0.890625, learning_rate 0.000120248
2017-10-11T11:14:02.322539: step 1344, loss 0.277637, acc 0.90625, learning_rate 0.000120165
2017-10-11T11:14:02.408295: step 1345, loss 0.282566, acc 0.9375, learning_rate 0.000120083
2017-10-11T11:14:02.498094: step 1346, loss 0.391958, acc 0.875, learning_rate 0.000120001
2017-10-11T11:14:02.587671: step 1347, loss 0.29495, acc 0.875, learning_rate 0.00011992
2017-10-11T11:14:02.676484: step 1348, loss 0.331066, acc 0.890625, learning_rate 0.000119838
2017-10-11T11:14:02.764585: step 1349, loss 0.31603, acc 0.90625, learning_rate 0.000119757
2017-10-11T11:14:02.851652: step 1350, loss 0.252057, acc 0.890625, learning_rate 0.000119677
2017-10-11T11:14:02.964068: step 1351, loss 0.435333, acc 0.859375, learning_rate 0.000119596
2017-10-11T11:14:03.083533: step 1352, loss 0.256741, acc 0.9375, learning_rate 0.000119516
2017-10-11T11:14:03.190187: step 1353, loss 0.269227, acc 0.90625, learning_rate 0.000119437
2017-10-11T11:14:03.311097: step 1354, loss 0.483805, acc 0.859375, learning_rate 0.000119357
2017-10-11T11:14:03.419161: step 1355, loss 0.397815, acc 0.84375, learning_rate 0.000119278
2017-10-11T11:14:03.537364: step 1356, loss 0.386574, acc 0.859375, learning_rate 0.0001192
2017-10-11T11:14:03.654944: step 1357, loss 0.2493, acc 0.9375, learning_rate 0.000119121
2017-10-11T11:14:03.771295: step 1358, loss 0.259153, acc 0.890625, learning_rate 0.000119043
2017-10-11T11:14:03.887787: step 1359, loss 0.503237, acc 0.828125, learning_rate 0.000118965
2017-10-11T11:14:04.006211: step 1360, loss 0.321121, acc 0.921875, learning_rate 0.000118888

Evaluation:
2017-10-11T11:14:04.247349: step 1360, loss 0.257895, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1360

2017-10-11T11:14:04.864862: step 1361, loss 0.506871, acc 0.875, learning_rate 0.000118811
2017-10-11T11:14:04.976568: step 1362, loss 0.246214, acc 0.9375, learning_rate 0.000118734
2017-10-11T11:14:05.089195: step 1363, loss 0.59155, acc 0.8125, learning_rate 0.000118658
2017-10-11T11:14:05.204277: step 1364, loss 0.283107, acc 0.9375, learning_rate 0.000118582
2017-10-11T11:14:05.313261: step 1365, loss 0.372807, acc 0.875, learning_rate 0.000118506
2017-10-11T11:14:05.424802: step 1366, loss 0.274652, acc 0.890625, learning_rate 0.00011843
2017-10-11T11:14:05.552951: step 1367, loss 0.364087, acc 0.859375, learning_rate 0.000118355
2017-10-11T11:14:05.670808: step 1368, loss 0.208911, acc 0.9375, learning_rate 0.00011828
2017-10-11T11:14:05.779924: step 1369, loss 0.456138, acc 0.8125, learning_rate 0.000118205
2017-10-11T11:14:05.896769: step 1370, loss 0.239258, acc 0.921875, learning_rate 0.000118131
2017-10-11T11:14:06.012650: step 1371, loss 0.312186, acc 0.953125, learning_rate 0.000118057
2017-10-11T11:14:06.111145: step 1372, loss 0.468057, acc 0.862745, learning_rate 0.000117983
2017-10-11T11:14:06.228887: step 1373, loss 0.369073, acc 0.890625, learning_rate 0.00011791
2017-10-11T11:14:06.352711: step 1374, loss 0.252832, acc 0.9375, learning_rate 0.000117837
2017-10-11T11:14:06.459923: step 1375, loss 0.400173, acc 0.859375, learning_rate 0.000117764
2017-10-11T11:14:06.577656: step 1376, loss 0.314123, acc 0.859375, learning_rate 0.000117692
2017-10-11T11:14:06.700983: step 1377, loss 0.335332, acc 0.875, learning_rate 0.000117619
2017-10-11T11:14:06.820048: step 1378, loss 0.209764, acc 0.9375, learning_rate 0.000117547
2017-10-11T11:14:06.931514: step 1379, loss 0.299713, acc 0.890625, learning_rate 0.000117476
2017-10-11T11:14:07.040505: step 1380, loss 0.220861, acc 0.890625, learning_rate 0.000117404
2017-10-11T11:14:07.149767: step 1381, loss 0.351843, acc 0.890625, learning_rate 0.000117333
2017-10-11T11:14:07.256917: step 1382, loss 0.413031, acc 0.875, learning_rate 0.000117263
2017-10-11T11:14:07.413360: step 1383, loss 0.336682, acc 0.9375, learning_rate 0.000117192
2017-10-11T11:14:07.515334: step 1384, loss 0.320503, acc 0.859375, learning_rate 0.000117122
2017-10-11T11:14:07.607641: step 1385, loss 0.229585, acc 0.9375, learning_rate 0.000117052
2017-10-11T11:14:07.700078: step 1386, loss 0.307896, acc 0.90625, learning_rate 0.000116983
2017-10-11T11:14:07.792677: step 1387, loss 0.313645, acc 0.84375, learning_rate 0.000116913
2017-10-11T11:14:07.890402: step 1388, loss 0.185623, acc 0.9375, learning_rate 0.000116844
2017-10-11T11:14:07.983746: step 1389, loss 0.262771, acc 0.921875, learning_rate 0.000116775
2017-10-11T11:14:08.073576: step 1390, loss 0.509282, acc 0.796875, learning_rate 0.000116707
2017-10-11T11:14:08.163209: step 1391, loss 0.407672, acc 0.859375, learning_rate 0.000116639
2017-10-11T11:14:08.262804: step 1392, loss 0.22534, acc 0.921875, learning_rate 0.000116571
2017-10-11T11:14:08.369068: step 1393, loss 0.241431, acc 0.953125, learning_rate 0.000116503
2017-10-11T11:14:08.480751: step 1394, loss 0.526404, acc 0.90625, learning_rate 0.000116436
2017-10-11T11:14:08.595507: step 1395, loss 0.366196, acc 0.828125, learning_rate 0.000116369
2017-10-11T11:14:08.708736: step 1396, loss 0.29515, acc 0.90625, learning_rate 0.000116302
2017-10-11T11:14:08.832056: step 1397, loss 0.315248, acc 0.90625, learning_rate 0.000116235
2017-10-11T11:14:08.956891: step 1398, loss 0.439224, acc 0.796875, learning_rate 0.000116169
2017-10-11T11:14:09.074372: step 1399, loss 0.394773, acc 0.828125, learning_rate 0.000116103
2017-10-11T11:14:09.181734: step 1400, loss 0.226418, acc 0.9375, learning_rate 0.000116037

Evaluation:
2017-10-11T11:14:09.418572: step 1400, loss 0.257766, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1400

2017-10-11T11:14:10.166950: step 1401, loss 0.417064, acc 0.859375, learning_rate 0.000115972
2017-10-11T11:14:10.281058: step 1402, loss 0.320692, acc 0.9375, learning_rate 0.000115907
2017-10-11T11:14:10.395240: step 1403, loss 0.369309, acc 0.875, learning_rate 0.000115842
2017-10-11T11:14:10.512580: step 1404, loss 0.148419, acc 0.984375, learning_rate 0.000115777
2017-10-11T11:14:10.622856: step 1405, loss 0.374589, acc 0.875, learning_rate 0.000115713
2017-10-11T11:14:10.730650: step 1406, loss 0.312735, acc 0.921875, learning_rate 0.000115649
2017-10-11T11:14:10.851876: step 1407, loss 0.406369, acc 0.828125, learning_rate 0.000115585
2017-10-11T11:14:10.965653: step 1408, loss 0.273969, acc 0.90625, learning_rate 0.000115521
2017-10-11T11:14:11.086286: step 1409, loss 0.298052, acc 0.9375, learning_rate 0.000115458
2017-10-11T11:14:11.204582: step 1410, loss 0.260687, acc 0.9375, learning_rate 0.000115395
2017-10-11T11:14:11.316275: step 1411, loss 0.441093, acc 0.8125, learning_rate 0.000115332
2017-10-11T11:14:11.432145: step 1412, loss 0.265522, acc 0.921875, learning_rate 0.000115269
2017-10-11T11:14:11.549542: step 1413, loss 0.269903, acc 0.90625, learning_rate 0.000115207
2017-10-11T11:14:11.661800: step 1414, loss 0.311144, acc 0.875, learning_rate 0.000115145
2017-10-11T11:14:11.766072: step 1415, loss 0.309843, acc 0.875, learning_rate 0.000115083
2017-10-11T11:14:11.880770: step 1416, loss 0.374504, acc 0.859375, learning_rate 0.000115022
2017-10-11T11:14:11.993386: step 1417, loss 0.366532, acc 0.859375, learning_rate 0.00011496
2017-10-11T11:14:12.110272: step 1418, loss 0.263102, acc 0.921875, learning_rate 0.000114899
2017-10-11T11:14:12.227895: step 1419, loss 0.540718, acc 0.796875, learning_rate 0.000114838
2017-10-11T11:14:12.341465: step 1420, loss 0.304404, acc 0.875, learning_rate 0.000114778
2017-10-11T11:14:12.458296: step 1421, loss 0.310995, acc 0.890625, learning_rate 0.000114717
2017-10-11T11:14:12.648339: step 1422, loss 0.224183, acc 0.953125, learning_rate 0.000114657
2017-10-11T11:14:12.733691: step 1423, loss 0.123341, acc 0.984375, learning_rate 0.000114598
2017-10-11T11:14:12.832178: step 1424, loss 0.245126, acc 0.90625, learning_rate 0.000114538
2017-10-11T11:14:12.920239: step 1425, loss 0.585385, acc 0.796875, learning_rate 0.000114479
2017-10-11T11:14:13.007023: step 1426, loss 0.279463, acc 0.890625, learning_rate 0.00011442
2017-10-11T11:14:13.095496: step 1427, loss 0.272541, acc 0.90625, learning_rate 0.000114361
2017-10-11T11:14:13.180145: step 1428, loss 0.217052, acc 0.9375, learning_rate 0.000114302
2017-10-11T11:14:13.266410: step 1429, loss 0.317896, acc 0.921875, learning_rate 0.000114244
2017-10-11T11:14:13.353693: step 1430, loss 0.328135, acc 0.875, learning_rate 0.000114186
2017-10-11T11:14:13.438984: step 1431, loss 0.691279, acc 0.8125, learning_rate 0.000114128
2017-10-11T11:14:13.526470: step 1432, loss 0.178152, acc 0.96875, learning_rate 0.00011407
2017-10-11T11:14:13.612988: step 1433, loss 0.237398, acc 0.921875, learning_rate 0.000114013
2017-10-11T11:14:13.730272: step 1434, loss 0.501191, acc 0.8125, learning_rate 0.000113955
2017-10-11T11:14:13.848176: step 1435, loss 0.373404, acc 0.875, learning_rate 0.000113898
2017-10-11T11:14:13.958578: step 1436, loss 0.235019, acc 0.953125, learning_rate 0.000113842
2017-10-11T11:14:14.072100: step 1437, loss 0.373167, acc 0.890625, learning_rate 0.000113785
2017-10-11T11:14:14.181240: step 1438, loss 0.284067, acc 0.9375, learning_rate 0.000113729
2017-10-11T11:14:14.307639: step 1439, loss 0.381404, acc 0.84375, learning_rate 0.000113673
2017-10-11T11:14:14.420940: step 1440, loss 0.21624, acc 0.96875, learning_rate 0.000113617

Evaluation:
2017-10-11T11:14:14.664855: step 1440, loss 0.257497, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1440

2017-10-11T11:14:15.370082: step 1441, loss 0.409941, acc 0.90625, learning_rate 0.000113561
2017-10-11T11:14:15.494286: step 1442, loss 0.216992, acc 0.953125, learning_rate 0.000113506
2017-10-11T11:14:15.613603: step 1443, loss 0.523323, acc 0.84375, learning_rate 0.000113451
2017-10-11T11:14:15.722873: step 1444, loss 0.439814, acc 0.84375, learning_rate 0.000113396
2017-10-11T11:14:15.845048: step 1445, loss 0.340504, acc 0.90625, learning_rate 0.000113341
2017-10-11T11:14:15.967732: step 1446, loss 0.397897, acc 0.890625, learning_rate 0.000113287
2017-10-11T11:14:16.082891: step 1447, loss 0.403202, acc 0.859375, learning_rate 0.000113233
2017-10-11T11:14:16.203217: step 1448, loss 0.454613, acc 0.875, learning_rate 0.000113179
2017-10-11T11:14:16.318990: step 1449, loss 0.236172, acc 0.921875, learning_rate 0.000113125
2017-10-11T11:14:16.431998: step 1450, loss 0.267954, acc 0.9375, learning_rate 0.000113071
2017-10-11T11:14:16.546066: step 1451, loss 0.328927, acc 0.890625, learning_rate 0.000113018
2017-10-11T11:14:16.657562: step 1452, loss 0.386093, acc 0.875, learning_rate 0.000112965
2017-10-11T11:14:16.768126: step 1453, loss 0.309889, acc 0.90625, learning_rate 0.000112912
2017-10-11T11:14:16.878569: step 1454, loss 0.439608, acc 0.875, learning_rate 0.000112859
2017-10-11T11:14:16.997571: step 1455, loss 0.374921, acc 0.875, learning_rate 0.000112807
2017-10-11T11:14:17.109448: step 1456, loss 0.28677, acc 0.921875, learning_rate 0.000112754
2017-10-11T11:14:17.234114: step 1457, loss 0.294152, acc 0.90625, learning_rate 0.000112702
2017-10-11T11:14:17.353254: step 1458, loss 0.41494, acc 0.859375, learning_rate 0.000112651
2017-10-11T11:14:17.464230: step 1459, loss 0.395334, acc 0.890625, learning_rate 0.000112599
2017-10-11T11:14:17.584981: step 1460, loss 0.328506, acc 0.890625, learning_rate 0.000112547
2017-10-11T11:14:17.695472: step 1461, loss 0.5986, acc 0.796875, learning_rate 0.000112496
2017-10-11T11:14:17.798143: step 1462, loss 0.356466, acc 0.875, learning_rate 0.000112445
2017-10-11T11:14:17.909967: step 1463, loss 0.335727, acc 0.90625, learning_rate 0.000112394
2017-10-11T11:14:18.077033: step 1464, loss 0.478662, acc 0.84375, learning_rate 0.000112344
2017-10-11T11:14:18.172515: step 1465, loss 0.427821, acc 0.890625, learning_rate 0.000112293
2017-10-11T11:14:18.262835: step 1466, loss 0.469995, acc 0.90625, learning_rate 0.000112243
2017-10-11T11:14:18.350115: step 1467, loss 0.392859, acc 0.90625, learning_rate 0.000112193
2017-10-11T11:14:18.439039: step 1468, loss 0.188186, acc 0.96875, learning_rate 0.000112144
2017-10-11T11:14:18.523696: step 1469, loss 0.294424, acc 0.890625, learning_rate 0.000112094
2017-10-11T11:14:18.598142: step 1470, loss 0.233539, acc 0.921569, learning_rate 0.000112045
2017-10-11T11:14:18.686871: step 1471, loss 0.460644, acc 0.828125, learning_rate 0.000111995
2017-10-11T11:14:18.784986: step 1472, loss 0.298885, acc 0.921875, learning_rate 0.000111946
2017-10-11T11:14:18.899370: step 1473, loss 0.363766, acc 0.859375, learning_rate 0.000111898
2017-10-11T11:14:19.006301: step 1474, loss 0.461609, acc 0.875, learning_rate 0.000111849
2017-10-11T11:14:19.115080: step 1475, loss 0.297227, acc 0.890625, learning_rate 0.000111801
2017-10-11T11:14:19.240264: step 1476, loss 0.318638, acc 0.890625, learning_rate 0.000111753
2017-10-11T11:14:19.351901: step 1477, loss 0.244558, acc 0.890625, learning_rate 0.000111705
2017-10-11T11:14:19.464883: step 1478, loss 0.354045, acc 0.890625, learning_rate 0.000111657
2017-10-11T11:14:19.581198: step 1479, loss 0.285048, acc 0.90625, learning_rate 0.000111609
2017-10-11T11:14:19.696541: step 1480, loss 0.289007, acc 0.875, learning_rate 0.000111562

Evaluation:
2017-10-11T11:14:19.957211: step 1480, loss 0.256129, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1480

2017-10-11T11:14:20.693341: step 1481, loss 0.328321, acc 0.890625, learning_rate 0.000111515
2017-10-11T11:14:20.812188: step 1482, loss 0.360594, acc 0.859375, learning_rate 0.000111468
2017-10-11T11:14:20.924791: step 1483, loss 0.460415, acc 0.859375, learning_rate 0.000111421
2017-10-11T11:14:21.035672: step 1484, loss 0.186606, acc 0.984375, learning_rate 0.000111374
2017-10-11T11:14:21.157276: step 1485, loss 0.369888, acc 0.875, learning_rate 0.000111328
2017-10-11T11:14:21.271635: step 1486, loss 0.314752, acc 0.90625, learning_rate 0.000111282
2017-10-11T11:14:21.387796: step 1487, loss 0.316014, acc 0.890625, learning_rate 0.000111236
2017-10-11T11:14:21.505178: step 1488, loss 0.283553, acc 0.90625, learning_rate 0.00011119
2017-10-11T11:14:21.621745: step 1489, loss 0.444205, acc 0.8125, learning_rate 0.000111144
2017-10-11T11:14:21.729301: step 1490, loss 0.287881, acc 0.90625, learning_rate 0.000111099
2017-10-11T11:14:21.838868: step 1491, loss 0.379182, acc 0.875, learning_rate 0.000111053
2017-10-11T11:14:21.954741: step 1492, loss 0.285058, acc 0.921875, learning_rate 0.000111008
2017-10-11T11:14:22.077379: step 1493, loss 0.298052, acc 0.921875, learning_rate 0.000110963
2017-10-11T11:14:22.196481: step 1494, loss 0.39319, acc 0.875, learning_rate 0.000110918
2017-10-11T11:14:22.313922: step 1495, loss 0.213839, acc 0.9375, learning_rate 0.000110874
2017-10-11T11:14:22.429788: step 1496, loss 0.269776, acc 0.921875, learning_rate 0.00011083
2017-10-11T11:14:22.544832: step 1497, loss 0.395735, acc 0.84375, learning_rate 0.000110785
2017-10-11T11:14:22.660860: step 1498, loss 0.263636, acc 0.921875, learning_rate 0.000110741
2017-10-11T11:14:22.772881: step 1499, loss 0.363509, acc 0.921875, learning_rate 0.000110697
2017-10-11T11:14:22.900544: step 1500, loss 0.236459, acc 0.921875, learning_rate 0.000110654
2017-10-11T11:14:23.020581: step 1501, loss 0.268504, acc 0.890625, learning_rate 0.00011061
2017-10-11T11:14:23.186472: step 1502, loss 0.166381, acc 0.953125, learning_rate 0.000110567
2017-10-11T11:14:23.293679: step 1503, loss 0.322041, acc 0.90625, learning_rate 0.000110524
2017-10-11T11:14:23.379721: step 1504, loss 0.365814, acc 0.921875, learning_rate 0.000110481
2017-10-11T11:14:23.468425: step 1505, loss 0.268122, acc 0.921875, learning_rate 0.000110438
2017-10-11T11:14:23.555639: step 1506, loss 0.329116, acc 0.875, learning_rate 0.000110396
2017-10-11T11:14:23.642027: step 1507, loss 0.252297, acc 0.921875, learning_rate 0.000110353
2017-10-11T11:14:23.733588: step 1508, loss 0.213223, acc 0.96875, learning_rate 0.000110311
2017-10-11T11:14:23.823929: step 1509, loss 0.261737, acc 0.90625, learning_rate 0.000110269
2017-10-11T11:14:23.921405: step 1510, loss 0.254219, acc 0.875, learning_rate 0.000110227
2017-10-11T11:14:24.010734: step 1511, loss 0.286785, acc 0.90625, learning_rate 0.000110185
2017-10-11T11:14:24.123511: step 1512, loss 0.281703, acc 0.921875, learning_rate 0.000110144
2017-10-11T11:14:24.245240: step 1513, loss 0.253033, acc 0.9375, learning_rate 0.000110102
2017-10-11T11:14:24.358515: step 1514, loss 0.312155, acc 0.90625, learning_rate 0.000110061
2017-10-11T11:14:24.478029: step 1515, loss 0.461849, acc 0.828125, learning_rate 0.00011002
2017-10-11T11:14:24.594997: step 1516, loss 0.369183, acc 0.890625, learning_rate 0.000109979
2017-10-11T11:14:24.719933: step 1517, loss 0.384928, acc 0.875, learning_rate 0.000109938
2017-10-11T11:14:24.836998: step 1518, loss 0.300263, acc 0.9375, learning_rate 0.000109898
2017-10-11T11:14:24.951421: step 1519, loss 0.316746, acc 0.859375, learning_rate 0.000109857
2017-10-11T11:14:25.062521: step 1520, loss 0.363192, acc 0.890625, learning_rate 0.000109817

Evaluation:
2017-10-11T11:14:25.315180: step 1520, loss 0.255337, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1520

2017-10-11T11:14:26.036915: step 1521, loss 0.395146, acc 0.828125, learning_rate 0.000109777
2017-10-11T11:14:26.149482: step 1522, loss 0.330581, acc 0.859375, learning_rate 0.000109737
2017-10-11T11:14:26.257108: step 1523, loss 0.227171, acc 0.921875, learning_rate 0.000109697
2017-10-11T11:14:26.367603: step 1524, loss 0.329445, acc 0.921875, learning_rate 0.000109658
2017-10-11T11:14:26.475006: step 1525, loss 0.284697, acc 0.90625, learning_rate 0.000109618
2017-10-11T11:14:26.587571: step 1526, loss 0.240134, acc 0.953125, learning_rate 0.000109579
2017-10-11T11:14:26.712619: step 1527, loss 0.339108, acc 0.890625, learning_rate 0.00010954
2017-10-11T11:14:26.836355: step 1528, loss 0.234343, acc 0.953125, learning_rate 0.000109501
2017-10-11T11:14:26.953294: step 1529, loss 0.39706, acc 0.890625, learning_rate 0.000109462
2017-10-11T11:14:27.070515: step 1530, loss 0.424814, acc 0.84375, learning_rate 0.000109424
2017-10-11T11:14:27.188042: step 1531, loss 0.360547, acc 0.859375, learning_rate 0.000109385
2017-10-11T11:14:27.307723: step 1532, loss 0.294277, acc 0.90625, learning_rate 0.000109347
2017-10-11T11:14:27.421510: step 1533, loss 0.293754, acc 0.921875, learning_rate 0.000109309
2017-10-11T11:14:27.542121: step 1534, loss 0.449846, acc 0.859375, learning_rate 0.000109271
2017-10-11T11:14:27.665094: step 1535, loss 0.345174, acc 0.921875, learning_rate 0.000109233
2017-10-11T11:14:27.778919: step 1536, loss 0.368804, acc 0.859375, learning_rate 0.000109195
2017-10-11T11:14:27.900141: step 1537, loss 0.325419, acc 0.890625, learning_rate 0.000109158
2017-10-11T11:14:28.010793: step 1538, loss 0.240999, acc 0.9375, learning_rate 0.00010912
2017-10-11T11:14:28.124062: step 1539, loss 0.312431, acc 0.890625, learning_rate 0.000109083
2017-10-11T11:14:28.232106: step 1540, loss 0.207938, acc 0.953125, learning_rate 0.000109046
2017-10-11T11:14:28.345844: step 1541, loss 0.45096, acc 0.8125, learning_rate 0.000109009
2017-10-11T11:14:28.533383: step 1542, loss 0.393175, acc 0.828125, learning_rate 0.000108972
2017-10-11T11:14:28.619325: step 1543, loss 0.343599, acc 0.875, learning_rate 0.000108936
2017-10-11T11:14:28.715689: step 1544, loss 0.302709, acc 0.90625, learning_rate 0.000108899
2017-10-11T11:14:28.813835: step 1545, loss 0.291804, acc 0.9375, learning_rate 0.000108863
2017-10-11T11:14:28.900969: step 1546, loss 0.459849, acc 0.8125, learning_rate 0.000108827
2017-10-11T11:14:28.986183: step 1547, loss 0.377983, acc 0.90625, learning_rate 0.000108791
2017-10-11T11:14:29.074490: step 1548, loss 0.352938, acc 0.828125, learning_rate 0.000108755
2017-10-11T11:14:29.162007: step 1549, loss 0.292275, acc 0.9375, learning_rate 0.000108719
2017-10-11T11:14:29.261110: step 1550, loss 0.281016, acc 0.875, learning_rate 0.000108683
2017-10-11T11:14:29.352357: step 1551, loss 0.346002, acc 0.90625, learning_rate 0.000108648
2017-10-11T11:14:29.460244: step 1552, loss 0.261455, acc 0.890625, learning_rate 0.000108613
2017-10-11T11:14:29.572876: step 1553, loss 0.283559, acc 0.921875, learning_rate 0.000108577
2017-10-11T11:14:29.684489: step 1554, loss 0.421438, acc 0.84375, learning_rate 0.000108542
2017-10-11T11:14:29.798992: step 1555, loss 0.335539, acc 0.921875, learning_rate 0.000108508
2017-10-11T11:14:29.928911: step 1556, loss 0.271909, acc 0.890625, learning_rate 0.000108473
2017-10-11T11:14:30.052492: step 1557, loss 0.231563, acc 0.921875, learning_rate 0.000108438
2017-10-11T11:14:30.158343: step 1558, loss 0.285007, acc 0.890625, learning_rate 0.000108404
2017-10-11T11:14:30.275362: step 1559, loss 0.235214, acc 0.9375, learning_rate 0.00010837
2017-10-11T11:14:30.383394: step 1560, loss 0.264894, acc 0.890625, learning_rate 0.000108335

Evaluation:
2017-10-11T11:14:30.626873: step 1560, loss 0.255112, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1560

2017-10-11T11:14:31.350318: step 1561, loss 0.245837, acc 0.921875, learning_rate 0.000108301
2017-10-11T11:14:31.468633: step 1562, loss 0.369187, acc 0.890625, learning_rate 0.000108267
2017-10-11T11:14:31.588176: step 1563, loss 0.482217, acc 0.8125, learning_rate 0.000108234
2017-10-11T11:14:31.704901: step 1564, loss 0.523016, acc 0.8125, learning_rate 0.0001082
2017-10-11T11:14:31.832877: step 1565, loss 0.249699, acc 0.921875, learning_rate 0.000108167
2017-10-11T11:14:31.941333: step 1566, loss 0.307601, acc 0.9375, learning_rate 0.000108133
2017-10-11T11:14:32.050795: step 1567, loss 0.341329, acc 0.875, learning_rate 0.0001081
2017-10-11T11:14:32.148434: step 1568, loss 0.410416, acc 0.862745, learning_rate 0.000108067
2017-10-11T11:14:32.272891: step 1569, loss 0.20938, acc 0.921875, learning_rate 0.000108034
2017-10-11T11:14:32.384845: step 1570, loss 0.27008, acc 0.890625, learning_rate 0.000108001
2017-10-11T11:14:32.496821: step 1571, loss 0.39763, acc 0.828125, learning_rate 0.000107969
2017-10-11T11:14:32.610109: step 1572, loss 0.435915, acc 0.8125, learning_rate 0.000107936
2017-10-11T11:14:32.720730: step 1573, loss 0.361, acc 0.890625, learning_rate 0.000107904
2017-10-11T11:14:32.834286: step 1574, loss 0.219435, acc 0.9375, learning_rate 0.000107871
2017-10-11T11:14:32.956521: step 1575, loss 0.208147, acc 0.90625, learning_rate 0.000107839
2017-10-11T11:14:33.072969: step 1576, loss 0.41239, acc 0.84375, learning_rate 0.000107807
2017-10-11T11:14:33.184060: step 1577, loss 0.328559, acc 0.859375, learning_rate 0.000107775
2017-10-11T11:14:33.299917: step 1578, loss 0.431374, acc 0.875, learning_rate 0.000107744
2017-10-11T11:14:33.404222: step 1579, loss 0.553349, acc 0.8125, learning_rate 0.000107712
2017-10-11T11:14:33.520627: step 1580, loss 0.59777, acc 0.8125, learning_rate 0.000107681
2017-10-11T11:14:33.626956: step 1581, loss 0.349365, acc 0.84375, learning_rate 0.000107649
2017-10-11T11:14:33.810589: step 1582, loss 0.234275, acc 0.90625, learning_rate 0.000107618
2017-10-11T11:14:33.894043: step 1583, loss 0.507556, acc 0.78125, learning_rate 0.000107587
2017-10-11T11:14:33.977016: step 1584, loss 0.27413, acc 0.90625, learning_rate 0.000107556
2017-10-11T11:14:34.060392: step 1585, loss 0.289214, acc 0.921875, learning_rate 0.000107525
2017-10-11T11:14:34.143928: step 1586, loss 0.212184, acc 0.9375, learning_rate 0.000107494
2017-10-11T11:14:34.230118: step 1587, loss 0.334171, acc 0.921875, learning_rate 0.000107464
2017-10-11T11:14:34.320192: step 1588, loss 0.284345, acc 0.890625, learning_rate 0.000107433
2017-10-11T11:14:34.419301: step 1589, loss 0.392287, acc 0.875, learning_rate 0.000107403
2017-10-11T11:14:34.506771: step 1590, loss 0.216901, acc 0.9375, learning_rate 0.000107373
2017-10-11T11:14:34.593852: step 1591, loss 0.374627, acc 0.9375, learning_rate 0.000107343
2017-10-11T11:14:34.681137: step 1592, loss 0.39738, acc 0.890625, learning_rate 0.000107313
2017-10-11T11:14:34.767187: step 1593, loss 0.225562, acc 0.9375, learning_rate 0.000107283
2017-10-11T11:14:34.851985: step 1594, loss 0.37348, acc 0.890625, learning_rate 0.000107253
2017-10-11T11:14:34.937747: step 1595, loss 0.261256, acc 0.9375, learning_rate 0.000107224
2017-10-11T11:14:35.025778: step 1596, loss 0.384367, acc 0.875, learning_rate 0.000107194
2017-10-11T11:14:35.120521: step 1597, loss 0.337195, acc 0.875, learning_rate 0.000107165
2017-10-11T11:14:35.241936: step 1598, loss 0.219207, acc 0.96875, learning_rate 0.000107136
2017-10-11T11:14:35.353627: step 1599, loss 0.233428, acc 0.9375, learning_rate 0.000107106
2017-10-11T11:14:35.472895: step 1600, loss 0.368783, acc 0.859375, learning_rate 0.000107077

Evaluation:
2017-10-11T11:14:35.741505: step 1600, loss 0.253819, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1600

2017-10-11T11:14:36.474708: step 1601, loss 0.497662, acc 0.796875, learning_rate 0.000107048
2017-10-11T11:14:36.584872: step 1602, loss 0.323011, acc 0.921875, learning_rate 0.00010702
2017-10-11T11:14:36.687566: step 1603, loss 0.302547, acc 0.921875, learning_rate 0.000106991
2017-10-11T11:14:36.797048: step 1604, loss 0.410905, acc 0.859375, learning_rate 0.000106963
2017-10-11T11:14:36.916890: step 1605, loss 0.315726, acc 0.890625, learning_rate 0.000106934
2017-10-11T11:14:37.035450: step 1606, loss 0.343668, acc 0.875, learning_rate 0.000106906
2017-10-11T11:14:37.148352: step 1607, loss 0.626929, acc 0.84375, learning_rate 0.000106878
2017-10-11T11:14:37.257892: step 1608, loss 0.328326, acc 0.859375, learning_rate 0.00010685
2017-10-11T11:14:37.368831: step 1609, loss 0.476672, acc 0.84375, learning_rate 0.000106822
2017-10-11T11:14:37.478526: step 1610, loss 0.225631, acc 0.921875, learning_rate 0.000106794
2017-10-11T11:14:37.599910: step 1611, loss 0.364335, acc 0.875, learning_rate 0.000106766
2017-10-11T11:14:37.705719: step 1612, loss 0.261254, acc 0.90625, learning_rate 0.000106738
2017-10-11T11:14:37.824550: step 1613, loss 0.288292, acc 0.921875, learning_rate 0.000106711
2017-10-11T11:14:37.934463: step 1614, loss 0.257887, acc 0.90625, learning_rate 0.000106684
2017-10-11T11:14:38.050976: step 1615, loss 0.255688, acc 0.953125, learning_rate 0.000106656
2017-10-11T11:14:38.171895: step 1616, loss 0.257746, acc 0.90625, learning_rate 0.000106629
2017-10-11T11:14:38.288689: step 1617, loss 0.241222, acc 0.9375, learning_rate 0.000106602
2017-10-11T11:14:38.403460: step 1618, loss 0.459201, acc 0.828125, learning_rate 0.000106575
2017-10-11T11:14:38.520511: step 1619, loss 0.348807, acc 0.84375, learning_rate 0.000106548
2017-10-11T11:14:38.640100: step 1620, loss 0.467706, acc 0.859375, learning_rate 0.000106521
2017-10-11T11:14:38.754720: step 1621, loss 0.31798, acc 0.90625, learning_rate 0.000106495
2017-10-11T11:14:38.872922: step 1622, loss 0.400864, acc 0.859375, learning_rate 0.000106468
2017-10-11T11:14:38.980899: step 1623, loss 0.174486, acc 0.90625, learning_rate 0.000106442
2017-10-11T11:14:39.095082: step 1624, loss 0.401101, acc 0.875, learning_rate 0.000106416
2017-10-11T11:14:39.219756: step 1625, loss 0.256559, acc 0.921875, learning_rate 0.000106389
2017-10-11T11:14:39.392891: step 1626, loss 0.204303, acc 0.9375, learning_rate 0.000106363
2017-10-11T11:14:39.492833: step 1627, loss 0.416941, acc 0.890625, learning_rate 0.000106337
2017-10-11T11:14:39.580080: step 1628, loss 0.441918, acc 0.859375, learning_rate 0.000106312
2017-10-11T11:14:39.667443: step 1629, loss 0.313105, acc 0.890625, learning_rate 0.000106286
2017-10-11T11:14:39.756069: step 1630, loss 0.29011, acc 0.90625, learning_rate 0.00010626
2017-10-11T11:14:39.843016: step 1631, loss 0.390849, acc 0.890625, learning_rate 0.000106235
2017-10-11T11:14:39.928017: step 1632, loss 0.408581, acc 0.859375, learning_rate 0.000106209
2017-10-11T11:14:40.013252: step 1633, loss 0.510325, acc 0.796875, learning_rate 0.000106184
2017-10-11T11:14:40.099726: step 1634, loss 0.267039, acc 0.921875, learning_rate 0.000106159
2017-10-11T11:14:40.220241: step 1635, loss 0.236487, acc 0.9375, learning_rate 0.000106133
2017-10-11T11:14:40.339807: step 1636, loss 0.221891, acc 0.90625, learning_rate 0.000106108
2017-10-11T11:14:40.468857: step 1637, loss 0.29838, acc 0.875, learning_rate 0.000106083
2017-10-11T11:14:40.580216: step 1638, loss 0.275343, acc 0.921875, learning_rate 0.000106059
2017-10-11T11:14:40.693085: step 1639, loss 0.313476, acc 0.90625, learning_rate 0.000106034
2017-10-11T11:14:40.810166: step 1640, loss 0.229911, acc 0.9375, learning_rate 0.000106009

Evaluation:
2017-10-11T11:14:41.063096: step 1640, loss 0.253635, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1640

2017-10-11T11:14:41.708935: step 1641, loss 0.388049, acc 0.84375, learning_rate 0.000105985
2017-10-11T11:14:41.814871: step 1642, loss 0.31424, acc 0.890625, learning_rate 0.00010596
2017-10-11T11:14:41.907477: step 1643, loss 0.464177, acc 0.8125, learning_rate 0.000105936
2017-10-11T11:14:42.024963: step 1644, loss 0.317569, acc 0.90625, learning_rate 0.000105912
2017-10-11T11:14:42.129863: step 1645, loss 0.324325, acc 0.890625, learning_rate 0.000105888
2017-10-11T11:14:42.253875: step 1646, loss 0.314763, acc 0.875, learning_rate 0.000105864
2017-10-11T11:14:42.379646: step 1647, loss 0.211717, acc 0.9375, learning_rate 0.00010584
2017-10-11T11:14:42.492409: step 1648, loss 0.427754, acc 0.890625, learning_rate 0.000105816
2017-10-11T11:14:42.602565: step 1649, loss 0.303739, acc 0.90625, learning_rate 0.000105792
2017-10-11T11:14:42.715445: step 1650, loss 0.300154, acc 0.890625, learning_rate 0.000105768
2017-10-11T11:14:42.829994: step 1651, loss 0.209518, acc 0.9375, learning_rate 0.000105745
2017-10-11T11:14:42.946619: step 1652, loss 0.318342, acc 0.859375, learning_rate 0.000105721
2017-10-11T11:14:43.068302: step 1653, loss 0.355007, acc 0.859375, learning_rate 0.000105698
2017-10-11T11:14:43.181460: step 1654, loss 0.427811, acc 0.859375, learning_rate 0.000105675
2017-10-11T11:14:43.294624: step 1655, loss 0.236099, acc 0.9375, learning_rate 0.000105652
2017-10-11T11:14:43.413301: step 1656, loss 0.376585, acc 0.875, learning_rate 0.000105629
2017-10-11T11:14:43.531086: step 1657, loss 0.309652, acc 0.921875, learning_rate 0.000105606
2017-10-11T11:14:43.659426: step 1658, loss 0.435796, acc 0.796875, learning_rate 0.000105583
2017-10-11T11:14:43.792907: step 1659, loss 0.301502, acc 0.921875, learning_rate 0.00010556
2017-10-11T11:14:43.911868: step 1660, loss 0.456934, acc 0.875, learning_rate 0.000105537
2017-10-11T11:14:44.033887: step 1661, loss 0.436075, acc 0.84375, learning_rate 0.000105515
2017-10-11T11:14:44.144837: step 1662, loss 0.269647, acc 0.890625, learning_rate 0.000105492
2017-10-11T11:14:44.264503: step 1663, loss 0.282489, acc 0.921875, learning_rate 0.00010547
2017-10-11T11:14:44.376968: step 1664, loss 0.345371, acc 0.875, learning_rate 0.000105447
2017-10-11T11:14:44.536885: step 1665, loss 0.179542, acc 0.953125, learning_rate 0.000105425
2017-10-11T11:14:44.635451: step 1666, loss 0.371931, acc 0.823529, learning_rate 0.000105403
2017-10-11T11:14:44.727996: step 1667, loss 0.227847, acc 0.953125, learning_rate 0.000105381
2017-10-11T11:14:44.818158: step 1668, loss 0.395014, acc 0.875, learning_rate 0.000105359
2017-10-11T11:14:44.907320: step 1669, loss 0.320504, acc 0.890625, learning_rate 0.000105337
2017-10-11T11:14:44.994126: step 1670, loss 0.440869, acc 0.890625, learning_rate 0.000105315
2017-10-11T11:14:45.079474: step 1671, loss 0.280968, acc 0.875, learning_rate 0.000105294
2017-10-11T11:14:45.166167: step 1672, loss 0.252509, acc 0.9375, learning_rate 0.000105272
2017-10-11T11:14:45.263430: step 1673, loss 0.398577, acc 0.859375, learning_rate 0.000105251
2017-10-11T11:14:45.350716: step 1674, loss 0.287374, acc 0.90625, learning_rate 0.000105229
2017-10-11T11:14:45.464293: step 1675, loss 0.216963, acc 0.9375, learning_rate 0.000105208
2017-10-11T11:14:45.588631: step 1676, loss 0.303592, acc 0.890625, learning_rate 0.000105186
2017-10-11T11:14:45.705722: step 1677, loss 0.441202, acc 0.859375, learning_rate 0.000105165
2017-10-11T11:14:45.817434: step 1678, loss 0.247934, acc 0.953125, learning_rate 0.000105144
2017-10-11T11:14:45.939176: step 1679, loss 0.262839, acc 0.90625, learning_rate 0.000105123
2017-10-11T11:14:46.052677: step 1680, loss 0.325589, acc 0.890625, learning_rate 0.000105102

Evaluation:
2017-10-11T11:14:46.316914: step 1680, loss 0.253747, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1680

2017-10-11T11:14:47.010342: step 1681, loss 0.40421, acc 0.875, learning_rate 0.000105081
2017-10-11T11:14:47.128859: step 1682, loss 0.219042, acc 0.953125, learning_rate 0.000105061
2017-10-11T11:14:47.247224: step 1683, loss 0.188484, acc 0.9375, learning_rate 0.00010504
2017-10-11T11:14:47.364151: step 1684, loss 0.455059, acc 0.84375, learning_rate 0.00010502
2017-10-11T11:14:47.473699: step 1685, loss 0.317143, acc 0.90625, learning_rate 0.000104999
2017-10-11T11:14:47.588759: step 1686, loss 0.153377, acc 0.96875, learning_rate 0.000104979
2017-10-11T11:14:47.699826: step 1687, loss 0.283436, acc 0.90625, learning_rate 0.000104958
2017-10-11T11:14:47.808863: step 1688, loss 0.338042, acc 0.875, learning_rate 0.000104938
2017-10-11T11:14:47.940244: step 1689, loss 0.324303, acc 0.90625, learning_rate 0.000104918
2017-10-11T11:14:48.057052: step 1690, loss 0.379246, acc 0.875, learning_rate 0.000104898
2017-10-11T11:14:48.174337: step 1691, loss 0.166213, acc 0.96875, learning_rate 0.000104878
2017-10-11T11:14:48.288015: step 1692, loss 0.303408, acc 0.90625, learning_rate 0.000104858
2017-10-11T11:14:48.400960: step 1693, loss 0.402835, acc 0.9375, learning_rate 0.000104838
2017-10-11T11:14:48.526453: step 1694, loss 0.254984, acc 0.890625, learning_rate 0.000104818
2017-10-11T11:14:48.649210: step 1695, loss 0.304795, acc 0.90625, learning_rate 0.000104799
2017-10-11T11:14:48.770596: step 1696, loss 0.314635, acc 0.875, learning_rate 0.000104779
2017-10-11T11:14:48.887818: step 1697, loss 0.259562, acc 0.921875, learning_rate 0.00010476
2017-10-11T11:14:49.000658: step 1698, loss 0.332818, acc 0.890625, learning_rate 0.00010474
2017-10-11T11:14:49.126012: step 1699, loss 0.160561, acc 1, learning_rate 0.000104721
2017-10-11T11:14:49.242432: step 1700, loss 0.269815, acc 0.875, learning_rate 0.000104702
2017-10-11T11:14:49.356871: step 1701, loss 0.387132, acc 0.859375, learning_rate 0.000104682
2017-10-11T11:14:49.477959: step 1702, loss 0.254704, acc 0.9375, learning_rate 0.000104663
2017-10-11T11:14:49.595532: step 1703, loss 0.332923, acc 0.90625, learning_rate 0.000104644
2017-10-11T11:14:49.713702: step 1704, loss 0.380476, acc 0.875, learning_rate 0.000104625
2017-10-11T11:14:49.901334: step 1705, loss 0.322569, acc 0.90625, learning_rate 0.000104606
2017-10-11T11:14:49.984584: step 1706, loss 0.221309, acc 0.96875, learning_rate 0.000104588
2017-10-11T11:14:50.066575: step 1707, loss 0.371613, acc 0.859375, learning_rate 0.000104569
2017-10-11T11:14:50.146465: step 1708, loss 0.21816, acc 0.9375, learning_rate 0.00010455
2017-10-11T11:14:50.500268: step 1709, loss 0.297257, acc 0.90625, learning_rate 0.000104532
2017-10-11T11:14:50.600950: step 1710, loss 0.306455, acc 0.90625, learning_rate 0.000104513
2017-10-11T11:14:50.687542: step 1711, loss 0.410492, acc 0.875, learning_rate 0.000104495
2017-10-11T11:14:50.771724: step 1712, loss 0.450415, acc 0.828125, learning_rate 0.000104476
2017-10-11T11:14:50.862002: step 1713, loss 0.263626, acc 0.921875, learning_rate 0.000104458
2017-10-11T11:14:50.951466: step 1714, loss 0.257787, acc 0.9375, learning_rate 0.00010444
2017-10-11T11:14:51.039356: step 1715, loss 0.370937, acc 0.84375, learning_rate 0.000104422
2017-10-11T11:14:51.125146: step 1716, loss 0.285004, acc 0.890625, learning_rate 0.000104404
2017-10-11T11:14:51.218253: step 1717, loss 0.410749, acc 0.859375, learning_rate 0.000104386
2017-10-11T11:14:51.304956: step 1718, loss 0.373124, acc 0.890625, learning_rate 0.000104368
2017-10-11T11:14:51.424135: step 1719, loss 0.352009, acc 0.84375, learning_rate 0.00010435
2017-10-11T11:14:51.534392: step 1720, loss 0.308495, acc 0.921875, learning_rate 0.000104332

Evaluation:
2017-10-11T11:14:51.786975: step 1720, loss 0.254145, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1720

2017-10-11T11:14:52.498991: step 1721, loss 0.422071, acc 0.84375, learning_rate 0.000104315
2017-10-11T11:14:52.615582: step 1722, loss 0.372855, acc 0.859375, learning_rate 0.000104297
2017-10-11T11:14:52.728897: step 1723, loss 0.45673, acc 0.875, learning_rate 0.000104279
2017-10-11T11:14:52.838307: step 1724, loss 0.201833, acc 0.921875, learning_rate 0.000104262
2017-10-11T11:14:52.947351: step 1725, loss 0.432351, acc 0.875, learning_rate 0.000104245
2017-10-11T11:14:53.072884: step 1726, loss 0.144273, acc 0.96875, learning_rate 0.000104227
2017-10-11T11:14:53.184947: step 1727, loss 0.348319, acc 0.90625, learning_rate 0.00010421
2017-10-11T11:14:53.299160: step 1728, loss 0.285391, acc 0.890625, learning_rate 0.000104193
2017-10-11T11:14:53.405847: step 1729, loss 0.306975, acc 0.890625, learning_rate 0.000104176
2017-10-11T11:14:53.526462: step 1730, loss 0.327353, acc 0.921875, learning_rate 0.000104159
2017-10-11T11:14:53.632889: step 1731, loss 0.278953, acc 0.921875, learning_rate 0.000104142
2017-10-11T11:14:53.760869: step 1732, loss 0.35951, acc 0.90625, learning_rate 0.000104125
2017-10-11T11:14:53.866848: step 1733, loss 0.21182, acc 0.9375, learning_rate 0.000104108
2017-10-11T11:14:53.983953: step 1734, loss 0.277936, acc 0.921875, learning_rate 0.000104091
2017-10-11T11:14:54.105729: step 1735, loss 0.486196, acc 0.8125, learning_rate 0.000104074
2017-10-11T11:14:54.221217: step 1736, loss 0.238111, acc 0.921875, learning_rate 0.000104058
2017-10-11T11:14:54.342427: step 1737, loss 0.203457, acc 0.9375, learning_rate 0.000104041
2017-10-11T11:14:54.461023: step 1738, loss 0.345421, acc 0.875, learning_rate 0.000104025
2017-10-11T11:14:54.574508: step 1739, loss 0.303804, acc 0.90625, learning_rate 0.000104008
2017-10-11T11:14:54.697997: step 1740, loss 0.364215, acc 0.875, learning_rate 0.000103992
2017-10-11T11:14:54.811739: step 1741, loss 0.229142, acc 0.9375, learning_rate 0.000103976
2017-10-11T11:14:54.927666: step 1742, loss 0.383136, acc 0.90625, learning_rate 0.000103959
2017-10-11T11:14:55.032313: step 1743, loss 0.320777, acc 0.9375, learning_rate 0.000103943
2017-10-11T11:14:55.146233: step 1744, loss 0.402831, acc 0.828125, learning_rate 0.000103927
2017-10-11T11:14:55.268851: step 1745, loss 0.220674, acc 0.90625, learning_rate 0.000103911
2017-10-11T11:14:55.383211: step 1746, loss 0.483661, acc 0.84375, learning_rate 0.000103895
2017-10-11T11:14:55.496334: step 1747, loss 0.279866, acc 0.90625, learning_rate 0.000103879
2017-10-11T11:14:55.899810: step 1748, loss 0.293476, acc 0.921875, learning_rate 0.000103863
2017-10-11T11:14:56.036689: step 1749, loss 0.248623, acc 0.9375, learning_rate 0.000103848
2017-10-11T11:14:56.127752: step 1750, loss 0.284016, acc 0.90625, learning_rate 0.000103832
2017-10-11T11:14:56.214560: step 1751, loss 0.387516, acc 0.84375, learning_rate 0.000103816
2017-10-11T11:14:56.303100: step 1752, loss 0.289357, acc 0.9375, learning_rate 0.000103801
2017-10-11T11:14:56.400325: step 1753, loss 0.268344, acc 0.90625, learning_rate 0.000103785
2017-10-11T11:14:56.487129: step 1754, loss 0.339501, acc 0.875, learning_rate 0.00010377
2017-10-11T11:14:56.574742: step 1755, loss 0.316676, acc 0.875, learning_rate 0.000103754
2017-10-11T11:14:56.669414: step 1756, loss 0.299557, acc 0.84375, learning_rate 0.000103739
2017-10-11T11:14:56.778355: step 1757, loss 0.355588, acc 0.875, learning_rate 0.000103724
2017-10-11T11:14:56.890751: step 1758, loss 0.392956, acc 0.796875, learning_rate 0.000103709
2017-10-11T11:14:57.014135: step 1759, loss 0.381209, acc 0.859375, learning_rate 0.000103694
2017-10-11T11:14:57.130518: step 1760, loss 0.217972, acc 0.9375, learning_rate 0.000103678

Evaluation:
2017-10-11T11:14:57.368601: step 1760, loss 0.251818, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1760

2017-10-11T11:14:58.080153: step 1761, loss 0.337389, acc 0.90625, learning_rate 0.000103663
2017-10-11T11:14:58.192171: step 1762, loss 0.245176, acc 0.890625, learning_rate 0.000103648
2017-10-11T11:14:58.301005: step 1763, loss 0.388852, acc 0.859375, learning_rate 0.000103634
2017-10-11T11:14:58.405763: step 1764, loss 0.295172, acc 0.862745, learning_rate 0.000103619
2017-10-11T11:14:58.529527: step 1765, loss 0.367422, acc 0.9375, learning_rate 0.000103604
2017-10-11T11:14:58.636858: step 1766, loss 0.382795, acc 0.890625, learning_rate 0.000103589
2017-10-11T11:14:58.742498: step 1767, loss 0.35644, acc 0.84375, learning_rate 0.000103575
2017-10-11T11:14:58.855257: step 1768, loss 0.243252, acc 0.921875, learning_rate 0.00010356
2017-10-11T11:14:58.969547: step 1769, loss 0.2841, acc 0.890625, learning_rate 0.000103545
2017-10-11T11:14:59.088894: step 1770, loss 0.218139, acc 0.9375, learning_rate 0.000103531
2017-10-11T11:14:59.188681: step 1771, loss 0.475029, acc 0.84375, learning_rate 0.000103517
2017-10-11T11:14:59.308870: step 1772, loss 0.369769, acc 0.84375, learning_rate 0.000103502
2017-10-11T11:14:59.420888: step 1773, loss 0.239113, acc 0.921875, learning_rate 0.000103488
2017-10-11T11:14:59.526001: step 1774, loss 0.270127, acc 0.921875, learning_rate 0.000103474
2017-10-11T11:14:59.652088: step 1775, loss 0.272842, acc 0.9375, learning_rate 0.00010346
2017-10-11T11:14:59.756857: step 1776, loss 0.178044, acc 0.96875, learning_rate 0.000103445
2017-10-11T11:14:59.866774: step 1777, loss 0.251557, acc 0.9375, learning_rate 0.000103431
2017-10-11T11:14:59.980635: step 1778, loss 0.513409, acc 0.84375, learning_rate 0.000103417
2017-10-11T11:15:00.085513: step 1779, loss 0.279543, acc 0.921875, learning_rate 0.000103403
2017-10-11T11:15:00.200060: step 1780, loss 0.369801, acc 0.859375, learning_rate 0.00010339
2017-10-11T11:15:00.307835: step 1781, loss 0.227669, acc 0.953125, learning_rate 0.000103376
2017-10-11T11:15:00.415088: step 1782, loss 0.344279, acc 0.84375, learning_rate 0.000103362
2017-10-11T11:15:00.531623: step 1783, loss 0.336279, acc 0.890625, learning_rate 0.000103348
2017-10-11T11:15:00.644985: step 1784, loss 0.586462, acc 0.828125, learning_rate 0.000103335
2017-10-11T11:15:00.764861: step 1785, loss 0.266416, acc 0.90625, learning_rate 0.000103321
2017-10-11T11:15:00.878606: step 1786, loss 0.434133, acc 0.875, learning_rate 0.000103307
2017-10-11T11:15:00.996885: step 1787, loss 0.373987, acc 0.921875, learning_rate 0.000103294
2017-10-11T11:15:01.182242: step 1788, loss 0.306715, acc 0.890625, learning_rate 0.00010328
2017-10-11T11:15:01.272227: step 1789, loss 0.240291, acc 0.9375, learning_rate 0.000103267
2017-10-11T11:15:01.356960: step 1790, loss 0.205929, acc 0.9375, learning_rate 0.000103254
2017-10-11T11:15:01.444885: step 1791, loss 0.372543, acc 0.84375, learning_rate 0.00010324
2017-10-11T11:15:01.533064: step 1792, loss 0.192418, acc 0.9375, learning_rate 0.000103227
2017-10-11T11:15:01.620572: step 1793, loss 0.28785, acc 0.875, learning_rate 0.000103214
2017-10-11T11:15:01.707226: step 1794, loss 0.432193, acc 0.84375, learning_rate 0.000103201
2017-10-11T11:15:01.794614: step 1795, loss 0.428132, acc 0.890625, learning_rate 0.000103188
2017-10-11T11:15:01.881874: step 1796, loss 0.358991, acc 0.921875, learning_rate 0.000103175
2017-10-11T11:15:01.971151: step 1797, loss 0.393894, acc 0.859375, learning_rate 0.000103162
2017-10-11T11:15:02.080198: step 1798, loss 0.377695, acc 0.921875, learning_rate 0.000103149
2017-10-11T11:15:02.189193: step 1799, loss 0.377538, acc 0.875, learning_rate 0.000103136
2017-10-11T11:15:02.303270: step 1800, loss 0.272596, acc 0.921875, learning_rate 0.000103123

Evaluation:
2017-10-11T11:15:02.552800: step 1800, loss 0.252358, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1800

2017-10-11T11:15:03.285711: step 1801, loss 0.47112, acc 0.859375, learning_rate 0.000103111
2017-10-11T11:15:03.398093: step 1802, loss 0.41727, acc 0.875, learning_rate 0.000103098
2017-10-11T11:15:03.513160: step 1803, loss 0.453358, acc 0.828125, learning_rate 0.000103085
2017-10-11T11:15:03.632594: step 1804, loss 0.284787, acc 0.9375, learning_rate 0.000103073
2017-10-11T11:15:03.746866: step 1805, loss 0.283315, acc 0.921875, learning_rate 0.00010306
2017-10-11T11:15:03.853657: step 1806, loss 0.327282, acc 0.875, learning_rate 0.000103048
2017-10-11T11:15:03.960475: step 1807, loss 0.361267, acc 0.859375, learning_rate 0.000103035
2017-10-11T11:15:04.072207: step 1808, loss 0.239003, acc 0.90625, learning_rate 0.000103023
2017-10-11T11:15:04.191269: step 1809, loss 0.240969, acc 0.953125, learning_rate 0.00010301
2017-10-11T11:15:04.307630: step 1810, loss 0.314865, acc 0.9375, learning_rate 0.000102998
2017-10-11T11:15:04.426039: step 1811, loss 0.313847, acc 0.890625, learning_rate 0.000102986
2017-10-11T11:15:04.548553: step 1812, loss 0.415171, acc 0.828125, learning_rate 0.000102974
2017-10-11T11:15:04.664774: step 1813, loss 0.311734, acc 0.875, learning_rate 0.000102962
2017-10-11T11:15:04.778177: step 1814, loss 0.326562, acc 0.890625, learning_rate 0.000102949
2017-10-11T11:15:04.889711: step 1815, loss 0.307133, acc 0.859375, learning_rate 0.000102937
2017-10-11T11:15:05.004672: step 1816, loss 0.387646, acc 0.90625, learning_rate 0.000102925
2017-10-11T11:15:05.120882: step 1817, loss 0.258068, acc 0.90625, learning_rate 0.000102913
2017-10-11T11:15:05.228907: step 1818, loss 0.344332, acc 0.90625, learning_rate 0.000102902
2017-10-11T11:15:05.331975: step 1819, loss 0.506583, acc 0.828125, learning_rate 0.00010289
2017-10-11T11:15:05.445862: step 1820, loss 0.329001, acc 0.90625, learning_rate 0.000102878
2017-10-11T11:15:05.556284: step 1821, loss 0.413983, acc 0.84375, learning_rate 0.000102866
2017-10-11T11:15:05.677545: step 1822, loss 0.267503, acc 0.921875, learning_rate 0.000102855
2017-10-11T11:15:05.791626: step 1823, loss 0.285393, acc 0.9375, learning_rate 0.000102843
2017-10-11T11:15:05.908021: step 1824, loss 0.358876, acc 0.890625, learning_rate 0.000102831
2017-10-11T11:15:06.028830: step 1825, loss 0.407079, acc 0.84375, learning_rate 0.00010282
2017-10-11T11:15:06.139608: step 1826, loss 0.377394, acc 0.890625, learning_rate 0.000102808
2017-10-11T11:15:06.250803: step 1827, loss 0.383868, acc 0.859375, learning_rate 0.000102797
2017-10-11T11:15:06.444560: step 1828, loss 0.315846, acc 0.859375, learning_rate 0.000102785
2017-10-11T11:15:06.533998: step 1829, loss 0.237558, acc 0.9375, learning_rate 0.000102774
2017-10-11T11:15:06.622633: step 1830, loss 0.242563, acc 0.953125, learning_rate 0.000102763
2017-10-11T11:15:06.712487: step 1831, loss 0.231223, acc 0.890625, learning_rate 0.000102751
2017-10-11T11:15:06.800699: step 1832, loss 0.306451, acc 0.921875, learning_rate 0.00010274
2017-10-11T11:15:06.886835: step 1833, loss 0.368295, acc 0.890625, learning_rate 0.000102729
2017-10-11T11:15:06.970088: step 1834, loss 0.184613, acc 0.953125, learning_rate 0.000102718
2017-10-11T11:15:07.056154: step 1835, loss 0.355147, acc 0.875, learning_rate 0.000102707
2017-10-11T11:15:07.143492: step 1836, loss 0.52761, acc 0.796875, learning_rate 0.000102696
2017-10-11T11:15:07.228206: step 1837, loss 0.445095, acc 0.90625, learning_rate 0.000102685
2017-10-11T11:15:07.320816: step 1838, loss 0.354615, acc 0.84375, learning_rate 0.000102674
2017-10-11T11:15:07.436780: step 1839, loss 0.272986, acc 0.90625, learning_rate 0.000102663
2017-10-11T11:15:07.552249: step 1840, loss 0.310754, acc 0.921875, learning_rate 0.000102652

Evaluation:
2017-10-11T11:15:07.789755: step 1840, loss 0.252048, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1840

2017-10-11T11:15:08.520862: step 1841, loss 0.385885, acc 0.875, learning_rate 0.000102641
2017-10-11T11:15:08.632772: step 1842, loss 0.385188, acc 0.90625, learning_rate 0.00010263
2017-10-11T11:15:08.741102: step 1843, loss 0.291785, acc 0.90625, learning_rate 0.00010262
2017-10-11T11:15:08.856506: step 1844, loss 0.427998, acc 0.921875, learning_rate 0.000102609
2017-10-11T11:15:08.966015: step 1845, loss 0.33875, acc 0.875, learning_rate 0.000102598
2017-10-11T11:15:09.081062: step 1846, loss 0.342248, acc 0.890625, learning_rate 0.000102588
2017-10-11T11:15:09.189437: step 1847, loss 0.269725, acc 0.921875, learning_rate 0.000102577
2017-10-11T11:15:09.305886: step 1848, loss 0.186435, acc 0.953125, learning_rate 0.000102567
2017-10-11T11:15:09.423763: step 1849, loss 0.244668, acc 0.921875, learning_rate 0.000102556
2017-10-11T11:15:09.549363: step 1850, loss 0.198501, acc 0.9375, learning_rate 0.000102546
2017-10-11T11:15:09.663332: step 1851, loss 0.29844, acc 0.90625, learning_rate 0.000102535
2017-10-11T11:15:09.775996: step 1852, loss 0.427302, acc 0.875, learning_rate 0.000102525
2017-10-11T11:15:09.892966: step 1853, loss 0.314194, acc 0.890625, learning_rate 0.000102515
2017-10-11T11:15:10.012751: step 1854, loss 0.396877, acc 0.859375, learning_rate 0.000102504
2017-10-11T11:15:10.127180: step 1855, loss 0.313911, acc 0.875, learning_rate 0.000102494
2017-10-11T11:15:10.238172: step 1856, loss 0.340235, acc 0.890625, learning_rate 0.000102484
2017-10-11T11:15:10.357787: step 1857, loss 0.346389, acc 0.875, learning_rate 0.000102474
2017-10-11T11:15:10.464533: step 1858, loss 0.323914, acc 0.875, learning_rate 0.000102464
2017-10-11T11:15:10.573101: step 1859, loss 0.303197, acc 0.9375, learning_rate 0.000102454
2017-10-11T11:15:10.690872: step 1860, loss 0.397907, acc 0.890625, learning_rate 0.000102444
2017-10-11T11:15:10.801630: step 1861, loss 0.206218, acc 0.953125, learning_rate 0.000102434
2017-10-11T11:15:10.896761: step 1862, loss 0.386362, acc 0.862745, learning_rate 0.000102424
2017-10-11T11:15:11.017191: step 1863, loss 0.212752, acc 0.9375, learning_rate 0.000102414
2017-10-11T11:15:11.133636: step 1864, loss 0.344444, acc 0.921875, learning_rate 0.000102404
2017-10-11T11:15:11.251180: step 1865, loss 0.254321, acc 0.90625, learning_rate 0.000102394
2017-10-11T11:15:11.379129: step 1866, loss 0.290896, acc 0.90625, learning_rate 0.000102384
2017-10-11T11:15:11.492058: step 1867, loss 0.31044, acc 0.875, learning_rate 0.000102375
2017-10-11T11:15:11.615993: step 1868, loss 0.21945, acc 0.96875, learning_rate 0.000102365
2017-10-11T11:15:11.782523: step 1869, loss 0.304478, acc 0.921875, learning_rate 0.000102355
2017-10-11T11:15:11.886871: step 1870, loss 0.241171, acc 0.9375, learning_rate 0.000102346
2017-10-11T11:15:11.987844: step 1871, loss 0.388993, acc 0.859375, learning_rate 0.000102336
2017-10-11T11:15:12.087574: step 1872, loss 0.383244, acc 0.859375, learning_rate 0.000102327
2017-10-11T11:15:12.185016: step 1873, loss 0.208447, acc 0.96875, learning_rate 0.000102317
2017-10-11T11:15:12.283722: step 1874, loss 0.513679, acc 0.828125, learning_rate 0.000102308
2017-10-11T11:15:12.385332: step 1875, loss 0.281204, acc 0.90625, learning_rate 0.000102298
2017-10-11T11:15:12.507838: step 1876, loss 0.328223, acc 0.90625, learning_rate 0.000102289
2017-10-11T11:15:12.628002: step 1877, loss 0.281307, acc 0.921875, learning_rate 0.000102279
2017-10-11T11:15:12.745607: step 1878, loss 0.359146, acc 0.859375, learning_rate 0.00010227
2017-10-11T11:15:12.860968: step 1879, loss 0.276961, acc 0.890625, learning_rate 0.000102261
2017-10-11T11:15:12.974455: step 1880, loss 0.231683, acc 0.953125, learning_rate 0.000102252

Evaluation:
2017-10-11T11:15:13.235775: step 1880, loss 0.252218, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1880

2017-10-11T11:15:13.972914: step 1881, loss 0.251692, acc 0.90625, learning_rate 0.000102242
2017-10-11T11:15:14.096213: step 1882, loss 0.3627, acc 0.875, learning_rate 0.000102233
2017-10-11T11:15:14.202747: step 1883, loss 0.321996, acc 0.875, learning_rate 0.000102224
2017-10-11T11:15:14.311175: step 1884, loss 0.322979, acc 0.90625, learning_rate 0.000102215
2017-10-11T11:15:14.423422: step 1885, loss 0.234599, acc 0.921875, learning_rate 0.000102206
2017-10-11T11:15:14.549845: step 1886, loss 0.312672, acc 0.859375, learning_rate 0.000102197
2017-10-11T11:15:14.662724: step 1887, loss 0.360232, acc 0.890625, learning_rate 0.000102188
2017-10-11T11:15:14.781276: step 1888, loss 0.432296, acc 0.8125, learning_rate 0.000102179
2017-10-11T11:15:14.897276: step 1889, loss 0.369125, acc 0.90625, learning_rate 0.00010217
2017-10-11T11:15:15.022808: step 1890, loss 0.265115, acc 0.921875, learning_rate 0.000102161
2017-10-11T11:15:15.136886: step 1891, loss 0.37401, acc 0.875, learning_rate 0.000102153
2017-10-11T11:15:15.252230: step 1892, loss 0.294089, acc 0.921875, learning_rate 0.000102144
2017-10-11T11:15:15.349595: step 1893, loss 0.362599, acc 0.875, learning_rate 0.000102135
2017-10-11T11:15:15.459036: step 1894, loss 0.284069, acc 0.90625, learning_rate 0.000102126
2017-10-11T11:15:15.579979: step 1895, loss 0.275244, acc 0.90625, learning_rate 0.000102118
2017-10-11T11:15:15.691338: step 1896, loss 0.232211, acc 0.953125, learning_rate 0.000102109
2017-10-11T11:15:15.802594: step 1897, loss 0.31853, acc 0.875, learning_rate 0.0001021
2017-10-11T11:15:15.924904: step 1898, loss 0.335265, acc 0.859375, learning_rate 0.000102092
2017-10-11T11:15:16.052131: step 1899, loss 0.487399, acc 0.84375, learning_rate 0.000102083
2017-10-11T11:15:16.180085: step 1900, loss 0.399812, acc 0.828125, learning_rate 0.000102075
2017-10-11T11:15:16.298601: step 1901, loss 0.500761, acc 0.796875, learning_rate 0.000102066
2017-10-11T11:15:16.404932: step 1902, loss 0.194905, acc 0.9375, learning_rate 0.000102058
2017-10-11T11:15:16.522553: step 1903, loss 0.321902, acc 0.84375, learning_rate 0.00010205
2017-10-11T11:15:16.638603: step 1904, loss 0.221059, acc 0.890625, learning_rate 0.000102041
2017-10-11T11:15:16.785004: step 1905, loss 0.456007, acc 0.828125, learning_rate 0.000102033
2017-10-11T11:15:16.909735: step 1906, loss 0.168797, acc 0.96875, learning_rate 0.000102025
2017-10-11T11:15:16.999554: step 1907, loss 0.177314, acc 0.9375, learning_rate 0.000102016
2017-10-11T11:15:17.086861: step 1908, loss 0.302458, acc 0.875, learning_rate 0.000102008
2017-10-11T11:15:17.186531: step 1909, loss 0.249411, acc 0.9375, learning_rate 0.000102
2017-10-11T11:15:17.272826: step 1910, loss 0.268735, acc 0.953125, learning_rate 0.000101992
2017-10-11T11:15:17.360190: step 1911, loss 0.350192, acc 0.875, learning_rate 0.000101984
2017-10-11T11:15:17.448408: step 1912, loss 0.225428, acc 0.953125, learning_rate 0.000101975
2017-10-11T11:15:17.535245: step 1913, loss 0.387521, acc 0.875, learning_rate 0.000101967
2017-10-11T11:15:17.619631: step 1914, loss 0.16943, acc 0.953125, learning_rate 0.000101959
2017-10-11T11:15:17.720912: step 1915, loss 0.378802, acc 0.84375, learning_rate 0.000101951
2017-10-11T11:15:17.838442: step 1916, loss 0.383641, acc 0.890625, learning_rate 0.000101943
2017-10-11T11:15:17.946552: step 1917, loss 0.303838, acc 0.890625, learning_rate 0.000101935
2017-10-11T11:15:18.069117: step 1918, loss 0.35413, acc 0.828125, learning_rate 0.000101928
2017-10-11T11:15:18.184986: step 1919, loss 0.23393, acc 0.921875, learning_rate 0.00010192
2017-10-11T11:15:18.299965: step 1920, loss 0.523563, acc 0.8125, learning_rate 0.000101912

Evaluation:
2017-10-11T11:15:18.528367: step 1920, loss 0.251626, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1920

2017-10-11T11:15:19.271486: step 1921, loss 0.270381, acc 0.921875, learning_rate 0.000101904
2017-10-11T11:15:19.387986: step 1922, loss 0.284667, acc 0.890625, learning_rate 0.000101896
2017-10-11T11:15:19.511280: step 1923, loss 0.421852, acc 0.84375, learning_rate 0.000101889
2017-10-11T11:15:19.625635: step 1924, loss 0.328086, acc 0.90625, learning_rate 0.000101881
2017-10-11T11:15:19.750910: step 1925, loss 0.329476, acc 0.90625, learning_rate 0.000101873
2017-10-11T11:15:19.871292: step 1926, loss 0.507701, acc 0.828125, learning_rate 0.000101865
2017-10-11T11:15:19.995563: step 1927, loss 0.387276, acc 0.859375, learning_rate 0.000101858
2017-10-11T11:15:20.107567: step 1928, loss 0.268208, acc 0.875, learning_rate 0.00010185
2017-10-11T11:15:20.227294: step 1929, loss 0.301873, acc 0.90625, learning_rate 0.000101843
2017-10-11T11:15:20.346543: step 1930, loss 0.251942, acc 0.84375, learning_rate 0.000101835
2017-10-11T11:15:20.472863: step 1931, loss 0.22669, acc 0.953125, learning_rate 0.000101828
2017-10-11T11:15:20.599224: step 1932, loss 0.271336, acc 0.890625, learning_rate 0.00010182
2017-10-11T11:15:20.704715: step 1933, loss 0.320341, acc 0.890625, learning_rate 0.000101813
2017-10-11T11:15:20.815800: step 1934, loss 0.300771, acc 0.890625, learning_rate 0.000101805
2017-10-11T11:15:20.931002: step 1935, loss 0.377988, acc 0.875, learning_rate 0.000101798
2017-10-11T11:15:21.040931: step 1936, loss 0.326484, acc 0.890625, learning_rate 0.000101791
2017-10-11T11:15:21.162818: step 1937, loss 0.311908, acc 0.890625, learning_rate 0.000101783
2017-10-11T11:15:21.273040: step 1938, loss 0.201169, acc 0.953125, learning_rate 0.000101776
2017-10-11T11:15:21.380630: step 1939, loss 0.382595, acc 0.859375, learning_rate 0.000101769
2017-10-11T11:15:21.496658: step 1940, loss 0.302797, acc 0.875, learning_rate 0.000101762
2017-10-11T11:15:21.611179: step 1941, loss 0.421726, acc 0.875, learning_rate 0.000101754
2017-10-11T11:15:21.723305: step 1942, loss 0.342237, acc 0.84375, learning_rate 0.000101747
2017-10-11T11:15:21.824871: step 1943, loss 0.446491, acc 0.859375, learning_rate 0.00010174
2017-10-11T11:15:21.941751: step 1944, loss 0.264838, acc 0.921875, learning_rate 0.000101733
2017-10-11T11:15:22.045521: step 1945, loss 0.247796, acc 0.921875, learning_rate 0.000101726
2017-10-11T11:15:22.205371: step 1946, loss 0.415194, acc 0.84375, learning_rate 0.000101719
2017-10-11T11:15:22.292735: step 1947, loss 0.506382, acc 0.828125, learning_rate 0.000101712
2017-10-11T11:15:22.379126: step 1948, loss 0.470201, acc 0.8125, learning_rate 0.000101705
2017-10-11T11:15:22.467278: step 1949, loss 0.313071, acc 0.875, learning_rate 0.000101698
2017-10-11T11:15:22.552886: step 1950, loss 0.329989, acc 0.875, learning_rate 0.000101691
2017-10-11T11:15:22.641762: step 1951, loss 0.255672, acc 0.90625, learning_rate 0.000101684
2017-10-11T11:15:22.730602: step 1952, loss 0.338647, acc 0.875, learning_rate 0.000101677
2017-10-11T11:15:22.817872: step 1953, loss 0.420811, acc 0.90625, learning_rate 0.00010167
2017-10-11T11:15:22.906755: step 1954, loss 0.302727, acc 0.875, learning_rate 0.000101664
2017-10-11T11:15:23.004022: step 1955, loss 0.404957, acc 0.859375, learning_rate 0.000101657
2017-10-11T11:15:23.108879: step 1956, loss 0.392867, acc 0.875, learning_rate 0.00010165
2017-10-11T11:15:23.220859: step 1957, loss 0.275245, acc 0.890625, learning_rate 0.000101643
2017-10-11T11:15:23.337133: step 1958, loss 0.403514, acc 0.875, learning_rate 0.000101637
2017-10-11T11:15:23.450692: step 1959, loss 0.317014, acc 0.921875, learning_rate 0.00010163
2017-10-11T11:15:23.539081: step 1960, loss 0.170531, acc 0.941176, learning_rate 0.000101623

Evaluation:
2017-10-11T11:15:23.788357: step 1960, loss 0.250834, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-1960

2017-10-11T11:15:24.387796: step 1961, loss 0.136597, acc 0.96875, learning_rate 0.000101617
2017-10-11T11:15:24.509044: step 1962, loss 0.274881, acc 0.890625, learning_rate 0.00010161
2017-10-11T11:15:24.628560: step 1963, loss 0.225714, acc 0.984375, learning_rate 0.000101604
2017-10-11T11:15:24.745098: step 1964, loss 0.515273, acc 0.84375, learning_rate 0.000101597
2017-10-11T11:15:24.857178: step 1965, loss 0.337872, acc 0.859375, learning_rate 0.00010159
2017-10-11T11:15:24.970301: step 1966, loss 0.417232, acc 0.875, learning_rate 0.000101584
2017-10-11T11:15:25.080744: step 1967, loss 0.258693, acc 0.953125, learning_rate 0.000101577
2017-10-11T11:15:25.188813: step 1968, loss 0.293164, acc 0.921875, learning_rate 0.000101571
2017-10-11T11:15:25.306193: step 1969, loss 0.407372, acc 0.90625, learning_rate 0.000101565
2017-10-11T11:15:25.423333: step 1970, loss 0.205146, acc 0.9375, learning_rate 0.000101558
2017-10-11T11:15:25.537251: step 1971, loss 0.307215, acc 0.875, learning_rate 0.000101552
2017-10-11T11:15:25.656036: step 1972, loss 0.363164, acc 0.90625, learning_rate 0.000101546
2017-10-11T11:15:25.769651: step 1973, loss 0.530932, acc 0.8125, learning_rate 0.000101539
2017-10-11T11:15:25.881026: step 1974, loss 0.144701, acc 0.96875, learning_rate 0.000101533
2017-10-11T11:15:25.999960: step 1975, loss 0.477957, acc 0.828125, learning_rate 0.000101527
2017-10-11T11:15:26.117353: step 1976, loss 0.26049, acc 0.953125, learning_rate 0.00010152
2017-10-11T11:15:26.228947: step 1977, loss 0.265876, acc 0.921875, learning_rate 0.000101514
2017-10-11T11:15:26.346189: step 1978, loss 0.13574, acc 0.96875, learning_rate 0.000101508
2017-10-11T11:15:26.468437: step 1979, loss 0.304537, acc 0.921875, learning_rate 0.000101502
2017-10-11T11:15:26.584051: step 1980, loss 0.330566, acc 0.890625, learning_rate 0.000101496
2017-10-11T11:15:26.704301: step 1981, loss 0.246909, acc 0.890625, learning_rate 0.00010149
2017-10-11T11:15:26.818450: step 1982, loss 0.327583, acc 0.890625, learning_rate 0.000101484
2017-10-11T11:15:26.934837: step 1983, loss 0.312402, acc 0.890625, learning_rate 0.000101478
2017-10-11T11:15:27.052405: step 1984, loss 0.388606, acc 0.859375, learning_rate 0.000101472
2017-10-11T11:15:27.176812: step 1985, loss 0.311147, acc 0.890625, learning_rate 0.000101466
2017-10-11T11:15:27.292867: step 1986, loss 0.334992, acc 0.890625, learning_rate 0.00010146
2017-10-11T11:15:27.395168: step 1987, loss 0.407094, acc 0.90625, learning_rate 0.000101454
2017-10-11T11:15:27.586491: step 1988, loss 0.384422, acc 0.90625, learning_rate 0.000101448
2017-10-11T11:15:27.666960: step 1989, loss 0.322634, acc 0.859375, learning_rate 0.000101442
2017-10-11T11:15:27.750569: step 1990, loss 0.248592, acc 0.9375, learning_rate 0.000101436
2017-10-11T11:15:27.830678: step 1991, loss 0.169053, acc 0.953125, learning_rate 0.00010143
2017-10-11T11:15:27.917146: step 1992, loss 0.286305, acc 0.90625, learning_rate 0.000101424
2017-10-11T11:15:28.006585: step 1993, loss 0.326144, acc 0.921875, learning_rate 0.000101418
2017-10-11T11:15:28.097987: step 1994, loss 0.369142, acc 0.875, learning_rate 0.000101413
2017-10-11T11:15:28.187872: step 1995, loss 0.200544, acc 0.921875, learning_rate 0.000101407
2017-10-11T11:15:28.277491: step 1996, loss 0.291533, acc 0.90625, learning_rate 0.000101401
2017-10-11T11:15:28.363267: step 1997, loss 0.410316, acc 0.859375, learning_rate 0.000101395
2017-10-11T11:15:28.452406: step 1998, loss 0.198673, acc 0.96875, learning_rate 0.00010139
2017-10-11T11:15:28.551432: step 1999, loss 0.392507, acc 0.859375, learning_rate 0.000101384
2017-10-11T11:15:28.643842: step 2000, loss 0.317677, acc 0.90625, learning_rate 0.000101378

Evaluation:
2017-10-11T11:15:28.854745: step 2000, loss 0.250069, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2000

2017-10-11T11:15:29.576987: step 2001, loss 0.212748, acc 0.9375, learning_rate 0.000101373
2017-10-11T11:15:29.692700: step 2002, loss 0.265673, acc 0.875, learning_rate 0.000101367
2017-10-11T11:15:29.812486: step 2003, loss 0.339851, acc 0.90625, learning_rate 0.000101362
2017-10-11T11:15:29.922312: step 2004, loss 0.408625, acc 0.875, learning_rate 0.000101356
2017-10-11T11:15:30.037529: step 2005, loss 0.294162, acc 0.921875, learning_rate 0.00010135
2017-10-11T11:15:30.152863: step 2006, loss 0.326629, acc 0.875, learning_rate 0.000101345
2017-10-11T11:15:30.265597: step 2007, loss 0.209764, acc 0.96875, learning_rate 0.000101339
2017-10-11T11:15:30.376317: step 2008, loss 0.5538, acc 0.75, learning_rate 0.000101334
2017-10-11T11:15:30.494397: step 2009, loss 0.397702, acc 0.875, learning_rate 0.000101328
2017-10-11T11:15:30.607884: step 2010, loss 0.629816, acc 0.78125, learning_rate 0.000101323
2017-10-11T11:15:30.711145: step 2011, loss 0.419198, acc 0.859375, learning_rate 0.000101318
2017-10-11T11:15:30.820796: step 2012, loss 0.534703, acc 0.84375, learning_rate 0.000101312
2017-10-11T11:15:30.936869: step 2013, loss 0.485665, acc 0.859375, learning_rate 0.000101307
2017-10-11T11:15:31.065165: step 2014, loss 0.269649, acc 0.953125, learning_rate 0.000101302
2017-10-11T11:15:31.192192: step 2015, loss 0.502828, acc 0.875, learning_rate 0.000101296
2017-10-11T11:15:31.321670: step 2016, loss 0.315805, acc 0.890625, learning_rate 0.000101291
2017-10-11T11:15:31.445626: step 2017, loss 0.405545, acc 0.875, learning_rate 0.000101286
2017-10-11T11:15:31.546379: step 2018, loss 0.291802, acc 0.875, learning_rate 0.00010128
2017-10-11T11:15:31.669022: step 2019, loss 0.279897, acc 0.90625, learning_rate 0.000101275
2017-10-11T11:15:31.788628: step 2020, loss 0.416545, acc 0.90625, learning_rate 0.00010127
2017-10-11T11:15:31.908175: step 2021, loss 0.305131, acc 0.890625, learning_rate 0.000101265
2017-10-11T11:15:32.021180: step 2022, loss 0.392277, acc 0.875, learning_rate 0.00010126
2017-10-11T11:15:32.134971: step 2023, loss 0.193468, acc 0.953125, learning_rate 0.000101255
2017-10-11T11:15:32.251524: step 2024, loss 0.201737, acc 0.921875, learning_rate 0.000101249
2017-10-11T11:15:32.361943: step 2025, loss 0.295613, acc 0.890625, learning_rate 0.000101244
2017-10-11T11:15:32.480212: step 2026, loss 0.291217, acc 0.90625, learning_rate 0.000101239
2017-10-11T11:15:32.593348: step 2027, loss 0.297726, acc 0.875, learning_rate 0.000101234
2017-10-11T11:15:32.707975: step 2028, loss 0.310535, acc 0.90625, learning_rate 0.000101229
2017-10-11T11:15:32.815370: step 2029, loss 0.318977, acc 0.921875, learning_rate 0.000101224
2017-10-11T11:15:32.928218: step 2030, loss 0.403529, acc 0.84375, learning_rate 0.000101219
2017-10-11T11:15:33.029416: step 2031, loss 0.252802, acc 0.890625, learning_rate 0.000101214
2017-10-11T11:15:33.180921: step 2032, loss 0.204687, acc 0.96875, learning_rate 0.000101209
2017-10-11T11:15:33.308028: step 2033, loss 0.225798, acc 0.9375, learning_rate 0.000101204
2017-10-11T11:15:33.398361: step 2034, loss 0.335876, acc 0.90625, learning_rate 0.000101199
2017-10-11T11:15:33.486690: step 2035, loss 0.271363, acc 0.90625, learning_rate 0.000101194
2017-10-11T11:15:33.572154: step 2036, loss 0.311739, acc 0.90625, learning_rate 0.00010119
2017-10-11T11:15:33.654666: step 2037, loss 0.248764, acc 0.9375, learning_rate 0.000101185
2017-10-11T11:15:33.742833: step 2038, loss 0.309659, acc 0.90625, learning_rate 0.00010118
2017-10-11T11:15:33.832754: step 2039, loss 0.373091, acc 0.828125, learning_rate 0.000101175
2017-10-11T11:15:33.920229: step 2040, loss 0.323274, acc 0.90625, learning_rate 0.00010117

Evaluation:
2017-10-11T11:15:34.161806: step 2040, loss 0.249677, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2040

2017-10-11T11:15:34.896805: step 2041, loss 0.370336, acc 0.84375, learning_rate 0.000101166
2017-10-11T11:15:35.010866: step 2042, loss 0.26647, acc 0.9375, learning_rate 0.000101161
2017-10-11T11:15:35.124875: step 2043, loss 0.294014, acc 0.90625, learning_rate 0.000101156
2017-10-11T11:15:35.234788: step 2044, loss 0.310357, acc 0.953125, learning_rate 0.000101151
2017-10-11T11:15:35.347633: step 2045, loss 0.262463, acc 0.953125, learning_rate 0.000101147
2017-10-11T11:15:35.464872: step 2046, loss 0.327144, acc 0.890625, learning_rate 0.000101142
2017-10-11T11:15:35.584965: step 2047, loss 0.461567, acc 0.828125, learning_rate 0.000101137
2017-10-11T11:15:35.693137: step 2048, loss 0.242164, acc 0.921875, learning_rate 0.000101133
2017-10-11T11:15:35.808240: step 2049, loss 0.384157, acc 0.84375, learning_rate 0.000101128
2017-10-11T11:15:35.923285: step 2050, loss 0.293859, acc 0.90625, learning_rate 0.000101123
2017-10-11T11:15:36.033273: step 2051, loss 0.208108, acc 0.953125, learning_rate 0.000101119
2017-10-11T11:15:36.143855: step 2052, loss 0.226117, acc 0.9375, learning_rate 0.000101114
2017-10-11T11:15:36.257240: step 2053, loss 0.217639, acc 0.921875, learning_rate 0.00010111
2017-10-11T11:15:36.366710: step 2054, loss 0.281876, acc 0.875, learning_rate 0.000101105
2017-10-11T11:15:36.483726: step 2055, loss 0.414838, acc 0.859375, learning_rate 0.000101101
2017-10-11T11:15:36.602046: step 2056, loss 0.322883, acc 0.875, learning_rate 0.000101096
2017-10-11T11:15:36.725988: step 2057, loss 0.219343, acc 0.921875, learning_rate 0.000101092
2017-10-11T11:15:36.823299: step 2058, loss 0.369581, acc 0.882353, learning_rate 0.000101087
2017-10-11T11:15:36.945998: step 2059, loss 0.317285, acc 0.890625, learning_rate 0.000101083
2017-10-11T11:15:37.076805: step 2060, loss 0.179992, acc 0.9375, learning_rate 0.000101078
2017-10-11T11:15:37.276319: step 2061, loss 0.369437, acc 0.90625, learning_rate 0.000101074
2017-10-11T11:15:37.389368: step 2062, loss 0.36299, acc 0.859375, learning_rate 0.00010107
2017-10-11T11:15:37.508862: step 2063, loss 0.396001, acc 0.875, learning_rate 0.000101065
2017-10-11T11:15:37.623461: step 2064, loss 0.201198, acc 0.96875, learning_rate 0.000101061
2017-10-11T11:15:37.737607: step 2065, loss 0.256803, acc 0.921875, learning_rate 0.000101057
2017-10-11T11:15:37.860286: step 2066, loss 0.300583, acc 0.921875, learning_rate 0.000101052
2017-10-11T11:15:37.984059: step 2067, loss 0.413935, acc 0.859375, learning_rate 0.000101048
2017-10-11T11:15:38.098368: step 2068, loss 0.121051, acc 0.984375, learning_rate 0.000101044
2017-10-11T11:15:38.220839: step 2069, loss 0.193185, acc 0.9375, learning_rate 0.000101039
2017-10-11T11:15:38.368078: step 2070, loss 0.366247, acc 0.890625, learning_rate 0.000101035
2017-10-11T11:15:38.461185: step 2071, loss 0.395953, acc 0.890625, learning_rate 0.000101031
2017-10-11T11:15:38.546681: step 2072, loss 0.335529, acc 0.890625, learning_rate 0.000101027
2017-10-11T11:15:38.635606: step 2073, loss 0.310161, acc 0.90625, learning_rate 0.000101023
2017-10-11T11:15:38.723077: step 2074, loss 0.423165, acc 0.859375, learning_rate 0.000101018
2017-10-11T11:15:38.810031: step 2075, loss 0.299074, acc 0.90625, learning_rate 0.000101014
2017-10-11T11:15:38.901340: step 2076, loss 0.367611, acc 0.859375, learning_rate 0.00010101
2017-10-11T11:15:38.988213: step 2077, loss 0.499629, acc 0.78125, learning_rate 0.000101006
2017-10-11T11:15:39.073445: step 2078, loss 0.343615, acc 0.90625, learning_rate 0.000101002
2017-10-11T11:15:39.172926: step 2079, loss 0.474782, acc 0.875, learning_rate 0.000100998
2017-10-11T11:15:39.280875: step 2080, loss 0.351652, acc 0.890625, learning_rate 0.000100994

Evaluation:
2017-10-11T11:15:39.524286: step 2080, loss 0.248736, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2080

2017-10-11T11:15:40.238056: step 2081, loss 0.407786, acc 0.90625, learning_rate 0.00010099
2017-10-11T11:15:40.355403: step 2082, loss 0.330751, acc 0.890625, learning_rate 0.000100986
2017-10-11T11:15:40.468348: step 2083, loss 0.485664, acc 0.796875, learning_rate 0.000100982
2017-10-11T11:15:40.586336: step 2084, loss 0.344172, acc 0.90625, learning_rate 0.000100978
2017-10-11T11:15:40.715217: step 2085, loss 0.236233, acc 0.953125, learning_rate 0.000100974
2017-10-11T11:15:40.831671: step 2086, loss 0.281438, acc 0.890625, learning_rate 0.00010097
2017-10-11T11:15:40.949986: step 2087, loss 0.531689, acc 0.8125, learning_rate 0.000100966
2017-10-11T11:15:41.058040: step 2088, loss 0.171857, acc 0.9375, learning_rate 0.000100962
2017-10-11T11:15:41.174856: step 2089, loss 0.284186, acc 0.90625, learning_rate 0.000100958
2017-10-11T11:15:41.291476: step 2090, loss 0.253311, acc 0.9375, learning_rate 0.000100954
2017-10-11T11:15:41.411217: step 2091, loss 0.456542, acc 0.828125, learning_rate 0.00010095
2017-10-11T11:15:41.533441: step 2092, loss 0.301923, acc 0.9375, learning_rate 0.000100946
2017-10-11T11:15:41.645452: step 2093, loss 0.333366, acc 0.890625, learning_rate 0.000100942
2017-10-11T11:15:41.762575: step 2094, loss 0.337142, acc 0.890625, learning_rate 0.000100938
2017-10-11T11:15:41.875230: step 2095, loss 0.466926, acc 0.828125, learning_rate 0.000100935
2017-10-11T11:15:41.985193: step 2096, loss 0.260631, acc 0.921875, learning_rate 0.000100931
2017-10-11T11:15:42.105084: step 2097, loss 0.254445, acc 0.9375, learning_rate 0.000100927
2017-10-11T11:15:42.226879: step 2098, loss 0.375576, acc 0.90625, learning_rate 0.000100923
2017-10-11T11:15:42.342580: step 2099, loss 0.371906, acc 0.828125, learning_rate 0.000100919
2017-10-11T11:15:42.459377: step 2100, loss 0.358424, acc 0.828125, learning_rate 0.000100916
2017-10-11T11:15:42.582652: step 2101, loss 0.298676, acc 0.875, learning_rate 0.000100912
2017-10-11T11:15:42.701823: step 2102, loss 0.206826, acc 0.921875, learning_rate 0.000100908
2017-10-11T11:15:42.803713: step 2103, loss 0.19762, acc 0.921875, learning_rate 0.000100904
2017-10-11T11:15:42.912211: step 2104, loss 0.40041, acc 0.90625, learning_rate 0.000100901
2017-10-11T11:15:43.025140: step 2105, loss 0.345919, acc 0.859375, learning_rate 0.000100897
2017-10-11T11:15:43.138582: step 2106, loss 0.218973, acc 0.953125, learning_rate 0.000100893
2017-10-11T11:15:43.256833: step 2107, loss 0.305089, acc 0.921875, learning_rate 0.00010089
2017-10-11T11:15:43.364703: step 2108, loss 0.297009, acc 0.90625, learning_rate 0.000100886
2017-10-11T11:15:43.480012: step 2109, loss 0.23495, acc 0.90625, learning_rate 0.000100883
2017-10-11T11:15:43.656372: step 2110, loss 0.332415, acc 0.890625, learning_rate 0.000100879
2017-10-11T11:15:43.744375: step 2111, loss 0.32334, acc 0.90625, learning_rate 0.000100875
2017-10-11T11:15:43.833448: step 2112, loss 0.316864, acc 0.9375, learning_rate 0.000100872
2017-10-11T11:15:43.922680: step 2113, loss 0.316062, acc 0.90625, learning_rate 0.000100868
2017-10-11T11:15:44.011196: step 2114, loss 0.531046, acc 0.828125, learning_rate 0.000100865
2017-10-11T11:15:44.099166: step 2115, loss 0.290001, acc 0.9375, learning_rate 0.000100861
2017-10-11T11:15:44.189917: step 2116, loss 0.210917, acc 0.921875, learning_rate 0.000100858
2017-10-11T11:15:44.280162: step 2117, loss 0.226791, acc 0.921875, learning_rate 0.000100854
2017-10-11T11:15:44.373049: step 2118, loss 0.33046, acc 0.890625, learning_rate 0.000100851
2017-10-11T11:15:44.469112: step 2119, loss 0.348113, acc 0.875, learning_rate 0.000100847
2017-10-11T11:15:44.593969: step 2120, loss 0.288682, acc 0.90625, learning_rate 0.000100844

Evaluation:
2017-10-11T11:15:44.844208: step 2120, loss 0.249781, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2120

2017-10-11T11:15:45.832826: step 2121, loss 0.26085, acc 0.9375, learning_rate 0.00010084
2017-10-11T11:15:45.917088: step 2122, loss 0.364557, acc 0.828125, learning_rate 0.000100837
2017-10-11T11:15:46.004623: step 2123, loss 0.180324, acc 0.953125, learning_rate 0.000100833
2017-10-11T11:15:46.114897: step 2124, loss 0.219776, acc 0.953125, learning_rate 0.00010083
2017-10-11T11:15:46.230479: step 2125, loss 0.191404, acc 0.9375, learning_rate 0.000100827
2017-10-11T11:15:46.330850: step 2126, loss 0.268068, acc 0.9375, learning_rate 0.000100823
2017-10-11T11:15:46.437406: step 2127, loss 0.341796, acc 0.90625, learning_rate 0.00010082
2017-10-11T11:15:46.553736: step 2128, loss 0.191963, acc 0.9375, learning_rate 0.000100817
2017-10-11T11:15:46.666458: step 2129, loss 0.354069, acc 0.890625, learning_rate 0.000100813
2017-10-11T11:15:46.776876: step 2130, loss 0.266513, acc 0.890625, learning_rate 0.00010081
2017-10-11T11:15:46.913766: step 2131, loss 0.309345, acc 0.859375, learning_rate 0.000100807
2017-10-11T11:15:47.036977: step 2132, loss 0.232894, acc 0.953125, learning_rate 0.000100803
2017-10-11T11:15:47.160995: step 2133, loss 0.294777, acc 0.9375, learning_rate 0.0001008
2017-10-11T11:15:47.279897: step 2134, loss 0.350115, acc 0.859375, learning_rate 0.000100797
2017-10-11T11:15:47.397301: step 2135, loss 0.216363, acc 0.90625, learning_rate 0.000100793
2017-10-11T11:15:47.512582: step 2136, loss 0.306641, acc 0.9375, learning_rate 0.00010079
2017-10-11T11:15:47.634885: step 2137, loss 0.397061, acc 0.84375, learning_rate 0.000100787
2017-10-11T11:15:47.757170: step 2138, loss 0.315699, acc 0.890625, learning_rate 0.000100784
2017-10-11T11:15:47.877256: step 2139, loss 0.281862, acc 0.890625, learning_rate 0.000100781
2017-10-11T11:15:47.999389: step 2140, loss 0.339266, acc 0.890625, learning_rate 0.000100777
2017-10-11T11:15:48.119116: step 2141, loss 0.216805, acc 0.921875, learning_rate 0.000100774
2017-10-11T11:15:48.219233: step 2142, loss 0.269391, acc 0.890625, learning_rate 0.000100771
2017-10-11T11:15:48.344707: step 2143, loss 0.283126, acc 0.90625, learning_rate 0.000100768
2017-10-11T11:15:48.464373: step 2144, loss 0.305251, acc 0.859375, learning_rate 0.000100765
2017-10-11T11:15:48.583291: step 2145, loss 0.319129, acc 0.875, learning_rate 0.000100762
2017-10-11T11:15:48.701956: step 2146, loss 0.286238, acc 0.875, learning_rate 0.000100759
2017-10-11T11:15:48.882378: step 2147, loss 0.315519, acc 0.921875, learning_rate 0.000100755
2017-10-11T11:15:48.965869: step 2148, loss 0.309301, acc 0.9375, learning_rate 0.000100752
2017-10-11T11:15:49.052830: step 2149, loss 0.385159, acc 0.875, learning_rate 0.000100749
2017-10-11T11:15:49.140978: step 2150, loss 0.221061, acc 0.921875, learning_rate 0.000100746
2017-10-11T11:15:49.230933: step 2151, loss 0.424951, acc 0.78125, learning_rate 0.000100743
2017-10-11T11:15:49.318982: step 2152, loss 0.39071, acc 0.9375, learning_rate 0.00010074
2017-10-11T11:15:49.410225: step 2153, loss 0.346701, acc 0.875, learning_rate 0.000100737
2017-10-11T11:15:49.498539: step 2154, loss 0.299524, acc 0.875, learning_rate 0.000100734
2017-10-11T11:15:49.587828: step 2155, loss 0.320391, acc 0.90625, learning_rate 0.000100731
2017-10-11T11:15:49.689621: step 2156, loss 0.267658, acc 0.882353, learning_rate 0.000100728
2017-10-11T11:15:49.815363: step 2157, loss 0.246668, acc 0.90625, learning_rate 0.000100725
2017-10-11T11:15:49.919021: step 2158, loss 0.205766, acc 0.953125, learning_rate 0.000100722
2017-10-11T11:15:50.028685: step 2159, loss 0.361932, acc 0.890625, learning_rate 0.000100719
2017-10-11T11:15:50.151289: step 2160, loss 0.306584, acc 0.90625, learning_rate 0.000100716

Evaluation:
2017-10-11T11:15:50.396287: step 2160, loss 0.249002, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2160

2017-10-11T11:15:51.127702: step 2161, loss 0.332937, acc 0.875, learning_rate 0.000100713
2017-10-11T11:15:51.243338: step 2162, loss 0.252611, acc 0.921875, learning_rate 0.000100711
2017-10-11T11:15:51.354094: step 2163, loss 0.252821, acc 0.921875, learning_rate 0.000100708
2017-10-11T11:15:51.467906: step 2164, loss 0.362682, acc 0.875, learning_rate 0.000100705
2017-10-11T11:15:51.581501: step 2165, loss 0.407307, acc 0.828125, learning_rate 0.000100702
2017-10-11T11:15:51.685322: step 2166, loss 0.336389, acc 0.875, learning_rate 0.000100699
2017-10-11T11:15:51.801246: step 2167, loss 0.252933, acc 0.90625, learning_rate 0.000100696
2017-10-11T11:15:51.920317: step 2168, loss 0.328066, acc 0.90625, learning_rate 0.000100693
2017-10-11T11:15:52.035660: step 2169, loss 0.306667, acc 0.875, learning_rate 0.00010069
2017-10-11T11:15:52.150681: step 2170, loss 0.312092, acc 0.9375, learning_rate 0.000100688
2017-10-11T11:15:52.258253: step 2171, loss 0.352117, acc 0.875, learning_rate 0.000100685
2017-10-11T11:15:52.378894: step 2172, loss 0.361487, acc 0.828125, learning_rate 0.000100682
2017-10-11T11:15:52.496971: step 2173, loss 0.47231, acc 0.859375, learning_rate 0.000100679
2017-10-11T11:15:52.615609: step 2174, loss 0.523702, acc 0.8125, learning_rate 0.000100677
2017-10-11T11:15:52.727489: step 2175, loss 0.358335, acc 0.859375, learning_rate 0.000100674
2017-10-11T11:15:52.832788: step 2176, loss 0.339411, acc 0.890625, learning_rate 0.000100671
2017-10-11T11:15:52.954621: step 2177, loss 0.251636, acc 0.90625, learning_rate 0.000100668
2017-10-11T11:15:53.078595: step 2178, loss 0.315519, acc 0.890625, learning_rate 0.000100666
2017-10-11T11:15:53.196072: step 2179, loss 0.447248, acc 0.84375, learning_rate 0.000100663
2017-10-11T11:15:53.312461: step 2180, loss 0.38037, acc 0.875, learning_rate 0.00010066
2017-10-11T11:15:53.417639: step 2181, loss 0.624214, acc 0.8125, learning_rate 0.000100657
2017-10-11T11:15:53.529876: step 2182, loss 0.459841, acc 0.875, learning_rate 0.000100655
2017-10-11T11:15:53.651553: step 2183, loss 0.426208, acc 0.890625, learning_rate 0.000100652
2017-10-11T11:15:53.765498: step 2184, loss 0.227648, acc 0.921875, learning_rate 0.000100649
2017-10-11T11:15:53.879161: step 2185, loss 0.173476, acc 0.953125, learning_rate 0.000100647
2017-10-11T11:15:54.044071: step 2186, loss 0.333851, acc 0.84375, learning_rate 0.000100644
2017-10-11T11:15:54.161875: step 2187, loss 0.37608, acc 0.875, learning_rate 0.000100641
2017-10-11T11:15:54.251285: step 2188, loss 0.517313, acc 0.796875, learning_rate 0.000100639
2017-10-11T11:15:54.337927: step 2189, loss 0.19176, acc 0.9375, learning_rate 0.000100636
2017-10-11T11:15:54.434169: step 2190, loss 0.172671, acc 0.953125, learning_rate 0.000100634
2017-10-11T11:15:54.522430: step 2191, loss 0.274844, acc 0.921875, learning_rate 0.000100631
2017-10-11T11:15:54.606793: step 2192, loss 0.284008, acc 0.890625, learning_rate 0.000100628
2017-10-11T11:15:54.691636: step 2193, loss 0.219103, acc 0.9375, learning_rate 0.000100626
2017-10-11T11:15:54.777539: step 2194, loss 0.328119, acc 0.859375, learning_rate 0.000100623
2017-10-11T11:15:54.866112: step 2195, loss 0.305286, acc 0.859375, learning_rate 0.000100621
2017-10-11T11:15:54.973401: step 2196, loss 0.30836, acc 0.890625, learning_rate 0.000100618
2017-10-11T11:15:55.072925: step 2197, loss 0.271936, acc 0.90625, learning_rate 0.000100616
2017-10-11T11:15:55.190426: step 2198, loss 0.22442, acc 0.953125, learning_rate 0.000100613
2017-10-11T11:15:55.310402: step 2199, loss 0.286723, acc 0.90625, learning_rate 0.000100611
2017-10-11T11:15:55.436312: step 2200, loss 0.333643, acc 0.90625, learning_rate 0.000100608

Evaluation:
2017-10-11T11:15:55.682483: step 2200, loss 0.248759, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2200

2017-10-11T11:15:56.406844: step 2201, loss 0.188589, acc 0.953125, learning_rate 0.000100606
2017-10-11T11:15:56.524818: step 2202, loss 0.366273, acc 0.921875, learning_rate 0.000100603
2017-10-11T11:15:56.642118: step 2203, loss 0.174683, acc 0.96875, learning_rate 0.000100601
2017-10-11T11:15:56.762819: step 2204, loss 0.276058, acc 0.875, learning_rate 0.000100598
2017-10-11T11:15:56.878728: step 2205, loss 0.313627, acc 0.875, learning_rate 0.000100596
2017-10-11T11:15:56.994566: step 2206, loss 0.146375, acc 0.984375, learning_rate 0.000100594
2017-10-11T11:15:57.111263: step 2207, loss 0.318362, acc 0.90625, learning_rate 0.000100591
2017-10-11T11:15:57.220120: step 2208, loss 0.191451, acc 0.96875, learning_rate 0.000100589
2017-10-11T11:15:57.337156: step 2209, loss 0.367681, acc 0.890625, learning_rate 0.000100586
2017-10-11T11:15:57.461778: step 2210, loss 0.281769, acc 0.875, learning_rate 0.000100584
2017-10-11T11:15:57.560976: step 2211, loss 0.2446, acc 0.96875, learning_rate 0.000100581
2017-10-11T11:15:57.671887: step 2212, loss 0.240923, acc 0.921875, learning_rate 0.000100579
2017-10-11T11:15:57.770224: step 2213, loss 0.274915, acc 0.9375, learning_rate 0.000100577
2017-10-11T11:15:57.899020: step 2214, loss 0.211031, acc 0.9375, learning_rate 0.000100574
2017-10-11T11:15:58.018396: step 2215, loss 0.471101, acc 0.828125, learning_rate 0.000100572
2017-10-11T11:15:58.123548: step 2216, loss 0.242069, acc 0.9375, learning_rate 0.00010057
2017-10-11T11:15:58.241226: step 2217, loss 0.285993, acc 0.921875, learning_rate 0.000100567
2017-10-11T11:15:58.360072: step 2218, loss 0.337371, acc 0.921875, learning_rate 0.000100565
2017-10-11T11:15:58.468001: step 2219, loss 0.257041, acc 0.90625, learning_rate 0.000100563
2017-10-11T11:15:58.583308: step 2220, loss 0.302031, acc 0.90625, learning_rate 0.00010056
2017-10-11T11:15:58.696222: step 2221, loss 0.264537, acc 0.9375, learning_rate 0.000100558
2017-10-11T11:15:58.803938: step 2222, loss 0.285769, acc 0.890625, learning_rate 0.000100556
2017-10-11T11:15:58.920733: step 2223, loss 0.367124, acc 0.859375, learning_rate 0.000100554
2017-10-11T11:15:59.032803: step 2224, loss 0.218693, acc 0.9375, learning_rate 0.000100551
2017-10-11T11:15:59.148058: step 2225, loss 0.263452, acc 0.90625, learning_rate 0.000100549
2017-10-11T11:15:59.328874: step 2226, loss 0.278945, acc 0.921875, learning_rate 0.000100547
2017-10-11T11:15:59.433642: step 2227, loss 0.345156, acc 0.890625, learning_rate 0.000100545
2017-10-11T11:15:59.528260: step 2228, loss 0.307751, acc 0.90625, learning_rate 0.000100542
2017-10-11T11:15:59.613805: step 2229, loss 0.290247, acc 0.890625, learning_rate 0.00010054
2017-10-11T11:15:59.702431: step 2230, loss 0.236913, acc 0.890625, learning_rate 0.000100538
2017-10-11T11:15:59.800717: step 2231, loss 0.312271, acc 0.921875, learning_rate 0.000100536
2017-10-11T11:15:59.894698: step 2232, loss 0.335846, acc 0.921875, learning_rate 0.000100534
2017-10-11T11:15:59.986906: step 2233, loss 0.393702, acc 0.890625, learning_rate 0.000100531
2017-10-11T11:16:00.072301: step 2234, loss 0.417774, acc 0.8125, learning_rate 0.000100529
2017-10-11T11:16:00.157468: step 2235, loss 0.420392, acc 0.890625, learning_rate 0.000100527
2017-10-11T11:16:00.243266: step 2236, loss 0.365729, acc 0.875, learning_rate 0.000100525
2017-10-11T11:16:00.359785: step 2237, loss 0.288152, acc 0.921875, learning_rate 0.000100523
2017-10-11T11:16:00.484894: step 2238, loss 0.394695, acc 0.90625, learning_rate 0.000100521
2017-10-11T11:16:00.595492: step 2239, loss 0.25402, acc 0.9375, learning_rate 0.000100519
2017-10-11T11:16:00.702434: step 2240, loss 0.165651, acc 0.96875, learning_rate 0.000100516

Evaluation:
2017-10-11T11:16:00.953721: step 2240, loss 0.248045, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2240

2017-10-11T11:16:01.678103: step 2241, loss 0.316946, acc 0.875, learning_rate 0.000100514
2017-10-11T11:16:01.795222: step 2242, loss 0.226527, acc 0.953125, learning_rate 0.000100512
2017-10-11T11:16:01.906651: step 2243, loss 0.382848, acc 0.84375, learning_rate 0.00010051
2017-10-11T11:16:02.022278: step 2244, loss 0.2075, acc 0.953125, learning_rate 0.000100508
2017-10-11T11:16:02.141898: step 2245, loss 0.209361, acc 0.921875, learning_rate 0.000100506
2017-10-11T11:16:02.254655: step 2246, loss 0.394259, acc 0.84375, learning_rate 0.000100504
2017-10-11T11:16:02.367418: step 2247, loss 0.295076, acc 0.921875, learning_rate 0.000100502
2017-10-11T11:16:02.483423: step 2248, loss 0.32016, acc 0.90625, learning_rate 0.0001005
2017-10-11T11:16:02.596680: step 2249, loss 0.36234, acc 0.890625, learning_rate 0.000100498
2017-10-11T11:16:02.711643: step 2250, loss 0.229808, acc 0.90625, learning_rate 0.000100496
2017-10-11T11:16:02.823922: step 2251, loss 0.403001, acc 0.875, learning_rate 0.000100494
2017-10-11T11:16:02.939383: step 2252, loss 0.40333, acc 0.890625, learning_rate 0.000100492
2017-10-11T11:16:03.046117: step 2253, loss 0.275086, acc 0.875, learning_rate 0.00010049
2017-10-11T11:16:03.142159: step 2254, loss 0.330024, acc 0.901961, learning_rate 0.000100488
2017-10-11T11:16:03.268022: step 2255, loss 0.337976, acc 0.921875, learning_rate 0.000100486
2017-10-11T11:16:03.388336: step 2256, loss 0.307342, acc 0.90625, learning_rate 0.000100484
2017-10-11T11:16:03.495115: step 2257, loss 0.357109, acc 0.890625, learning_rate 0.000100482
2017-10-11T11:16:03.614047: step 2258, loss 0.259171, acc 0.921875, learning_rate 0.00010048
2017-10-11T11:16:03.733661: step 2259, loss 0.311938, acc 0.875, learning_rate 0.000100478
2017-10-11T11:16:03.851898: step 2260, loss 0.312493, acc 0.890625, learning_rate 0.000100476
2017-10-11T11:16:03.967703: step 2261, loss 0.217203, acc 0.921875, learning_rate 0.000100474
2017-10-11T11:16:04.089027: step 2262, loss 0.280015, acc 0.921875, learning_rate 0.000100472
2017-10-11T11:16:04.201158: step 2263, loss 0.253557, acc 0.90625, learning_rate 0.00010047
2017-10-11T11:16:04.321134: step 2264, loss 0.213586, acc 0.953125, learning_rate 0.000100468
2017-10-11T11:16:04.435884: step 2265, loss 0.265038, acc 0.890625, learning_rate 0.000100466
2017-10-11T11:16:04.557817: step 2266, loss 0.329413, acc 0.890625, learning_rate 0.000100464
2017-10-11T11:16:04.732877: step 2267, loss 0.254603, acc 0.90625, learning_rate 0.000100462
2017-10-11T11:16:04.830219: step 2268, loss 0.269956, acc 0.9375, learning_rate 0.000100461
2017-10-11T11:16:04.915986: step 2269, loss 0.299873, acc 0.90625, learning_rate 0.000100459
2017-10-11T11:16:05.003516: step 2270, loss 0.379418, acc 0.875, learning_rate 0.000100457
2017-10-11T11:16:05.091869: step 2271, loss 0.382379, acc 0.859375, learning_rate 0.000100455
2017-10-11T11:16:05.177754: step 2272, loss 0.231155, acc 0.9375, learning_rate 0.000100453
2017-10-11T11:16:05.263626: step 2273, loss 0.351367, acc 0.84375, learning_rate 0.000100451
2017-10-11T11:16:05.353303: step 2274, loss 0.202203, acc 0.921875, learning_rate 0.000100449
2017-10-11T11:16:05.439188: step 2275, loss 0.249113, acc 0.9375, learning_rate 0.000100448
2017-10-11T11:16:05.523600: step 2276, loss 0.184158, acc 0.953125, learning_rate 0.000100446
2017-10-11T11:16:05.635993: step 2277, loss 0.34456, acc 0.90625, learning_rate 0.000100444
2017-10-11T11:16:05.759942: step 2278, loss 0.296744, acc 0.890625, learning_rate 0.000100442
2017-10-11T11:16:05.886631: step 2279, loss 0.355848, acc 0.859375, learning_rate 0.00010044
2017-10-11T11:16:06.001169: step 2280, loss 0.46428, acc 0.84375, learning_rate 0.000100439

Evaluation:
2017-10-11T11:16:06.253851: step 2280, loss 0.247834, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2280

2017-10-11T11:16:06.978825: step 2281, loss 0.284968, acc 0.90625, learning_rate 0.000100437
2017-10-11T11:16:07.101419: step 2282, loss 0.292835, acc 0.90625, learning_rate 0.000100435
2017-10-11T11:16:07.207755: step 2283, loss 0.353589, acc 0.84375, learning_rate 0.000100433
2017-10-11T11:16:07.314705: step 2284, loss 0.187885, acc 0.921875, learning_rate 0.000100431
2017-10-11T11:16:07.427273: step 2285, loss 0.439407, acc 0.84375, learning_rate 0.00010043
2017-10-11T11:16:07.542340: step 2286, loss 0.242331, acc 0.953125, learning_rate 0.000100428
2017-10-11T11:16:07.652424: step 2287, loss 0.360752, acc 0.90625, learning_rate 0.000100426
2017-10-11T11:16:07.776790: step 2288, loss 0.312923, acc 0.84375, learning_rate 0.000100424
2017-10-11T11:16:07.895327: step 2289, loss 0.292834, acc 0.90625, learning_rate 0.000100423
2017-10-11T11:16:08.006097: step 2290, loss 0.3305, acc 0.875, learning_rate 0.000100421
2017-10-11T11:16:08.114294: step 2291, loss 0.163589, acc 0.953125, learning_rate 0.000100419
2017-10-11T11:16:08.233672: step 2292, loss 0.298233, acc 0.90625, learning_rate 0.000100418
2017-10-11T11:16:08.355042: step 2293, loss 0.351983, acc 0.875, learning_rate 0.000100416
2017-10-11T11:16:08.465123: step 2294, loss 0.399704, acc 0.859375, learning_rate 0.000100414
2017-10-11T11:16:08.580750: step 2295, loss 0.14231, acc 0.953125, learning_rate 0.000100412
2017-10-11T11:16:08.699690: step 2296, loss 0.31458, acc 0.921875, learning_rate 0.000100411
2017-10-11T11:16:08.811080: step 2297, loss 0.271888, acc 0.953125, learning_rate 0.000100409
2017-10-11T11:16:08.934386: step 2298, loss 0.396219, acc 0.859375, learning_rate 0.000100407
2017-10-11T11:16:09.038047: step 2299, loss 0.480273, acc 0.875, learning_rate 0.000100406
2017-10-11T11:16:09.148899: step 2300, loss 0.386228, acc 0.8125, learning_rate 0.000100404
2017-10-11T11:16:09.271139: step 2301, loss 0.353043, acc 0.890625, learning_rate 0.000100402
2017-10-11T11:16:09.388890: step 2302, loss 0.356032, acc 0.859375, learning_rate 0.000100401
2017-10-11T11:16:09.512849: step 2303, loss 0.137379, acc 0.96875, learning_rate 0.000100399
2017-10-11T11:16:09.631111: step 2304, loss 0.276427, acc 0.90625, learning_rate 0.000100398
2017-10-11T11:16:09.752199: step 2305, loss 0.298941, acc 0.875, learning_rate 0.000100396
2017-10-11T11:16:09.908883: step 2306, loss 0.249139, acc 0.90625, learning_rate 0.000100394
2017-10-11T11:16:10.024454: step 2307, loss 0.279482, acc 0.890625, learning_rate 0.000100393
2017-10-11T11:16:10.110653: step 2308, loss 0.368217, acc 0.828125, learning_rate 0.000100391
2017-10-11T11:16:10.194120: step 2309, loss 0.32353, acc 0.890625, learning_rate 0.000100389
2017-10-11T11:16:10.276967: step 2310, loss 0.43453, acc 0.84375, learning_rate 0.000100388
2017-10-11T11:16:10.362697: step 2311, loss 0.223423, acc 0.96875, learning_rate 0.000100386
2017-10-11T11:16:10.448109: step 2312, loss 0.28856, acc 0.875, learning_rate 0.000100385
2017-10-11T11:16:10.537181: step 2313, loss 0.299538, acc 0.859375, learning_rate 0.000100383
2017-10-11T11:16:10.623397: step 2314, loss 0.339943, acc 0.84375, learning_rate 0.000100382
2017-10-11T11:16:10.709757: step 2315, loss 0.26909, acc 0.890625, learning_rate 0.00010038
2017-10-11T11:16:10.827303: step 2316, loss 0.403898, acc 0.8125, learning_rate 0.000100378
2017-10-11T11:16:10.954779: step 2317, loss 0.195877, acc 0.96875, learning_rate 0.000100377
2017-10-11T11:16:11.066362: step 2318, loss 0.405718, acc 0.828125, learning_rate 0.000100375
2017-10-11T11:16:11.185357: step 2319, loss 0.305225, acc 0.9375, learning_rate 0.000100374
2017-10-11T11:16:11.299723: step 2320, loss 0.259492, acc 0.921875, learning_rate 0.000100372

Evaluation:
2017-10-11T11:16:11.539826: step 2320, loss 0.24731, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2320

2017-10-11T11:16:12.154274: step 2321, loss 0.19859, acc 0.953125, learning_rate 0.000100371
2017-10-11T11:16:12.273363: step 2322, loss 0.276602, acc 0.859375, learning_rate 0.000100369
2017-10-11T11:16:12.395851: step 2323, loss 0.181296, acc 0.96875, learning_rate 0.000100368
2017-10-11T11:16:12.509865: step 2324, loss 0.288877, acc 0.875, learning_rate 0.000100366
2017-10-11T11:16:12.622839: step 2325, loss 0.38943, acc 0.859375, learning_rate 0.000100365
2017-10-11T11:16:12.742093: step 2326, loss 0.228723, acc 0.921875, learning_rate 0.000100363
2017-10-11T11:16:12.875052: step 2327, loss 0.293002, acc 0.90625, learning_rate 0.000100362
2017-10-11T11:16:12.990475: step 2328, loss 0.193587, acc 0.9375, learning_rate 0.00010036
2017-10-11T11:16:13.102858: step 2329, loss 0.355226, acc 0.921875, learning_rate 0.000100359
2017-10-11T11:16:13.210478: step 2330, loss 0.312248, acc 0.890625, learning_rate 0.000100357
2017-10-11T11:16:13.326809: step 2331, loss 0.262916, acc 0.9375, learning_rate 0.000100356
2017-10-11T11:16:13.440768: step 2332, loss 0.214926, acc 0.953125, learning_rate 0.000100354
2017-10-11T11:16:13.556862: step 2333, loss 0.262396, acc 0.921875, learning_rate 0.000100353
2017-10-11T11:16:13.660405: step 2334, loss 0.271302, acc 0.953125, learning_rate 0.000100352
2017-10-11T11:16:13.775320: step 2335, loss 0.256311, acc 0.921875, learning_rate 0.00010035
2017-10-11T11:16:13.890593: step 2336, loss 0.39713, acc 0.84375, learning_rate 0.000100349
2017-10-11T11:16:13.996702: step 2337, loss 0.318658, acc 0.875, learning_rate 0.000100347
2017-10-11T11:16:14.119199: step 2338, loss 0.306719, acc 0.953125, learning_rate 0.000100346
2017-10-11T11:16:14.241421: step 2339, loss 0.447022, acc 0.875, learning_rate 0.000100344
2017-10-11T11:16:14.367687: step 2340, loss 0.22497, acc 0.890625, learning_rate 0.000100343
2017-10-11T11:16:14.488908: step 2341, loss 0.181326, acc 0.9375, learning_rate 0.000100342
2017-10-11T11:16:14.602424: step 2342, loss 0.171767, acc 0.953125, learning_rate 0.00010034
2017-10-11T11:16:14.729508: step 2343, loss 0.355883, acc 0.859375, learning_rate 0.000100339
2017-10-11T11:16:14.843297: step 2344, loss 0.302455, acc 0.890625, learning_rate 0.000100338
2017-10-11T11:16:14.964106: step 2345, loss 0.413642, acc 0.90625, learning_rate 0.000100336
2017-10-11T11:16:15.104391: step 2346, loss 0.421351, acc 0.84375, learning_rate 0.000100335
2017-10-11T11:16:15.235591: step 2347, loss 0.236635, acc 0.921875, learning_rate 0.000100333
2017-10-11T11:16:15.322374: step 2348, loss 0.320648, acc 0.890625, learning_rate 0.000100332
2017-10-11T11:16:15.412294: step 2349, loss 0.437049, acc 0.890625, learning_rate 0.000100331
2017-10-11T11:16:15.501640: step 2350, loss 0.300892, acc 0.921875, learning_rate 0.000100329
2017-10-11T11:16:15.590535: step 2351, loss 0.359909, acc 0.875, learning_rate 0.000100328
2017-10-11T11:16:15.671217: step 2352, loss 0.253538, acc 0.901961, learning_rate 0.000100327
2017-10-11T11:16:15.761419: step 2353, loss 0.237782, acc 0.953125, learning_rate 0.000100325
2017-10-11T11:16:15.847918: step 2354, loss 0.279093, acc 0.953125, learning_rate 0.000100324
2017-10-11T11:16:15.932328: step 2355, loss 0.318777, acc 0.921875, learning_rate 0.000100323
2017-10-11T11:16:16.017259: step 2356, loss 0.357115, acc 0.875, learning_rate 0.000100321
2017-10-11T11:16:16.104456: step 2357, loss 0.328887, acc 0.921875, learning_rate 0.00010032
2017-10-11T11:16:16.223293: step 2358, loss 0.290044, acc 0.953125, learning_rate 0.000100319
2017-10-11T11:16:16.342616: step 2359, loss 0.336551, acc 0.84375, learning_rate 0.000100317
2017-10-11T11:16:16.457233: step 2360, loss 0.289521, acc 0.9375, learning_rate 0.000100316

Evaluation:
2017-10-11T11:16:16.695664: step 2360, loss 0.246715, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2360

2017-10-11T11:16:17.452951: step 2361, loss 0.396371, acc 0.84375, learning_rate 0.000100315
2017-10-11T11:16:17.569885: step 2362, loss 0.36813, acc 0.859375, learning_rate 0.000100314
2017-10-11T11:16:17.684914: step 2363, loss 0.420647, acc 0.8125, learning_rate 0.000100312
2017-10-11T11:16:17.804871: step 2364, loss 0.559442, acc 0.828125, learning_rate 0.000100311
2017-10-11T11:16:17.915793: step 2365, loss 0.290993, acc 0.9375, learning_rate 0.00010031
2017-10-11T11:16:18.030039: step 2366, loss 0.387118, acc 0.90625, learning_rate 0.000100308
2017-10-11T11:16:18.143521: step 2367, loss 0.351425, acc 0.875, learning_rate 0.000100307
2017-10-11T11:16:18.256281: step 2368, loss 0.309383, acc 0.90625, learning_rate 0.000100306
2017-10-11T11:16:18.367224: step 2369, loss 0.212182, acc 0.9375, learning_rate 0.000100305
2017-10-11T11:16:18.488916: step 2370, loss 0.39553, acc 0.859375, learning_rate 0.000100303
2017-10-11T11:16:18.603502: step 2371, loss 0.385277, acc 0.84375, learning_rate 0.000100302
2017-10-11T11:16:18.720484: step 2372, loss 0.157116, acc 0.953125, learning_rate 0.000100301
2017-10-11T11:16:18.847006: step 2373, loss 0.380281, acc 0.921875, learning_rate 0.0001003
2017-10-11T11:16:18.962510: step 2374, loss 0.202525, acc 0.953125, learning_rate 0.000100299
2017-10-11T11:16:19.093274: step 2375, loss 0.425228, acc 0.921875, learning_rate 0.000100297
2017-10-11T11:16:19.218754: step 2376, loss 0.297945, acc 0.859375, learning_rate 0.000100296
2017-10-11T11:16:19.336099: step 2377, loss 0.289118, acc 0.859375, learning_rate 0.000100295
2017-10-11T11:16:19.444306: step 2378, loss 0.317169, acc 0.890625, learning_rate 0.000100294
2017-10-11T11:16:19.573015: step 2379, loss 0.231367, acc 0.90625, learning_rate 0.000100292
2017-10-11T11:16:19.693757: step 2380, loss 0.320514, acc 0.875, learning_rate 0.000100291
2017-10-11T11:16:19.808445: step 2381, loss 0.261626, acc 0.953125, learning_rate 0.00010029
2017-10-11T11:16:19.924854: step 2382, loss 0.265005, acc 0.9375, learning_rate 0.000100289
2017-10-11T11:16:20.042244: step 2383, loss 0.37975, acc 0.84375, learning_rate 0.000100288
2017-10-11T11:16:20.155677: step 2384, loss 0.362633, acc 0.890625, learning_rate 0.000100287
2017-10-11T11:16:20.279172: step 2385, loss 0.241088, acc 0.953125, learning_rate 0.000100285
2017-10-11T11:16:20.392854: step 2386, loss 0.241915, acc 0.953125, learning_rate 0.000100284
2017-10-11T11:16:20.557070: step 2387, loss 0.492696, acc 0.859375, learning_rate 0.000100283
2017-10-11T11:16:20.666791: step 2388, loss 0.333965, acc 0.875, learning_rate 0.000100282
2017-10-11T11:16:20.758374: step 2389, loss 0.240163, acc 0.921875, learning_rate 0.000100281
2017-10-11T11:16:20.847136: step 2390, loss 0.449649, acc 0.859375, learning_rate 0.00010028
2017-10-11T11:16:20.937185: step 2391, loss 0.272212, acc 0.890625, learning_rate 0.000100278
2017-10-11T11:16:21.027975: step 2392, loss 0.22238, acc 0.90625, learning_rate 0.000100277
2017-10-11T11:16:21.119171: step 2393, loss 0.36402, acc 0.890625, learning_rate 0.000100276
2017-10-11T11:16:21.215510: step 2394, loss 0.240552, acc 0.9375, learning_rate 0.000100275
2017-10-11T11:16:21.324555: step 2395, loss 0.31921, acc 0.90625, learning_rate 0.000100274
2017-10-11T11:16:21.452787: step 2396, loss 0.305397, acc 0.890625, learning_rate 0.000100273
2017-10-11T11:16:21.572963: step 2397, loss 0.277941, acc 0.953125, learning_rate 0.000100272
2017-10-11T11:16:21.691009: step 2398, loss 0.212602, acc 0.953125, learning_rate 0.000100271
2017-10-11T11:16:21.797894: step 2399, loss 0.339047, acc 0.890625, learning_rate 0.00010027
2017-10-11T11:16:21.911885: step 2400, loss 0.410107, acc 0.828125, learning_rate 0.000100268

Evaluation:
2017-10-11T11:16:22.154057: step 2400, loss 0.245651, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2400

2017-10-11T11:16:23.125388: step 2401, loss 0.307729, acc 0.890625, learning_rate 0.000100267
2017-10-11T11:16:23.241730: step 2402, loss 0.329094, acc 0.9375, learning_rate 0.000100266
2017-10-11T11:16:23.351460: step 2403, loss 0.256574, acc 0.90625, learning_rate 0.000100265
2017-10-11T11:16:23.466884: step 2404, loss 0.243026, acc 0.90625, learning_rate 0.000100264
2017-10-11T11:16:23.580872: step 2405, loss 0.167697, acc 0.96875, learning_rate 0.000100263
2017-10-11T11:16:23.704939: step 2406, loss 0.348668, acc 0.828125, learning_rate 0.000100262
2017-10-11T11:16:23.808921: step 2407, loss 0.344125, acc 0.875, learning_rate 0.000100261
2017-10-11T11:16:23.927759: step 2408, loss 0.304761, acc 0.890625, learning_rate 0.00010026
2017-10-11T11:16:24.038142: step 2409, loss 0.23547, acc 0.953125, learning_rate 0.000100259
2017-10-11T11:16:24.145519: step 2410, loss 0.335701, acc 0.890625, learning_rate 0.000100258
2017-10-11T11:16:24.256271: step 2411, loss 0.654009, acc 0.84375, learning_rate 0.000100257
2017-10-11T11:16:24.385927: step 2412, loss 0.463235, acc 0.859375, learning_rate 0.000100256
2017-10-11T11:16:24.508604: step 2413, loss 0.473676, acc 0.828125, learning_rate 0.000100255
2017-10-11T11:16:24.626289: step 2414, loss 0.183761, acc 0.953125, learning_rate 0.000100253
2017-10-11T11:16:24.738974: step 2415, loss 0.261068, acc 0.921875, learning_rate 0.000100252
2017-10-11T11:16:24.848593: step 2416, loss 0.367572, acc 0.859375, learning_rate 0.000100251
2017-10-11T11:16:24.962117: step 2417, loss 0.484856, acc 0.890625, learning_rate 0.00010025
2017-10-11T11:16:25.071087: step 2418, loss 0.231731, acc 0.90625, learning_rate 0.000100249
2017-10-11T11:16:25.183871: step 2419, loss 0.236446, acc 0.9375, learning_rate 0.000100248
2017-10-11T11:16:25.288857: step 2420, loss 0.445433, acc 0.875, learning_rate 0.000100247
2017-10-11T11:16:25.401938: step 2421, loss 0.165351, acc 0.96875, learning_rate 0.000100246
2017-10-11T11:16:25.532875: step 2422, loss 0.327399, acc 0.90625, learning_rate 0.000100245
2017-10-11T11:16:25.676565: step 2423, loss 0.301039, acc 0.90625, learning_rate 0.000100244
2017-10-11T11:16:25.771597: step 2424, loss 0.323391, acc 0.859375, learning_rate 0.000100243
2017-10-11T11:16:25.862908: step 2425, loss 0.567796, acc 0.828125, learning_rate 0.000100242
2017-10-11T11:16:25.959864: step 2426, loss 0.263592, acc 0.890625, learning_rate 0.000100241
2017-10-11T11:16:26.054656: step 2427, loss 0.246965, acc 0.90625, learning_rate 0.00010024
2017-10-11T11:16:26.153951: step 2428, loss 0.321684, acc 0.859375, learning_rate 0.000100239
2017-10-11T11:16:26.247898: step 2429, loss 0.361179, acc 0.890625, learning_rate 0.000100238
2017-10-11T11:16:26.342063: step 2430, loss 0.204129, acc 0.9375, learning_rate 0.000100237
2017-10-11T11:16:26.434332: step 2431, loss 0.264457, acc 0.890625, learning_rate 0.000100236
2017-10-11T11:16:26.540962: step 2432, loss 0.232978, acc 0.890625, learning_rate 0.000100235
2017-10-11T11:16:26.647694: step 2433, loss 0.184024, acc 0.9375, learning_rate 0.000100235
2017-10-11T11:16:26.757904: step 2434, loss 0.280463, acc 0.9375, learning_rate 0.000100234
2017-10-11T11:16:26.860854: step 2435, loss 0.348389, acc 0.859375, learning_rate 0.000100233
2017-10-11T11:16:26.984935: step 2436, loss 0.121598, acc 0.96875, learning_rate 0.000100232
2017-10-11T11:16:27.088584: step 2437, loss 0.388139, acc 0.890625, learning_rate 0.000100231
2017-10-11T11:16:27.190488: step 2438, loss 0.360134, acc 0.84375, learning_rate 0.00010023
2017-10-11T11:16:27.320360: step 2439, loss 0.240809, acc 0.90625, learning_rate 0.000100229
2017-10-11T11:16:27.432778: step 2440, loss 0.310585, acc 0.890625, learning_rate 0.000100228

Evaluation:
2017-10-11T11:16:27.695447: step 2440, loss 0.245702, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2440

2017-10-11T11:16:28.406140: step 2441, loss 0.388311, acc 0.859375, learning_rate 0.000100227
2017-10-11T11:16:28.510417: step 2442, loss 0.380391, acc 0.875, learning_rate 0.000100226
2017-10-11T11:16:28.644831: step 2443, loss 0.305634, acc 0.890625, learning_rate 0.000100225
2017-10-11T11:16:28.762666: step 2444, loss 0.278344, acc 0.9375, learning_rate 0.000100224
2017-10-11T11:16:28.875724: step 2445, loss 0.471546, acc 0.859375, learning_rate 0.000100223
2017-10-11T11:16:28.998421: step 2446, loss 0.237036, acc 0.921875, learning_rate 0.000100222
2017-10-11T11:16:29.114875: step 2447, loss 0.255652, acc 0.90625, learning_rate 0.000100221
2017-10-11T11:16:29.232916: step 2448, loss 0.263195, acc 0.9375, learning_rate 0.000100221
2017-10-11T11:16:29.349200: step 2449, loss 0.431536, acc 0.84375, learning_rate 0.00010022
2017-10-11T11:16:29.449592: step 2450, loss 0.342327, acc 0.803922, learning_rate 0.000100219
2017-10-11T11:16:29.557839: step 2451, loss 0.245851, acc 0.90625, learning_rate 0.000100218
2017-10-11T11:16:29.658904: step 2452, loss 0.238988, acc 0.921875, learning_rate 0.000100217
2017-10-11T11:16:29.772383: step 2453, loss 0.255768, acc 0.9375, learning_rate 0.000100216
2017-10-11T11:16:29.879178: step 2454, loss 0.296983, acc 0.90625, learning_rate 0.000100215
2017-10-11T11:16:29.986458: step 2455, loss 0.372624, acc 0.859375, learning_rate 0.000100214
2017-10-11T11:16:30.106535: step 2456, loss 0.267859, acc 0.90625, learning_rate 0.000100213
2017-10-11T11:16:30.206583: step 2457, loss 0.36188, acc 0.90625, learning_rate 0.000100213
2017-10-11T11:16:30.332135: step 2458, loss 0.185281, acc 0.953125, learning_rate 0.000100212
2017-10-11T11:16:30.441985: step 2459, loss 0.324203, acc 0.875, learning_rate 0.000100211
2017-10-11T11:16:30.563811: step 2460, loss 0.316626, acc 0.90625, learning_rate 0.00010021
2017-10-11T11:16:30.683244: step 2461, loss 0.264325, acc 0.90625, learning_rate 0.000100209
2017-10-11T11:16:30.848887: step 2462, loss 0.391251, acc 0.828125, learning_rate 0.000100208
2017-10-11T11:16:30.955776: step 2463, loss 0.261626, acc 0.921875, learning_rate 0.000100207
2017-10-11T11:16:31.045527: step 2464, loss 0.313628, acc 0.875, learning_rate 0.000100207
2017-10-11T11:16:31.133210: step 2465, loss 0.471488, acc 0.875, learning_rate 0.000100206
2017-10-11T11:16:31.218757: step 2466, loss 0.286113, acc 0.921875, learning_rate 0.000100205
2017-10-11T11:16:31.306481: step 2467, loss 0.255679, acc 0.921875, learning_rate 0.000100204
2017-10-11T11:16:31.397186: step 2468, loss 0.327159, acc 0.890625, learning_rate 0.000100203
2017-10-11T11:16:31.489507: step 2469, loss 0.248085, acc 0.90625, learning_rate 0.000100202
2017-10-11T11:16:31.578241: step 2470, loss 0.440951, acc 0.84375, learning_rate 0.000100202
2017-10-11T11:16:31.665943: step 2471, loss 0.180528, acc 0.953125, learning_rate 0.000100201
2017-10-11T11:16:31.760465: step 2472, loss 0.267243, acc 0.890625, learning_rate 0.0001002
2017-10-11T11:16:31.848845: step 2473, loss 0.252999, acc 0.953125, learning_rate 0.000100199
2017-10-11T11:16:31.966312: step 2474, loss 0.32147, acc 0.875, learning_rate 0.000100198
2017-10-11T11:16:32.089787: step 2475, loss 0.418846, acc 0.859375, learning_rate 0.000100198
2017-10-11T11:16:32.196403: step 2476, loss 0.240358, acc 0.96875, learning_rate 0.000100197
2017-10-11T11:16:32.315494: step 2477, loss 0.494839, acc 0.84375, learning_rate 0.000100196
2017-10-11T11:16:32.427625: step 2478, loss 0.411721, acc 0.859375, learning_rate 0.000100195
2017-10-11T11:16:32.531655: step 2479, loss 0.356532, acc 0.875, learning_rate 0.000100194
2017-10-11T11:16:32.650734: step 2480, loss 0.176884, acc 0.9375, learning_rate 0.000100194

Evaluation:
2017-10-11T11:16:32.904631: step 2480, loss 0.244486, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2480

2017-10-11T11:16:33.628927: step 2481, loss 0.289656, acc 0.828125, learning_rate 0.000100193
2017-10-11T11:16:33.751258: step 2482, loss 0.523466, acc 0.859375, learning_rate 0.000100192
2017-10-11T11:16:33.863487: step 2483, loss 0.357862, acc 0.84375, learning_rate 0.000100191
2017-10-11T11:16:33.969201: step 2484, loss 0.162441, acc 0.9375, learning_rate 0.00010019
2017-10-11T11:16:34.074087: step 2485, loss 0.20739, acc 0.921875, learning_rate 0.00010019
2017-10-11T11:16:34.172891: step 2486, loss 0.279923, acc 0.90625, learning_rate 0.000100189
2017-10-11T11:16:34.278141: step 2487, loss 0.360401, acc 0.859375, learning_rate 0.000100188
2017-10-11T11:16:34.380553: step 2488, loss 0.352319, acc 0.90625, learning_rate 0.000100187
2017-10-11T11:16:34.495992: step 2489, loss 0.212811, acc 0.921875, learning_rate 0.000100187
2017-10-11T11:16:34.620725: step 2490, loss 0.366284, acc 0.875, learning_rate 0.000100186
2017-10-11T11:16:34.741419: step 2491, loss 0.318027, acc 0.921875, learning_rate 0.000100185
2017-10-11T11:16:34.850068: step 2492, loss 0.326808, acc 0.921875, learning_rate 0.000100184
2017-10-11T11:16:34.963580: step 2493, loss 0.326672, acc 0.90625, learning_rate 0.000100183
2017-10-11T11:16:35.074800: step 2494, loss 0.381024, acc 0.859375, learning_rate 0.000100183
2017-10-11T11:16:35.202474: step 2495, loss 0.344767, acc 0.921875, learning_rate 0.000100182
2017-10-11T11:16:35.323204: step 2496, loss 0.273436, acc 0.921875, learning_rate 0.000100181
2017-10-11T11:16:35.430622: step 2497, loss 0.277135, acc 0.875, learning_rate 0.000100181
2017-10-11T11:16:35.551289: step 2498, loss 0.373594, acc 0.875, learning_rate 0.00010018
2017-10-11T11:16:35.661195: step 2499, loss 0.412107, acc 0.796875, learning_rate 0.000100179
2017-10-11T11:16:35.767438: step 2500, loss 0.412448, acc 0.890625, learning_rate 0.000100178
2017-10-11T11:16:35.893033: step 2501, loss 0.374707, acc 0.859375, learning_rate 0.000100178
2017-10-11T11:16:36.006190: step 2502, loss 0.301843, acc 0.859375, learning_rate 0.000100177
2017-10-11T11:16:36.114744: step 2503, loss 0.252131, acc 0.90625, learning_rate 0.000100176
2017-10-11T11:16:36.218360: step 2504, loss 0.334901, acc 0.890625, learning_rate 0.000100175
2017-10-11T11:16:36.390223: step 2505, loss 0.214695, acc 0.9375, learning_rate 0.000100175
2017-10-11T11:16:36.480479: step 2506, loss 0.32567, acc 0.875, learning_rate 0.000100174
2017-10-11T11:16:36.585829: step 2507, loss 0.184382, acc 0.984375, learning_rate 0.000100173
2017-10-11T11:16:36.670944: step 2508, loss 0.342475, acc 0.890625, learning_rate 0.000100173
2017-10-11T11:16:36.759547: step 2509, loss 0.422771, acc 0.90625, learning_rate 0.000100172
2017-10-11T11:16:36.847418: step 2510, loss 0.25607, acc 0.921875, learning_rate 0.000100171
2017-10-11T11:16:36.940521: step 2511, loss 0.327875, acc 0.875, learning_rate 0.00010017
2017-10-11T11:16:37.027434: step 2512, loss 0.252218, acc 0.90625, learning_rate 0.00010017
2017-10-11T11:16:37.124465: step 2513, loss 0.24578, acc 0.90625, learning_rate 0.000100169
2017-10-11T11:16:37.232896: step 2514, loss 0.444718, acc 0.84375, learning_rate 0.000100168
2017-10-11T11:16:37.350730: step 2515, loss 0.288063, acc 0.875, learning_rate 0.000100168
2017-10-11T11:16:37.467931: step 2516, loss 0.332596, acc 0.890625, learning_rate 0.000100167
2017-10-11T11:16:37.576165: step 2517, loss 0.376636, acc 0.90625, learning_rate 0.000100166
2017-10-11T11:16:37.687791: step 2518, loss 0.285395, acc 0.921875, learning_rate 0.000100166
2017-10-11T11:16:37.802737: step 2519, loss 0.291086, acc 0.875, learning_rate 0.000100165
2017-10-11T11:16:37.921810: step 2520, loss 0.233534, acc 0.953125, learning_rate 0.000100164

Evaluation:
2017-10-11T11:16:38.158187: step 2520, loss 0.244618, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2520

2017-10-11T11:16:38.907358: step 2521, loss 0.212098, acc 0.9375, learning_rate 0.000100164
2017-10-11T11:16:39.022752: step 2522, loss 0.462435, acc 0.8125, learning_rate 0.000100163
2017-10-11T11:16:39.140862: step 2523, loss 0.164688, acc 0.9375, learning_rate 0.000100162
2017-10-11T11:16:39.257802: step 2524, loss 0.21371, acc 0.890625, learning_rate 0.000100162
2017-10-11T11:16:39.362172: step 2525, loss 0.298128, acc 0.921875, learning_rate 0.000100161
2017-10-11T11:16:39.473228: step 2526, loss 0.205136, acc 0.953125, learning_rate 0.00010016
2017-10-11T11:16:39.584844: step 2527, loss 0.248831, acc 0.90625, learning_rate 0.00010016
2017-10-11T11:16:39.698351: step 2528, loss 0.246516, acc 0.921875, learning_rate 0.000100159
2017-10-11T11:16:39.808429: step 2529, loss 0.301038, acc 0.90625, learning_rate 0.000100158
2017-10-11T11:16:39.935692: step 2530, loss 0.349069, acc 0.875, learning_rate 0.000100158
2017-10-11T11:16:40.046214: step 2531, loss 0.386213, acc 0.84375, learning_rate 0.000100157
2017-10-11T11:16:40.159602: step 2532, loss 0.22918, acc 0.953125, learning_rate 0.000100156
2017-10-11T11:16:40.279156: step 2533, loss 0.266468, acc 0.921875, learning_rate 0.000100156
2017-10-11T11:16:40.391269: step 2534, loss 0.337463, acc 0.875, learning_rate 0.000100155
2017-10-11T11:16:40.502707: step 2535, loss 0.324048, acc 0.90625, learning_rate 0.000100155
2017-10-11T11:16:40.619802: step 2536, loss 0.211984, acc 0.921875, learning_rate 0.000100154
2017-10-11T11:16:40.736410: step 2537, loss 0.324805, acc 0.875, learning_rate 0.000100153
2017-10-11T11:16:40.844560: step 2538, loss 0.264508, acc 0.875, learning_rate 0.000100153
2017-10-11T11:16:40.954576: step 2539, loss 0.244606, acc 0.90625, learning_rate 0.000100152
2017-10-11T11:16:41.071134: step 2540, loss 0.304509, acc 0.875, learning_rate 0.000100151
2017-10-11T11:16:41.186965: step 2541, loss 0.305339, acc 0.890625, learning_rate 0.000100151
2017-10-11T11:16:41.298988: step 2542, loss 0.258826, acc 0.921875, learning_rate 0.00010015
2017-10-11T11:16:41.412287: step 2543, loss 0.341132, acc 0.890625, learning_rate 0.00010015
2017-10-11T11:16:41.590648: step 2544, loss 0.225802, acc 0.9375, learning_rate 0.000100149
2017-10-11T11:16:41.676933: step 2545, loss 0.408274, acc 0.90625, learning_rate 0.000100148
2017-10-11T11:16:41.759525: step 2546, loss 0.280768, acc 0.90625, learning_rate 0.000100148
2017-10-11T11:16:41.843132: step 2547, loss 0.265251, acc 0.890625, learning_rate 0.000100147
2017-10-11T11:16:41.914799: step 2548, loss 0.154267, acc 0.960784, learning_rate 0.000100147
2017-10-11T11:16:41.997744: step 2549, loss 0.286565, acc 0.890625, learning_rate 0.000100146
2017-10-11T11:16:42.077203: step 2550, loss 0.340466, acc 0.90625, learning_rate 0.000100145
2017-10-11T11:16:42.159453: step 2551, loss 0.400242, acc 0.828125, learning_rate 0.000100145
2017-10-11T11:16:42.251377: step 2552, loss 0.286585, acc 0.921875, learning_rate 0.000100144
2017-10-11T11:16:42.339129: step 2553, loss 0.245806, acc 0.9375, learning_rate 0.000100144
2017-10-11T11:16:42.428688: step 2554, loss 0.282126, acc 0.875, learning_rate 0.000100143
2017-10-11T11:16:42.518201: step 2555, loss 0.251503, acc 0.9375, learning_rate 0.000100142
2017-10-11T11:16:42.605953: step 2556, loss 0.323392, acc 0.9375, learning_rate 0.000100142
2017-10-11T11:16:42.692548: step 2557, loss 0.294214, acc 0.875, learning_rate 0.000100141
2017-10-11T11:16:42.782158: step 2558, loss 0.318329, acc 0.890625, learning_rate 0.000100141
2017-10-11T11:16:42.868205: step 2559, loss 0.200299, acc 0.984375, learning_rate 0.00010014
2017-10-11T11:16:42.985913: step 2560, loss 0.480278, acc 0.8125, learning_rate 0.00010014

Evaluation:
2017-10-11T11:16:43.227131: step 2560, loss 0.243677, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2560

2017-10-11T11:16:43.937617: step 2561, loss 0.293251, acc 0.90625, learning_rate 0.000100139
2017-10-11T11:16:44.046650: step 2562, loss 0.144342, acc 0.9375, learning_rate 0.000100138
2017-10-11T11:16:44.164065: step 2563, loss 0.336009, acc 0.875, learning_rate 0.000100138
2017-10-11T11:16:44.279762: step 2564, loss 0.481943, acc 0.828125, learning_rate 0.000100137
2017-10-11T11:16:44.386358: step 2565, loss 0.214855, acc 0.953125, learning_rate 0.000100137
2017-10-11T11:16:44.512477: step 2566, loss 0.47046, acc 0.890625, learning_rate 0.000100136
2017-10-11T11:16:44.629212: step 2567, loss 0.376455, acc 0.875, learning_rate 0.000100136
2017-10-11T11:16:44.747721: step 2568, loss 0.247118, acc 0.921875, learning_rate 0.000100135
2017-10-11T11:16:44.872854: step 2569, loss 0.588657, acc 0.8125, learning_rate 0.000100134
2017-10-11T11:16:44.978717: step 2570, loss 0.270823, acc 0.921875, learning_rate 0.000100134
2017-10-11T11:16:45.096243: step 2571, loss 0.124177, acc 0.96875, learning_rate 0.000100133
2017-10-11T11:16:45.214733: step 2572, loss 0.38202, acc 0.90625, learning_rate 0.000100133
2017-10-11T11:16:45.338579: step 2573, loss 0.487898, acc 0.84375, learning_rate 0.000100132
2017-10-11T11:16:45.465526: step 2574, loss 0.316531, acc 0.90625, learning_rate 0.000100132
2017-10-11T11:16:45.580299: step 2575, loss 0.182811, acc 0.984375, learning_rate 0.000100131
2017-10-11T11:16:45.700156: step 2576, loss 0.261518, acc 0.890625, learning_rate 0.000100131
2017-10-11T11:16:45.822166: step 2577, loss 0.431626, acc 0.890625, learning_rate 0.00010013
2017-10-11T11:16:45.939575: step 2578, loss 0.422221, acc 0.84375, learning_rate 0.00010013
2017-10-11T11:16:46.051719: step 2579, loss 0.323645, acc 0.921875, learning_rate 0.000100129
2017-10-11T11:16:46.170088: step 2580, loss 0.246147, acc 0.90625, learning_rate 0.000100129
2017-10-11T11:16:46.281551: step 2581, loss 0.515632, acc 0.859375, learning_rate 0.000100128
2017-10-11T11:16:46.390976: step 2582, loss 0.20988, acc 0.953125, learning_rate 0.000100128
2017-10-11T11:16:46.520870: step 2583, loss 0.43535, acc 0.859375, learning_rate 0.000100127
2017-10-11T11:16:46.636944: step 2584, loss 0.352775, acc 0.859375, learning_rate 0.000100126
2017-10-11T11:16:46.745246: step 2585, loss 0.247759, acc 0.921875, learning_rate 0.000100126
2017-10-11T11:16:46.856934: step 2586, loss 0.37645, acc 0.890625, learning_rate 0.000100125
2017-10-11T11:16:46.985256: step 2587, loss 0.180229, acc 0.96875, learning_rate 0.000100125
2017-10-11T11:16:47.101571: step 2588, loss 0.273832, acc 0.921875, learning_rate 0.000100124
2017-10-11T11:16:47.217578: step 2589, loss 0.233407, acc 0.921875, learning_rate 0.000100124
2017-10-11T11:16:47.371249: step 2590, loss 0.449309, acc 0.828125, learning_rate 0.000100123
2017-10-11T11:16:47.459946: step 2591, loss 0.37097, acc 0.875, learning_rate 0.000100123
2017-10-11T11:16:47.546738: step 2592, loss 0.260608, acc 0.9375, learning_rate 0.000100122
2017-10-11T11:16:47.637701: step 2593, loss 0.412303, acc 0.90625, learning_rate 0.000100122
2017-10-11T11:16:47.726019: step 2594, loss 0.245397, acc 0.921875, learning_rate 0.000100121
2017-10-11T11:16:47.816252: step 2595, loss 0.218646, acc 0.921875, learning_rate 0.000100121
2017-10-11T11:16:47.902704: step 2596, loss 0.325359, acc 0.890625, learning_rate 0.00010012
2017-10-11T11:16:47.994378: step 2597, loss 0.239724, acc 0.96875, learning_rate 0.00010012
2017-10-11T11:16:48.081412: step 2598, loss 0.318863, acc 0.875, learning_rate 0.000100119
2017-10-11T11:16:48.179451: step 2599, loss 0.362523, acc 0.859375, learning_rate 0.000100119
2017-10-11T11:16:48.288940: step 2600, loss 0.136756, acc 0.96875, learning_rate 0.000100118

Evaluation:
2017-10-11T11:16:48.560859: step 2600, loss 0.244031, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2600

2017-10-11T11:16:49.288396: step 2601, loss 0.361639, acc 0.90625, learning_rate 0.000100118
2017-10-11T11:16:49.397133: step 2602, loss 0.395468, acc 0.875, learning_rate 0.000100117
2017-10-11T11:16:49.507011: step 2603, loss 0.271222, acc 0.9375, learning_rate 0.000100117
2017-10-11T11:16:49.621193: step 2604, loss 0.337854, acc 0.859375, learning_rate 0.000100117
2017-10-11T11:16:49.724402: step 2605, loss 0.228445, acc 0.921875, learning_rate 0.000100116
2017-10-11T11:16:49.828834: step 2606, loss 0.251548, acc 0.921875, learning_rate 0.000100116
2017-10-11T11:16:49.944909: step 2607, loss 0.373544, acc 0.859375, learning_rate 0.000100115
2017-10-11T11:16:50.063935: step 2608, loss 0.352474, acc 0.859375, learning_rate 0.000100115
2017-10-11T11:16:50.181370: step 2609, loss 0.348006, acc 0.90625, learning_rate 0.000100114
2017-10-11T11:16:50.298147: step 2610, loss 0.201696, acc 0.921875, learning_rate 0.000100114
2017-10-11T11:16:50.403262: step 2611, loss 0.392586, acc 0.859375, learning_rate 0.000100113
2017-10-11T11:16:50.522130: step 2612, loss 0.182882, acc 0.953125, learning_rate 0.000100113
2017-10-11T11:16:50.643131: step 2613, loss 0.225787, acc 0.9375, learning_rate 0.000100112
2017-10-11T11:16:50.761106: step 2614, loss 0.207903, acc 0.9375, learning_rate 0.000100112
2017-10-11T11:16:50.873153: step 2615, loss 0.360792, acc 0.828125, learning_rate 0.000100111
2017-10-11T11:16:50.984095: step 2616, loss 0.308503, acc 0.890625, learning_rate 0.000100111
2017-10-11T11:16:51.102625: step 2617, loss 0.269549, acc 0.890625, learning_rate 0.000100111
2017-10-11T11:16:51.226534: step 2618, loss 0.194367, acc 0.90625, learning_rate 0.00010011
2017-10-11T11:16:51.340793: step 2619, loss 0.339169, acc 0.859375, learning_rate 0.00010011
2017-10-11T11:16:51.449255: step 2620, loss 0.32428, acc 0.875, learning_rate 0.000100109
2017-10-11T11:16:51.564118: step 2621, loss 0.238045, acc 0.9375, learning_rate 0.000100109
2017-10-11T11:16:51.678844: step 2622, loss 0.273108, acc 0.90625, learning_rate 0.000100108
2017-10-11T11:16:51.787537: step 2623, loss 0.375867, acc 0.84375, learning_rate 0.000100108
2017-10-11T11:16:51.900096: step 2624, loss 0.363427, acc 0.90625, learning_rate 0.000100107
2017-10-11T11:16:52.019984: step 2625, loss 0.269094, acc 0.90625, learning_rate 0.000100107
2017-10-11T11:16:52.136196: step 2626, loss 0.256443, acc 0.921875, learning_rate 0.000100107
2017-10-11T11:16:52.247939: step 2627, loss 0.296563, acc 0.921875, learning_rate 0.000100106
2017-10-11T11:16:52.368444: step 2628, loss 0.368203, acc 0.859375, learning_rate 0.000100106
2017-10-11T11:16:52.528874: step 2629, loss 0.217819, acc 0.953125, learning_rate 0.000100105
2017-10-11T11:16:52.641758: step 2630, loss 0.500765, acc 0.859375, learning_rate 0.000100105
2017-10-11T11:16:52.729649: step 2631, loss 0.312918, acc 0.90625, learning_rate 0.000100104
2017-10-11T11:16:52.829665: step 2632, loss 0.364961, acc 0.921875, learning_rate 0.000100104
2017-10-11T11:16:52.922791: step 2633, loss 0.284883, acc 0.90625, learning_rate 0.000100104
2017-10-11T11:16:53.017839: step 2634, loss 0.215805, acc 0.9375, learning_rate 0.000100103
2017-10-11T11:16:53.111430: step 2635, loss 0.2624, acc 0.953125, learning_rate 0.000100103
2017-10-11T11:16:53.206513: step 2636, loss 0.396505, acc 0.890625, learning_rate 0.000100102
2017-10-11T11:16:53.297803: step 2637, loss 0.240673, acc 0.9375, learning_rate 0.000100102
2017-10-11T11:16:53.412397: step 2638, loss 0.293869, acc 0.921875, learning_rate 0.000100101
2017-10-11T11:16:53.519932: step 2639, loss 0.297625, acc 0.90625, learning_rate 0.000100101
2017-10-11T11:16:53.630905: step 2640, loss 0.260135, acc 0.921875, learning_rate 0.000100101

Evaluation:
2017-10-11T11:16:53.860361: step 2640, loss 0.244044, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2640

2017-10-11T11:16:54.515118: step 2641, loss 0.339742, acc 0.921875, learning_rate 0.0001001
2017-10-11T11:16:54.634651: step 2642, loss 0.197987, acc 0.96875, learning_rate 0.0001001
2017-10-11T11:16:54.756854: step 2643, loss 0.306242, acc 0.921875, learning_rate 0.000100099
2017-10-11T11:16:54.876843: step 2644, loss 0.182799, acc 0.953125, learning_rate 0.000100099
2017-10-11T11:16:54.984944: step 2645, loss 0.436782, acc 0.84375, learning_rate 0.000100099
2017-10-11T11:16:55.076931: step 2646, loss 0.244682, acc 0.901961, learning_rate 0.000100098
2017-10-11T11:16:55.183703: step 2647, loss 0.302114, acc 0.875, learning_rate 0.000100098
2017-10-11T11:16:55.289099: step 2648, loss 0.157312, acc 0.96875, learning_rate 0.000100097
2017-10-11T11:16:55.398146: step 2649, loss 0.268787, acc 0.9375, learning_rate 0.000100097
2017-10-11T11:16:55.514660: step 2650, loss 0.258915, acc 0.90625, learning_rate 0.000100097
2017-10-11T11:16:55.624571: step 2651, loss 0.219986, acc 0.90625, learning_rate 0.000100096
2017-10-11T11:16:55.729612: step 2652, loss 0.271015, acc 0.90625, learning_rate 0.000100096
2017-10-11T11:16:55.838225: step 2653, loss 0.402193, acc 0.859375, learning_rate 0.000100095
2017-10-11T11:16:55.953756: step 2654, loss 0.221244, acc 0.953125, learning_rate 0.000100095
2017-10-11T11:16:56.063293: step 2655, loss 0.231466, acc 0.90625, learning_rate 0.000100095
2017-10-11T11:16:56.172544: step 2656, loss 0.328217, acc 0.890625, learning_rate 0.000100094
2017-10-11T11:16:56.288860: step 2657, loss 0.231847, acc 0.90625, learning_rate 0.000100094
2017-10-11T11:16:56.395587: step 2658, loss 0.378825, acc 0.90625, learning_rate 0.000100093
2017-10-11T11:16:56.501417: step 2659, loss 0.139894, acc 0.96875, learning_rate 0.000100093
2017-10-11T11:16:56.615589: step 2660, loss 0.238992, acc 0.921875, learning_rate 0.000100093
2017-10-11T11:16:56.729036: step 2661, loss 0.304367, acc 0.859375, learning_rate 0.000100092
2017-10-11T11:16:56.844879: step 2662, loss 0.398722, acc 0.921875, learning_rate 0.000100092
2017-10-11T11:16:56.956875: step 2663, loss 0.262901, acc 0.921875, learning_rate 0.000100092
2017-10-11T11:16:57.068885: step 2664, loss 0.467836, acc 0.859375, learning_rate 0.000100091
2017-10-11T11:16:57.188922: step 2665, loss 0.279823, acc 0.9375, learning_rate 0.000100091
2017-10-11T11:16:57.290903: step 2666, loss 0.456713, acc 0.875, learning_rate 0.00010009
2017-10-11T11:16:57.396876: step 2667, loss 0.331264, acc 0.875, learning_rate 0.00010009
2017-10-11T11:16:57.507856: step 2668, loss 0.396173, acc 0.890625, learning_rate 0.00010009
2017-10-11T11:16:57.613391: step 2669, loss 0.306049, acc 0.875, learning_rate 0.000100089
2017-10-11T11:16:57.806470: step 2670, loss 0.402924, acc 0.8125, learning_rate 0.000100089
2017-10-11T11:16:57.891524: step 2671, loss 0.405232, acc 0.84375, learning_rate 0.000100089
2017-10-11T11:16:57.982776: step 2672, loss 0.298885, acc 0.921875, learning_rate 0.000100088
2017-10-11T11:16:58.069167: step 2673, loss 0.244674, acc 0.921875, learning_rate 0.000100088
2017-10-11T11:16:58.151944: step 2674, loss 0.291803, acc 0.921875, learning_rate 0.000100088
2017-10-11T11:16:58.237725: step 2675, loss 0.194098, acc 0.96875, learning_rate 0.000100087
2017-10-11T11:16:58.323611: step 2676, loss 0.342103, acc 0.921875, learning_rate 0.000100087
2017-10-11T11:16:58.409537: step 2677, loss 0.245253, acc 0.890625, learning_rate 0.000100086
2017-10-11T11:16:58.514715: step 2678, loss 0.283744, acc 0.90625, learning_rate 0.000100086
2017-10-11T11:16:58.637482: step 2679, loss 0.166834, acc 0.921875, learning_rate 0.000100086
2017-10-11T11:16:58.754609: step 2680, loss 0.271802, acc 0.890625, learning_rate 0.000100085

Evaluation:
2017-10-11T11:16:58.985480: step 2680, loss 0.243451, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2680

2017-10-11T11:16:59.736428: step 2681, loss 0.226824, acc 0.9375, learning_rate 0.000100085
2017-10-11T11:16:59.853089: step 2682, loss 0.192922, acc 0.921875, learning_rate 0.000100085
2017-10-11T11:16:59.970164: step 2683, loss 0.172907, acc 0.953125, learning_rate 0.000100084
2017-10-11T11:17:00.087752: step 2684, loss 0.393921, acc 0.875, learning_rate 0.000100084
2017-10-11T11:17:00.204914: step 2685, loss 0.528268, acc 0.828125, learning_rate 0.000100084
2017-10-11T11:17:00.315454: step 2686, loss 0.358765, acc 0.875, learning_rate 0.000100083
2017-10-11T11:17:00.430378: step 2687, loss 0.185856, acc 0.96875, learning_rate 0.000100083
2017-10-11T11:17:00.541083: step 2688, loss 0.217801, acc 0.90625, learning_rate 0.000100083
2017-10-11T11:17:00.668851: step 2689, loss 0.157252, acc 0.984375, learning_rate 0.000100082
2017-10-11T11:17:00.777914: step 2690, loss 0.318696, acc 0.90625, learning_rate 0.000100082
2017-10-11T11:17:00.888617: step 2691, loss 0.233688, acc 0.90625, learning_rate 0.000100082
2017-10-11T11:17:01.008892: step 2692, loss 0.229938, acc 0.921875, learning_rate 0.000100081
2017-10-11T11:17:01.126373: step 2693, loss 0.502128, acc 0.8125, learning_rate 0.000100081
2017-10-11T11:17:01.239403: step 2694, loss 0.338271, acc 0.859375, learning_rate 0.000100081
2017-10-11T11:17:01.350783: step 2695, loss 0.402343, acc 0.890625, learning_rate 0.00010008
2017-10-11T11:17:01.470558: step 2696, loss 0.245836, acc 0.90625, learning_rate 0.00010008
2017-10-11T11:17:01.587859: step 2697, loss 0.379888, acc 0.828125, learning_rate 0.00010008
2017-10-11T11:17:01.701627: step 2698, loss 0.427747, acc 0.828125, learning_rate 0.000100079
2017-10-11T11:17:01.818866: step 2699, loss 0.213226, acc 0.9375, learning_rate 0.000100079
2017-10-11T11:17:01.938142: step 2700, loss 0.270819, acc 0.9375, learning_rate 0.000100079
2017-10-11T11:17:02.061786: step 2701, loss 0.261189, acc 0.953125, learning_rate 0.000100078
2017-10-11T11:17:02.170169: step 2702, loss 0.357843, acc 0.890625, learning_rate 0.000100078
2017-10-11T11:17:02.288959: step 2703, loss 0.35766, acc 0.859375, learning_rate 0.000100078
2017-10-11T11:17:02.404471: step 2704, loss 0.194385, acc 0.953125, learning_rate 0.000100077
2017-10-11T11:17:02.525488: step 2705, loss 0.316436, acc 0.859375, learning_rate 0.000100077
2017-10-11T11:17:02.644806: step 2706, loss 0.439549, acc 0.84375, learning_rate 0.000100077
2017-10-11T11:17:02.755040: step 2707, loss 0.331541, acc 0.875, learning_rate 0.000100076
2017-10-11T11:17:02.917140: step 2708, loss 0.252068, acc 0.890625, learning_rate 0.000100076
2017-10-11T11:17:03.006519: step 2709, loss 0.335198, acc 0.875, learning_rate 0.000100076
2017-10-11T11:17:03.090398: step 2710, loss 0.17173, acc 0.953125, learning_rate 0.000100076
2017-10-11T11:17:03.174587: step 2711, loss 0.316684, acc 0.90625, learning_rate 0.000100075
2017-10-11T11:17:03.266925: step 2712, loss 0.221461, acc 0.921875, learning_rate 0.000100075
2017-10-11T11:17:03.355333: step 2713, loss 0.314939, acc 0.890625, learning_rate 0.000100075
2017-10-11T11:17:03.440387: step 2714, loss 0.328095, acc 0.890625, learning_rate 0.000100074
2017-10-11T11:17:03.525544: step 2715, loss 0.168454, acc 0.953125, learning_rate 0.000100074
2017-10-11T11:17:03.614118: step 2716, loss 0.25562, acc 0.90625, learning_rate 0.000100074
2017-10-11T11:17:03.738344: step 2717, loss 0.315315, acc 0.890625, learning_rate 0.000100073
2017-10-11T11:17:03.861458: step 2718, loss 0.157796, acc 0.921875, learning_rate 0.000100073
2017-10-11T11:17:03.982705: step 2719, loss 0.415483, acc 0.828125, learning_rate 0.000100073
2017-10-11T11:17:04.103562: step 2720, loss 0.252931, acc 0.90625, learning_rate 0.000100073

Evaluation:
2017-10-11T11:17:04.360687: step 2720, loss 0.242827, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2720

2017-10-11T11:17:05.009270: step 2721, loss 0.299394, acc 0.890625, learning_rate 0.000100072
2017-10-11T11:17:05.127020: step 2722, loss 0.363612, acc 0.890625, learning_rate 0.000100072
2017-10-11T11:17:05.247822: step 2723, loss 0.202609, acc 0.9375, learning_rate 0.000100072
2017-10-11T11:17:05.365160: step 2724, loss 0.202426, acc 0.953125, learning_rate 0.000100071
2017-10-11T11:17:05.485006: step 2725, loss 0.271541, acc 0.90625, learning_rate 0.000100071
2017-10-11T11:17:05.603048: step 2726, loss 0.175957, acc 0.984375, learning_rate 0.000100071
2017-10-11T11:17:05.713696: step 2727, loss 0.178117, acc 0.953125, learning_rate 0.00010007
2017-10-11T11:17:05.842385: step 2728, loss 0.136267, acc 0.96875, learning_rate 0.00010007
2017-10-11T11:17:05.956521: step 2729, loss 0.256034, acc 0.921875, learning_rate 0.00010007
2017-10-11T11:17:06.074843: step 2730, loss 0.487409, acc 0.8125, learning_rate 0.00010007
2017-10-11T11:17:06.189677: step 2731, loss 0.213938, acc 0.921875, learning_rate 0.000100069
2017-10-11T11:17:06.303694: step 2732, loss 0.167373, acc 0.96875, learning_rate 0.000100069
2017-10-11T11:17:06.418583: step 2733, loss 0.247783, acc 0.90625, learning_rate 0.000100069
2017-10-11T11:17:06.544865: step 2734, loss 0.287342, acc 0.890625, learning_rate 0.000100068
2017-10-11T11:17:06.651955: step 2735, loss 0.404002, acc 0.859375, learning_rate 0.000100068
2017-10-11T11:17:06.761615: step 2736, loss 0.358818, acc 0.875, learning_rate 0.000100068
2017-10-11T11:17:06.871612: step 2737, loss 0.386367, acc 0.875, learning_rate 0.000100068
2017-10-11T11:17:06.986134: step 2738, loss 0.404723, acc 0.84375, learning_rate 0.000100067
2017-10-11T11:17:07.097343: step 2739, loss 0.483596, acc 0.78125, learning_rate 0.000100067
2017-10-11T11:17:07.205194: step 2740, loss 0.280385, acc 0.90625, learning_rate 0.000100067
2017-10-11T11:17:07.316145: step 2741, loss 0.306192, acc 0.921875, learning_rate 0.000100067
2017-10-11T11:17:07.428865: step 2742, loss 0.277596, acc 0.921875, learning_rate 0.000100066
2017-10-11T11:17:07.546966: step 2743, loss 0.218011, acc 0.953125, learning_rate 0.000100066
2017-10-11T11:17:07.642030: step 2744, loss 0.270089, acc 0.901961, learning_rate 0.000100066
2017-10-11T11:17:07.759762: step 2745, loss 0.249255, acc 0.921875, learning_rate 0.000100065
2017-10-11T11:17:07.865025: step 2746, loss 0.407996, acc 0.796875, learning_rate 0.000100065
2017-10-11T11:17:07.979801: step 2747, loss 0.350629, acc 0.875, learning_rate 0.000100065
2017-10-11T11:17:08.165643: step 2748, loss 0.298975, acc 0.890625, learning_rate 0.000100065
2017-10-11T11:17:08.254543: step 2749, loss 0.203854, acc 0.953125, learning_rate 0.000100064
2017-10-11T11:17:08.339198: step 2750, loss 0.203831, acc 0.9375, learning_rate 0.000100064
2017-10-11T11:17:08.428756: step 2751, loss 0.415579, acc 0.90625, learning_rate 0.000100064
2017-10-11T11:17:08.524774: step 2752, loss 0.316219, acc 0.90625, learning_rate 0.000100064
2017-10-11T11:17:08.612590: step 2753, loss 0.369942, acc 0.890625, learning_rate 0.000100063
2017-10-11T11:17:08.697659: step 2754, loss 0.308641, acc 0.921875, learning_rate 0.000100063
2017-10-11T11:17:08.784372: step 2755, loss 0.411519, acc 0.828125, learning_rate 0.000100063
2017-10-11T11:17:08.873704: step 2756, loss 0.434622, acc 0.84375, learning_rate 0.000100063
2017-10-11T11:17:08.961559: step 2757, loss 0.199716, acc 0.96875, learning_rate 0.000100062
2017-10-11T11:17:09.048892: step 2758, loss 0.363533, acc 0.921875, learning_rate 0.000100062
2017-10-11T11:17:09.161385: step 2759, loss 0.52674, acc 0.859375, learning_rate 0.000100062
2017-10-11T11:17:09.284473: step 2760, loss 0.259617, acc 0.90625, learning_rate 0.000100062

Evaluation:
2017-10-11T11:17:09.537506: step 2760, loss 0.242046, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2760

2017-10-11T11:17:10.278725: step 2761, loss 0.243688, acc 0.90625, learning_rate 0.000100061
2017-10-11T11:17:10.387531: step 2762, loss 0.320841, acc 0.921875, learning_rate 0.000100061
2017-10-11T11:17:10.501490: step 2763, loss 0.311253, acc 0.890625, learning_rate 0.000100061
2017-10-11T11:17:10.625838: step 2764, loss 0.290187, acc 0.953125, learning_rate 0.000100061
2017-10-11T11:17:10.738445: step 2765, loss 0.235371, acc 0.90625, learning_rate 0.00010006
2017-10-11T11:17:10.861846: step 2766, loss 0.279456, acc 0.90625, learning_rate 0.00010006
2017-10-11T11:17:10.978057: step 2767, loss 0.307358, acc 0.90625, learning_rate 0.00010006
2017-10-11T11:17:11.092266: step 2768, loss 0.227554, acc 0.9375, learning_rate 0.00010006
2017-10-11T11:17:11.208156: step 2769, loss 0.271524, acc 0.921875, learning_rate 0.000100059
2017-10-11T11:17:11.320541: step 2770, loss 0.24632, acc 0.9375, learning_rate 0.000100059
2017-10-11T11:17:11.432569: step 2771, loss 0.360646, acc 0.8125, learning_rate 0.000100059
2017-10-11T11:17:11.544068: step 2772, loss 0.200907, acc 0.953125, learning_rate 0.000100059
2017-10-11T11:17:11.655778: step 2773, loss 0.270844, acc 0.875, learning_rate 0.000100058
2017-10-11T11:17:11.783588: step 2774, loss 0.411625, acc 0.84375, learning_rate 0.000100058
2017-10-11T11:17:11.908792: step 2775, loss 0.21509, acc 0.96875, learning_rate 0.000100058
2017-10-11T11:17:12.024055: step 2776, loss 0.365067, acc 0.8125, learning_rate 0.000100058
2017-10-11T11:17:12.142544: step 2777, loss 0.191665, acc 0.9375, learning_rate 0.000100057
2017-10-11T11:17:12.252775: step 2778, loss 0.22646, acc 0.9375, learning_rate 0.000100057
2017-10-11T11:17:12.372053: step 2779, loss 0.211167, acc 0.921875, learning_rate 0.000100057
2017-10-11T11:17:12.486947: step 2780, loss 0.296556, acc 0.90625, learning_rate 0.000100057
2017-10-11T11:17:12.602782: step 2781, loss 0.329394, acc 0.859375, learning_rate 0.000100056
2017-10-11T11:17:12.719451: step 2782, loss 0.38528, acc 0.84375, learning_rate 0.000100056
2017-10-11T11:17:12.840877: step 2783, loss 0.280915, acc 0.90625, learning_rate 0.000100056
2017-10-11T11:17:12.955903: step 2784, loss 0.204978, acc 0.921875, learning_rate 0.000100056
2017-10-11T11:17:13.076602: step 2785, loss 0.224118, acc 0.9375, learning_rate 0.000100056
2017-10-11T11:17:13.189592: step 2786, loss 0.421906, acc 0.859375, learning_rate 0.000100055
2017-10-11T11:17:13.288237: step 2787, loss 0.210351, acc 0.9375, learning_rate 0.000100055
2017-10-11T11:17:13.476632: step 2788, loss 0.326171, acc 0.875, learning_rate 0.000100055
2017-10-11T11:17:13.564555: step 2789, loss 0.317279, acc 0.890625, learning_rate 0.000100055
2017-10-11T11:17:13.663466: step 2790, loss 0.372265, acc 0.828125, learning_rate 0.000100054
2017-10-11T11:17:13.761125: step 2791, loss 0.28437, acc 0.890625, learning_rate 0.000100054
2017-10-11T11:17:13.848001: step 2792, loss 0.184412, acc 0.96875, learning_rate 0.000100054
2017-10-11T11:17:13.936556: step 2793, loss 0.427726, acc 0.84375, learning_rate 0.000100054
2017-10-11T11:17:14.032129: step 2794, loss 0.253608, acc 0.9375, learning_rate 0.000100054
2017-10-11T11:17:14.121483: step 2795, loss 0.355339, acc 0.90625, learning_rate 0.000100053
2017-10-11T11:17:14.245559: step 2796, loss 0.352136, acc 0.875, learning_rate 0.000100053
2017-10-11T11:17:14.368609: step 2797, loss 0.238836, acc 0.90625, learning_rate 0.000100053
2017-10-11T11:17:14.485082: step 2798, loss 0.343758, acc 0.890625, learning_rate 0.000100053
2017-10-11T11:17:14.600818: step 2799, loss 0.292909, acc 0.921875, learning_rate 0.000100052
2017-10-11T11:17:14.716098: step 2800, loss 0.250136, acc 0.921875, learning_rate 0.000100052

Evaluation:
2017-10-11T11:17:14.969292: step 2800, loss 0.241533, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2800

2017-10-11T11:17:15.676251: step 2801, loss 0.206663, acc 0.90625, learning_rate 0.000100052
2017-10-11T11:17:15.784412: step 2802, loss 0.27102, acc 0.90625, learning_rate 0.000100052
2017-10-11T11:17:15.894752: step 2803, loss 0.272499, acc 0.90625, learning_rate 0.000100052
2017-10-11T11:17:16.009044: step 2804, loss 0.169706, acc 0.9375, learning_rate 0.000100051
2017-10-11T11:17:16.124168: step 2805, loss 0.301974, acc 0.921875, learning_rate 0.000100051
2017-10-11T11:17:16.240844: step 2806, loss 0.409249, acc 0.8125, learning_rate 0.000100051
2017-10-11T11:17:16.346037: step 2807, loss 0.416177, acc 0.921875, learning_rate 0.000100051
2017-10-11T11:17:16.465276: step 2808, loss 0.289594, acc 0.90625, learning_rate 0.000100051
2017-10-11T11:17:16.594369: step 2809, loss 0.336523, acc 0.890625, learning_rate 0.00010005
2017-10-11T11:17:16.707969: step 2810, loss 0.339241, acc 0.875, learning_rate 0.00010005
2017-10-11T11:17:16.827887: step 2811, loss 0.371044, acc 0.921875, learning_rate 0.00010005
2017-10-11T11:17:16.945059: step 2812, loss 0.249157, acc 0.953125, learning_rate 0.00010005
2017-10-11T11:17:17.065358: step 2813, loss 0.275041, acc 0.890625, learning_rate 0.00010005
2017-10-11T11:17:17.188567: step 2814, loss 0.307427, acc 0.890625, learning_rate 0.000100049
2017-10-11T11:17:17.301921: step 2815, loss 0.182724, acc 0.9375, learning_rate 0.000100049
2017-10-11T11:17:17.417567: step 2816, loss 0.1753, acc 0.9375, learning_rate 0.000100049
2017-10-11T11:17:17.534676: step 2817, loss 0.211694, acc 0.9375, learning_rate 0.000100049
2017-10-11T11:17:17.651092: step 2818, loss 0.404383, acc 0.859375, learning_rate 0.000100049
2017-10-11T11:17:17.768353: step 2819, loss 0.190033, acc 0.9375, learning_rate 0.000100048
2017-10-11T11:17:17.892958: step 2820, loss 0.253279, acc 0.90625, learning_rate 0.000100048
2017-10-11T11:17:18.003997: step 2821, loss 0.563971, acc 0.875, learning_rate 0.000100048
2017-10-11T11:17:18.132911: step 2822, loss 0.382288, acc 0.84375, learning_rate 0.000100048
2017-10-11T11:17:18.242193: step 2823, loss 0.194533, acc 0.921875, learning_rate 0.000100048
2017-10-11T11:17:18.349500: step 2824, loss 0.256772, acc 0.890625, learning_rate 0.000100047
2017-10-11T11:17:18.464867: step 2825, loss 0.415256, acc 0.90625, learning_rate 0.000100047
2017-10-11T11:17:18.644238: step 2826, loss 0.38074, acc 0.890625, learning_rate 0.000100047
2017-10-11T11:17:18.729018: step 2827, loss 0.251476, acc 0.90625, learning_rate 0.000100047
2017-10-11T11:17:18.818749: step 2828, loss 0.223744, acc 0.90625, learning_rate 0.000100047
2017-10-11T11:17:18.909644: step 2829, loss 0.354057, acc 0.84375, learning_rate 0.000100046
2017-10-11T11:17:18.997906: step 2830, loss 0.206664, acc 0.90625, learning_rate 0.000100046
2017-10-11T11:17:19.088911: step 2831, loss 0.29553, acc 0.875, learning_rate 0.000100046
2017-10-11T11:17:19.187508: step 2832, loss 0.283268, acc 0.9375, learning_rate 0.000100046
2017-10-11T11:17:19.272672: step 2833, loss 0.399184, acc 0.859375, learning_rate 0.000100046
2017-10-11T11:17:19.358918: step 2834, loss 0.362656, acc 0.90625, learning_rate 0.000100045
2017-10-11T11:17:19.444810: step 2835, loss 0.256471, acc 0.90625, learning_rate 0.000100045
2017-10-11T11:17:19.538934: step 2836, loss 0.342466, acc 0.890625, learning_rate 0.000100045
2017-10-11T11:17:19.656220: step 2837, loss 0.346298, acc 0.859375, learning_rate 0.000100045
2017-10-11T11:17:19.780312: step 2838, loss 0.242914, acc 0.921875, learning_rate 0.000100045
2017-10-11T11:17:19.899756: step 2839, loss 0.186051, acc 0.9375, learning_rate 0.000100045
2017-10-11T11:17:20.014298: step 2840, loss 0.212449, acc 0.9375, learning_rate 0.000100044

Evaluation:
2017-10-11T11:17:20.259872: step 2840, loss 0.240731, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2840

2017-10-11T11:17:20.994459: step 2841, loss 0.215075, acc 0.921875, learning_rate 0.000100044
2017-10-11T11:17:21.093037: step 2842, loss 0.320258, acc 0.882353, learning_rate 0.000100044
2017-10-11T11:17:21.215864: step 2843, loss 0.247876, acc 0.921875, learning_rate 0.000100044
2017-10-11T11:17:21.324854: step 2844, loss 0.252452, acc 0.953125, learning_rate 0.000100044
2017-10-11T11:17:21.448323: step 2845, loss 0.414069, acc 0.9375, learning_rate 0.000100043
2017-10-11T11:17:21.567129: step 2846, loss 0.15991, acc 0.953125, learning_rate 0.000100043
2017-10-11T11:17:21.680220: step 2847, loss 0.340881, acc 0.90625, learning_rate 0.000100043
2017-10-11T11:17:21.789951: step 2848, loss 0.25318, acc 0.890625, learning_rate 0.000100043
2017-10-11T11:17:21.906486: step 2849, loss 0.270204, acc 0.875, learning_rate 0.000100043
2017-10-11T11:17:22.030752: step 2850, loss 0.376837, acc 0.890625, learning_rate 0.000100043
2017-10-11T11:17:22.141863: step 2851, loss 0.345452, acc 0.875, learning_rate 0.000100042
2017-10-11T11:17:22.266459: step 2852, loss 0.229075, acc 0.921875, learning_rate 0.000100042
2017-10-11T11:17:22.394141: step 2853, loss 0.336751, acc 0.890625, learning_rate 0.000100042
2017-10-11T11:17:22.512979: step 2854, loss 0.178903, acc 0.96875, learning_rate 0.000100042
2017-10-11T11:17:22.631901: step 2855, loss 0.418154, acc 0.828125, learning_rate 0.000100042
2017-10-11T11:17:22.743722: step 2856, loss 0.267144, acc 0.921875, learning_rate 0.000100042
2017-10-11T11:17:22.859608: step 2857, loss 0.318469, acc 0.859375, learning_rate 0.000100041
2017-10-11T11:17:22.976767: step 2858, loss 0.194633, acc 0.90625, learning_rate 0.000100041
2017-10-11T11:17:23.092118: step 2859, loss 0.451164, acc 0.8125, learning_rate 0.000100041
2017-10-11T11:17:23.214448: step 2860, loss 0.247133, acc 0.9375, learning_rate 0.000100041
2017-10-11T11:17:23.336776: step 2861, loss 0.229393, acc 0.921875, learning_rate 0.000100041
2017-10-11T11:17:23.445741: step 2862, loss 0.28501, acc 0.875, learning_rate 0.000100041
2017-10-11T11:17:23.565212: step 2863, loss 0.359468, acc 0.9375, learning_rate 0.00010004
2017-10-11T11:17:23.683878: step 2864, loss 0.39001, acc 0.875, learning_rate 0.00010004
2017-10-11T11:17:23.798331: step 2865, loss 0.128126, acc 0.96875, learning_rate 0.00010004
2017-10-11T11:17:23.979050: step 2866, loss 0.226449, acc 0.9375, learning_rate 0.00010004
2017-10-11T11:17:24.065821: step 2867, loss 0.167154, acc 0.96875, learning_rate 0.00010004
2017-10-11T11:17:24.153137: step 2868, loss 0.2509, acc 0.9375, learning_rate 0.00010004
2017-10-11T11:17:24.240720: step 2869, loss 0.329659, acc 0.875, learning_rate 0.000100039
2017-10-11T11:17:24.326763: step 2870, loss 0.235329, acc 0.921875, learning_rate 0.000100039
2017-10-11T11:17:24.419237: step 2871, loss 0.248472, acc 0.953125, learning_rate 0.000100039
2017-10-11T11:17:24.509497: step 2872, loss 0.319072, acc 0.9375, learning_rate 0.000100039
2017-10-11T11:17:24.598066: step 2873, loss 0.291519, acc 0.890625, learning_rate 0.000100039
2017-10-11T11:17:24.687574: step 2874, loss 0.301889, acc 0.890625, learning_rate 0.000100039
2017-10-11T11:17:24.777587: step 2875, loss 0.207585, acc 0.9375, learning_rate 0.000100038
2017-10-11T11:17:24.863743: step 2876, loss 0.152742, acc 0.953125, learning_rate 0.000100038
2017-10-11T11:17:24.968902: step 2877, loss 0.299761, acc 0.890625, learning_rate 0.000100038
2017-10-11T11:17:25.094277: step 2878, loss 0.184442, acc 0.953125, learning_rate 0.000100038
2017-10-11T11:17:25.202900: step 2879, loss 0.361871, acc 0.875, learning_rate 0.000100038
2017-10-11T11:17:25.324576: step 2880, loss 0.31181, acc 0.890625, learning_rate 0.000100038

Evaluation:
2017-10-11T11:17:25.549665: step 2880, loss 0.241204, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2880

2017-10-11T11:17:26.290769: step 2881, loss 0.303649, acc 0.90625, learning_rate 0.000100038
2017-10-11T11:17:26.406746: step 2882, loss 0.290539, acc 0.90625, learning_rate 0.000100037
2017-10-11T11:17:26.519896: step 2883, loss 0.283611, acc 0.890625, learning_rate 0.000100037
2017-10-11T11:17:26.634170: step 2884, loss 0.256388, acc 0.90625, learning_rate 0.000100037
2017-10-11T11:17:26.748864: step 2885, loss 0.260247, acc 0.921875, learning_rate 0.000100037
2017-10-11T11:17:26.863754: step 2886, loss 0.387144, acc 0.828125, learning_rate 0.000100037
2017-10-11T11:17:26.984875: step 2887, loss 0.275908, acc 0.90625, learning_rate 0.000100037
2017-10-11T11:17:27.094901: step 2888, loss 0.352599, acc 0.875, learning_rate 0.000100036
2017-10-11T11:17:27.211568: step 2889, loss 0.276923, acc 0.859375, learning_rate 0.000100036
2017-10-11T11:17:27.331027: step 2890, loss 0.291803, acc 0.90625, learning_rate 0.000100036
2017-10-11T11:17:27.448812: step 2891, loss 0.387446, acc 0.84375, learning_rate 0.000100036
2017-10-11T11:17:27.575795: step 2892, loss 0.31855, acc 0.921875, learning_rate 0.000100036
2017-10-11T11:17:27.684546: step 2893, loss 0.297909, acc 0.90625, learning_rate 0.000100036
2017-10-11T11:17:27.795740: step 2894, loss 0.288918, acc 0.921875, learning_rate 0.000100036
2017-10-11T11:17:27.916243: step 2895, loss 0.308801, acc 0.9375, learning_rate 0.000100035
2017-10-11T11:17:28.033964: step 2896, loss 0.46665, acc 0.84375, learning_rate 0.000100035
2017-10-11T11:17:28.157252: step 2897, loss 0.259872, acc 0.90625, learning_rate 0.000100035
2017-10-11T11:17:28.280761: step 2898, loss 0.282166, acc 0.890625, learning_rate 0.000100035
2017-10-11T11:17:28.390362: step 2899, loss 0.325473, acc 0.859375, learning_rate 0.000100035
2017-10-11T11:17:28.501466: step 2900, loss 0.307976, acc 0.921875, learning_rate 0.000100035
2017-10-11T11:17:28.631134: step 2901, loss 0.235929, acc 0.921875, learning_rate 0.000100035
2017-10-11T11:17:28.738123: step 2902, loss 0.229365, acc 0.921875, learning_rate 0.000100034
2017-10-11T11:17:28.850581: step 2903, loss 0.317836, acc 0.921875, learning_rate 0.000100034
2017-10-11T11:17:28.971232: step 2904, loss 0.241139, acc 0.9375, learning_rate 0.000100034
2017-10-11T11:17:29.095576: step 2905, loss 0.210638, acc 0.921875, learning_rate 0.000100034
2017-10-11T11:17:29.288882: step 2906, loss 0.424789, acc 0.890625, learning_rate 0.000100034
2017-10-11T11:17:29.377389: step 2907, loss 0.375621, acc 0.859375, learning_rate 0.000100034
2017-10-11T11:17:29.466184: step 2908, loss 0.226261, acc 0.90625, learning_rate 0.000100034
2017-10-11T11:17:29.552244: step 2909, loss 0.461853, acc 0.875, learning_rate 0.000100033
2017-10-11T11:17:29.640914: step 2910, loss 0.221241, acc 0.9375, learning_rate 0.000100033
2017-10-11T11:17:29.730844: step 2911, loss 0.262533, acc 0.921875, learning_rate 0.000100033
2017-10-11T11:17:29.820116: step 2912, loss 0.249479, acc 0.921875, learning_rate 0.000100033
2017-10-11T11:17:29.908965: step 2913, loss 0.364768, acc 0.859375, learning_rate 0.000100033
2017-10-11T11:17:30.020129: step 2914, loss 0.194905, acc 0.96875, learning_rate 0.000100033
2017-10-11T11:17:30.133475: step 2915, loss 0.347717, acc 0.875, learning_rate 0.000100033
2017-10-11T11:17:30.255976: step 2916, loss 0.215025, acc 0.953125, learning_rate 0.000100033
2017-10-11T11:17:30.371444: step 2917, loss 0.193299, acc 0.9375, learning_rate 0.000100032
2017-10-11T11:17:30.486480: step 2918, loss 0.177, acc 0.96875, learning_rate 0.000100032
2017-10-11T11:17:30.610858: step 2919, loss 0.37929, acc 0.84375, learning_rate 0.000100032
2017-10-11T11:17:30.727489: step 2920, loss 0.359001, acc 0.890625, learning_rate 0.000100032

Evaluation:
2017-10-11T11:17:30.971518: step 2920, loss 0.24124, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2920

2017-10-11T11:17:31.634639: step 2921, loss 0.212797, acc 0.921875, learning_rate 0.000100032
2017-10-11T11:17:31.762123: step 2922, loss 0.336197, acc 0.875, learning_rate 0.000100032
2017-10-11T11:17:31.870902: step 2923, loss 0.228617, acc 0.921875, learning_rate 0.000100032
2017-10-11T11:17:31.984608: step 2924, loss 0.115678, acc 0.984375, learning_rate 0.000100031
2017-10-11T11:17:32.105180: step 2925, loss 0.314598, acc 0.90625, learning_rate 0.000100031
2017-10-11T11:17:32.208985: step 2926, loss 0.409244, acc 0.875, learning_rate 0.000100031
2017-10-11T11:17:32.319238: step 2927, loss 0.22996, acc 0.9375, learning_rate 0.000100031
2017-10-11T11:17:32.435611: step 2928, loss 0.35949, acc 0.875, learning_rate 0.000100031
2017-10-11T11:17:32.539853: step 2929, loss 0.324322, acc 0.875, learning_rate 0.000100031
2017-10-11T11:17:32.673088: step 2930, loss 0.540544, acc 0.875, learning_rate 0.000100031
2017-10-11T11:17:32.786131: step 2931, loss 0.394107, acc 0.84375, learning_rate 0.000100031
2017-10-11T11:17:32.908404: step 2932, loss 0.412972, acc 0.890625, learning_rate 0.00010003
2017-10-11T11:17:33.005609: step 2933, loss 0.319084, acc 0.875, learning_rate 0.00010003
2017-10-11T11:17:33.115052: step 2934, loss 0.265199, acc 0.90625, learning_rate 0.00010003
2017-10-11T11:17:33.228804: step 2935, loss 0.372547, acc 0.859375, learning_rate 0.00010003
2017-10-11T11:17:33.335335: step 2936, loss 0.294857, acc 0.90625, learning_rate 0.00010003
2017-10-11T11:17:33.453013: step 2937, loss 0.353914, acc 0.84375, learning_rate 0.00010003
2017-10-11T11:17:33.570368: step 2938, loss 0.189376, acc 0.953125, learning_rate 0.00010003
2017-10-11T11:17:33.687402: step 2939, loss 0.404661, acc 0.890625, learning_rate 0.00010003
2017-10-11T11:17:33.792914: step 2940, loss 0.181982, acc 0.960784, learning_rate 0.000100029
2017-10-11T11:17:33.912055: step 2941, loss 0.168836, acc 0.96875, learning_rate 0.000100029
2017-10-11T11:17:34.025665: step 2942, loss 0.409112, acc 0.90625, learning_rate 0.000100029
2017-10-11T11:17:34.145198: step 2943, loss 0.530377, acc 0.8125, learning_rate 0.000100029
2017-10-11T11:17:34.264928: step 2944, loss 0.288266, acc 0.859375, learning_rate 0.000100029
2017-10-11T11:17:34.424870: step 2945, loss 0.271468, acc 0.875, learning_rate 0.000100029
2017-10-11T11:17:34.532192: step 2946, loss 0.258921, acc 0.9375, learning_rate 0.000100029
2017-10-11T11:17:34.628088: step 2947, loss 0.226048, acc 0.9375, learning_rate 0.000100029
2017-10-11T11:17:34.715264: step 2948, loss 0.260154, acc 0.921875, learning_rate 0.000100029
2017-10-11T11:17:34.799749: step 2949, loss 0.3952, acc 0.859375, learning_rate 0.000100028
2017-10-11T11:17:34.883960: step 2950, loss 0.394906, acc 0.828125, learning_rate 0.000100028
2017-10-11T11:17:34.968666: step 2951, loss 0.451789, acc 0.859375, learning_rate 0.000100028
2017-10-11T11:17:35.054567: step 2952, loss 0.329024, acc 0.84375, learning_rate 0.000100028
2017-10-11T11:17:35.140249: step 2953, loss 0.431449, acc 0.859375, learning_rate 0.000100028
2017-10-11T11:17:35.228364: step 2954, loss 0.237975, acc 0.90625, learning_rate 0.000100028
2017-10-11T11:17:35.324088: step 2955, loss 0.208944, acc 0.9375, learning_rate 0.000100028
2017-10-11T11:17:35.451977: step 2956, loss 0.329477, acc 0.90625, learning_rate 0.000100028
2017-10-11T11:17:35.572714: step 2957, loss 0.343446, acc 0.90625, learning_rate 0.000100028
2017-10-11T11:17:35.688530: step 2958, loss 0.253968, acc 0.921875, learning_rate 0.000100027
2017-10-11T11:17:35.812554: step 2959, loss 0.298622, acc 0.9375, learning_rate 0.000100027
2017-10-11T11:17:35.921781: step 2960, loss 0.254467, acc 0.9375, learning_rate 0.000100027

Evaluation:
2017-10-11T11:17:36.152362: step 2960, loss 0.240341, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-2960

2017-10-11T11:17:37.276279: step 2961, loss 0.291524, acc 0.90625, learning_rate 0.000100027
2017-10-11T11:17:37.392778: step 2962, loss 0.376225, acc 0.90625, learning_rate 0.000100027
2017-10-11T11:17:37.508928: step 2963, loss 0.239415, acc 0.90625, learning_rate 0.000100027
2017-10-11T11:17:37.624324: step 2964, loss 0.239267, acc 0.90625, learning_rate 0.000100027
2017-10-11T11:17:37.746699: step 2965, loss 0.260386, acc 0.9375, learning_rate 0.000100027
2017-10-11T11:17:37.854950: step 2966, loss 0.30339, acc 0.890625, learning_rate 0.000100027
2017-10-11T11:17:37.968852: step 2967, loss 0.470126, acc 0.859375, learning_rate 0.000100026
2017-10-11T11:17:38.083088: step 2968, loss 0.191925, acc 0.921875, learning_rate 0.000100026
2017-10-11T11:17:38.199237: step 2969, loss 0.300555, acc 0.90625, learning_rate 0.000100026
2017-10-11T11:17:38.313735: step 2970, loss 0.423583, acc 0.890625, learning_rate 0.000100026
2017-10-11T11:17:38.430681: step 2971, loss 0.236303, acc 0.90625, learning_rate 0.000100026
2017-10-11T11:17:38.539238: step 2972, loss 0.46443, acc 0.84375, learning_rate 0.000100026
2017-10-11T11:17:38.653526: step 2973, loss 0.187357, acc 0.9375, learning_rate 0.000100026
2017-10-11T11:17:38.783093: step 2974, loss 0.277803, acc 0.90625, learning_rate 0.000100026
2017-10-11T11:17:38.900952: step 2975, loss 0.41148, acc 0.859375, learning_rate 0.000100026
2017-10-11T11:17:39.018546: step 2976, loss 0.19015, acc 0.90625, learning_rate 0.000100025
2017-10-11T11:17:39.134291: step 2977, loss 0.261437, acc 0.90625, learning_rate 0.000100025
2017-10-11T11:17:39.245501: step 2978, loss 0.389184, acc 0.921875, learning_rate 0.000100025
2017-10-11T11:17:39.367603: step 2979, loss 0.196831, acc 0.9375, learning_rate 0.000100025
2017-10-11T11:17:39.520877: step 2980, loss 0.350255, acc 0.953125, learning_rate 0.000100025
2017-10-11T11:17:39.637179: step 2981, loss 0.35382, acc 0.890625, learning_rate 0.000100025
2017-10-11T11:17:39.724626: step 2982, loss 0.235007, acc 0.9375, learning_rate 0.000100025
2017-10-11T11:17:39.809746: step 2983, loss 0.457935, acc 0.859375, learning_rate 0.000100025
2017-10-11T11:17:39.895904: step 2984, loss 0.242872, acc 0.953125, learning_rate 0.000100025
2017-10-11T11:17:39.982330: step 2985, loss 0.221849, acc 0.921875, learning_rate 0.000100025
2017-10-11T11:17:40.068465: step 2986, loss 0.284193, acc 0.875, learning_rate 0.000100024
2017-10-11T11:17:40.156121: step 2987, loss 0.356611, acc 0.875, learning_rate 0.000100024
2017-10-11T11:17:40.244385: step 2988, loss 0.276871, acc 0.90625, learning_rate 0.000100024
2017-10-11T11:17:40.330594: step 2989, loss 0.383715, acc 0.859375, learning_rate 0.000100024
2017-10-11T11:17:40.430086: step 2990, loss 0.326268, acc 0.921875, learning_rate 0.000100024
2017-10-11T11:17:40.547827: step 2991, loss 0.159347, acc 0.9375, learning_rate 0.000100024
2017-10-11T11:17:40.667868: step 2992, loss 0.406491, acc 0.875, learning_rate 0.000100024
2017-10-11T11:17:40.777184: step 2993, loss 0.143692, acc 0.953125, learning_rate 0.000100024
2017-10-11T11:17:40.899718: step 2994, loss 0.15852, acc 0.9375, learning_rate 0.000100024
2017-10-11T11:17:41.017899: step 2995, loss 0.100516, acc 0.984375, learning_rate 0.000100024
2017-10-11T11:17:41.139841: step 2996, loss 0.236026, acc 0.890625, learning_rate 0.000100023
2017-10-11T11:17:41.251658: step 2997, loss 0.215828, acc 0.921875, learning_rate 0.000100023
2017-10-11T11:17:41.358012: step 2998, loss 0.307945, acc 0.890625, learning_rate 0.000100023
2017-10-11T11:17:41.481570: step 2999, loss 0.256726, acc 0.9375, learning_rate 0.000100023
2017-10-11T11:17:41.590843: step 3000, loss 0.251202, acc 0.90625, learning_rate 0.000100023

Evaluation:
2017-10-11T11:17:41.829791: step 3000, loss 0.239651, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3000

2017-10-11T11:17:42.474757: step 3001, loss 0.327148, acc 0.875, learning_rate 0.000100023
2017-10-11T11:17:42.587848: step 3002, loss 0.315183, acc 0.90625, learning_rate 0.000100023
2017-10-11T11:17:42.710412: step 3003, loss 0.325753, acc 0.953125, learning_rate 0.000100023
2017-10-11T11:17:42.830193: step 3004, loss 0.250124, acc 0.921875, learning_rate 0.000100023
2017-10-11T11:17:42.946977: step 3005, loss 0.201424, acc 0.9375, learning_rate 0.000100023
2017-10-11T11:17:43.063870: step 3006, loss 0.367751, acc 0.890625, learning_rate 0.000100023
2017-10-11T11:17:43.182262: step 3007, loss 0.465401, acc 0.859375, learning_rate 0.000100022
2017-10-11T11:17:43.296485: step 3008, loss 0.323961, acc 0.890625, learning_rate 0.000100022
2017-10-11T11:17:43.405781: step 3009, loss 0.176611, acc 0.953125, learning_rate 0.000100022
2017-10-11T11:17:43.521340: step 3010, loss 0.543174, acc 0.859375, learning_rate 0.000100022
2017-10-11T11:17:43.642318: step 3011, loss 0.212928, acc 0.9375, learning_rate 0.000100022
2017-10-11T11:17:43.760314: step 3012, loss 0.166074, acc 0.96875, learning_rate 0.000100022
2017-10-11T11:17:43.867878: step 3013, loss 0.270036, acc 0.890625, learning_rate 0.000100022
2017-10-11T11:17:43.984237: step 3014, loss 0.294555, acc 0.953125, learning_rate 0.000100022
2017-10-11T11:17:44.099887: step 3015, loss 0.387808, acc 0.84375, learning_rate 0.000100022
2017-10-11T11:17:44.221399: step 3016, loss 0.350203, acc 0.875, learning_rate 0.000100022
2017-10-11T11:17:44.337747: step 3017, loss 0.455061, acc 0.828125, learning_rate 0.000100022
2017-10-11T11:17:44.457001: step 3018, loss 0.300312, acc 0.90625, learning_rate 0.000100021
2017-10-11T11:17:44.573947: step 3019, loss 0.390265, acc 0.875, learning_rate 0.000100021
2017-10-11T11:17:44.695711: step 3020, loss 0.269586, acc 0.921875, learning_rate 0.000100021
2017-10-11T11:17:44.815279: step 3021, loss 0.283911, acc 0.875, learning_rate 0.000100021
2017-10-11T11:17:44.984844: step 3022, loss 0.241536, acc 0.921875, learning_rate 0.000100021
2017-10-11T11:17:45.079056: step 3023, loss 0.373746, acc 0.890625, learning_rate 0.000100021
2017-10-11T11:17:45.164908: step 3024, loss 0.220264, acc 0.9375, learning_rate 0.000100021
2017-10-11T11:17:45.252365: step 3025, loss 0.261058, acc 0.9375, learning_rate 0.000100021
2017-10-11T11:17:45.338895: step 3026, loss 0.269058, acc 0.9375, learning_rate 0.000100021
2017-10-11T11:17:45.427021: step 3027, loss 0.20202, acc 0.953125, learning_rate 0.000100021
2017-10-11T11:17:45.514370: step 3028, loss 0.527311, acc 0.8125, learning_rate 0.000100021
2017-10-11T11:17:45.602547: step 3029, loss 0.250396, acc 0.921875, learning_rate 0.00010002
2017-10-11T11:17:45.687624: step 3030, loss 0.249926, acc 0.90625, learning_rate 0.00010002
2017-10-11T11:17:45.776792: step 3031, loss 0.319076, acc 0.890625, learning_rate 0.00010002
2017-10-11T11:17:45.863633: step 3032, loss 0.421186, acc 0.890625, learning_rate 0.00010002
2017-10-11T11:17:45.951286: step 3033, loss 0.317151, acc 0.90625, learning_rate 0.00010002
2017-10-11T11:17:46.072305: step 3034, loss 0.430072, acc 0.84375, learning_rate 0.00010002
2017-10-11T11:17:46.195047: step 3035, loss 0.179917, acc 0.921875, learning_rate 0.00010002
2017-10-11T11:17:46.312303: step 3036, loss 0.369672, acc 0.90625, learning_rate 0.00010002
2017-10-11T11:17:46.431836: step 3037, loss 0.216063, acc 0.921875, learning_rate 0.00010002
2017-10-11T11:17:46.527096: step 3038, loss 0.273328, acc 0.941176, learning_rate 0.00010002
2017-10-11T11:17:46.641547: step 3039, loss 0.309336, acc 0.890625, learning_rate 0.00010002
2017-10-11T11:17:46.750400: step 3040, loss 0.308019, acc 0.890625, learning_rate 0.00010002

Evaluation:
2017-10-11T11:17:46.986052: step 3040, loss 0.239026, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3040

2017-10-11T11:17:47.695267: step 3041, loss 0.258882, acc 0.90625, learning_rate 0.00010002
2017-10-11T11:17:47.811934: step 3042, loss 0.326552, acc 0.890625, learning_rate 0.000100019
2017-10-11T11:17:47.927167: step 3043, loss 0.311917, acc 0.875, learning_rate 0.000100019
2017-10-11T11:17:48.041541: step 3044, loss 0.29612, acc 0.921875, learning_rate 0.000100019
2017-10-11T11:17:48.162753: step 3045, loss 0.240322, acc 0.890625, learning_rate 0.000100019
2017-10-11T11:17:48.270163: step 3046, loss 0.170713, acc 0.953125, learning_rate 0.000100019
2017-10-11T11:17:48.392560: step 3047, loss 0.332432, acc 0.875, learning_rate 0.000100019
2017-10-11T11:17:48.510868: step 3048, loss 0.151694, acc 0.953125, learning_rate 0.000100019
2017-10-11T11:17:48.625834: step 3049, loss 0.339865, acc 0.828125, learning_rate 0.000100019
2017-10-11T11:17:48.736969: step 3050, loss 0.271334, acc 0.96875, learning_rate 0.000100019
2017-10-11T11:17:48.851131: step 3051, loss 0.298215, acc 0.90625, learning_rate 0.000100019
2017-10-11T11:17:48.963507: step 3052, loss 0.269165, acc 0.921875, learning_rate 0.000100019
2017-10-11T11:17:49.088851: step 3053, loss 0.209062, acc 0.9375, learning_rate 0.000100019
2017-10-11T11:17:49.194434: step 3054, loss 0.511759, acc 0.8125, learning_rate 0.000100018
2017-10-11T11:17:49.316870: step 3055, loss 0.197178, acc 0.9375, learning_rate 0.000100018
2017-10-11T11:17:49.436956: step 3056, loss 0.247378, acc 0.921875, learning_rate 0.000100018
2017-10-11T11:17:49.556844: step 3057, loss 0.127579, acc 1, learning_rate 0.000100018
2017-10-11T11:17:49.663107: step 3058, loss 0.347556, acc 0.828125, learning_rate 0.000100018
2017-10-11T11:17:49.777851: step 3059, loss 0.208764, acc 0.9375, learning_rate 0.000100018
2017-10-11T11:17:49.904248: step 3060, loss 0.27, acc 0.90625, learning_rate 0.000100018
2017-10-11T11:17:50.021072: step 3061, loss 0.220627, acc 0.9375, learning_rate 0.000100018
2017-10-11T11:17:50.134372: step 3062, loss 0.35651, acc 0.875, learning_rate 0.000100018
2017-10-11T11:17:50.253374: step 3063, loss 0.365215, acc 0.890625, learning_rate 0.000100018
2017-10-11T11:17:50.392982: step 3064, loss 0.374948, acc 0.890625, learning_rate 0.000100018
2017-10-11T11:17:50.530460: step 3065, loss 0.304818, acc 0.890625, learning_rate 0.000100018
2017-10-11T11:17:50.618036: step 3066, loss 0.28415, acc 0.890625, learning_rate 0.000100018
2017-10-11T11:17:50.705040: step 3067, loss 0.315397, acc 0.890625, learning_rate 0.000100018
2017-10-11T11:17:50.793598: step 3068, loss 0.205495, acc 0.90625, learning_rate 0.000100017
2017-10-11T11:17:50.883960: step 3069, loss 0.483583, acc 0.8125, learning_rate 0.000100017
2017-10-11T11:17:50.970122: step 3070, loss 0.242255, acc 0.9375, learning_rate 0.000100017
2017-10-11T11:17:51.055098: step 3071, loss 0.380729, acc 0.84375, learning_rate 0.000100017
2017-10-11T11:17:51.144420: step 3072, loss 0.351102, acc 0.875, learning_rate 0.000100017
2017-10-11T11:17:51.248795: step 3073, loss 0.236073, acc 0.90625, learning_rate 0.000100017
2017-10-11T11:17:51.367487: step 3074, loss 0.355009, acc 0.875, learning_rate 0.000100017
2017-10-11T11:17:51.482271: step 3075, loss 0.195338, acc 0.96875, learning_rate 0.000100017
2017-10-11T11:17:51.595281: step 3076, loss 0.233908, acc 0.90625, learning_rate 0.000100017
2017-10-11T11:17:51.708987: step 3077, loss 0.280278, acc 0.890625, learning_rate 0.000100017
2017-10-11T11:17:51.819802: step 3078, loss 0.418632, acc 0.890625, learning_rate 0.000100017
2017-10-11T11:17:51.934885: step 3079, loss 0.374384, acc 0.859375, learning_rate 0.000100017
2017-10-11T11:17:52.051276: step 3080, loss 0.246084, acc 0.875, learning_rate 0.000100017

Evaluation:
2017-10-11T11:17:52.294522: step 3080, loss 0.238784, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3080

2017-10-11T11:17:53.010466: step 3081, loss 0.337679, acc 0.890625, learning_rate 0.000100017
2017-10-11T11:17:53.126809: step 3082, loss 0.281934, acc 0.90625, learning_rate 0.000100016
2017-10-11T11:17:53.243601: step 3083, loss 0.191438, acc 0.9375, learning_rate 0.000100016
2017-10-11T11:17:53.360560: step 3084, loss 0.344686, acc 0.859375, learning_rate 0.000100016
2017-10-11T11:17:53.481195: step 3085, loss 0.354541, acc 0.921875, learning_rate 0.000100016
2017-10-11T11:17:53.590655: step 3086, loss 0.300983, acc 0.890625, learning_rate 0.000100016
2017-10-11T11:17:53.708761: step 3087, loss 0.288017, acc 0.875, learning_rate 0.000100016
2017-10-11T11:17:53.812329: step 3088, loss 0.293436, acc 0.84375, learning_rate 0.000100016
2017-10-11T11:17:53.926021: step 3089, loss 0.315546, acc 0.90625, learning_rate 0.000100016
2017-10-11T11:17:54.052880: step 3090, loss 0.245734, acc 0.921875, learning_rate 0.000100016
2017-10-11T11:17:54.164937: step 3091, loss 0.328304, acc 0.890625, learning_rate 0.000100016
2017-10-11T11:17:54.275311: step 3092, loss 0.405037, acc 0.84375, learning_rate 0.000100016
2017-10-11T11:17:54.384703: step 3093, loss 0.213683, acc 0.921875, learning_rate 0.000100016
2017-10-11T11:17:54.503771: step 3094, loss 0.342036, acc 0.90625, learning_rate 0.000100016
2017-10-11T11:17:54.612167: step 3095, loss 0.472309, acc 0.890625, learning_rate 0.000100016
2017-10-11T11:17:54.720682: step 3096, loss 0.369052, acc 0.859375, learning_rate 0.000100016
2017-10-11T11:17:54.850912: step 3097, loss 0.358794, acc 0.890625, learning_rate 0.000100016
2017-10-11T11:17:54.968912: step 3098, loss 0.319423, acc 0.84375, learning_rate 0.000100015
2017-10-11T11:17:55.089928: step 3099, loss 0.340742, acc 0.921875, learning_rate 0.000100015
2017-10-11T11:17:55.204815: step 3100, loss 0.401282, acc 0.890625, learning_rate 0.000100015
2017-10-11T11:17:55.327823: step 3101, loss 0.3545, acc 0.859375, learning_rate 0.000100015
2017-10-11T11:17:55.442370: step 3102, loss 0.162463, acc 0.96875, learning_rate 0.000100015
2017-10-11T11:17:55.604870: step 3103, loss 0.220978, acc 0.921875, learning_rate 0.000100015
2017-10-11T11:17:55.708559: step 3104, loss 0.253222, acc 0.859375, learning_rate 0.000100015
2017-10-11T11:17:55.793259: step 3105, loss 0.273214, acc 0.90625, learning_rate 0.000100015
2017-10-11T11:17:55.890933: step 3106, loss 0.280825, acc 0.9375, learning_rate 0.000100015
2017-10-11T11:17:55.977335: step 3107, loss 0.286269, acc 0.90625, learning_rate 0.000100015
2017-10-11T11:17:56.064102: step 3108, loss 0.20894, acc 0.90625, learning_rate 0.000100015
2017-10-11T11:17:56.147719: step 3109, loss 0.239609, acc 0.90625, learning_rate 0.000100015
2017-10-11T11:17:56.234070: step 3110, loss 0.333696, acc 0.859375, learning_rate 0.000100015
2017-10-11T11:17:56.327969: step 3111, loss 0.282837, acc 0.890625, learning_rate 0.000100015
2017-10-11T11:17:56.428204: step 3112, loss 0.31969, acc 0.890625, learning_rate 0.000100015
2017-10-11T11:17:56.511760: step 3113, loss 0.19359, acc 0.9375, learning_rate 0.000100015
2017-10-11T11:17:56.608404: step 3114, loss 0.184252, acc 0.953125, learning_rate 0.000100014
2017-10-11T11:17:56.724311: step 3115, loss 0.341869, acc 0.859375, learning_rate 0.000100014
2017-10-11T11:17:56.849520: step 3116, loss 0.232096, acc 0.859375, learning_rate 0.000100014
2017-10-11T11:17:56.970948: step 3117, loss 0.318478, acc 0.90625, learning_rate 0.000100014
2017-10-11T11:17:57.089067: step 3118, loss 0.175247, acc 0.9375, learning_rate 0.000100014
2017-10-11T11:17:57.199286: step 3119, loss 0.215237, acc 0.9375, learning_rate 0.000100014
2017-10-11T11:17:57.327207: step 3120, loss 0.503513, acc 0.8125, learning_rate 0.000100014

Evaluation:
2017-10-11T11:17:57.564345: step 3120, loss 0.239112, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3120

2017-10-11T11:17:58.282533: step 3121, loss 0.269007, acc 0.921875, learning_rate 0.000100014
2017-10-11T11:17:58.726360: step 3122, loss 0.263473, acc 0.921875, learning_rate 0.000100014
2017-10-11T11:17:58.847218: step 3123, loss 0.296217, acc 0.890625, learning_rate 0.000100014
2017-10-11T11:17:58.962089: step 3124, loss 0.252088, acc 0.9375, learning_rate 0.000100014
2017-10-11T11:17:59.080060: step 3125, loss 0.330274, acc 0.859375, learning_rate 0.000100014
2017-10-11T11:17:59.202241: step 3126, loss 0.220015, acc 0.921875, learning_rate 0.000100014
2017-10-11T11:17:59.316391: step 3127, loss 0.279356, acc 0.890625, learning_rate 0.000100014
2017-10-11T11:17:59.431181: step 3128, loss 0.292552, acc 0.890625, learning_rate 0.000100014
2017-10-11T11:17:59.537287: step 3129, loss 0.255914, acc 0.90625, learning_rate 0.000100014
2017-10-11T11:17:59.658290: step 3130, loss 0.198927, acc 0.9375, learning_rate 0.000100014
2017-10-11T11:17:59.775546: step 3131, loss 0.133724, acc 0.96875, learning_rate 0.000100014
2017-10-11T11:17:59.890656: step 3132, loss 0.266141, acc 0.953125, learning_rate 0.000100013
2017-10-11T11:18:00.012506: step 3133, loss 0.261766, acc 0.921875, learning_rate 0.000100013
2017-10-11T11:18:00.128615: step 3134, loss 0.246689, acc 0.921875, learning_rate 0.000100013
2017-10-11T11:18:00.251379: step 3135, loss 0.263971, acc 0.90625, learning_rate 0.000100013
2017-10-11T11:18:00.347807: step 3136, loss 0.278109, acc 0.901961, learning_rate 0.000100013
2017-10-11T11:18:00.468488: step 3137, loss 0.41656, acc 0.84375, learning_rate 0.000100013
2017-10-11T11:18:00.593097: step 3138, loss 0.288036, acc 0.90625, learning_rate 0.000100013
2017-10-11T11:18:00.706790: step 3139, loss 0.277263, acc 0.9375, learning_rate 0.000100013
2017-10-11T11:18:00.864963: step 3140, loss 0.229673, acc 0.921875, learning_rate 0.000100013
2017-10-11T11:18:00.975554: step 3141, loss 0.261452, acc 0.96875, learning_rate 0.000100013
2017-10-11T11:18:01.060315: step 3142, loss 0.18542, acc 0.953125, learning_rate 0.000100013
2017-10-11T11:18:01.149002: step 3143, loss 0.201486, acc 0.9375, learning_rate 0.000100013
2017-10-11T11:18:01.236219: step 3144, loss 0.267856, acc 0.875, learning_rate 0.000100013
2017-10-11T11:18:01.326276: step 3145, loss 0.142938, acc 0.9375, learning_rate 0.000100013
2017-10-11T11:18:01.418974: step 3146, loss 0.255958, acc 0.921875, learning_rate 0.000100013
2017-10-11T11:18:01.519459: step 3147, loss 0.291415, acc 0.890625, learning_rate 0.000100013
2017-10-11T11:18:01.615177: step 3148, loss 0.183469, acc 0.953125, learning_rate 0.000100013
2017-10-11T11:18:01.702489: step 3149, loss 0.212078, acc 0.953125, learning_rate 0.000100013
2017-10-11T11:18:01.787848: step 3150, loss 0.190768, acc 0.921875, learning_rate 0.000100012
2017-10-11T11:18:01.881709: step 3151, loss 0.231556, acc 0.921875, learning_rate 0.000100012
2017-10-11T11:18:02.012442: step 3152, loss 0.186812, acc 0.921875, learning_rate 0.000100012
2017-10-11T11:18:02.126815: step 3153, loss 0.252845, acc 0.953125, learning_rate 0.000100012
2017-10-11T11:18:02.241193: step 3154, loss 0.356084, acc 0.875, learning_rate 0.000100012
2017-10-11T11:18:02.361110: step 3155, loss 0.368351, acc 0.828125, learning_rate 0.000100012
2017-10-11T11:18:02.477769: step 3156, loss 0.270394, acc 0.9375, learning_rate 0.000100012
2017-10-11T11:18:02.594957: step 3157, loss 0.255266, acc 0.90625, learning_rate 0.000100012
2017-10-11T11:18:02.714912: step 3158, loss 0.337602, acc 0.890625, learning_rate 0.000100012
2017-10-11T11:18:02.825486: step 3159, loss 0.34001, acc 0.890625, learning_rate 0.000100012
2017-10-11T11:18:02.945084: step 3160, loss 0.284161, acc 0.890625, learning_rate 0.000100012

Evaluation:
2017-10-11T11:18:03.200398: step 3160, loss 0.239241, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3160

2017-10-11T11:18:03.896230: step 3161, loss 0.289174, acc 0.890625, learning_rate 0.000100012
2017-10-11T11:18:03.997691: step 3162, loss 0.30443, acc 0.90625, learning_rate 0.000100012
2017-10-11T11:18:04.120864: step 3163, loss 0.365938, acc 0.890625, learning_rate 0.000100012
2017-10-11T11:18:04.228878: step 3164, loss 0.25053, acc 0.9375, learning_rate 0.000100012
2017-10-11T11:18:04.336574: step 3165, loss 0.23786, acc 0.9375, learning_rate 0.000100012
2017-10-11T11:18:04.450836: step 3166, loss 0.321918, acc 0.890625, learning_rate 0.000100012
2017-10-11T11:18:04.561437: step 3167, loss 0.483416, acc 0.828125, learning_rate 0.000100012
2017-10-11T11:18:04.680878: step 3168, loss 0.312946, acc 0.90625, learning_rate 0.000100012
2017-10-11T11:18:04.791738: step 3169, loss 0.464153, acc 0.859375, learning_rate 0.000100012
2017-10-11T11:18:04.903976: step 3170, loss 0.189036, acc 0.9375, learning_rate 0.000100012
2017-10-11T11:18:05.043224: step 3171, loss 0.214761, acc 0.890625, learning_rate 0.000100011
2017-10-11T11:18:05.147767: step 3172, loss 0.243922, acc 0.90625, learning_rate 0.000100011
2017-10-11T11:18:05.265095: step 3173, loss 0.268183, acc 0.890625, learning_rate 0.000100011
2017-10-11T11:18:05.374570: step 3174, loss 0.332608, acc 0.859375, learning_rate 0.000100011
2017-10-11T11:18:05.496240: step 3175, loss 0.330206, acc 0.890625, learning_rate 0.000100011
2017-10-11T11:18:05.611431: step 3176, loss 0.339355, acc 0.90625, learning_rate 0.000100011
2017-10-11T11:18:05.740830: step 3177, loss 0.313358, acc 0.890625, learning_rate 0.000100011
2017-10-11T11:18:05.854682: step 3178, loss 0.312686, acc 0.859375, learning_rate 0.000100011
2017-10-11T11:18:05.964616: step 3179, loss 0.232991, acc 0.890625, learning_rate 0.000100011
2017-10-11T11:18:06.078671: step 3180, loss 0.272259, acc 0.9375, learning_rate 0.000100011
2017-10-11T11:18:06.261036: step 3181, loss 0.222214, acc 0.90625, learning_rate 0.000100011
2017-10-11T11:18:06.351052: step 3182, loss 0.336681, acc 0.859375, learning_rate 0.000100011
2017-10-11T11:18:06.439873: step 3183, loss 0.343636, acc 0.90625, learning_rate 0.000100011
2017-10-11T11:18:06.531016: step 3184, loss 0.314656, acc 0.890625, learning_rate 0.000100011
2017-10-11T11:18:06.614883: step 3185, loss 0.285212, acc 0.890625, learning_rate 0.000100011
2017-10-11T11:18:06.713428: step 3186, loss 0.255181, acc 0.890625, learning_rate 0.000100011
2017-10-11T11:18:06.801911: step 3187, loss 0.210165, acc 0.921875, learning_rate 0.000100011
2017-10-11T11:18:06.886386: step 3188, loss 0.32374, acc 0.875, learning_rate 0.000100011
2017-10-11T11:18:06.979569: step 3189, loss 0.424314, acc 0.859375, learning_rate 0.000100011
2017-10-11T11:18:07.078270: step 3190, loss 0.36934, acc 0.859375, learning_rate 0.000100011
2017-10-11T11:18:07.200326: step 3191, loss 0.360797, acc 0.921875, learning_rate 0.000100011
2017-10-11T11:18:07.312454: step 3192, loss 0.328915, acc 0.890625, learning_rate 0.000100011
2017-10-11T11:18:07.434571: step 3193, loss 0.361879, acc 0.90625, learning_rate 0.00010001
2017-10-11T11:18:07.543937: step 3194, loss 0.183605, acc 0.953125, learning_rate 0.00010001
2017-10-11T11:18:07.654316: step 3195, loss 0.317774, acc 0.859375, learning_rate 0.00010001
2017-10-11T11:18:07.760194: step 3196, loss 0.270104, acc 0.921875, learning_rate 0.00010001
2017-10-11T11:18:07.871265: step 3197, loss 0.223274, acc 0.9375, learning_rate 0.00010001
2017-10-11T11:18:07.987540: step 3198, loss 0.46276, acc 0.875, learning_rate 0.00010001
2017-10-11T11:18:08.104688: step 3199, loss 0.24176, acc 0.921875, learning_rate 0.00010001
2017-10-11T11:18:08.206962: step 3200, loss 0.279721, acc 0.875, learning_rate 0.00010001

Evaluation:
2017-10-11T11:18:08.464777: step 3200, loss 0.238708, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3200

2017-10-11T11:18:09.165434: step 3201, loss 0.249364, acc 0.875, learning_rate 0.00010001
2017-10-11T11:18:09.299255: step 3202, loss 0.324191, acc 0.890625, learning_rate 0.00010001
2017-10-11T11:18:09.403858: step 3203, loss 0.209805, acc 0.921875, learning_rate 0.00010001
2017-10-11T11:18:09.524359: step 3204, loss 0.34451, acc 0.90625, learning_rate 0.00010001
2017-10-11T11:18:09.630064: step 3205, loss 0.197163, acc 0.953125, learning_rate 0.00010001
2017-10-11T11:18:09.744339: step 3206, loss 0.429173, acc 0.890625, learning_rate 0.00010001
2017-10-11T11:18:09.869379: step 3207, loss 0.198687, acc 0.9375, learning_rate 0.00010001
2017-10-11T11:18:09.987515: step 3208, loss 0.282071, acc 0.90625, learning_rate 0.00010001
2017-10-11T11:18:10.094519: step 3209, loss 0.450501, acc 0.8125, learning_rate 0.00010001
2017-10-11T11:18:10.210905: step 3210, loss 0.292854, acc 0.890625, learning_rate 0.00010001
2017-10-11T11:18:10.327759: step 3211, loss 0.314001, acc 0.890625, learning_rate 0.00010001
2017-10-11T11:18:10.444300: step 3212, loss 0.299625, acc 0.90625, learning_rate 0.00010001
2017-10-11T11:18:10.557586: step 3213, loss 0.234359, acc 0.921875, learning_rate 0.00010001
2017-10-11T11:18:10.672456: step 3214, loss 0.281275, acc 0.921875, learning_rate 0.00010001
2017-10-11T11:18:10.786122: step 3215, loss 0.32618, acc 0.890625, learning_rate 0.00010001
2017-10-11T11:18:10.907338: step 3216, loss 0.164213, acc 1, learning_rate 0.00010001
2017-10-11T11:18:11.026659: step 3217, loss 0.285708, acc 0.921875, learning_rate 0.000100009
2017-10-11T11:18:11.135036: step 3218, loss 0.214639, acc 0.9375, learning_rate 0.000100009
2017-10-11T11:18:11.247001: step 3219, loss 0.222798, acc 0.9375, learning_rate 0.000100009
2017-10-11T11:18:11.381044: step 3220, loss 0.352975, acc 0.859375, learning_rate 0.000100009
2017-10-11T11:18:11.516481: step 3221, loss 0.228832, acc 0.90625, learning_rate 0.000100009
2017-10-11T11:18:11.608267: step 3222, loss 0.219154, acc 0.921875, learning_rate 0.000100009
2017-10-11T11:18:11.697879: step 3223, loss 0.276033, acc 0.921875, learning_rate 0.000100009
2017-10-11T11:18:11.786251: step 3224, loss 0.20335, acc 0.953125, learning_rate 0.000100009
2017-10-11T11:18:11.876136: step 3225, loss 0.436169, acc 0.890625, learning_rate 0.000100009
2017-10-11T11:18:11.962591: step 3226, loss 0.13063, acc 0.96875, learning_rate 0.000100009
2017-10-11T11:18:12.049517: step 3227, loss 0.375401, acc 0.859375, learning_rate 0.000100009
2017-10-11T11:18:12.134603: step 3228, loss 0.269185, acc 0.9375, learning_rate 0.000100009
2017-10-11T11:18:12.219916: step 3229, loss 0.116685, acc 0.953125, learning_rate 0.000100009
2017-10-11T11:18:12.308883: step 3230, loss 0.41681, acc 0.859375, learning_rate 0.000100009
2017-10-11T11:18:12.389910: step 3231, loss 0.36201, acc 0.921875, learning_rate 0.000100009
2017-10-11T11:18:12.471885: step 3232, loss 0.264519, acc 0.9375, learning_rate 0.000100009
2017-10-11T11:18:12.555542: step 3233, loss 0.279842, acc 0.890625, learning_rate 0.000100009
2017-10-11T11:18:12.632838: step 3234, loss 0.419443, acc 0.823529, learning_rate 0.000100009
2017-10-11T11:18:12.715881: step 3235, loss 0.255703, acc 0.890625, learning_rate 0.000100009
2017-10-11T11:18:12.798934: step 3236, loss 0.233024, acc 0.921875, learning_rate 0.000100009
2017-10-11T11:18:12.887984: step 3237, loss 0.358413, acc 0.875, learning_rate 0.000100009
2017-10-11T11:18:12.975412: step 3238, loss 0.285577, acc 0.921875, learning_rate 0.000100009
2017-10-11T11:18:13.054688: step 3239, loss 0.302049, acc 0.890625, learning_rate 0.000100009
2017-10-11T11:18:13.144595: step 3240, loss 0.280042, acc 0.921875, learning_rate 0.000100009

Evaluation:
2017-10-11T11:18:13.351391: step 3240, loss 0.237226, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3240

2017-10-11T11:18:13.859825: step 3241, loss 0.466397, acc 0.859375, learning_rate 0.000100009
2017-10-11T11:18:13.942693: step 3242, loss 0.193905, acc 0.96875, learning_rate 0.000100009
2017-10-11T11:18:14.026697: step 3243, loss 0.302218, acc 0.875, learning_rate 0.000100009
2017-10-11T11:18:14.106740: step 3244, loss 0.197213, acc 0.921875, learning_rate 0.000100009
2017-10-11T11:18:14.190925: step 3245, loss 0.352567, acc 0.859375, learning_rate 0.000100008
2017-10-11T11:18:14.271223: step 3246, loss 0.216997, acc 0.90625, learning_rate 0.000100008
2017-10-11T11:18:14.354016: step 3247, loss 0.32118, acc 0.890625, learning_rate 0.000100008
2017-10-11T11:18:14.438605: step 3248, loss 0.32197, acc 0.921875, learning_rate 0.000100008
2017-10-11T11:18:14.525932: step 3249, loss 0.321366, acc 0.9375, learning_rate 0.000100008
2017-10-11T11:18:14.610992: step 3250, loss 0.197647, acc 0.921875, learning_rate 0.000100008
2017-10-11T11:18:14.695990: step 3251, loss 0.24296, acc 0.90625, learning_rate 0.000100008
2017-10-11T11:18:14.777164: step 3252, loss 0.256072, acc 0.90625, learning_rate 0.000100008
2017-10-11T11:18:14.861626: step 3253, loss 0.228084, acc 0.890625, learning_rate 0.000100008
2017-10-11T11:18:14.942704: step 3254, loss 0.31715, acc 0.9375, learning_rate 0.000100008
2017-10-11T11:18:15.026844: step 3255, loss 0.211202, acc 0.953125, learning_rate 0.000100008
2017-10-11T11:18:15.109920: step 3256, loss 0.308114, acc 0.875, learning_rate 0.000100008
2017-10-11T11:18:15.191424: step 3257, loss 0.413573, acc 0.84375, learning_rate 0.000100008
2017-10-11T11:18:15.273692: step 3258, loss 0.268537, acc 0.90625, learning_rate 0.000100008
2017-10-11T11:18:15.357412: step 3259, loss 0.361182, acc 0.90625, learning_rate 0.000100008
2017-10-11T11:18:15.438497: step 3260, loss 0.22398, acc 0.9375, learning_rate 0.000100008
2017-10-11T11:18:15.518123: step 3261, loss 0.251457, acc 0.90625, learning_rate 0.000100008
2017-10-11T11:18:15.600098: step 3262, loss 0.330176, acc 0.953125, learning_rate 0.000100008
2017-10-11T11:18:15.680563: step 3263, loss 0.247843, acc 0.890625, learning_rate 0.000100008
2017-10-11T11:18:15.762651: step 3264, loss 0.452527, acc 0.84375, learning_rate 0.000100008
2017-10-11T11:18:15.847114: step 3265, loss 0.359072, acc 0.890625, learning_rate 0.000100008
2017-10-11T11:18:15.927718: step 3266, loss 0.184913, acc 0.921875, learning_rate 0.000100008
2017-10-11T11:18:16.010427: step 3267, loss 0.397492, acc 0.890625, learning_rate 0.000100008
2017-10-11T11:18:16.090610: step 3268, loss 0.216192, acc 0.9375, learning_rate 0.000100008
2017-10-11T11:18:16.172664: step 3269, loss 0.297562, acc 0.921875, learning_rate 0.000100008
2017-10-11T11:18:16.254751: step 3270, loss 0.36559, acc 0.9375, learning_rate 0.000100008
2017-10-11T11:18:16.339238: step 3271, loss 0.322435, acc 0.875, learning_rate 0.000100008
2017-10-11T11:18:16.423252: step 3272, loss 0.288716, acc 0.859375, learning_rate 0.000100008
2017-10-11T11:18:16.506783: step 3273, loss 0.172799, acc 0.9375, learning_rate 0.000100008
2017-10-11T11:18:16.588311: step 3274, loss 0.374468, acc 0.921875, learning_rate 0.000100008
2017-10-11T11:18:16.672629: step 3275, loss 0.290035, acc 0.875, learning_rate 0.000100007
2017-10-11T11:18:16.755321: step 3276, loss 0.196329, acc 0.921875, learning_rate 0.000100007
2017-10-11T11:18:16.838171: step 3277, loss 0.323127, acc 0.890625, learning_rate 0.000100007
2017-10-11T11:18:16.920763: step 3278, loss 0.229122, acc 0.953125, learning_rate 0.000100007
2017-10-11T11:18:17.003077: step 3279, loss 0.247405, acc 0.90625, learning_rate 0.000100007
2017-10-11T11:18:17.085570: step 3280, loss 0.271901, acc 0.921875, learning_rate 0.000100007

Evaluation:
2017-10-11T11:18:17.274283: step 3280, loss 0.23725, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3280

2017-10-11T11:18:17.781379: step 3281, loss 0.221231, acc 0.90625, learning_rate 0.000100007
2017-10-11T11:18:17.865172: step 3282, loss 0.252781, acc 0.90625, learning_rate 0.000100007
2017-10-11T11:18:17.946489: step 3283, loss 0.281895, acc 0.875, learning_rate 0.000100007
2017-10-11T11:18:18.029469: step 3284, loss 0.303494, acc 0.90625, learning_rate 0.000100007
2017-10-11T11:18:18.112229: step 3285, loss 0.21014, acc 0.921875, learning_rate 0.000100007
2017-10-11T11:18:18.195624: step 3286, loss 0.226124, acc 0.921875, learning_rate 0.000100007
2017-10-11T11:18:18.278814: step 3287, loss 0.206588, acc 0.953125, learning_rate 0.000100007
2017-10-11T11:18:18.361172: step 3288, loss 0.128299, acc 0.96875, learning_rate 0.000100007
2017-10-11T11:18:18.445073: step 3289, loss 0.366219, acc 0.875, learning_rate 0.000100007
2017-10-11T11:18:18.526414: step 3290, loss 0.315925, acc 0.921875, learning_rate 0.000100007
2017-10-11T11:18:18.608904: step 3291, loss 0.365337, acc 0.875, learning_rate 0.000100007
2017-10-11T11:18:18.691503: step 3292, loss 0.348985, acc 0.875, learning_rate 0.000100007
2017-10-11T11:18:18.773692: step 3293, loss 0.185837, acc 0.9375, learning_rate 0.000100007
2017-10-11T11:18:18.857358: step 3294, loss 0.295339, acc 0.953125, learning_rate 0.000100007
2017-10-11T11:18:18.938695: step 3295, loss 0.30583, acc 0.90625, learning_rate 0.000100007
2017-10-11T11:18:19.022426: step 3296, loss 0.202797, acc 0.953125, learning_rate 0.000100007
2017-10-11T11:18:19.104540: step 3297, loss 0.38024, acc 0.890625, learning_rate 0.000100007
2017-10-11T11:18:19.188856: step 3298, loss 0.302851, acc 0.90625, learning_rate 0.000100007
2017-10-11T11:18:19.270930: step 3299, loss 0.173075, acc 0.953125, learning_rate 0.000100007
2017-10-11T11:18:19.353140: step 3300, loss 0.30722, acc 0.890625, learning_rate 0.000100007
2017-10-11T11:18:19.434335: step 3301, loss 0.336303, acc 0.890625, learning_rate 0.000100007
2017-10-11T11:18:19.516968: step 3302, loss 0.333852, acc 0.828125, learning_rate 0.000100007
2017-10-11T11:18:19.599821: step 3303, loss 0.316175, acc 0.859375, learning_rate 0.000100007
2017-10-11T11:18:19.681755: step 3304, loss 0.237382, acc 0.9375, learning_rate 0.000100007
2017-10-11T11:18:19.766708: step 3305, loss 0.270374, acc 0.921875, learning_rate 0.000100007
2017-10-11T11:18:19.846673: step 3306, loss 0.243625, acc 0.921875, learning_rate 0.000100007
2017-10-11T11:18:19.932251: step 3307, loss 0.150184, acc 1, learning_rate 0.000100007
2017-10-11T11:18:20.014100: step 3308, loss 0.183745, acc 0.921875, learning_rate 0.000100007
2017-10-11T11:18:20.095183: step 3309, loss 0.439128, acc 0.859375, learning_rate 0.000100007
2017-10-11T11:18:20.180405: step 3310, loss 0.246971, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:18:20.262592: step 3311, loss 0.205179, acc 0.9375, learning_rate 0.000100006
2017-10-11T11:18:20.343052: step 3312, loss 0.221874, acc 0.9375, learning_rate 0.000100006
2017-10-11T11:18:20.426619: step 3313, loss 0.346652, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:18:20.509319: step 3314, loss 0.367213, acc 0.859375, learning_rate 0.000100006
2017-10-11T11:18:20.590393: step 3315, loss 0.211911, acc 0.921875, learning_rate 0.000100006
2017-10-11T11:18:20.672613: step 3316, loss 0.287529, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:18:20.754550: step 3317, loss 0.183948, acc 0.953125, learning_rate 0.000100006
2017-10-11T11:18:20.837490: step 3318, loss 0.321932, acc 0.890625, learning_rate 0.000100006
2017-10-11T11:18:20.917328: step 3319, loss 0.18403, acc 0.921875, learning_rate 0.000100006
2017-10-11T11:18:20.999557: step 3320, loss 0.293163, acc 0.875, learning_rate 0.000100006

Evaluation:
2017-10-11T11:18:21.191829: step 3320, loss 0.236066, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3320

2017-10-11T11:18:21.630892: step 3321, loss 0.36644, acc 0.828125, learning_rate 0.000100006
2017-10-11T11:18:21.712424: step 3322, loss 0.181706, acc 0.9375, learning_rate 0.000100006
2017-10-11T11:18:21.794233: step 3323, loss 0.205851, acc 0.96875, learning_rate 0.000100006
2017-10-11T11:18:21.877042: step 3324, loss 0.316411, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:18:21.959935: step 3325, loss 0.259499, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:18:22.042940: step 3326, loss 0.335507, acc 0.875, learning_rate 0.000100006
2017-10-11T11:18:22.123881: step 3327, loss 0.237092, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:18:22.207663: step 3328, loss 0.287637, acc 0.921875, learning_rate 0.000100006
2017-10-11T11:18:22.288949: step 3329, loss 0.262852, acc 0.921875, learning_rate 0.000100006
2017-10-11T11:18:22.371350: step 3330, loss 0.393511, acc 0.875, learning_rate 0.000100006
2017-10-11T11:18:22.454998: step 3331, loss 0.31433, acc 0.875, learning_rate 0.000100006
2017-10-11T11:18:22.525290: step 3332, loss 0.286334, acc 0.901961, learning_rate 0.000100006
2017-10-11T11:18:22.608664: step 3333, loss 0.319154, acc 0.890625, learning_rate 0.000100006
2017-10-11T11:18:22.690959: step 3334, loss 0.223946, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:18:22.773795: step 3335, loss 0.234104, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:18:22.856411: step 3336, loss 0.289854, acc 0.875, learning_rate 0.000100006
2017-10-11T11:18:22.938074: step 3337, loss 0.294831, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:18:23.027051: step 3338, loss 0.304158, acc 0.890625, learning_rate 0.000100006
2017-10-11T11:18:23.109584: step 3339, loss 0.182699, acc 0.921875, learning_rate 0.000100006
2017-10-11T11:18:23.194470: step 3340, loss 0.246853, acc 0.921875, learning_rate 0.000100006
2017-10-11T11:18:23.275871: step 3341, loss 0.256495, acc 0.9375, learning_rate 0.000100006
2017-10-11T11:18:23.359446: step 3342, loss 0.143882, acc 0.96875, learning_rate 0.000100006
2017-10-11T11:18:23.439797: step 3343, loss 0.310588, acc 0.890625, learning_rate 0.000100006
2017-10-11T11:18:23.528717: step 3344, loss 0.225906, acc 0.9375, learning_rate 0.000100006
2017-10-11T11:18:23.612395: step 3345, loss 0.215852, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:18:23.693376: step 3346, loss 0.339922, acc 0.890625, learning_rate 0.000100006
2017-10-11T11:18:23.774876: step 3347, loss 0.228096, acc 0.90625, learning_rate 0.000100006
2017-10-11T11:18:23.857833: step 3348, loss 0.294712, acc 0.890625, learning_rate 0.000100006
2017-10-11T11:18:23.942784: step 3349, loss 0.351247, acc 0.875, learning_rate 0.000100006
2017-10-11T11:18:24.025813: step 3350, loss 0.164466, acc 0.921875, learning_rate 0.000100006
2017-10-11T11:18:24.108153: step 3351, loss 0.353298, acc 0.890625, learning_rate 0.000100005
2017-10-11T11:18:24.191651: step 3352, loss 0.363418, acc 0.875, learning_rate 0.000100005
2017-10-11T11:18:24.274044: step 3353, loss 0.172609, acc 0.9375, learning_rate 0.000100005
2017-10-11T11:18:24.354522: step 3354, loss 0.207455, acc 0.9375, learning_rate 0.000100005
2017-10-11T11:18:24.439493: step 3355, loss 0.236322, acc 0.9375, learning_rate 0.000100005
2017-10-11T11:18:24.522557: step 3356, loss 0.401977, acc 0.84375, learning_rate 0.000100005
2017-10-11T11:18:24.606520: step 3357, loss 0.246444, acc 0.9375, learning_rate 0.000100005
2017-10-11T11:18:24.690321: step 3358, loss 0.242006, acc 0.9375, learning_rate 0.000100005
2017-10-11T11:18:24.774806: step 3359, loss 0.206936, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:18:24.860156: step 3360, loss 0.268214, acc 0.921875, learning_rate 0.000100005

Evaluation:
2017-10-11T11:18:25.050809: step 3360, loss 0.23519, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3360

2017-10-11T11:18:25.549404: step 3361, loss 0.40064, acc 0.84375, learning_rate 0.000100005
2017-10-11T11:18:25.631134: step 3362, loss 0.230621, acc 0.9375, learning_rate 0.000100005
2017-10-11T11:18:25.711876: step 3363, loss 0.234005, acc 0.859375, learning_rate 0.000100005
2017-10-11T11:18:25.794220: step 3364, loss 0.411801, acc 0.875, learning_rate 0.000100005
2017-10-11T11:18:25.877449: step 3365, loss 0.203554, acc 0.953125, learning_rate 0.000100005
2017-10-11T11:18:25.961584: step 3366, loss 0.374345, acc 0.84375, learning_rate 0.000100005
2017-10-11T11:18:26.043738: step 3367, loss 0.321317, acc 0.859375, learning_rate 0.000100005
2017-10-11T11:18:26.125912: step 3368, loss 0.320252, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:18:26.208770: step 3369, loss 0.143529, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:18:26.293510: step 3370, loss 0.171737, acc 0.96875, learning_rate 0.000100005
2017-10-11T11:18:26.377259: step 3371, loss 0.258474, acc 0.875, learning_rate 0.000100005
2017-10-11T11:18:26.456633: step 3372, loss 0.279492, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:18:26.535694: step 3373, loss 0.387765, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:18:26.618998: step 3374, loss 0.343997, acc 0.875, learning_rate 0.000100005
2017-10-11T11:18:26.703071: step 3375, loss 0.263605, acc 0.890625, learning_rate 0.000100005
2017-10-11T11:18:26.786206: step 3376, loss 0.277642, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:18:26.869375: step 3377, loss 0.209511, acc 0.9375, learning_rate 0.000100005
2017-10-11T11:18:26.949684: step 3378, loss 0.296625, acc 0.859375, learning_rate 0.000100005
2017-10-11T11:18:27.032529: step 3379, loss 0.260709, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:18:27.116743: step 3380, loss 0.326214, acc 0.890625, learning_rate 0.000100005
2017-10-11T11:18:27.199255: step 3381, loss 0.349289, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:18:27.284298: step 3382, loss 0.429725, acc 0.890625, learning_rate 0.000100005
2017-10-11T11:18:27.369518: step 3383, loss 0.216691, acc 0.9375, learning_rate 0.000100005
2017-10-11T11:18:27.452288: step 3384, loss 0.221306, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:18:27.533208: step 3385, loss 0.249024, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:18:27.615447: step 3386, loss 0.24133, acc 0.953125, learning_rate 0.000100005
2017-10-11T11:18:27.700528: step 3387, loss 0.143993, acc 0.96875, learning_rate 0.000100005
2017-10-11T11:18:27.782299: step 3388, loss 0.181779, acc 0.96875, learning_rate 0.000100005
2017-10-11T11:18:27.866571: step 3389, loss 0.366434, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:18:27.944609: step 3390, loss 0.297468, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:18:28.028687: step 3391, loss 0.219977, acc 0.9375, learning_rate 0.000100005
2017-10-11T11:18:28.111816: step 3392, loss 0.325258, acc 0.890625, learning_rate 0.000100005
2017-10-11T11:18:28.194934: step 3393, loss 0.390366, acc 0.859375, learning_rate 0.000100005
2017-10-11T11:18:28.274708: step 3394, loss 0.309468, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:18:28.355399: step 3395, loss 0.117469, acc 0.96875, learning_rate 0.000100005
2017-10-11T11:18:28.436440: step 3396, loss 0.310288, acc 0.921875, learning_rate 0.000100005
2017-10-11T11:18:28.521036: step 3397, loss 0.502504, acc 0.828125, learning_rate 0.000100005
2017-10-11T11:18:28.604098: step 3398, loss 0.379997, acc 0.90625, learning_rate 0.000100005
2017-10-11T11:18:28.686563: step 3399, loss 0.237935, acc 0.96875, learning_rate 0.000100005
2017-10-11T11:18:28.769989: step 3400, loss 0.383411, acc 0.9375, learning_rate 0.000100004

Evaluation:
2017-10-11T11:18:28.954788: step 3400, loss 0.235156, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3400

2017-10-11T11:18:29.450026: step 3401, loss 0.265204, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:18:29.534091: step 3402, loss 0.372005, acc 0.859375, learning_rate 0.000100004
2017-10-11T11:18:29.616610: step 3403, loss 0.238307, acc 0.90625, learning_rate 0.000100004
2017-10-11T11:18:29.697139: step 3404, loss 0.250463, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:18:29.777059: step 3405, loss 0.182962, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:18:29.859919: step 3406, loss 0.402478, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:18:29.940479: step 3407, loss 0.383889, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:18:30.020756: step 3408, loss 0.301286, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:18:30.104191: step 3409, loss 0.290584, acc 0.875, learning_rate 0.000100004
2017-10-11T11:18:30.188487: step 3410, loss 0.375135, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:18:30.270034: step 3411, loss 0.267506, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:18:30.352308: step 3412, loss 0.244513, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:18:30.433235: step 3413, loss 0.139451, acc 0.96875, learning_rate 0.000100004
2017-10-11T11:18:30.515251: step 3414, loss 0.137098, acc 0.984375, learning_rate 0.000100004
2017-10-11T11:18:30.594156: step 3415, loss 0.20134, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:18:30.676462: step 3416, loss 0.332625, acc 0.875, learning_rate 0.000100004
2017-10-11T11:18:30.757626: step 3417, loss 0.307371, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:18:30.839608: step 3418, loss 0.315731, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:18:30.919435: step 3419, loss 0.296453, acc 0.875, learning_rate 0.000100004
2017-10-11T11:18:31.003311: step 3420, loss 0.269754, acc 0.90625, learning_rate 0.000100004
2017-10-11T11:18:31.084825: step 3421, loss 0.360769, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:18:31.166490: step 3422, loss 0.260614, acc 0.90625, learning_rate 0.000100004
2017-10-11T11:18:31.249968: step 3423, loss 0.225483, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:18:31.331149: step 3424, loss 0.344017, acc 0.90625, learning_rate 0.000100004
2017-10-11T11:18:31.410906: step 3425, loss 0.415258, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:18:31.492879: step 3426, loss 0.523827, acc 0.8125, learning_rate 0.000100004
2017-10-11T11:18:31.574407: step 3427, loss 0.208012, acc 0.90625, learning_rate 0.000100004
2017-10-11T11:18:31.655807: step 3428, loss 0.200581, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:18:31.738306: step 3429, loss 0.22101, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:18:31.809259: step 3430, loss 0.340741, acc 0.901961, learning_rate 0.000100004
2017-10-11T11:18:31.891220: step 3431, loss 0.227102, acc 0.90625, learning_rate 0.000100004
2017-10-11T11:18:31.971956: step 3432, loss 0.229442, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:18:32.055276: step 3433, loss 0.22709, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:18:32.135852: step 3434, loss 0.231059, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:18:32.218287: step 3435, loss 0.415123, acc 0.875, learning_rate 0.000100004
2017-10-11T11:18:32.303268: step 3436, loss 0.213195, acc 0.96875, learning_rate 0.000100004
2017-10-11T11:18:32.386742: step 3437, loss 0.331758, acc 0.875, learning_rate 0.000100004
2017-10-11T11:18:32.469451: step 3438, loss 0.362852, acc 0.859375, learning_rate 0.000100004
2017-10-11T11:18:32.552857: step 3439, loss 0.157475, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:18:32.635728: step 3440, loss 0.490347, acc 0.859375, learning_rate 0.000100004

Evaluation:
2017-10-11T11:18:32.828822: step 3440, loss 0.234204, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3440

2017-10-11T11:18:33.333612: step 3441, loss 0.502349, acc 0.859375, learning_rate 0.000100004
2017-10-11T11:18:33.414917: step 3442, loss 0.326592, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:18:33.496822: step 3443, loss 0.393423, acc 0.875, learning_rate 0.000100004
2017-10-11T11:18:33.576379: step 3444, loss 0.303437, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:18:33.658063: step 3445, loss 0.278655, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:18:33.739593: step 3446, loss 0.446773, acc 0.8125, learning_rate 0.000100004
2017-10-11T11:18:33.822184: step 3447, loss 0.286428, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:18:33.905771: step 3448, loss 0.274681, acc 0.90625, learning_rate 0.000100004
2017-10-11T11:18:33.985930: step 3449, loss 0.221877, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:18:34.067658: step 3450, loss 0.209891, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:18:34.151868: step 3451, loss 0.100027, acc 0.984375, learning_rate 0.000100004
2017-10-11T11:18:34.232830: step 3452, loss 0.317636, acc 0.875, learning_rate 0.000100004
2017-10-11T11:18:34.324000: step 3453, loss 0.163106, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:18:34.405864: step 3454, loss 0.186156, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:18:34.488959: step 3455, loss 0.221052, acc 0.9375, learning_rate 0.000100004
2017-10-11T11:18:34.571218: step 3456, loss 0.292379, acc 0.921875, learning_rate 0.000100004
2017-10-11T11:18:34.655374: step 3457, loss 0.290825, acc 0.90625, learning_rate 0.000100004
2017-10-11T11:18:34.735968: step 3458, loss 0.235026, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:18:34.815696: step 3459, loss 0.193858, acc 0.953125, learning_rate 0.000100004
2017-10-11T11:18:34.900156: step 3460, loss 0.342237, acc 0.875, learning_rate 0.000100004
2017-10-11T11:18:34.979864: step 3461, loss 0.335935, acc 0.890625, learning_rate 0.000100004
2017-10-11T11:18:35.060937: step 3462, loss 0.247298, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:18:35.144194: step 3463, loss 0.253826, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:35.232081: step 3464, loss 0.332851, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:35.312971: step 3465, loss 0.346935, acc 0.859375, learning_rate 0.000100003
2017-10-11T11:18:35.394801: step 3466, loss 0.527487, acc 0.84375, learning_rate 0.000100003
2017-10-11T11:18:35.476004: step 3467, loss 0.440193, acc 0.8125, learning_rate 0.000100003
2017-10-11T11:18:35.561484: step 3468, loss 0.188604, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:35.648043: step 3469, loss 0.234248, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:18:35.728528: step 3470, loss 0.237391, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:35.811966: step 3471, loss 0.271038, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:18:35.895221: step 3472, loss 0.219998, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:18:35.979581: step 3473, loss 0.147627, acc 0.96875, learning_rate 0.000100003
2017-10-11T11:18:36.062196: step 3474, loss 0.338211, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:18:36.147509: step 3475, loss 0.22179, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:18:36.232882: step 3476, loss 0.285806, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:36.317396: step 3477, loss 0.290648, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:36.400401: step 3478, loss 0.341854, acc 0.875, learning_rate 0.000100003
2017-10-11T11:18:36.482996: step 3479, loss 0.205852, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:18:36.562933: step 3480, loss 0.32335, acc 0.90625, learning_rate 0.000100003

Evaluation:
2017-10-11T11:18:36.749806: step 3480, loss 0.23303, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3480

2017-10-11T11:18:37.254372: step 3481, loss 0.287359, acc 0.875, learning_rate 0.000100003
2017-10-11T11:18:37.338776: step 3482, loss 0.291404, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:18:37.421124: step 3483, loss 0.176741, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:18:37.500895: step 3484, loss 0.293853, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:37.579400: step 3485, loss 0.294748, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:18:37.663150: step 3486, loss 0.108173, acc 0.96875, learning_rate 0.000100003
2017-10-11T11:18:37.746674: step 3487, loss 0.116597, acc 0.984375, learning_rate 0.000100003
2017-10-11T11:18:37.829021: step 3488, loss 0.319078, acc 0.875, learning_rate 0.000100003
2017-10-11T11:18:37.911358: step 3489, loss 0.245993, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:18:37.993691: step 3490, loss 0.213198, acc 0.96875, learning_rate 0.000100003
2017-10-11T11:18:38.073995: step 3491, loss 0.364406, acc 0.84375, learning_rate 0.000100003
2017-10-11T11:18:38.161797: step 3492, loss 0.170534, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:18:38.245687: step 3493, loss 0.312672, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:18:38.325871: step 3494, loss 0.189548, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:18:38.408540: step 3495, loss 0.311156, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:38.490521: step 3496, loss 0.205388, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:18:38.571307: step 3497, loss 0.237882, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:18:38.655290: step 3498, loss 0.66878, acc 0.78125, learning_rate 0.000100003
2017-10-11T11:18:38.738555: step 3499, loss 0.179201, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:18:38.821105: step 3500, loss 0.296169, acc 0.859375, learning_rate 0.000100003
2017-10-11T11:18:38.903581: step 3501, loss 0.404052, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:18:38.986559: step 3502, loss 0.297783, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:18:39.068985: step 3503, loss 0.371475, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:39.153631: step 3504, loss 0.250273, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:18:39.236533: step 3505, loss 0.301379, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:39.321801: step 3506, loss 0.230859, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:18:39.403010: step 3507, loss 0.340196, acc 0.84375, learning_rate 0.000100003
2017-10-11T11:18:39.484910: step 3508, loss 0.211507, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:18:39.566144: step 3509, loss 0.427093, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:18:39.643961: step 3510, loss 0.415287, acc 0.8125, learning_rate 0.000100003
2017-10-11T11:18:39.728283: step 3511, loss 0.298029, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:39.806113: step 3512, loss 0.365644, acc 0.875, learning_rate 0.000100003
2017-10-11T11:18:39.887321: step 3513, loss 0.200648, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:18:39.967927: step 3514, loss 0.381469, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:18:40.049184: step 3515, loss 0.367144, acc 0.859375, learning_rate 0.000100003
2017-10-11T11:18:40.130370: step 3516, loss 0.186623, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:18:40.210013: step 3517, loss 0.165733, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:18:40.294389: step 3518, loss 0.193964, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:18:40.377161: step 3519, loss 0.290153, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:18:40.459838: step 3520, loss 0.299387, acc 0.890625, learning_rate 0.000100003

Evaluation:
2017-10-11T11:18:40.646679: step 3520, loss 0.232924, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3520

2017-10-11T11:18:41.152722: step 3521, loss 0.310045, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:41.235075: step 3522, loss 0.411964, acc 0.859375, learning_rate 0.000100003
2017-10-11T11:18:41.318271: step 3523, loss 0.271013, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:18:41.399262: step 3524, loss 0.245889, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:18:41.482194: step 3525, loss 0.274934, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:18:41.565784: step 3526, loss 0.29666, acc 0.890625, learning_rate 0.000100003
2017-10-11T11:18:41.648887: step 3527, loss 0.231257, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:41.720037: step 3528, loss 0.109527, acc 0.960784, learning_rate 0.000100003
2017-10-11T11:18:41.801644: step 3529, loss 0.38205, acc 0.84375, learning_rate 0.000100003
2017-10-11T11:18:41.888446: step 3530, loss 0.157683, acc 0.96875, learning_rate 0.000100003
2017-10-11T11:18:41.972396: step 3531, loss 0.339839, acc 0.828125, learning_rate 0.000100003
2017-10-11T11:18:42.057069: step 3532, loss 0.130196, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:18:42.138340: step 3533, loss 0.333785, acc 0.859375, learning_rate 0.000100003
2017-10-11T11:18:42.223402: step 3534, loss 0.196305, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:18:42.305855: step 3535, loss 0.153088, acc 0.953125, learning_rate 0.000100003
2017-10-11T11:18:42.388951: step 3536, loss 0.265748, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:42.472955: step 3537, loss 0.290642, acc 0.859375, learning_rate 0.000100003
2017-10-11T11:18:42.556952: step 3538, loss 0.295302, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:18:42.637287: step 3539, loss 0.314038, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:18:42.721069: step 3540, loss 0.234072, acc 0.921875, learning_rate 0.000100003
2017-10-11T11:18:42.805517: step 3541, loss 0.304106, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:42.889778: step 3542, loss 0.283326, acc 0.9375, learning_rate 0.000100003
2017-10-11T11:18:42.970430: step 3543, loss 0.295041, acc 0.90625, learning_rate 0.000100003
2017-10-11T11:18:43.050313: step 3544, loss 0.254491, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:18:43.136706: step 3545, loss 0.135686, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:18:43.219071: step 3546, loss 0.478798, acc 0.828125, learning_rate 0.000100002
2017-10-11T11:18:43.302108: step 3547, loss 0.203558, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:43.381052: step 3548, loss 0.189781, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:43.467094: step 3549, loss 0.271126, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:43.553840: step 3550, loss 0.246804, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:43.638904: step 3551, loss 0.194318, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:18:43.721521: step 3552, loss 0.220702, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:43.806319: step 3553, loss 0.226259, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:43.887956: step 3554, loss 0.163215, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:18:43.970465: step 3555, loss 0.259726, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:44.053182: step 3556, loss 0.228508, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:44.131364: step 3557, loss 0.423476, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:18:44.215717: step 3558, loss 0.319083, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:18:44.297989: step 3559, loss 0.278616, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:44.380158: step 3560, loss 0.285643, acc 0.90625, learning_rate 0.000100002

Evaluation:
2017-10-11T11:18:44.572149: step 3560, loss 0.233322, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3560

2017-10-11T11:18:45.077629: step 3561, loss 0.23853, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:18:45.158853: step 3562, loss 0.223252, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:45.241269: step 3563, loss 0.4619, acc 0.828125, learning_rate 0.000100002
2017-10-11T11:18:45.326461: step 3564, loss 0.191824, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:45.410199: step 3565, loss 0.180717, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:45.492842: step 3566, loss 0.283244, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:18:45.575663: step 3567, loss 0.213464, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:45.656754: step 3568, loss 0.235814, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:18:45.737779: step 3569, loss 0.261304, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:45.817961: step 3570, loss 0.330173, acc 0.859375, learning_rate 0.000100002
2017-10-11T11:18:45.900436: step 3571, loss 0.401494, acc 0.84375, learning_rate 0.000100002
2017-10-11T11:18:45.983773: step 3572, loss 0.214912, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:18:46.064819: step 3573, loss 0.285279, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:46.145312: step 3574, loss 0.463802, acc 0.859375, learning_rate 0.000100002
2017-10-11T11:18:46.227663: step 3575, loss 0.201492, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:46.312178: step 3576, loss 0.300344, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:46.394175: step 3577, loss 0.32756, acc 0.84375, learning_rate 0.000100002
2017-10-11T11:18:46.477498: step 3578, loss 0.228925, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:46.559413: step 3579, loss 0.372119, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:18:46.642701: step 3580, loss 0.249775, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:46.726290: step 3581, loss 0.241072, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:46.807518: step 3582, loss 0.173376, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:18:46.890234: step 3583, loss 0.304798, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:46.976427: step 3584, loss 0.440494, acc 0.859375, learning_rate 0.000100002
2017-10-11T11:18:47.060157: step 3585, loss 0.16224, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:47.142072: step 3586, loss 0.336343, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:47.223039: step 3587, loss 0.161342, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:18:47.302043: step 3588, loss 0.169054, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:18:47.386506: step 3589, loss 0.195409, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:47.470270: step 3590, loss 0.402413, acc 0.859375, learning_rate 0.000100002
2017-10-11T11:18:47.553960: step 3591, loss 0.17574, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:18:47.637366: step 3592, loss 0.336259, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:47.721147: step 3593, loss 0.3433, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:47.803992: step 3594, loss 0.249809, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:47.886992: step 3595, loss 0.341388, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:47.969547: step 3596, loss 0.23982, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:18:48.052635: step 3597, loss 0.410686, acc 0.84375, learning_rate 0.000100002
2017-10-11T11:18:48.136427: step 3598, loss 0.208022, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:48.220082: step 3599, loss 0.189641, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:48.300562: step 3600, loss 0.338475, acc 0.921875, learning_rate 0.000100002

Evaluation:
2017-10-11T11:18:48.486946: step 3600, loss 0.232968, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3600

2017-10-11T11:18:49.000878: step 3601, loss 0.219487, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:18:49.085398: step 3602, loss 0.328554, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:49.169730: step 3603, loss 0.217003, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:49.253283: step 3604, loss 0.182762, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:49.334277: step 3605, loss 0.252667, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:49.416099: step 3606, loss 0.31796, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:49.498181: step 3607, loss 0.4045, acc 0.859375, learning_rate 0.000100002
2017-10-11T11:18:49.581174: step 3608, loss 0.353245, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:49.664072: step 3609, loss 0.327364, acc 0.84375, learning_rate 0.000100002
2017-10-11T11:18:49.746500: step 3610, loss 0.207011, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:49.827627: step 3611, loss 0.343822, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:49.909741: step 3612, loss 0.196705, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:49.992489: step 3613, loss 0.280097, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:18:50.074149: step 3614, loss 0.308765, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:18:50.157027: step 3615, loss 0.341567, acc 0.859375, learning_rate 0.000100002
2017-10-11T11:18:50.236983: step 3616, loss 0.337495, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:50.317417: step 3617, loss 0.277983, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:50.404448: step 3618, loss 0.199427, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:50.488480: step 3619, loss 0.263408, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:50.570349: step 3620, loss 0.318612, acc 0.828125, learning_rate 0.000100002
2017-10-11T11:18:50.651451: step 3621, loss 0.353243, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:18:50.733411: step 3622, loss 0.472409, acc 0.84375, learning_rate 0.000100002
2017-10-11T11:18:50.815162: step 3623, loss 0.332756, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:50.898240: step 3624, loss 0.276944, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:50.980566: step 3625, loss 0.323475, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:51.052387: step 3626, loss 0.378063, acc 0.901961, learning_rate 0.000100002
2017-10-11T11:18:51.138182: step 3627, loss 0.415811, acc 0.859375, learning_rate 0.000100002
2017-10-11T11:18:51.218927: step 3628, loss 0.223654, acc 0.96875, learning_rate 0.000100002
2017-10-11T11:18:51.301613: step 3629, loss 0.221253, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:51.386363: step 3630, loss 0.275391, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:51.469867: step 3631, loss 0.216632, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:18:51.551228: step 3632, loss 0.351918, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:51.632631: step 3633, loss 0.34468, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:18:51.714684: step 3634, loss 0.240231, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:51.793267: step 3635, loss 0.221617, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:51.876431: step 3636, loss 0.595314, acc 0.796875, learning_rate 0.000100002
2017-10-11T11:18:51.957251: step 3637, loss 0.161104, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:52.038556: step 3638, loss 0.266633, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:52.121850: step 3639, loss 0.184774, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:52.203861: step 3640, loss 0.164959, acc 0.9375, learning_rate 0.000100002

Evaluation:
2017-10-11T11:18:52.392558: step 3640, loss 0.233197, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3640

2017-10-11T11:18:52.903873: step 3641, loss 0.551806, acc 0.84375, learning_rate 0.000100002
2017-10-11T11:18:52.987642: step 3642, loss 0.31981, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:53.069702: step 3643, loss 0.279096, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:53.150461: step 3644, loss 0.277752, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:53.230590: step 3645, loss 0.190975, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:53.314657: step 3646, loss 0.237627, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:53.395400: step 3647, loss 0.278014, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:53.479045: step 3648, loss 0.198871, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:18:53.562454: step 3649, loss 0.231515, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:53.643900: step 3650, loss 0.282191, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:53.727832: step 3651, loss 0.219848, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:18:53.812382: step 3652, loss 0.248874, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:53.892750: step 3653, loss 0.299265, acc 0.859375, learning_rate 0.000100002
2017-10-11T11:18:53.974564: step 3654, loss 0.376026, acc 0.890625, learning_rate 0.000100002
2017-10-11T11:18:54.054376: step 3655, loss 0.233187, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:54.141102: step 3656, loss 0.268038, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:54.222418: step 3657, loss 0.195436, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:18:54.306966: step 3658, loss 0.210313, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:54.388138: step 3659, loss 0.319543, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:54.470473: step 3660, loss 0.333815, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:54.552885: step 3661, loss 0.349889, acc 0.859375, learning_rate 0.000100002
2017-10-11T11:18:54.636846: step 3662, loss 0.315377, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:54.719171: step 3663, loss 0.249462, acc 0.921875, learning_rate 0.000100002
2017-10-11T11:18:54.802703: step 3664, loss 0.235656, acc 0.9375, learning_rate 0.000100002
2017-10-11T11:18:54.886342: step 3665, loss 0.373852, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:54.969863: step 3666, loss 0.375609, acc 0.875, learning_rate 0.000100002
2017-10-11T11:18:55.053564: step 3667, loss 0.247017, acc 0.90625, learning_rate 0.000100002
2017-10-11T11:18:55.135149: step 3668, loss 0.185212, acc 0.953125, learning_rate 0.000100002
2017-10-11T11:18:55.214572: step 3669, loss 0.122852, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:18:55.302570: step 3670, loss 0.265886, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:18:55.381552: step 3671, loss 0.264019, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:18:55.466395: step 3672, loss 0.351954, acc 0.875, learning_rate 0.000100001
2017-10-11T11:18:55.551527: step 3673, loss 0.361627, acc 0.875, learning_rate 0.000100001
2017-10-11T11:18:55.632198: step 3674, loss 0.278488, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:18:55.717047: step 3675, loss 0.254898, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:18:55.800757: step 3676, loss 0.207314, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:18:55.885663: step 3677, loss 0.334508, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:18:55.968065: step 3678, loss 0.313297, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:18:56.052350: step 3679, loss 0.198975, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:18:56.133342: step 3680, loss 0.23758, acc 0.921875, learning_rate 0.000100001

Evaluation:
2017-10-11T11:18:56.326860: step 3680, loss 0.232249, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3680

2017-10-11T11:18:56.810906: step 3681, loss 0.175655, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:18:56.893138: step 3682, loss 0.268005, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:18:56.977702: step 3683, loss 0.26431, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:18:57.058249: step 3684, loss 0.229505, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:18:57.143182: step 3685, loss 0.277543, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:18:57.223225: step 3686, loss 0.193469, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:18:57.306113: step 3687, loss 0.35026, acc 0.84375, learning_rate 0.000100001
2017-10-11T11:18:57.387611: step 3688, loss 0.313986, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:18:57.470695: step 3689, loss 0.349889, acc 0.84375, learning_rate 0.000100001
2017-10-11T11:18:57.552826: step 3690, loss 0.278879, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:18:57.638020: step 3691, loss 0.31682, acc 0.875, learning_rate 0.000100001
2017-10-11T11:18:57.719702: step 3692, loss 0.174143, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:18:57.800931: step 3693, loss 0.37846, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:18:57.885468: step 3694, loss 0.295327, acc 0.875, learning_rate 0.000100001
2017-10-11T11:18:57.966967: step 3695, loss 0.239122, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:18:58.050702: step 3696, loss 0.318745, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:18:58.134538: step 3697, loss 0.288551, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:18:58.215285: step 3698, loss 0.198944, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:18:58.297946: step 3699, loss 0.364481, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:18:58.379552: step 3700, loss 0.282537, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:18:58.462050: step 3701, loss 0.268656, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:18:58.545470: step 3702, loss 0.260128, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:18:58.626687: step 3703, loss 0.278468, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:18:58.707069: step 3704, loss 0.365128, acc 0.875, learning_rate 0.000100001
2017-10-11T11:18:58.795842: step 3705, loss 0.227488, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:18:58.876363: step 3706, loss 0.16007, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:18:58.959928: step 3707, loss 0.209359, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:18:59.041707: step 3708, loss 0.350351, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:18:59.125210: step 3709, loss 0.338468, acc 0.875, learning_rate 0.000100001
2017-10-11T11:18:59.210083: step 3710, loss 0.377203, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:18:59.293113: step 3711, loss 0.348564, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:18:59.375185: step 3712, loss 0.249947, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:18:59.458781: step 3713, loss 0.175933, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:18:59.541528: step 3714, loss 0.314166, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:18:59.623345: step 3715, loss 0.316792, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:18:59.708862: step 3716, loss 0.394222, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:18:59.791569: step 3717, loss 0.238418, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:18:59.875683: step 3718, loss 0.208669, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:18:59.958529: step 3719, loss 0.194867, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:00.039796: step 3720, loss 0.371213, acc 0.84375, learning_rate 0.000100001

Evaluation:
2017-10-11T11:19:00.230075: step 3720, loss 0.232067, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3720

2017-10-11T11:19:00.729847: step 3721, loss 0.147545, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:00.809783: step 3722, loss 0.256816, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:00.891983: step 3723, loss 0.218115, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:00.963184: step 3724, loss 0.279724, acc 0.882353, learning_rate 0.000100001
2017-10-11T11:19:01.047643: step 3725, loss 0.190484, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:01.129115: step 3726, loss 0.278911, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:01.210875: step 3727, loss 0.235448, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:01.289739: step 3728, loss 0.311957, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:01.370092: step 3729, loss 0.184074, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:01.451791: step 3730, loss 0.364612, acc 0.8125, learning_rate 0.000100001
2017-10-11T11:19:01.533591: step 3731, loss 0.283945, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:01.616816: step 3732, loss 0.352414, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:01.700720: step 3733, loss 0.286158, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:01.784390: step 3734, loss 0.197698, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:01.865794: step 3735, loss 0.271734, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:01.947291: step 3736, loss 0.24096, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:02.029049: step 3737, loss 0.159617, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:02.108845: step 3738, loss 0.293325, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:02.194060: step 3739, loss 0.235988, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:02.276599: step 3740, loss 0.313466, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:02.357674: step 3741, loss 0.333979, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:02.436961: step 3742, loss 0.411694, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:02.518897: step 3743, loss 0.367969, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:02.600639: step 3744, loss 0.387437, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:02.683032: step 3745, loss 0.2543, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:02.763778: step 3746, loss 0.168562, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:02.846394: step 3747, loss 0.225963, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:02.927536: step 3748, loss 0.219356, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:03.008782: step 3749, loss 0.272383, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:03.092081: step 3750, loss 0.149532, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:03.176405: step 3751, loss 0.244388, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:03.259760: step 3752, loss 0.308128, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:19:03.343544: step 3753, loss 0.190768, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:03.425768: step 3754, loss 0.326205, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:03.506054: step 3755, loss 0.218818, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:03.589178: step 3756, loss 0.289089, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:03.673147: step 3757, loss 0.286514, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:03.757586: step 3758, loss 0.225871, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:03.839530: step 3759, loss 0.36219, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:03.922910: step 3760, loss 0.308304, acc 0.890625, learning_rate 0.000100001

Evaluation:
2017-10-11T11:19:04.112127: step 3760, loss 0.232815, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3760

2017-10-11T11:19:04.611905: step 3761, loss 0.416939, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:04.695932: step 3762, loss 0.202291, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:04.776453: step 3763, loss 0.277135, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:04.856966: step 3764, loss 0.354644, acc 0.84375, learning_rate 0.000100001
2017-10-11T11:19:04.936854: step 3765, loss 0.233066, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:05.018126: step 3766, loss 0.235961, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:05.099285: step 3767, loss 0.207695, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:05.181604: step 3768, loss 0.161284, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:05.261956: step 3769, loss 0.227572, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:05.340019: step 3770, loss 0.374388, acc 0.828125, learning_rate 0.000100001
2017-10-11T11:19:05.424980: step 3771, loss 0.285618, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:05.506951: step 3772, loss 0.30672, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:05.588815: step 3773, loss 0.158784, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:05.668846: step 3774, loss 0.159124, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:05.753423: step 3775, loss 0.284355, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:05.834545: step 3776, loss 0.276582, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:05.918713: step 3777, loss 0.205138, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:06.003328: step 3778, loss 0.222447, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:06.083989: step 3779, loss 0.180967, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:06.168129: step 3780, loss 0.36732, acc 0.828125, learning_rate 0.000100001
2017-10-11T11:19:06.251872: step 3781, loss 0.179221, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:06.331969: step 3782, loss 0.328681, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:06.412673: step 3783, loss 0.329792, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:06.494376: step 3784, loss 0.178753, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:06.572410: step 3785, loss 0.248864, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:06.654449: step 3786, loss 0.295403, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:06.733557: step 3787, loss 0.336177, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:19:06.815023: step 3788, loss 0.144457, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:19:06.894735: step 3789, loss 0.331894, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:06.976409: step 3790, loss 0.415688, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:07.056112: step 3791, loss 0.277358, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:07.138666: step 3792, loss 0.348872, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:07.221877: step 3793, loss 0.277813, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:07.303321: step 3794, loss 0.284643, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:07.384784: step 3795, loss 0.228634, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:07.467199: step 3796, loss 0.402439, acc 0.8125, learning_rate 0.000100001
2017-10-11T11:19:07.550060: step 3797, loss 0.249159, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:07.632697: step 3798, loss 0.158629, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:07.717089: step 3799, loss 0.238714, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:07.798712: step 3800, loss 0.187019, acc 0.953125, learning_rate 0.000100001

Evaluation:
2017-10-11T11:19:07.985994: step 3800, loss 0.231908, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3800

2017-10-11T11:19:08.483934: step 3801, loss 0.129391, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:08.568680: step 3802, loss 0.436728, acc 0.84375, learning_rate 0.000100001
2017-10-11T11:19:08.650604: step 3803, loss 0.18579, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:08.734047: step 3804, loss 0.246271, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:08.818112: step 3805, loss 0.446075, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:08.905762: step 3806, loss 0.323787, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:08.986355: step 3807, loss 0.210526, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:09.066846: step 3808, loss 0.281959, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:09.147887: step 3809, loss 0.468865, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:09.233127: step 3810, loss 0.296932, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:09.314554: step 3811, loss 0.139949, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:09.397777: step 3812, loss 0.318822, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:09.480201: step 3813, loss 0.340632, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:19:09.564080: step 3814, loss 0.287506, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:09.645301: step 3815, loss 0.238001, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:09.727635: step 3816, loss 0.3202, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:09.808686: step 3817, loss 0.216536, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:09.891651: step 3818, loss 0.39293, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:09.971569: step 3819, loss 0.225414, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:10.053776: step 3820, loss 0.236721, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:10.133842: step 3821, loss 0.366643, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:10.202834: step 3822, loss 0.185478, acc 0.941176, learning_rate 0.000100001
2017-10-11T11:19:10.286650: step 3823, loss 0.396149, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:19:10.367568: step 3824, loss 0.246807, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:10.449097: step 3825, loss 0.274997, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:10.531457: step 3826, loss 0.194123, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:10.612740: step 3827, loss 0.270602, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:10.694861: step 3828, loss 0.230817, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:10.777128: step 3829, loss 0.156147, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:10.859396: step 3830, loss 0.183325, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:10.943491: step 3831, loss 0.0965553, acc 0.984375, learning_rate 0.000100001
2017-10-11T11:19:11.029172: step 3832, loss 0.191891, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:11.111576: step 3833, loss 0.1488, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:11.194571: step 3834, loss 0.17267, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:11.278024: step 3835, loss 0.206319, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:11.362327: step 3836, loss 0.438419, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:11.443639: step 3837, loss 0.389329, acc 0.828125, learning_rate 0.000100001
2017-10-11T11:19:11.524494: step 3838, loss 0.235532, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:11.607180: step 3839, loss 0.158611, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:11.688782: step 3840, loss 0.192262, acc 0.96875, learning_rate 0.000100001

Evaluation:
2017-10-11T11:19:11.879723: step 3840, loss 0.231175, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3840

2017-10-11T11:19:12.379987: step 3841, loss 0.189083, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:12.460427: step 3842, loss 0.24582, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:12.542126: step 3843, loss 0.311651, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:19:12.623148: step 3844, loss 0.221814, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:12.704820: step 3845, loss 0.545633, acc 0.828125, learning_rate 0.000100001
2017-10-11T11:19:12.782777: step 3846, loss 0.247633, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:12.867376: step 3847, loss 0.40992, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:19:12.949740: step 3848, loss 0.202323, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:13.031607: step 3849, loss 0.21543, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:13.112744: step 3850, loss 0.205858, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:13.191537: step 3851, loss 0.244095, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:13.274249: step 3852, loss 0.280499, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:13.356112: step 3853, loss 0.231813, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:13.439628: step 3854, loss 0.170952, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:13.520024: step 3855, loss 0.226927, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:13.603939: step 3856, loss 0.358719, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:19:13.686104: step 3857, loss 0.184865, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:13.770387: step 3858, loss 0.257521, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:13.853323: step 3859, loss 0.189671, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:13.934689: step 3860, loss 0.26747, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:14.019654: step 3861, loss 0.272699, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:14.100472: step 3862, loss 0.392688, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:14.183134: step 3863, loss 0.258111, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:14.267384: step 3864, loss 0.48143, acc 0.78125, learning_rate 0.000100001
2017-10-11T11:19:14.348278: step 3865, loss 0.2227, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:14.430372: step 3866, loss 0.292343, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:14.514779: step 3867, loss 0.376547, acc 0.84375, learning_rate 0.000100001
2017-10-11T11:19:14.596287: step 3868, loss 0.259339, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:14.674468: step 3869, loss 0.223266, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:14.756681: step 3870, loss 0.250387, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:14.841478: step 3871, loss 0.348194, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:14.923805: step 3872, loss 0.36377, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:15.005738: step 3873, loss 0.261138, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:15.086456: step 3874, loss 0.373103, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:19:15.169568: step 3875, loss 0.272077, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:15.254395: step 3876, loss 0.213014, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:15.335000: step 3877, loss 0.360783, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:15.416314: step 3878, loss 0.226753, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:15.496602: step 3879, loss 0.325669, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:15.577781: step 3880, loss 0.209563, acc 0.9375, learning_rate 0.000100001

Evaluation:
2017-10-11T11:19:15.767634: step 3880, loss 0.232477, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3880

2017-10-11T11:19:16.289649: step 3881, loss 0.278723, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:16.371063: step 3882, loss 0.298356, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:16.452396: step 3883, loss 0.327967, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:16.535292: step 3884, loss 0.294196, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:16.618563: step 3885, loss 0.305611, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:16.701811: step 3886, loss 0.214471, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:16.783451: step 3887, loss 0.316462, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:16.865788: step 3888, loss 0.168747, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:16.950658: step 3889, loss 0.242076, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:17.033529: step 3890, loss 0.250464, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:17.115373: step 3891, loss 0.276787, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:17.199160: step 3892, loss 0.309704, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:17.279054: step 3893, loss 0.234598, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:17.363534: step 3894, loss 0.530294, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:17.444280: step 3895, loss 0.296221, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:17.525658: step 3896, loss 0.244155, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:17.608565: step 3897, loss 0.286079, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:17.690544: step 3898, loss 0.266834, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:17.771548: step 3899, loss 0.415842, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:17.853068: step 3900, loss 0.32446, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:17.934219: step 3901, loss 0.281128, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:19:18.013557: step 3902, loss 0.338759, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:18.100795: step 3903, loss 0.373951, acc 0.859375, learning_rate 0.000100001
2017-10-11T11:19:18.181409: step 3904, loss 0.322985, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:18.263257: step 3905, loss 0.263539, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:18.344880: step 3906, loss 0.184921, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:18.429142: step 3907, loss 0.289273, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:18.513656: step 3908, loss 0.352275, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:18.595445: step 3909, loss 0.110878, acc 1, learning_rate 0.000100001
2017-10-11T11:19:18.677468: step 3910, loss 0.215652, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:18.761528: step 3911, loss 0.420772, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:18.846444: step 3912, loss 0.236581, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:18.926273: step 3913, loss 0.271062, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:19.006681: step 3914, loss 0.202251, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:19.090978: step 3915, loss 0.258477, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:19.173250: step 3916, loss 0.272373, acc 0.921875, learning_rate 0.000100001
2017-10-11T11:19:19.255088: step 3917, loss 0.482005, acc 0.8125, learning_rate 0.000100001
2017-10-11T11:19:19.337113: step 3918, loss 0.274462, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:19.419687: step 3919, loss 0.214222, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:19.488791: step 3920, loss 0.195226, acc 0.921569, learning_rate 0.000100001

Evaluation:
2017-10-11T11:19:19.680820: step 3920, loss 0.231815, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3920

2017-10-11T11:19:20.188796: step 3921, loss 0.248922, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:20.270220: step 3922, loss 0.336204, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:20.354196: step 3923, loss 0.213915, acc 0.96875, learning_rate 0.000100001
2017-10-11T11:19:20.436968: step 3924, loss 0.286066, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:20.516481: step 3925, loss 0.25452, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:20.598651: step 3926, loss 0.276832, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:20.679669: step 3927, loss 0.228902, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:20.760149: step 3928, loss 0.327731, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:20.841878: step 3929, loss 0.27397, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:20.923432: step 3930, loss 0.241242, acc 0.90625, learning_rate 0.000100001
2017-10-11T11:19:21.006388: step 3931, loss 0.24875, acc 0.875, learning_rate 0.000100001
2017-10-11T11:19:21.087488: step 3932, loss 0.249481, acc 0.9375, learning_rate 0.000100001
2017-10-11T11:19:21.170548: step 3933, loss 0.321227, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:21.251323: step 3934, loss 0.511915, acc 0.890625, learning_rate 0.000100001
2017-10-11T11:19:21.333839: step 3935, loss 0.185581, acc 0.953125, learning_rate 0.000100001
2017-10-11T11:19:21.415740: step 3936, loss 0.341483, acc 0.84375, learning_rate 0.000100001
2017-10-11T11:19:21.498622: step 3937, loss 0.277895, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:21.580213: step 3938, loss 0.271277, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:21.669786: step 3939, loss 0.27257, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:21.749884: step 3940, loss 0.228157, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:21.830633: step 3941, loss 0.221238, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:21.913073: step 3942, loss 0.279753, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:21.998172: step 3943, loss 0.258762, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:22.082850: step 3944, loss 0.171317, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:22.167112: step 3945, loss 0.279975, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:22.249425: step 3946, loss 0.256144, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:22.330299: step 3947, loss 0.294913, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:22.409652: step 3948, loss 0.312632, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:22.492145: step 3949, loss 0.244299, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:22.578094: step 3950, loss 0.402491, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:22.660873: step 3951, loss 0.332904, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:22.742392: step 3952, loss 0.289849, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:22.826210: step 3953, loss 0.273225, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:22.908164: step 3954, loss 0.174787, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:22.993721: step 3955, loss 0.233559, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:23.076613: step 3956, loss 0.122255, acc 0.984375, learning_rate 0.0001
2017-10-11T11:19:23.159968: step 3957, loss 0.191051, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:23.238800: step 3958, loss 0.179089, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:23.321109: step 3959, loss 0.395671, acc 0.84375, learning_rate 0.0001
2017-10-11T11:19:23.403734: step 3960, loss 0.224572, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:19:23.589242: step 3960, loss 0.231922, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-3960

2017-10-11T11:19:24.096086: step 3961, loss 0.371491, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:24.176848: step 3962, loss 0.208904, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:24.258237: step 3963, loss 0.144458, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:24.338545: step 3964, loss 0.309034, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:24.420685: step 3965, loss 0.301729, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:24.503699: step 3966, loss 0.231586, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:24.584450: step 3967, loss 0.180506, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:24.669867: step 3968, loss 0.237646, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:24.751879: step 3969, loss 0.296918, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:24.836590: step 3970, loss 0.232443, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:24.921219: step 3971, loss 0.352682, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:25.003090: step 3972, loss 0.266523, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:25.084390: step 3973, loss 0.203308, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:25.166729: step 3974, loss 0.207241, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:25.249503: step 3975, loss 0.196564, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:25.334966: step 3976, loss 0.226705, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:25.416401: step 3977, loss 0.237773, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:25.497000: step 3978, loss 0.257674, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:25.576463: step 3979, loss 0.257379, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:25.661906: step 3980, loss 0.216699, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:25.743484: step 3981, loss 0.175167, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:25.828000: step 3982, loss 0.11948, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:25.910576: step 3983, loss 0.271146, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:25.992343: step 3984, loss 0.37321, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:26.075929: step 3985, loss 0.256409, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:26.157701: step 3986, loss 0.230751, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:26.239887: step 3987, loss 0.161773, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:26.323014: step 3988, loss 0.279436, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:26.406781: step 3989, loss 0.335252, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:26.490551: step 3990, loss 0.412808, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:26.572563: step 3991, loss 0.232519, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:26.654512: step 3992, loss 0.226537, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:26.735468: step 3993, loss 0.249647, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:26.816015: step 3994, loss 0.213303, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:26.898899: step 3995, loss 0.381551, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:26.979366: step 3996, loss 0.375757, acc 0.84375, learning_rate 0.0001
2017-10-11T11:19:27.061477: step 3997, loss 0.359892, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:27.142970: step 3998, loss 0.356745, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:27.222527: step 3999, loss 0.269058, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:27.307784: step 4000, loss 0.1536, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:19:27.497807: step 4000, loss 0.231894, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4000

2017-10-11T11:19:27.932363: step 4001, loss 0.443384, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:28.020349: step 4002, loss 0.502734, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:28.101522: step 4003, loss 0.251209, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:28.184900: step 4004, loss 0.173254, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:28.266758: step 4005, loss 0.295093, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:28.351292: step 4006, loss 0.198263, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:28.433747: step 4007, loss 0.380829, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:28.515357: step 4008, loss 0.340047, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:28.600466: step 4009, loss 0.269656, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:28.682374: step 4010, loss 0.423646, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:28.762145: step 4011, loss 0.369082, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:28.842863: step 4012, loss 0.313223, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:28.924585: step 4013, loss 0.20882, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:29.007348: step 4014, loss 0.328775, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:29.089443: step 4015, loss 0.465071, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:29.171406: step 4016, loss 0.211811, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:29.253039: step 4017, loss 0.282679, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:29.324482: step 4018, loss 0.277585, acc 0.882353, learning_rate 0.0001
2017-10-11T11:19:29.408678: step 4019, loss 0.2521, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:29.490620: step 4020, loss 0.201709, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:29.572736: step 4021, loss 0.266149, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:29.654266: step 4022, loss 0.527808, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:29.730734: step 4023, loss 0.230424, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:29.814323: step 4024, loss 0.357081, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:29.897012: step 4025, loss 0.330419, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:29.981836: step 4026, loss 0.317503, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:30.065500: step 4027, loss 0.320785, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:30.146376: step 4028, loss 0.207757, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:30.225224: step 4029, loss 0.279147, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:30.307378: step 4030, loss 0.246521, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:30.389795: step 4031, loss 0.343941, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:30.475036: step 4032, loss 0.254849, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:30.557139: step 4033, loss 0.177359, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:30.639215: step 4034, loss 0.250415, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:30.721162: step 4035, loss 0.347088, acc 0.84375, learning_rate 0.0001
2017-10-11T11:19:30.804742: step 4036, loss 0.199383, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:30.894217: step 4037, loss 0.227679, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:30.974538: step 4038, loss 0.228687, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:31.062064: step 4039, loss 0.167645, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:31.145620: step 4040, loss 0.2777, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:19:31.332777: step 4040, loss 0.231165, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4040

2017-10-11T11:19:31.831815: step 4041, loss 0.17076, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:31.915012: step 4042, loss 0.180278, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:31.995122: step 4043, loss 0.233393, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:32.076716: step 4044, loss 0.272656, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:32.157797: step 4045, loss 0.401972, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:32.240268: step 4046, loss 0.343331, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:32.322762: step 4047, loss 0.307509, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:32.405700: step 4048, loss 0.294099, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:32.486087: step 4049, loss 0.255236, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:32.565889: step 4050, loss 0.267382, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:32.647305: step 4051, loss 0.270428, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:32.727866: step 4052, loss 0.342452, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:32.810865: step 4053, loss 0.293301, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:32.892980: step 4054, loss 0.335659, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:32.974799: step 4055, loss 0.306619, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:33.055903: step 4056, loss 0.174029, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:33.139328: step 4057, loss 0.299445, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:33.220817: step 4058, loss 0.187849, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:33.305213: step 4059, loss 0.173695, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:33.387317: step 4060, loss 0.227778, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:33.469246: step 4061, loss 0.307748, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:33.552106: step 4062, loss 0.358659, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:33.635990: step 4063, loss 0.186833, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:33.718243: step 4064, loss 0.311273, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:33.801003: step 4065, loss 0.230847, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:33.883454: step 4066, loss 0.240065, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:33.965492: step 4067, loss 0.331006, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:34.049050: step 4068, loss 0.148437, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:34.129384: step 4069, loss 0.229858, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:34.210377: step 4070, loss 0.41448, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:34.293359: step 4071, loss 0.407334, acc 0.828125, learning_rate 0.0001
2017-10-11T11:19:34.375785: step 4072, loss 0.219021, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:34.458020: step 4073, loss 0.252052, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:34.539793: step 4074, loss 0.239977, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:34.622102: step 4075, loss 0.337184, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:34.706636: step 4076, loss 0.194643, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:34.789179: step 4077, loss 0.273, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:34.873544: step 4078, loss 0.216682, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:34.960599: step 4079, loss 0.306937, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:35.042121: step 4080, loss 0.423434, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-11T11:19:35.234360: step 4080, loss 0.230782, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4080

2017-10-11T11:19:35.736025: step 4081, loss 0.293824, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:35.821761: step 4082, loss 0.287037, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:35.903849: step 4083, loss 0.305324, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:35.987395: step 4084, loss 0.15546, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:36.068551: step 4085, loss 0.255452, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:36.149899: step 4086, loss 0.210898, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:36.230902: step 4087, loss 0.299572, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:36.315347: step 4088, loss 0.292414, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:36.396424: step 4089, loss 0.252005, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:36.477667: step 4090, loss 0.339629, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:36.556413: step 4091, loss 0.288859, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:36.637832: step 4092, loss 0.19332, acc 0.984375, learning_rate 0.0001
2017-10-11T11:19:36.717630: step 4093, loss 0.271479, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:36.803397: step 4094, loss 0.186812, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:36.884859: step 4095, loss 0.206392, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:36.965423: step 4096, loss 0.161101, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:37.048688: step 4097, loss 0.16601, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:37.132473: step 4098, loss 0.23669, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:37.212961: step 4099, loss 0.171012, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:37.294244: step 4100, loss 0.170778, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:37.376163: step 4101, loss 0.295378, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:37.455408: step 4102, loss 0.394855, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:37.536484: step 4103, loss 0.295621, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:37.619296: step 4104, loss 0.238381, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:37.702009: step 4105, loss 0.335517, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:37.785531: step 4106, loss 0.236868, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:37.870819: step 4107, loss 0.240542, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:37.955396: step 4108, loss 0.29091, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:38.036859: step 4109, loss 0.309408, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:38.116783: step 4110, loss 0.242876, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:38.198149: step 4111, loss 0.292216, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:38.278774: step 4112, loss 0.193531, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:38.361442: step 4113, loss 0.296223, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:38.446776: step 4114, loss 0.311261, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:38.532766: step 4115, loss 0.276094, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:38.603987: step 4116, loss 0.224606, acc 0.921569, learning_rate 0.0001
2017-10-11T11:19:38.688458: step 4117, loss 0.214367, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:38.771121: step 4118, loss 0.234021, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:38.855698: step 4119, loss 0.155214, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:38.938483: step 4120, loss 0.211219, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:19:39.125262: step 4120, loss 0.229529, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4120

2017-10-11T11:19:39.623730: step 4121, loss 0.220698, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:39.707354: step 4122, loss 0.211313, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:39.791231: step 4123, loss 0.31543, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:39.873728: step 4124, loss 0.547306, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:39.954282: step 4125, loss 0.246853, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:40.037072: step 4126, loss 0.237394, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:40.118338: step 4127, loss 0.302464, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:40.201911: step 4128, loss 0.358831, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:40.281981: step 4129, loss 0.20698, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:40.365083: step 4130, loss 0.182417, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:40.447728: step 4131, loss 0.194562, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:40.529412: step 4132, loss 0.487593, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:40.609426: step 4133, loss 0.26429, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:40.690326: step 4134, loss 0.233347, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:40.771038: step 4135, loss 0.260988, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:40.851106: step 4136, loss 0.259056, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:40.933481: step 4137, loss 0.208931, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:41.016430: step 4138, loss 0.254476, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:41.096322: step 4139, loss 0.239109, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:41.177791: step 4140, loss 0.360019, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:41.260318: step 4141, loss 0.203205, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:41.344306: step 4142, loss 0.351865, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:41.425025: step 4143, loss 0.19555, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:41.507231: step 4144, loss 0.102996, acc 0.984375, learning_rate 0.0001
2017-10-11T11:19:41.589111: step 4145, loss 0.397252, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:41.674085: step 4146, loss 0.263449, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:41.754950: step 4147, loss 0.147953, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:41.834959: step 4148, loss 0.283446, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:41.925119: step 4149, loss 0.261873, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:42.008366: step 4150, loss 0.146729, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:42.094065: step 4151, loss 0.236265, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:42.175054: step 4152, loss 0.229045, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:42.257559: step 4153, loss 0.23247, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:42.340708: step 4154, loss 0.286482, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:42.424055: step 4155, loss 0.174299, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:42.506642: step 4156, loss 0.138784, acc 0.984375, learning_rate 0.0001
2017-10-11T11:19:42.590416: step 4157, loss 0.178989, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:42.671571: step 4158, loss 0.176797, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:42.749788: step 4159, loss 0.466243, acc 0.828125, learning_rate 0.0001
2017-10-11T11:19:42.833097: step 4160, loss 0.446701, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-11T11:19:43.017252: step 4160, loss 0.229092, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4160

2017-10-11T11:19:43.522355: step 4161, loss 0.235527, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:43.603939: step 4162, loss 0.310181, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:43.685572: step 4163, loss 0.19117, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:43.768635: step 4164, loss 0.17518, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:43.848767: step 4165, loss 0.124825, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:43.933035: step 4166, loss 0.17455, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:44.016858: step 4167, loss 0.221618, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:44.097668: step 4168, loss 0.222196, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:44.178420: step 4169, loss 0.418098, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:44.258839: step 4170, loss 0.163553, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:44.346210: step 4171, loss 0.32913, acc 0.84375, learning_rate 0.0001
2017-10-11T11:19:44.431409: step 4172, loss 0.336459, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:44.514067: step 4173, loss 0.22399, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:44.594593: step 4174, loss 0.308084, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:44.676634: step 4175, loss 0.26498, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:44.759700: step 4176, loss 0.403416, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:44.840345: step 4177, loss 0.210195, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:44.930351: step 4178, loss 0.258041, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:45.014294: step 4179, loss 0.274508, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:45.097891: step 4180, loss 0.409118, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:45.179372: step 4181, loss 0.212347, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:45.258949: step 4182, loss 0.383921, acc 0.828125, learning_rate 0.0001
2017-10-11T11:19:45.340608: step 4183, loss 0.229194, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:45.420955: step 4184, loss 0.268777, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:45.502204: step 4185, loss 0.32924, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:45.583829: step 4186, loss 0.181992, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:45.666682: step 4187, loss 0.292023, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:45.749368: step 4188, loss 0.235347, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:45.833518: step 4189, loss 0.198512, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:45.915797: step 4190, loss 0.32662, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:45.997707: step 4191, loss 0.269021, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:46.079146: step 4192, loss 0.331665, acc 0.84375, learning_rate 0.0001
2017-10-11T11:19:46.162435: step 4193, loss 0.40134, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:46.247551: step 4194, loss 0.300666, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:46.330413: step 4195, loss 0.370821, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:46.409532: step 4196, loss 0.183378, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:46.491483: step 4197, loss 0.258372, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:46.573793: step 4198, loss 0.203956, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:46.658554: step 4199, loss 0.546759, acc 0.84375, learning_rate 0.0001
2017-10-11T11:19:46.743960: step 4200, loss 0.287608, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:19:46.935483: step 4200, loss 0.227982, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4200

2017-10-11T11:19:47.440796: step 4201, loss 0.150929, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:47.522369: step 4202, loss 0.316429, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:47.604786: step 4203, loss 0.326902, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:47.689679: step 4204, loss 0.255843, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:47.771595: step 4205, loss 0.50641, acc 0.828125, learning_rate 0.0001
2017-10-11T11:19:47.855697: step 4206, loss 0.292535, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:47.935831: step 4207, loss 0.23544, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:48.017956: step 4208, loss 0.323873, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:48.101843: step 4209, loss 0.386036, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:48.185200: step 4210, loss 0.376483, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:48.268414: step 4211, loss 0.258673, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:48.350780: step 4212, loss 0.309256, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:48.433782: step 4213, loss 0.279413, acc 0.828125, learning_rate 0.0001
2017-10-11T11:19:48.505307: step 4214, loss 0.227054, acc 0.960784, learning_rate 0.0001
2017-10-11T11:19:48.587409: step 4215, loss 0.210689, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:48.670468: step 4216, loss 0.106824, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:48.754142: step 4217, loss 0.264457, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:48.835990: step 4218, loss 0.372261, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:48.920193: step 4219, loss 0.377876, acc 0.828125, learning_rate 0.0001
2017-10-11T11:19:49.003887: step 4220, loss 0.260284, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:49.087211: step 4221, loss 0.31782, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:49.170276: step 4222, loss 0.274479, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:49.254124: step 4223, loss 0.204026, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:49.341997: step 4224, loss 0.288696, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:49.424564: step 4225, loss 0.317952, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:49.505539: step 4226, loss 0.419652, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:49.586436: step 4227, loss 0.254251, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:49.667226: step 4228, loss 0.313893, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:49.750790: step 4229, loss 0.170843, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:49.831257: step 4230, loss 0.353788, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:49.915142: step 4231, loss 0.235509, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:50.001693: step 4232, loss 0.284103, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:50.087281: step 4233, loss 0.260355, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:50.169944: step 4234, loss 0.148176, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:50.251238: step 4235, loss 0.184546, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:50.333198: step 4236, loss 0.366968, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:50.416503: step 4237, loss 0.195165, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:50.497215: step 4238, loss 0.198976, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:50.578355: step 4239, loss 0.28141, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:50.660174: step 4240, loss 0.398087, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-11T11:19:50.843704: step 4240, loss 0.227441, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4240

2017-10-11T11:19:51.348345: step 4241, loss 0.280136, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:51.427269: step 4242, loss 0.257685, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:51.508135: step 4243, loss 0.201733, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:51.589401: step 4244, loss 0.265333, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:51.671047: step 4245, loss 0.240351, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:51.750308: step 4246, loss 0.41105, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:51.831139: step 4247, loss 0.324112, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:51.915126: step 4248, loss 0.228862, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:51.996003: step 4249, loss 0.266848, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:52.076965: step 4250, loss 0.29716, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:52.159389: step 4251, loss 0.237282, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:52.243486: step 4252, loss 0.182925, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:52.325757: step 4253, loss 0.283919, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:52.408557: step 4254, loss 0.378357, acc 0.84375, learning_rate 0.0001
2017-10-11T11:19:52.489385: step 4255, loss 0.27855, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:52.570064: step 4256, loss 0.195353, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:52.650726: step 4257, loss 0.410642, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:52.732526: step 4258, loss 0.273525, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:52.817346: step 4259, loss 0.302903, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:52.901482: step 4260, loss 0.278378, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:52.984243: step 4261, loss 0.200449, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:53.065587: step 4262, loss 0.369639, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:53.146119: step 4263, loss 0.253699, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:53.229027: step 4264, loss 0.306068, acc 0.84375, learning_rate 0.0001
2017-10-11T11:19:53.309405: step 4265, loss 0.216314, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:53.393305: step 4266, loss 0.238385, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:53.475370: step 4267, loss 0.285248, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:53.559603: step 4268, loss 0.265254, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:53.641800: step 4269, loss 0.213978, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:53.725715: step 4270, loss 0.304998, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:53.809613: step 4271, loss 0.327544, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:53.894663: step 4272, loss 0.2718, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:53.977063: step 4273, loss 0.322697, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:54.059357: step 4274, loss 0.210663, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:54.139653: step 4275, loss 0.293407, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:54.220150: step 4276, loss 0.17716, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:54.306170: step 4277, loss 0.274438, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:54.387169: step 4278, loss 0.225206, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:54.471132: step 4279, loss 0.265711, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:54.555144: step 4280, loss 0.43691, acc 0.828125, learning_rate 0.0001

Evaluation:
2017-10-11T11:19:54.743117: step 4280, loss 0.227717, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4280

2017-10-11T11:19:55.248843: step 4281, loss 0.21676, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:55.330216: step 4282, loss 0.215842, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:55.412591: step 4283, loss 0.205804, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:55.495154: step 4284, loss 0.247884, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:55.575869: step 4285, loss 0.250355, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:55.658281: step 4286, loss 0.248571, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:55.741260: step 4287, loss 0.245989, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:55.823987: step 4288, loss 0.188605, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:55.906387: step 4289, loss 0.236096, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:55.986090: step 4290, loss 0.226225, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:56.066210: step 4291, loss 0.270951, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:56.147735: step 4292, loss 0.155353, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:56.230585: step 4293, loss 0.195106, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:56.313717: step 4294, loss 0.331575, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:56.393503: step 4295, loss 0.278368, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:56.475840: step 4296, loss 0.275638, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:56.557460: step 4297, loss 0.128165, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:56.640436: step 4298, loss 0.22525, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:56.721522: step 4299, loss 0.207603, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:56.804874: step 4300, loss 0.203382, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:56.888901: step 4301, loss 0.235705, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:56.973361: step 4302, loss 0.192658, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:57.058059: step 4303, loss 0.104515, acc 0.984375, learning_rate 0.0001
2017-10-11T11:19:57.139883: step 4304, loss 0.26373, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:57.219526: step 4305, loss 0.196286, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:57.300345: step 4306, loss 0.206146, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:57.383171: step 4307, loss 0.273238, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:57.465057: step 4308, loss 0.478868, acc 0.84375, learning_rate 0.0001
2017-10-11T11:19:57.551146: step 4309, loss 0.281094, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:57.636416: step 4310, loss 0.1063, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:57.719761: step 4311, loss 0.253301, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:57.791256: step 4312, loss 0.409558, acc 0.862745, learning_rate 0.0001
2017-10-11T11:19:57.873096: step 4313, loss 0.185473, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:57.954927: step 4314, loss 0.346681, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:58.038595: step 4315, loss 0.278688, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:58.122416: step 4316, loss 0.248461, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:58.203952: step 4317, loss 0.237665, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:58.286086: step 4318, loss 0.181217, acc 0.953125, learning_rate 0.0001
2017-10-11T11:19:58.364410: step 4319, loss 0.237457, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:58.446872: step 4320, loss 0.303865, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-11T11:19:58.633051: step 4320, loss 0.228102, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4320

2017-10-11T11:19:59.144163: step 4321, loss 0.263109, acc 0.859375, learning_rate 0.0001
2017-10-11T11:19:59.227466: step 4322, loss 0.256989, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:59.308898: step 4323, loss 0.218376, acc 0.890625, learning_rate 0.0001
2017-10-11T11:19:59.391053: step 4324, loss 0.268498, acc 0.921875, learning_rate 0.0001
2017-10-11T11:19:59.471847: step 4325, loss 0.381825, acc 0.875, learning_rate 0.0001
2017-10-11T11:19:59.554588: step 4326, loss 0.259479, acc 0.90625, learning_rate 0.0001
2017-10-11T11:19:59.636044: step 4327, loss 0.131766, acc 0.96875, learning_rate 0.0001
2017-10-11T11:19:59.718695: step 4328, loss 0.176761, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:59.800631: step 4329, loss 0.160811, acc 0.9375, learning_rate 0.0001
2017-10-11T11:19:59.880493: step 4330, loss 0.406848, acc 0.84375, learning_rate 0.0001
2017-10-11T11:19:59.962508: step 4331, loss 0.349248, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:00.044208: step 4332, loss 0.223902, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:00.126859: step 4333, loss 0.328569, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:00.209405: step 4334, loss 0.207718, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:00.291082: step 4335, loss 0.224217, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:00.370703: step 4336, loss 0.465719, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:00.449831: step 4337, loss 0.183473, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:00.531043: step 4338, loss 0.370042, acc 0.84375, learning_rate 0.0001
2017-10-11T11:20:00.615849: step 4339, loss 0.105251, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:00.697030: step 4340, loss 0.165516, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:00.781566: step 4341, loss 0.312462, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:00.862499: step 4342, loss 0.273985, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:00.944946: step 4343, loss 0.161963, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:01.028149: step 4344, loss 0.305345, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:01.116188: step 4345, loss 0.171761, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:01.199879: step 4346, loss 0.326965, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:01.282808: step 4347, loss 0.232208, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:01.363769: step 4348, loss 0.373157, acc 0.828125, learning_rate 0.0001
2017-10-11T11:20:01.443684: step 4349, loss 0.276216, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:01.524035: step 4350, loss 0.256208, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:01.607161: step 4351, loss 0.127293, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:01.689387: step 4352, loss 0.243273, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:01.771230: step 4353, loss 0.316451, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:01.854948: step 4354, loss 0.305332, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:01.938333: step 4355, loss 0.365693, acc 0.84375, learning_rate 0.0001
2017-10-11T11:20:02.018533: step 4356, loss 0.264656, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:02.099157: step 4357, loss 0.284076, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:02.183302: step 4358, loss 0.269126, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:02.265694: step 4359, loss 0.333637, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:02.349412: step 4360, loss 0.297202, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:02.538497: step 4360, loss 0.227745, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4360

2017-10-11T11:20:02.976598: step 4361, loss 0.225683, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:03.058054: step 4362, loss 0.330804, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:03.142246: step 4363, loss 0.39733, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:03.226616: step 4364, loss 0.188202, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:03.309132: step 4365, loss 0.25735, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:03.393922: step 4366, loss 0.417108, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:03.477552: step 4367, loss 0.254735, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:03.559405: step 4368, loss 0.256426, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:03.647743: step 4369, loss 0.15959, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:03.727926: step 4370, loss 0.211526, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:03.808411: step 4371, loss 0.193314, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:03.894977: step 4372, loss 0.315576, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:03.978268: step 4373, loss 0.379477, acc 0.84375, learning_rate 0.0001
2017-10-11T11:20:04.059591: step 4374, loss 0.283593, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:04.139130: step 4375, loss 0.247023, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:04.220721: step 4376, loss 0.221617, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:04.305708: step 4377, loss 0.270768, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:04.390605: step 4378, loss 0.227509, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:04.476747: step 4379, loss 0.443808, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:04.560028: step 4380, loss 0.263377, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:04.639207: step 4381, loss 0.288646, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:04.719905: step 4382, loss 0.378534, acc 0.84375, learning_rate 0.0001
2017-10-11T11:20:04.802959: step 4383, loss 0.210374, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:04.887190: step 4384, loss 0.161336, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:04.969190: step 4385, loss 0.215156, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:05.051034: step 4386, loss 0.347595, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:05.136388: step 4387, loss 0.258536, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:05.220285: step 4388, loss 0.251908, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:05.303530: step 4389, loss 0.196797, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:05.387027: step 4390, loss 0.206542, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:05.470048: step 4391, loss 0.24736, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:05.549764: step 4392, loss 0.267386, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:05.634444: step 4393, loss 0.212882, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:05.715810: step 4394, loss 0.297802, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:05.800965: step 4395, loss 0.211951, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:05.883113: step 4396, loss 0.440716, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:05.963518: step 4397, loss 0.19484, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:06.043598: step 4398, loss 0.20363, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:06.128536: step 4399, loss 0.150717, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:06.212388: step 4400, loss 0.222732, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:06.401258: step 4400, loss 0.227747, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4400

2017-10-11T11:20:06.903680: step 4401, loss 0.258348, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:06.985380: step 4402, loss 0.181371, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:07.068681: step 4403, loss 0.0994421, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:07.154278: step 4404, loss 0.203195, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:07.237006: step 4405, loss 0.192694, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:07.317612: step 4406, loss 0.197063, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:07.401257: step 4407, loss 0.278625, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:07.482447: step 4408, loss 0.258394, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:07.563439: step 4409, loss 0.211201, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:07.633256: step 4410, loss 0.265278, acc 0.901961, learning_rate 0.0001
2017-10-11T11:20:07.717051: step 4411, loss 0.298955, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:07.799552: step 4412, loss 0.134691, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:07.883415: step 4413, loss 0.314528, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:07.968061: step 4414, loss 0.319588, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:08.053548: step 4415, loss 0.284976, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:08.136956: step 4416, loss 0.260408, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:08.218427: step 4417, loss 0.145166, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:08.303692: step 4418, loss 0.282378, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:08.387136: step 4419, loss 0.306036, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:08.469327: step 4420, loss 0.233229, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:08.553098: step 4421, loss 0.216127, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:08.634426: step 4422, loss 0.246609, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:08.715652: step 4423, loss 0.264277, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:08.796294: step 4424, loss 0.294362, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:08.879729: step 4425, loss 0.15695, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:08.963228: step 4426, loss 0.214408, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:09.046801: step 4427, loss 0.348063, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:09.129909: step 4428, loss 0.195774, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:09.213741: step 4429, loss 0.280183, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:09.294052: step 4430, loss 0.412482, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:09.376282: step 4431, loss 0.285098, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:09.460550: step 4432, loss 0.218841, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:09.540994: step 4433, loss 0.262099, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:09.623350: step 4434, loss 0.455244, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:09.705543: step 4435, loss 0.26809, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:09.790252: step 4436, loss 0.326799, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:09.876783: step 4437, loss 0.267724, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:09.959605: step 4438, loss 0.15155, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:10.041842: step 4439, loss 0.26886, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:10.123774: step 4440, loss 0.380945, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:10.305912: step 4440, loss 0.226963, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4440

2017-10-11T11:20:10.806227: step 4441, loss 0.247268, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:10.889830: step 4442, loss 0.336738, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:10.971444: step 4443, loss 0.222255, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:11.054495: step 4444, loss 0.363374, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:11.135157: step 4445, loss 0.164316, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:11.216197: step 4446, loss 0.153832, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:11.297841: step 4447, loss 0.366138, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:11.380243: step 4448, loss 0.269808, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:11.461192: step 4449, loss 0.220747, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:11.544475: step 4450, loss 0.112019, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:11.628168: step 4451, loss 0.317282, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:11.710584: step 4452, loss 0.291233, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:11.792761: step 4453, loss 0.139412, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:11.877233: step 4454, loss 0.211838, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:11.959015: step 4455, loss 0.199989, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:12.042410: step 4456, loss 0.265908, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:12.120907: step 4457, loss 0.428381, acc 0.78125, learning_rate 0.0001
2017-10-11T11:20:12.203912: step 4458, loss 0.263089, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:12.286036: step 4459, loss 0.367694, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:12.369038: step 4460, loss 0.252717, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:12.446793: step 4461, loss 0.258893, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:12.531970: step 4462, loss 0.268586, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:12.616093: step 4463, loss 0.138113, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:12.699512: step 4464, loss 0.34992, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:12.782271: step 4465, loss 0.17685, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:12.865794: step 4466, loss 0.272435, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:12.946721: step 4467, loss 0.196543, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:13.030858: step 4468, loss 0.287335, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:13.113056: step 4469, loss 0.210639, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:13.196037: step 4470, loss 0.309347, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:13.278752: step 4471, loss 0.253604, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:13.363070: step 4472, loss 0.338479, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:13.444507: step 4473, loss 0.178827, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:13.529361: step 4474, loss 0.199781, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:13.610914: step 4475, loss 0.303871, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:13.692163: step 4476, loss 0.363183, acc 0.8125, learning_rate 0.0001
2017-10-11T11:20:13.775842: step 4477, loss 0.360886, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:13.858803: step 4478, loss 0.333755, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:13.940204: step 4479, loss 0.14639, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:14.020419: step 4480, loss 0.183148, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:14.213406: step 4480, loss 0.226154, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4480

2017-10-11T11:20:14.713306: step 4481, loss 0.256205, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:14.798210: step 4482, loss 0.282365, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:14.881979: step 4483, loss 0.237025, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:14.963410: step 4484, loss 0.251859, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:15.046543: step 4485, loss 0.313508, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:15.125266: step 4486, loss 0.225404, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:15.208207: step 4487, loss 0.397456, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:15.290620: step 4488, loss 0.393107, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:15.371426: step 4489, loss 0.218528, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:15.455455: step 4490, loss 0.231133, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:15.538866: step 4491, loss 0.343784, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:15.620965: step 4492, loss 0.314666, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:15.703095: step 4493, loss 0.290015, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:15.785605: step 4494, loss 0.290268, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:15.870884: step 4495, loss 0.183356, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:15.951172: step 4496, loss 0.178041, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:16.032589: step 4497, loss 0.304889, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:16.117830: step 4498, loss 0.186022, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:16.197706: step 4499, loss 0.179067, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:16.285632: step 4500, loss 0.191328, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:16.368659: step 4501, loss 0.343749, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:16.451673: step 4502, loss 0.221493, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:16.533512: step 4503, loss 0.313572, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:16.613802: step 4504, loss 0.237415, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:16.695927: step 4505, loss 0.19523, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:16.777996: step 4506, loss 0.525319, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:16.861364: step 4507, loss 0.334447, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:16.930151: step 4508, loss 0.273909, acc 0.901961, learning_rate 0.0001
2017-10-11T11:20:17.012195: step 4509, loss 0.199562, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:17.093553: step 4510, loss 0.201731, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:17.178837: step 4511, loss 0.177287, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:17.259789: step 4512, loss 0.201905, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:17.342600: step 4513, loss 0.260469, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:17.422852: step 4514, loss 0.246471, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:17.503473: step 4515, loss 0.237806, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:17.585722: step 4516, loss 0.184653, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:17.664662: step 4517, loss 0.330263, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:17.747014: step 4518, loss 0.263382, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:17.829070: step 4519, loss 0.206896, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:17.909476: step 4520, loss 0.323448, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:18.094595: step 4520, loss 0.225746, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4520

2017-10-11T11:20:18.597750: step 4521, loss 0.236461, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:18.681357: step 4522, loss 0.321635, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:18.763976: step 4523, loss 0.354994, acc 0.828125, learning_rate 0.0001
2017-10-11T11:20:18.847179: step 4524, loss 0.116389, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:18.928639: step 4525, loss 0.290323, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:19.012358: step 4526, loss 0.333556, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:19.094563: step 4527, loss 0.353637, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:19.177686: step 4528, loss 0.193664, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:19.259294: step 4529, loss 0.182919, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:19.341751: step 4530, loss 0.260552, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:19.425305: step 4531, loss 0.235177, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:19.508812: step 4532, loss 0.141075, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:19.591549: step 4533, loss 0.292526, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:19.675411: step 4534, loss 0.264147, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:19.760819: step 4535, loss 0.215067, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:19.843513: step 4536, loss 0.338894, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:19.926954: step 4537, loss 0.160376, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:20.006313: step 4538, loss 0.170474, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:20.090068: step 4539, loss 0.249521, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:20.172820: step 4540, loss 0.314559, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:20.255017: step 4541, loss 0.141099, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:20.337926: step 4542, loss 0.282928, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:20.416166: step 4543, loss 0.214899, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:20.498917: step 4544, loss 0.236284, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:20.582886: step 4545, loss 0.274102, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:20.664540: step 4546, loss 0.227376, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:20.745915: step 4547, loss 0.461237, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:20.834509: step 4548, loss 0.195916, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:20.917804: step 4549, loss 0.298522, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:21.003179: step 4550, loss 0.211956, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:21.085213: step 4551, loss 0.195049, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:21.165117: step 4552, loss 0.447484, acc 0.84375, learning_rate 0.0001
2017-10-11T11:20:21.248107: step 4553, loss 0.174031, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:21.329212: step 4554, loss 0.336516, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:21.416418: step 4555, loss 0.324375, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:21.498406: step 4556, loss 0.143282, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:21.581682: step 4557, loss 0.235732, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:21.664616: step 4558, loss 0.187061, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:21.746105: step 4559, loss 0.269765, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:21.828569: step 4560, loss 0.448757, acc 0.859375, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:22.019954: step 4560, loss 0.225337, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4560

2017-10-11T11:20:22.522970: step 4561, loss 0.269641, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:22.603628: step 4562, loss 0.175888, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:22.686361: step 4563, loss 0.334197, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:22.769597: step 4564, loss 0.314754, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:22.848228: step 4565, loss 0.21042, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:22.928286: step 4566, loss 0.239586, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:23.006708: step 4567, loss 0.228798, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:23.090039: step 4568, loss 0.172781, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:23.174083: step 4569, loss 0.278759, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:23.255687: step 4570, loss 0.154916, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:23.342788: step 4571, loss 0.43587, acc 0.828125, learning_rate 0.0001
2017-10-11T11:20:23.426052: step 4572, loss 0.3479, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:23.510160: step 4573, loss 0.209197, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:23.588734: step 4574, loss 0.319043, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:23.669950: step 4575, loss 0.267395, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:23.751734: step 4576, loss 0.173023, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:23.835641: step 4577, loss 0.278538, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:23.919474: step 4578, loss 0.25681, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:24.004573: step 4579, loss 0.366217, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:24.088914: step 4580, loss 0.208784, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:24.171053: step 4581, loss 0.296883, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:24.253279: step 4582, loss 0.252101, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:24.334944: step 4583, loss 0.394123, acc 0.84375, learning_rate 0.0001
2017-10-11T11:20:24.415766: step 4584, loss 0.266752, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:24.499909: step 4585, loss 0.309748, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:24.579354: step 4586, loss 0.186236, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:24.663382: step 4587, loss 0.299983, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:24.746483: step 4588, loss 0.293319, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:24.830112: step 4589, loss 0.322896, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:24.911795: step 4590, loss 0.202516, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:24.994087: step 4591, loss 0.181887, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:25.074275: step 4592, loss 0.225134, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:25.158272: step 4593, loss 0.275471, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:25.240004: step 4594, loss 0.379478, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:25.326305: step 4595, loss 0.255725, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:25.409070: step 4596, loss 0.487197, acc 0.828125, learning_rate 0.0001
2017-10-11T11:20:25.497150: step 4597, loss 0.316165, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:25.578634: step 4598, loss 0.208755, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:25.660000: step 4599, loss 0.33009, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:25.741714: step 4600, loss 0.155729, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:25.931083: step 4600, loss 0.225524, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4600

2017-10-11T11:20:26.436095: step 4601, loss 0.272683, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:26.517519: step 4602, loss 0.173401, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:26.599676: step 4603, loss 0.208268, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:26.684896: step 4604, loss 0.231947, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:26.767714: step 4605, loss 0.146924, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:26.838048: step 4606, loss 0.271137, acc 0.941176, learning_rate 0.0001
2017-10-11T11:20:26.922007: step 4607, loss 0.285801, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:27.005643: step 4608, loss 0.184031, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:27.087611: step 4609, loss 0.347833, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:27.172176: step 4610, loss 0.279022, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:27.254752: step 4611, loss 0.360448, acc 0.84375, learning_rate 0.0001
2017-10-11T11:20:27.337617: step 4612, loss 0.265659, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:27.416692: step 4613, loss 0.104034, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:27.499117: step 4614, loss 0.222801, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:27.579717: step 4615, loss 0.243321, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:27.661062: step 4616, loss 0.329151, acc 0.84375, learning_rate 0.0001
2017-10-11T11:20:27.742064: step 4617, loss 0.195384, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:27.823173: step 4618, loss 0.285469, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:27.907987: step 4619, loss 0.189871, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:27.991467: step 4620, loss 0.25263, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:28.073493: step 4621, loss 0.282826, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:28.153583: step 4622, loss 0.395117, acc 0.828125, learning_rate 0.0001
2017-10-11T11:20:28.235189: step 4623, loss 0.273057, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:28.319102: step 4624, loss 0.299822, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:28.401158: step 4625, loss 0.307866, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:28.480548: step 4626, loss 0.263203, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:28.563308: step 4627, loss 0.159645, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:28.646530: step 4628, loss 0.170217, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:28.730074: step 4629, loss 0.163253, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:28.810523: step 4630, loss 0.240581, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:28.892737: step 4631, loss 0.310577, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:28.975721: step 4632, loss 0.180885, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:29.054167: step 4633, loss 0.560404, acc 0.796875, learning_rate 0.0001
2017-10-11T11:20:29.134138: step 4634, loss 0.229886, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:29.214667: step 4635, loss 0.205827, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:29.296926: step 4636, loss 0.245517, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:29.380697: step 4637, loss 0.280647, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:29.464181: step 4638, loss 0.359336, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:29.545990: step 4639, loss 0.254695, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:29.627579: step 4640, loss 0.188348, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:29.815991: step 4640, loss 0.22459, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4640

2017-10-11T11:20:30.322682: step 4641, loss 0.409101, acc 0.8125, learning_rate 0.0001
2017-10-11T11:20:30.402581: step 4642, loss 0.373602, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:30.484897: step 4643, loss 0.325356, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:30.565611: step 4644, loss 0.153065, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:30.648565: step 4645, loss 0.223361, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:30.730331: step 4646, loss 0.181211, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:30.810919: step 4647, loss 0.213175, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:30.896721: step 4648, loss 0.227289, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:30.980439: step 4649, loss 0.185596, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:31.060448: step 4650, loss 0.386829, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:31.143429: step 4651, loss 0.282279, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:31.223183: step 4652, loss 0.273167, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:31.305070: step 4653, loss 0.166753, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:31.386582: step 4654, loss 0.230599, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:31.467255: step 4655, loss 0.192084, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:31.550367: step 4656, loss 0.208894, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:31.633572: step 4657, loss 0.304935, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:31.712075: step 4658, loss 0.185581, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:31.795709: step 4659, loss 0.328378, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:31.879786: step 4660, loss 0.228358, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:31.962348: step 4661, loss 0.195331, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:32.044576: step 4662, loss 0.251539, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:32.129795: step 4663, loss 0.281129, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:32.208909: step 4664, loss 0.292812, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:32.290274: step 4665, loss 0.128452, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:32.375769: step 4666, loss 0.322341, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:32.458034: step 4667, loss 0.386541, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:32.540569: step 4668, loss 0.428803, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:32.626107: step 4669, loss 0.297236, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:32.709440: step 4670, loss 0.175717, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:32.791244: step 4671, loss 0.319278, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:32.872029: step 4672, loss 0.246727, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:32.955596: step 4673, loss 0.296308, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:33.038959: step 4674, loss 0.249449, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:33.121281: step 4675, loss 0.247412, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:33.203388: step 4676, loss 0.353765, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:33.286272: step 4677, loss 0.215063, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:33.368442: step 4678, loss 0.217614, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:33.450377: step 4679, loss 0.287155, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:33.534328: step 4680, loss 0.223413, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:33.717943: step 4680, loss 0.224711, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4680

2017-10-11T11:20:34.151555: step 4681, loss 0.213479, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:34.233203: step 4682, loss 0.298189, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:34.316496: step 4683, loss 0.272972, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:34.400371: step 4684, loss 0.219591, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:34.484863: step 4685, loss 0.210268, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:34.566985: step 4686, loss 0.29064, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:34.647512: step 4687, loss 0.216461, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:34.730832: step 4688, loss 0.197519, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:34.815564: step 4689, loss 0.296646, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:34.898008: step 4690, loss 0.155925, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:34.978902: step 4691, loss 0.204664, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:35.059355: step 4692, loss 0.166059, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:35.143427: step 4693, loss 0.289715, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:35.227225: step 4694, loss 0.196711, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:35.308539: step 4695, loss 0.343215, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:35.394282: step 4696, loss 0.238857, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:35.476471: step 4697, loss 0.38563, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:35.560564: step 4698, loss 0.368833, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:35.643609: step 4699, loss 0.177773, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:35.724400: step 4700, loss 0.274423, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:35.806405: step 4701, loss 0.146452, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:35.891485: step 4702, loss 0.180292, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:35.975428: step 4703, loss 0.312673, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:36.047577: step 4704, loss 0.327284, acc 0.882353, learning_rate 0.0001
2017-10-11T11:20:36.131061: step 4705, loss 0.454404, acc 0.84375, learning_rate 0.0001
2017-10-11T11:20:36.212991: step 4706, loss 0.226054, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:36.295424: step 4707, loss 0.212761, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:36.377641: step 4708, loss 0.295411, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:36.461706: step 4709, loss 0.312281, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:36.543602: step 4710, loss 0.284049, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:36.626334: step 4711, loss 0.263547, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:36.709614: step 4712, loss 0.326281, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:36.791902: step 4713, loss 0.188485, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:36.872696: step 4714, loss 0.19069, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:36.956540: step 4715, loss 0.147005, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:37.038445: step 4716, loss 0.264013, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:37.123507: step 4717, loss 0.255054, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:37.205693: step 4718, loss 0.160299, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:37.289741: step 4719, loss 0.342582, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:37.372526: step 4720, loss 0.265317, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:37.567887: step 4720, loss 0.225289, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4720

2017-10-11T11:20:38.071789: step 4721, loss 0.228629, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:38.155378: step 4722, loss 0.338443, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:38.235460: step 4723, loss 0.199775, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:38.316585: step 4724, loss 0.380006, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:38.397849: step 4725, loss 0.227417, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:38.480164: step 4726, loss 0.239054, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:38.563798: step 4727, loss 0.223957, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:38.646291: step 4728, loss 0.180212, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:38.727850: step 4729, loss 0.262916, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:38.810805: step 4730, loss 0.37937, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:38.892072: step 4731, loss 0.318928, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:38.977039: step 4732, loss 0.16162, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:39.061317: step 4733, loss 0.372, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:39.143934: step 4734, loss 0.258014, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:39.226616: step 4735, loss 0.343572, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:39.309227: step 4736, loss 0.325177, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:39.389921: step 4737, loss 0.334844, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:39.472354: step 4738, loss 0.272579, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:39.558367: step 4739, loss 0.232607, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:39.638880: step 4740, loss 0.12441, acc 1, learning_rate 0.0001
2017-10-11T11:20:39.720693: step 4741, loss 0.140133, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:39.807818: step 4742, loss 0.262491, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:39.892159: step 4743, loss 0.333953, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:39.972858: step 4744, loss 0.252527, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:40.056808: step 4745, loss 0.218141, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:40.137681: step 4746, loss 0.522897, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:40.222097: step 4747, loss 0.25512, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:40.305539: step 4748, loss 0.168104, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:40.388714: step 4749, loss 0.272507, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:40.468563: step 4750, loss 0.141395, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:40.550436: step 4751, loss 0.277556, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:40.633650: step 4752, loss 0.283344, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:40.717394: step 4753, loss 0.175157, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:40.800608: step 4754, loss 0.283465, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:40.880577: step 4755, loss 0.297045, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:40.961826: step 4756, loss 0.344092, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:41.044273: step 4757, loss 0.166801, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:41.125510: step 4758, loss 0.315926, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:41.208268: step 4759, loss 0.333655, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:41.291423: step 4760, loss 0.332429, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:41.481680: step 4760, loss 0.225907, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4760

2017-10-11T11:20:41.983195: step 4761, loss 0.24658, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:42.066469: step 4762, loss 0.17521, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:42.145314: step 4763, loss 0.215338, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:42.227273: step 4764, loss 0.156889, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:42.310938: step 4765, loss 0.22616, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:42.393454: step 4766, loss 0.252482, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:42.476582: step 4767, loss 0.364057, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:42.557394: step 4768, loss 0.131742, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:42.638207: step 4769, loss 0.40775, acc 0.84375, learning_rate 0.0001
2017-10-11T11:20:42.719690: step 4770, loss 0.409359, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:42.804214: step 4771, loss 0.242109, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:42.887197: step 4772, loss 0.193844, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:42.971032: step 4773, loss 0.285825, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:43.054867: step 4774, loss 0.216611, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:43.140210: step 4775, loss 0.248791, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:43.225109: step 4776, loss 0.244305, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:43.309574: step 4777, loss 0.271872, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:43.395564: step 4778, loss 0.176637, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:43.477258: step 4779, loss 0.290886, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:43.557957: step 4780, loss 0.203409, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:43.640080: step 4781, loss 0.120763, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:43.722554: step 4782, loss 0.241901, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:43.805241: step 4783, loss 0.204659, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:43.885776: step 4784, loss 0.219544, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:43.967383: step 4785, loss 0.231219, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:44.049514: step 4786, loss 0.109458, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:44.131165: step 4787, loss 0.159132, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:44.215199: step 4788, loss 0.200078, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:44.300806: step 4789, loss 0.33022, acc 0.828125, learning_rate 0.0001
2017-10-11T11:20:44.382156: step 4790, loss 0.243153, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:44.464135: step 4791, loss 0.176765, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:44.552084: step 4792, loss 0.250832, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:44.635867: step 4793, loss 0.217117, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:44.721683: step 4794, loss 0.229025, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:44.805255: step 4795, loss 0.207502, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:44.887820: step 4796, loss 0.291438, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:44.967949: step 4797, loss 0.221789, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:45.050032: step 4798, loss 0.205671, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:45.130122: step 4799, loss 0.206855, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:45.214233: step 4800, loss 0.30952, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:45.402317: step 4800, loss 0.224595, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4800

2017-10-11T11:20:45.903364: step 4801, loss 0.231296, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:45.974847: step 4802, loss 0.252767, acc 0.901961, learning_rate 0.0001
2017-10-11T11:20:46.057311: step 4803, loss 0.198587, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:46.141611: step 4804, loss 0.37296, acc 0.84375, learning_rate 0.0001
2017-10-11T11:20:46.223694: step 4805, loss 0.485613, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:46.307857: step 4806, loss 0.13035, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:46.390195: step 4807, loss 0.156091, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:46.474123: step 4808, loss 0.32536, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:46.556061: step 4809, loss 0.545351, acc 0.765625, learning_rate 0.0001
2017-10-11T11:20:46.637956: step 4810, loss 0.329204, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:46.720134: step 4811, loss 0.195768, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:46.802949: step 4812, loss 0.359278, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:46.889962: step 4813, loss 0.468993, acc 0.78125, learning_rate 0.0001
2017-10-11T11:20:46.969085: step 4814, loss 0.195781, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:47.051480: step 4815, loss 0.258284, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:47.132818: step 4816, loss 0.304395, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:47.216876: step 4817, loss 0.252446, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:47.298427: step 4818, loss 0.237265, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:47.380773: step 4819, loss 0.290857, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:47.465684: step 4820, loss 0.323222, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:47.544951: step 4821, loss 0.211694, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:47.623927: step 4822, loss 0.25816, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:47.707630: step 4823, loss 0.251731, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:47.790899: step 4824, loss 0.167379, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:47.873521: step 4825, loss 0.258386, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:47.954633: step 4826, loss 0.22456, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:48.038150: step 4827, loss 0.237622, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:48.117804: step 4828, loss 0.235259, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:48.201745: step 4829, loss 0.133195, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:48.287607: step 4830, loss 0.19472, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:48.368525: step 4831, loss 0.249212, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:48.450081: step 4832, loss 0.191363, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:48.529721: step 4833, loss 0.197241, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:48.611940: step 4834, loss 0.242919, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:48.696825: step 4835, loss 0.292063, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:48.778077: step 4836, loss 0.287466, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:48.860904: step 4837, loss 0.277957, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:48.944570: step 4838, loss 0.138131, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:49.026266: step 4839, loss 0.307433, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:49.107520: step 4840, loss 0.293305, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:49.290783: step 4840, loss 0.224625, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4840

2017-10-11T11:20:49.794455: step 4841, loss 0.317157, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:49.878199: step 4842, loss 0.141489, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:49.958824: step 4843, loss 0.267533, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:50.039078: step 4844, loss 0.298081, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:50.119807: step 4845, loss 0.190154, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:50.201235: step 4846, loss 0.232189, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:50.281344: step 4847, loss 0.23829, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:50.362744: step 4848, loss 0.273674, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:50.443749: step 4849, loss 0.297074, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:50.529407: step 4850, loss 0.276536, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:50.611113: step 4851, loss 0.213328, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:50.692400: step 4852, loss 0.15994, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:50.775283: step 4853, loss 0.443395, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:50.856218: step 4854, loss 0.185062, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:50.935719: step 4855, loss 0.255082, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:51.017614: step 4856, loss 0.26992, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:51.100776: step 4857, loss 0.309804, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:51.182741: step 4858, loss 0.22635, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:51.264924: step 4859, loss 0.180757, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:51.346214: step 4860, loss 0.300847, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:51.428026: step 4861, loss 0.345387, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:51.512773: step 4862, loss 0.257977, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:51.594556: step 4863, loss 0.170041, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:51.679109: step 4864, loss 0.249005, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:51.759173: step 4865, loss 0.379655, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:51.841628: step 4866, loss 0.28028, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:51.927253: step 4867, loss 0.279533, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:52.009395: step 4868, loss 0.240862, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:52.093148: step 4869, loss 0.195669, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:52.178535: step 4870, loss 0.201756, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:52.259701: step 4871, loss 0.23694, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:52.344094: step 4872, loss 0.178276, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:52.424591: step 4873, loss 0.116649, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:52.505072: step 4874, loss 0.433686, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:52.587859: step 4875, loss 0.275551, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:52.672463: step 4876, loss 0.349553, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:52.752226: step 4877, loss 0.117668, acc 1, learning_rate 0.0001
2017-10-11T11:20:52.832191: step 4878, loss 0.285561, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:52.914708: step 4879, loss 0.160813, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:52.998427: step 4880, loss 0.249897, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:53.191748: step 4880, loss 0.223368, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4880

2017-10-11T11:20:53.693457: step 4881, loss 0.16321, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:53.780247: step 4882, loss 0.309674, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:53.864544: step 4883, loss 0.319296, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:53.946987: step 4884, loss 0.170505, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:54.028740: step 4885, loss 0.283018, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:54.111902: step 4886, loss 0.164975, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:54.196306: step 4887, loss 0.344675, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:54.277683: step 4888, loss 0.293164, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:54.357496: step 4889, loss 0.258191, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:54.440136: step 4890, loss 0.188735, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:54.523105: step 4891, loss 0.143231, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:54.601853: step 4892, loss 0.265579, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:54.683980: step 4893, loss 0.154138, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:54.770493: step 4894, loss 0.26635, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:54.856606: step 4895, loss 0.284226, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:54.936043: step 4896, loss 0.227754, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:55.018018: step 4897, loss 0.355943, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:55.099555: step 4898, loss 0.166559, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:55.182021: step 4899, loss 0.256835, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:55.255084: step 4900, loss 0.219697, acc 0.921569, learning_rate 0.0001
2017-10-11T11:20:55.338960: step 4901, loss 0.170895, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:55.421532: step 4902, loss 0.221513, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:55.501180: step 4903, loss 0.320115, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:55.583972: step 4904, loss 0.113768, acc 0.984375, learning_rate 0.0001
2017-10-11T11:20:55.665360: step 4905, loss 0.197532, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:55.745785: step 4906, loss 0.170948, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:55.827923: step 4907, loss 0.287484, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:55.908859: step 4908, loss 0.250932, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:55.991705: step 4909, loss 0.362482, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:56.073214: step 4910, loss 0.198862, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:56.156973: step 4911, loss 0.393196, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:56.237200: step 4912, loss 0.18803, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:56.317532: step 4913, loss 0.256168, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:56.402952: step 4914, loss 0.262695, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:56.483459: step 4915, loss 0.334838, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:56.563391: step 4916, loss 0.26806, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:56.647723: step 4917, loss 0.294742, acc 0.875, learning_rate 0.0001
2017-10-11T11:20:56.729932: step 4918, loss 0.257363, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:56.811380: step 4919, loss 0.234095, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:56.895649: step 4920, loss 0.281744, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:20:57.087783: step 4920, loss 0.222937, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4920

2017-10-11T11:20:57.596890: step 4921, loss 0.281132, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:57.679483: step 4922, loss 0.258734, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:57.762881: step 4923, loss 0.208089, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:57.846151: step 4924, loss 0.260782, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:57.926986: step 4925, loss 0.298912, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:58.006805: step 4926, loss 0.149764, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:58.090538: step 4927, loss 0.255214, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:58.175429: step 4928, loss 0.221918, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:58.255809: step 4929, loss 0.274052, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:58.339428: step 4930, loss 0.332919, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:58.419049: step 4931, loss 0.25119, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:58.503067: step 4932, loss 0.242044, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:58.584049: step 4933, loss 0.156438, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:58.669244: step 4934, loss 0.178333, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:58.751058: step 4935, loss 0.28457, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:58.835299: step 4936, loss 0.141407, acc 0.953125, learning_rate 0.0001
2017-10-11T11:20:58.917990: step 4937, loss 0.340582, acc 0.859375, learning_rate 0.0001
2017-10-11T11:20:58.998356: step 4938, loss 0.208971, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:59.081619: step 4939, loss 0.239569, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:59.164605: step 4940, loss 0.23268, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:59.245669: step 4941, loss 0.255861, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:59.327722: step 4942, loss 0.224762, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:59.411902: step 4943, loss 0.236993, acc 0.90625, learning_rate 0.0001
2017-10-11T11:20:59.492529: step 4944, loss 0.281107, acc 0.9375, learning_rate 0.0001
2017-10-11T11:20:59.575269: step 4945, loss 0.401451, acc 0.890625, learning_rate 0.0001
2017-10-11T11:20:59.658125: step 4946, loss 0.224696, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:59.739219: step 4947, loss 0.21721, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:59.820208: step 4948, loss 0.323466, acc 0.921875, learning_rate 0.0001
2017-10-11T11:20:59.900060: step 4949, loss 0.125466, acc 0.96875, learning_rate 0.0001
2017-10-11T11:20:59.979697: step 4950, loss 0.304484, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:00.061307: step 4951, loss 0.308487, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:00.145469: step 4952, loss 0.313205, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:00.225014: step 4953, loss 0.283863, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:00.305970: step 4954, loss 0.240492, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:00.386123: step 4955, loss 0.192315, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:00.467121: step 4956, loss 0.227531, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:00.550570: step 4957, loss 0.233473, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:00.634073: step 4958, loss 0.265488, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:00.715827: step 4959, loss 0.268182, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:00.796605: step 4960, loss 0.130812, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:00.987043: step 4960, loss 0.222139, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-4960

2017-10-11T11:21:01.493475: step 4961, loss 0.167919, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:01.575836: step 4962, loss 0.417115, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:01.658466: step 4963, loss 0.175048, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:01.740109: step 4964, loss 0.190127, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:01.822652: step 4965, loss 0.192049, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:01.905448: step 4966, loss 0.349903, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:01.987136: step 4967, loss 0.181234, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:02.067173: step 4968, loss 0.183229, acc 0.984375, learning_rate 0.0001
2017-10-11T11:21:02.152216: step 4969, loss 0.235834, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:02.235116: step 4970, loss 0.324475, acc 0.84375, learning_rate 0.0001
2017-10-11T11:21:02.314436: step 4971, loss 0.252519, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:02.395057: step 4972, loss 0.154316, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:02.477566: step 4973, loss 0.254052, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:02.561464: step 4974, loss 0.179959, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:02.644989: step 4975, loss 0.337728, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:02.726913: step 4976, loss 0.179571, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:02.810869: step 4977, loss 0.233211, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:02.895074: step 4978, loss 0.217866, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:02.978378: step 4979, loss 0.450809, acc 0.828125, learning_rate 0.0001
2017-10-11T11:21:03.063540: step 4980, loss 0.313834, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:03.143132: step 4981, loss 0.164712, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:03.223408: step 4982, loss 0.268729, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:03.306051: step 4983, loss 0.261917, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:03.386578: step 4984, loss 0.307026, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:03.470847: step 4985, loss 0.216937, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:03.553583: step 4986, loss 0.249752, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:03.637391: step 4987, loss 0.253489, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:03.718131: step 4988, loss 0.28389, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:03.800658: step 4989, loss 0.210408, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:03.885072: step 4990, loss 0.285787, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:03.970957: step 4991, loss 0.267561, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:04.053293: step 4992, loss 0.308131, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:04.134766: step 4993, loss 0.142192, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:04.220455: step 4994, loss 0.25622, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:04.303962: step 4995, loss 0.304852, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:04.387975: step 4996, loss 0.214357, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:04.469145: step 4997, loss 0.189372, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:04.539305: step 4998, loss 0.206659, acc 0.941176, learning_rate 0.0001
2017-10-11T11:21:04.624196: step 4999, loss 0.190029, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:04.707498: step 5000, loss 0.234252, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:04.892248: step 5000, loss 0.222533, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5000

2017-10-11T11:21:05.396954: step 5001, loss 0.195287, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:05.480125: step 5002, loss 0.148287, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:05.560025: step 5003, loss 0.217607, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:05.640234: step 5004, loss 0.415771, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:05.723419: step 5005, loss 0.183897, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:05.803464: step 5006, loss 0.260634, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:05.888793: step 5007, loss 0.300038, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:05.971774: step 5008, loss 0.188613, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:06.053429: step 5009, loss 0.286635, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:06.135801: step 5010, loss 0.311664, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:06.218077: step 5011, loss 0.11437, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:06.300371: step 5012, loss 0.347167, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:06.383947: step 5013, loss 0.208926, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:06.467694: step 5014, loss 0.309359, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:06.547805: step 5015, loss 0.181988, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:06.627383: step 5016, loss 0.231969, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:06.711092: step 5017, loss 0.259024, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:06.794666: step 5018, loss 0.291443, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:06.877095: step 5019, loss 0.18784, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:06.959634: step 5020, loss 0.34231, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:07.040296: step 5021, loss 0.432275, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:07.127228: step 5022, loss 0.193435, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:07.205599: step 5023, loss 0.153141, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:07.288258: step 5024, loss 0.266104, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:07.369893: step 5025, loss 0.430735, acc 0.84375, learning_rate 0.0001
2017-10-11T11:21:07.452598: step 5026, loss 0.180283, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:07.532561: step 5027, loss 0.300082, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:07.613399: step 5028, loss 0.256399, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:07.700068: step 5029, loss 0.271185, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:07.782080: step 5030, loss 0.317124, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:07.864906: step 5031, loss 0.278543, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:07.948673: step 5032, loss 0.21927, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:08.032170: step 5033, loss 0.277162, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:08.117020: step 5034, loss 0.237252, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:08.199802: step 5035, loss 0.219364, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:08.281955: step 5036, loss 0.161121, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:08.362829: step 5037, loss 0.263, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:08.444207: step 5038, loss 0.295056, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:08.524250: step 5039, loss 0.227958, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:08.608008: step 5040, loss 0.165259, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:08.790197: step 5040, loss 0.222033, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5040

2017-10-11T11:21:09.226630: step 5041, loss 0.213622, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:09.309065: step 5042, loss 0.176358, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:09.392180: step 5043, loss 0.259222, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:09.478178: step 5044, loss 0.252076, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:09.558924: step 5045, loss 0.197768, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:09.642487: step 5046, loss 0.223703, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:09.725399: step 5047, loss 0.298138, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:09.808382: step 5048, loss 0.163794, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:09.892095: step 5049, loss 0.233739, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:09.970833: step 5050, loss 0.115024, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:10.054733: step 5051, loss 0.370751, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:10.135711: step 5052, loss 0.27085, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:10.216761: step 5053, loss 0.249223, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:10.296490: step 5054, loss 0.186872, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:10.378855: step 5055, loss 0.153664, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:10.462898: step 5056, loss 0.232574, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:10.547679: step 5057, loss 0.291042, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:10.630720: step 5058, loss 0.253112, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:10.710314: step 5059, loss 0.18454, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:10.794027: step 5060, loss 0.373809, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:10.878244: step 5061, loss 0.10722, acc 1, learning_rate 0.0001
2017-10-11T11:21:10.959879: step 5062, loss 0.18539, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:11.043071: step 5063, loss 0.255167, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:11.126061: step 5064, loss 0.326521, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:11.208454: step 5065, loss 0.192467, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:11.288896: step 5066, loss 0.44733, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:11.371595: step 5067, loss 0.257595, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:11.451538: step 5068, loss 0.200571, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:11.534365: step 5069, loss 0.207388, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:11.616857: step 5070, loss 0.261384, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:11.699857: step 5071, loss 0.26982, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:11.782211: step 5072, loss 0.181138, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:11.865825: step 5073, loss 0.245765, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:11.946111: step 5074, loss 0.292514, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:12.030500: step 5075, loss 0.396723, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:12.113822: step 5076, loss 0.254786, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:12.197445: step 5077, loss 0.215485, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:12.278083: step 5078, loss 0.154506, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:12.359270: step 5079, loss 0.278093, acc 0.84375, learning_rate 0.0001
2017-10-11T11:21:12.444498: step 5080, loss 0.253972, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:12.633442: step 5080, loss 0.221987, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5080

2017-10-11T11:21:13.132392: step 5081, loss 0.241187, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:13.213478: step 5082, loss 0.14873, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:13.295365: step 5083, loss 0.384791, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:13.376409: step 5084, loss 0.257366, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:13.462441: step 5085, loss 0.284416, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:13.545307: step 5086, loss 0.297098, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:13.631987: step 5087, loss 0.350692, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:13.715500: step 5088, loss 0.342559, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:13.797988: step 5089, loss 0.222203, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:13.882347: step 5090, loss 0.518796, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:13.965588: step 5091, loss 0.193967, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:14.055999: step 5092, loss 0.401964, acc 0.84375, learning_rate 0.0001
2017-10-11T11:21:14.142575: step 5093, loss 0.272499, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:14.224783: step 5094, loss 0.161707, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:14.307103: step 5095, loss 0.441693, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:14.376340: step 5096, loss 0.180828, acc 0.921569, learning_rate 0.0001
2017-10-11T11:21:14.459617: step 5097, loss 0.20321, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:14.544074: step 5098, loss 0.404111, acc 0.8125, learning_rate 0.0001
2017-10-11T11:21:14.625634: step 5099, loss 0.171243, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:14.708097: step 5100, loss 0.287252, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:14.788843: step 5101, loss 0.242121, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:14.871801: step 5102, loss 0.225505, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:14.953799: step 5103, loss 0.293743, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:15.036972: step 5104, loss 0.189129, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:15.118912: step 5105, loss 0.28685, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:15.198130: step 5106, loss 0.172113, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:15.278287: step 5107, loss 0.278566, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:15.360676: step 5108, loss 0.351354, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:15.444842: step 5109, loss 0.220477, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:15.527947: step 5110, loss 0.326972, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:15.610993: step 5111, loss 0.478823, acc 0.828125, learning_rate 0.0001
2017-10-11T11:21:15.693459: step 5112, loss 0.0639589, acc 1, learning_rate 0.0001
2017-10-11T11:21:15.779111: step 5113, loss 0.299241, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:15.861733: step 5114, loss 0.296712, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:15.945729: step 5115, loss 0.250088, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:16.026270: step 5116, loss 0.202684, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:16.108385: step 5117, loss 0.18569, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:16.188112: step 5118, loss 0.335302, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:16.270334: step 5119, loss 0.332279, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:16.353846: step 5120, loss 0.308095, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:16.551094: step 5120, loss 0.221061, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5120

2017-10-11T11:21:17.057406: step 5121, loss 0.216529, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:17.139149: step 5122, loss 0.330339, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:17.222632: step 5123, loss 0.276744, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:17.307901: step 5124, loss 0.340552, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:17.389627: step 5125, loss 0.269949, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:17.472311: step 5126, loss 0.429536, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:17.555619: step 5127, loss 0.163716, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:17.638945: step 5128, loss 0.350069, acc 0.84375, learning_rate 0.0001
2017-10-11T11:21:17.721976: step 5129, loss 0.215901, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:17.801160: step 5130, loss 0.169893, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:17.881335: step 5131, loss 0.187422, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:17.970047: step 5132, loss 0.451738, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:18.048300: step 5133, loss 0.264144, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:18.132909: step 5134, loss 0.233641, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:18.218087: step 5135, loss 0.29857, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:18.305275: step 5136, loss 0.323429, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:18.389767: step 5137, loss 0.257266, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:18.471888: step 5138, loss 0.399565, acc 0.84375, learning_rate 0.0001
2017-10-11T11:21:18.555580: step 5139, loss 0.302723, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:18.640464: step 5140, loss 0.135064, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:18.722399: step 5141, loss 0.177921, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:18.804565: step 5142, loss 0.252396, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:18.886269: step 5143, loss 0.354356, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:18.967854: step 5144, loss 0.172426, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:19.052102: step 5145, loss 0.20859, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:19.137151: step 5146, loss 0.265535, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:19.218721: step 5147, loss 0.369666, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:19.298656: step 5148, loss 0.20675, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:19.380763: step 5149, loss 0.165312, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:19.463201: step 5150, loss 0.179627, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:19.544894: step 5151, loss 0.25335, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:19.629542: step 5152, loss 0.19354, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:19.712853: step 5153, loss 0.180663, acc 0.984375, learning_rate 0.0001
2017-10-11T11:21:19.794713: step 5154, loss 0.251364, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:19.876071: step 5155, loss 0.347899, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:19.958573: step 5156, loss 0.174001, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:20.042897: step 5157, loss 0.210874, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:20.125244: step 5158, loss 0.202461, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:20.206632: step 5159, loss 0.215317, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:20.290760: step 5160, loss 0.174412, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:20.481725: step 5160, loss 0.221599, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5160

2017-10-11T11:21:20.990669: step 5161, loss 0.244169, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:21.074150: step 5162, loss 0.224114, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:21.155177: step 5163, loss 0.123988, acc 0.984375, learning_rate 0.0001
2017-10-11T11:21:21.237070: step 5164, loss 0.149851, acc 0.984375, learning_rate 0.0001
2017-10-11T11:21:21.318299: step 5165, loss 0.242419, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:21.402055: step 5166, loss 0.137147, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:21.483941: step 5167, loss 0.154817, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:21.568842: step 5168, loss 0.307786, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:21.652949: step 5169, loss 0.210287, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:21.736467: step 5170, loss 0.212796, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:21.817632: step 5171, loss 0.236325, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:21.900235: step 5172, loss 0.190432, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:21.983942: step 5173, loss 0.483764, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:22.067686: step 5174, loss 0.221546, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:22.151154: step 5175, loss 0.273008, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:22.238426: step 5176, loss 0.198144, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:22.323303: step 5177, loss 0.257076, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:22.403006: step 5178, loss 0.199282, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:22.483149: step 5179, loss 0.274744, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:22.563230: step 5180, loss 0.310123, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:22.646116: step 5181, loss 0.181948, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:22.730201: step 5182, loss 0.172985, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:22.813271: step 5183, loss 0.332578, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:22.895981: step 5184, loss 0.224482, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:22.979213: step 5185, loss 0.227084, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:23.061870: step 5186, loss 0.31384, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:23.141137: step 5187, loss 0.212769, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:23.223655: step 5188, loss 0.207205, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:23.306492: step 5189, loss 0.219309, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:23.390687: step 5190, loss 0.264503, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:23.475059: step 5191, loss 0.177415, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:23.559803: step 5192, loss 0.15293, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:23.643615: step 5193, loss 0.253031, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:23.713654: step 5194, loss 0.203354, acc 0.960784, learning_rate 0.0001
2017-10-11T11:21:23.797508: step 5195, loss 0.275918, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:23.879255: step 5196, loss 0.268051, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:23.963553: step 5197, loss 0.301465, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:24.044489: step 5198, loss 0.188943, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:24.126048: step 5199, loss 0.326024, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:24.213036: step 5200, loss 0.21131, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:24.404938: step 5200, loss 0.221105, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5200

2017-10-11T11:21:24.915321: step 5201, loss 0.223382, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:24.996887: step 5202, loss 0.277059, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:25.078364: step 5203, loss 0.199619, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:25.161339: step 5204, loss 0.295926, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:25.247441: step 5205, loss 0.20003, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:25.334102: step 5206, loss 0.260242, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:25.416534: step 5207, loss 0.239949, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:25.500053: step 5208, loss 0.26321, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:25.582365: step 5209, loss 0.286466, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:25.668175: step 5210, loss 0.214619, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:25.749647: step 5211, loss 0.267116, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:25.829947: step 5212, loss 0.237681, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:25.914985: step 5213, loss 0.249721, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:25.994917: step 5214, loss 0.489935, acc 0.84375, learning_rate 0.0001
2017-10-11T11:21:26.082057: step 5215, loss 0.248104, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:26.163429: step 5216, loss 0.315321, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:26.245930: step 5217, loss 0.273603, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:26.328810: step 5218, loss 0.221193, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:26.409790: step 5219, loss 0.199208, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:26.494979: step 5220, loss 0.154401, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:26.579294: step 5221, loss 0.216932, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:26.660189: step 5222, loss 0.197367, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:26.745107: step 5223, loss 0.253011, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:26.827013: step 5224, loss 0.146757, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:26.912337: step 5225, loss 0.267097, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:26.995480: step 5226, loss 0.361553, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:27.073140: step 5227, loss 0.180837, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:27.152763: step 5228, loss 0.193849, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:27.238571: step 5229, loss 0.265393, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:27.323179: step 5230, loss 0.163167, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:27.405760: step 5231, loss 0.19768, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:27.490441: step 5232, loss 0.198446, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:27.572013: step 5233, loss 0.130662, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:27.657412: step 5234, loss 0.238654, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:27.737914: step 5235, loss 0.394132, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:27.819027: step 5236, loss 0.185807, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:27.901192: step 5237, loss 0.284452, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:27.984557: step 5238, loss 0.171291, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:28.070260: step 5239, loss 0.19057, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:28.151612: step 5240, loss 0.218103, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:28.346732: step 5240, loss 0.220508, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5240

2017-10-11T11:21:28.851517: step 5241, loss 0.134927, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:28.934569: step 5242, loss 0.454466, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:29.018831: step 5243, loss 0.223078, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:29.100824: step 5244, loss 0.267976, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:29.182932: step 5245, loss 0.368705, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:29.264969: step 5246, loss 0.156032, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:29.346922: step 5247, loss 0.329304, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:29.428780: step 5248, loss 0.155817, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:29.508575: step 5249, loss 0.101972, acc 0.984375, learning_rate 0.0001
2017-10-11T11:21:29.594559: step 5250, loss 0.206864, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:29.675071: step 5251, loss 0.323128, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:29.757465: step 5252, loss 0.217511, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:29.837147: step 5253, loss 0.304429, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:29.920904: step 5254, loss 0.184869, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:30.002976: step 5255, loss 0.290272, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:30.086534: step 5256, loss 0.437155, acc 0.796875, learning_rate 0.0001
2017-10-11T11:21:30.167307: step 5257, loss 0.303473, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:30.251616: step 5258, loss 0.136616, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:30.334651: step 5259, loss 0.157445, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:30.418130: step 5260, loss 0.249825, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:30.504057: step 5261, loss 0.17643, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:30.584782: step 5262, loss 0.387057, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:30.668387: step 5263, loss 0.208024, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:30.749176: step 5264, loss 0.197665, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:30.835819: step 5265, loss 0.191257, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:30.915967: step 5266, loss 0.192047, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:30.997677: step 5267, loss 0.159891, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:31.079540: step 5268, loss 0.189471, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:31.162715: step 5269, loss 0.304259, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:31.243936: step 5270, loss 0.313335, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:31.324909: step 5271, loss 0.3333, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:31.406350: step 5272, loss 0.369868, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:31.490263: step 5273, loss 0.196828, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:31.575427: step 5274, loss 0.312292, acc 0.84375, learning_rate 0.0001
2017-10-11T11:21:31.658135: step 5275, loss 0.222806, acc 0.984375, learning_rate 0.0001
2017-10-11T11:21:31.739296: step 5276, loss 0.281393, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:31.820860: step 5277, loss 0.284159, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:31.906987: step 5278, loss 0.292416, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:31.990103: step 5279, loss 0.288477, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:32.074426: step 5280, loss 0.208694, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:32.272403: step 5280, loss 0.220618, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5280

2017-10-11T11:21:32.778024: step 5281, loss 0.178754, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:32.861141: step 5282, loss 0.385205, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:32.945368: step 5283, loss 0.21469, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:33.024993: step 5284, loss 0.313598, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:33.113346: step 5285, loss 0.245513, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:33.196791: step 5286, loss 0.298213, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:33.279329: step 5287, loss 0.334332, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:33.362849: step 5288, loss 0.299485, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:33.444749: step 5289, loss 0.19, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:33.529596: step 5290, loss 0.137395, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:33.610484: step 5291, loss 0.138994, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:33.682192: step 5292, loss 0.285051, acc 0.921569, learning_rate 0.0001
2017-10-11T11:21:33.761731: step 5293, loss 0.597982, acc 0.8125, learning_rate 0.0001
2017-10-11T11:21:33.847963: step 5294, loss 0.160437, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:33.931163: step 5295, loss 0.20393, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:34.013965: step 5296, loss 0.3248, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:34.096810: step 5297, loss 0.200833, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:34.179471: step 5298, loss 0.230413, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:34.262282: step 5299, loss 0.247551, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:34.345430: step 5300, loss 0.222805, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:34.424595: step 5301, loss 0.135561, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:34.507201: step 5302, loss 0.18777, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:34.591779: step 5303, loss 0.282837, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:34.673853: step 5304, loss 0.28409, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:34.754161: step 5305, loss 0.179745, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:34.834211: step 5306, loss 0.215724, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:34.917829: step 5307, loss 0.248571, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:35.001324: step 5308, loss 0.159433, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:35.084759: step 5309, loss 0.255596, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:35.165481: step 5310, loss 0.211048, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:35.250976: step 5311, loss 0.336227, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:35.333839: step 5312, loss 0.173235, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:35.416836: step 5313, loss 0.243434, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:35.499572: step 5314, loss 0.203465, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:35.581124: step 5315, loss 0.207121, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:35.662264: step 5316, loss 0.122388, acc 0.984375, learning_rate 0.0001
2017-10-11T11:21:35.747054: step 5317, loss 0.234112, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:35.829553: step 5318, loss 0.239179, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:35.912099: step 5319, loss 0.317582, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:35.993315: step 5320, loss 0.148647, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:36.183140: step 5320, loss 0.221191, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5320

2017-10-11T11:21:36.689842: step 5321, loss 0.218573, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:36.773215: step 5322, loss 0.393193, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:36.857108: step 5323, loss 0.369781, acc 0.84375, learning_rate 0.0001
2017-10-11T11:21:36.936483: step 5324, loss 0.165969, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:37.025832: step 5325, loss 0.267053, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:37.109164: step 5326, loss 0.137647, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:37.192129: step 5327, loss 0.158527, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:37.276573: step 5328, loss 0.107751, acc 0.984375, learning_rate 0.0001
2017-10-11T11:21:37.358488: step 5329, loss 0.231618, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:37.439741: step 5330, loss 0.234746, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:37.520965: step 5331, loss 0.120684, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:37.604373: step 5332, loss 0.316095, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:37.686806: step 5333, loss 0.252349, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:37.769851: step 5334, loss 0.253674, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:37.853604: step 5335, loss 0.223665, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:37.934470: step 5336, loss 0.244771, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:38.017080: step 5337, loss 0.307802, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:38.099825: step 5338, loss 0.311162, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:38.183367: step 5339, loss 0.149863, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:38.261699: step 5340, loss 0.161751, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:38.344881: step 5341, loss 0.258293, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:38.427538: step 5342, loss 0.331919, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:38.510957: step 5343, loss 0.281307, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:38.595182: step 5344, loss 0.172025, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:38.678517: step 5345, loss 0.368683, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:38.760352: step 5346, loss 0.37085, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:38.842262: step 5347, loss 0.228056, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:38.924631: step 5348, loss 0.131034, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:39.006591: step 5349, loss 0.156407, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:39.087322: step 5350, loss 0.280534, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:39.169841: step 5351, loss 0.485602, acc 0.828125, learning_rate 0.0001
2017-10-11T11:21:39.252310: step 5352, loss 0.413847, acc 0.84375, learning_rate 0.0001
2017-10-11T11:21:39.340181: step 5353, loss 0.138868, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:39.419920: step 5354, loss 0.218415, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:39.502137: step 5355, loss 0.200163, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:39.586891: step 5356, loss 0.331847, acc 0.84375, learning_rate 0.0001
2017-10-11T11:21:39.670072: step 5357, loss 0.167184, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:39.755190: step 5358, loss 0.26951, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:39.836945: step 5359, loss 0.133097, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:39.920926: step 5360, loss 0.354919, acc 0.859375, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:40.113315: step 5360, loss 0.220787, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5360

2017-10-11T11:21:40.548639: step 5361, loss 0.1546, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:40.629707: step 5362, loss 0.239213, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:40.713692: step 5363, loss 0.218155, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:40.795585: step 5364, loss 0.18824, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:40.878681: step 5365, loss 0.32925, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:40.959946: step 5366, loss 0.158443, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:41.043184: step 5367, loss 0.296573, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:41.124095: step 5368, loss 0.314333, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:41.205658: step 5369, loss 0.226334, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:41.288343: step 5370, loss 0.21677, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:41.370944: step 5371, loss 0.171986, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:41.453431: step 5372, loss 0.282136, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:41.536803: step 5373, loss 0.359146, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:41.621019: step 5374, loss 0.317568, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:41.702723: step 5375, loss 0.196708, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:41.782259: step 5376, loss 0.196298, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:41.867028: step 5377, loss 0.184, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:41.949011: step 5378, loss 0.209099, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:42.032102: step 5379, loss 0.229643, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:42.116500: step 5380, loss 0.197225, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:42.198595: step 5381, loss 0.322818, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:42.279360: step 5382, loss 0.236897, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:42.361955: step 5383, loss 0.238958, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:42.443387: step 5384, loss 0.154881, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:42.522225: step 5385, loss 0.250655, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:42.603530: step 5386, loss 0.215249, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:42.689939: step 5387, loss 0.169614, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:42.773058: step 5388, loss 0.22151, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:42.856385: step 5389, loss 0.171091, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:42.930208: step 5390, loss 0.350935, acc 0.862745, learning_rate 0.0001
2017-10-11T11:21:43.015268: step 5391, loss 0.225578, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:43.098839: step 5392, loss 0.276591, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:43.181671: step 5393, loss 0.0947586, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:43.264289: step 5394, loss 0.309765, acc 0.84375, learning_rate 0.0001
2017-10-11T11:21:43.345970: step 5395, loss 0.203661, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:43.425250: step 5396, loss 0.161894, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:43.507674: step 5397, loss 0.354877, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:43.591856: step 5398, loss 0.180771, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:43.674646: step 5399, loss 0.3645, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:43.757275: step 5400, loss 0.185858, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:43.940631: step 5400, loss 0.220606, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5400

2017-10-11T11:21:44.441960: step 5401, loss 0.207491, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:44.524546: step 5402, loss 0.321773, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:44.610359: step 5403, loss 0.376506, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:44.694366: step 5404, loss 0.39301, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:44.777877: step 5405, loss 0.206206, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:44.863727: step 5406, loss 0.23046, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:44.948555: step 5407, loss 0.170914, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:45.029684: step 5408, loss 0.243905, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:45.110681: step 5409, loss 0.113575, acc 0.984375, learning_rate 0.0001
2017-10-11T11:21:45.194697: step 5410, loss 0.261618, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:45.277421: step 5411, loss 0.436606, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:45.359819: step 5412, loss 0.239025, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:45.443570: step 5413, loss 0.285446, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:45.524877: step 5414, loss 0.205365, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:45.605863: step 5415, loss 0.309902, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:45.688602: step 5416, loss 0.16427, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:45.771625: step 5417, loss 0.160692, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:45.855337: step 5418, loss 0.231761, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:45.938999: step 5419, loss 0.195868, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:46.021358: step 5420, loss 0.13483, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:46.105296: step 5421, loss 0.111836, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:46.186199: step 5422, loss 0.163461, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:46.271226: step 5423, loss 0.351325, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:46.351352: step 5424, loss 0.172475, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:46.434223: step 5425, loss 0.206933, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:46.515557: step 5426, loss 0.181637, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:46.598015: step 5427, loss 0.256838, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:46.679284: step 5428, loss 0.334573, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:46.762152: step 5429, loss 0.327692, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:46.844332: step 5430, loss 0.240463, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:46.930054: step 5431, loss 0.245819, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:47.011528: step 5432, loss 0.177157, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:47.092929: step 5433, loss 0.194588, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:47.174461: step 5434, loss 0.185266, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:47.257394: step 5435, loss 0.270026, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:47.342492: step 5436, loss 0.26162, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:47.424246: step 5437, loss 0.267052, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:47.508419: step 5438, loss 0.216924, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:47.593396: step 5439, loss 0.189865, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:47.677203: step 5440, loss 0.226451, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:47.868347: step 5440, loss 0.219349, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5440

2017-10-11T11:21:48.367162: step 5441, loss 0.176558, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:48.446930: step 5442, loss 0.140606, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:48.530994: step 5443, loss 0.208429, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:48.615653: step 5444, loss 0.283626, acc 0.828125, learning_rate 0.0001
2017-10-11T11:21:48.702452: step 5445, loss 0.208422, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:48.785973: step 5446, loss 0.195865, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:48.868717: step 5447, loss 0.269621, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:48.951102: step 5448, loss 0.208951, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:49.032474: step 5449, loss 0.115869, acc 0.984375, learning_rate 0.0001
2017-10-11T11:21:49.125507: step 5450, loss 0.253967, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:49.209662: step 5451, loss 0.273461, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:49.291622: step 5452, loss 0.34894, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:49.379712: step 5453, loss 0.297837, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:49.461785: step 5454, loss 0.255685, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:49.544870: step 5455, loss 0.155451, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:49.627322: step 5456, loss 0.292886, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:49.709398: step 5457, loss 0.43787, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:49.794299: step 5458, loss 0.222788, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:49.875898: step 5459, loss 0.152261, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:49.958835: step 5460, loss 0.456834, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:50.040552: step 5461, loss 0.28271, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:50.121286: step 5462, loss 0.319895, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:50.204393: step 5463, loss 0.266258, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:50.286736: step 5464, loss 0.248347, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:50.368425: step 5465, loss 0.23676, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:50.452911: step 5466, loss 0.192929, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:50.535292: step 5467, loss 0.28347, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:50.619515: step 5468, loss 0.216014, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:50.701130: step 5469, loss 0.343407, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:50.783607: step 5470, loss 0.299995, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:50.870397: step 5471, loss 0.204206, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:50.953120: step 5472, loss 0.2667, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:51.035244: step 5473, loss 0.386427, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:51.118952: step 5474, loss 0.247465, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:51.202711: step 5475, loss 0.25573, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:51.284086: step 5476, loss 0.301239, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:51.365614: step 5477, loss 0.175372, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:51.446162: step 5478, loss 0.182461, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:51.526190: step 5479, loss 0.170338, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:51.609831: step 5480, loss 0.204474, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:51.798994: step 5480, loss 0.21883, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5480

2017-10-11T11:21:52.301686: step 5481, loss 0.206104, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:52.387607: step 5482, loss 0.293724, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:52.470154: step 5483, loss 0.216265, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:52.555042: step 5484, loss 0.250142, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:52.638715: step 5485, loss 0.24754, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:52.723559: step 5486, loss 0.249769, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:52.805070: step 5487, loss 0.381171, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:52.878018: step 5488, loss 0.376959, acc 0.862745, learning_rate 0.0001
2017-10-11T11:21:52.960790: step 5489, loss 0.248025, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:53.043969: step 5490, loss 0.222854, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:53.122691: step 5491, loss 0.322397, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:53.208525: step 5492, loss 0.169407, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:53.289119: step 5493, loss 0.423764, acc 0.84375, learning_rate 0.0001
2017-10-11T11:21:53.371466: step 5494, loss 0.379681, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:53.453026: step 5495, loss 0.283238, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:53.536272: step 5496, loss 0.342058, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:53.616161: step 5497, loss 0.169885, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:53.697853: step 5498, loss 0.467562, acc 0.84375, learning_rate 0.0001
2017-10-11T11:21:53.779178: step 5499, loss 0.228131, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:53.863311: step 5500, loss 0.151205, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:53.947283: step 5501, loss 0.386403, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:54.028960: step 5502, loss 0.234746, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:54.108773: step 5503, loss 0.2055, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:54.191814: step 5504, loss 0.317445, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:54.273665: step 5505, loss 0.190253, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:54.360945: step 5506, loss 0.297917, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:54.446405: step 5507, loss 0.139866, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:54.527625: step 5508, loss 0.226659, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:54.608789: step 5509, loss 0.229448, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:54.692596: step 5510, loss 0.307484, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:54.776513: step 5511, loss 0.209359, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:54.860191: step 5512, loss 0.21588, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:54.942836: step 5513, loss 0.139615, acc 0.984375, learning_rate 0.0001
2017-10-11T11:21:55.023916: step 5514, loss 0.259536, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:55.106838: step 5515, loss 0.325095, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:55.192260: step 5516, loss 0.274436, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:55.276639: step 5517, loss 0.195057, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:55.358884: step 5518, loss 0.177356, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:55.442964: step 5519, loss 0.245765, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:55.523393: step 5520, loss 0.262876, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:55.714971: step 5520, loss 0.218668, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5520

2017-10-11T11:21:56.215395: step 5521, loss 0.279526, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:56.297630: step 5522, loss 0.247538, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:56.380967: step 5523, loss 0.20444, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:56.462818: step 5524, loss 0.19175, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:56.544146: step 5525, loss 0.177541, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:56.626525: step 5526, loss 0.171459, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:56.708571: step 5527, loss 0.286264, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:56.792880: step 5528, loss 0.305022, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:56.879956: step 5529, loss 0.294247, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:56.962462: step 5530, loss 0.259895, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:57.045783: step 5531, loss 0.238102, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:57.127983: step 5532, loss 0.175348, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:57.209976: step 5533, loss 0.232935, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:57.293051: step 5534, loss 0.223048, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:57.374894: step 5535, loss 0.276904, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:57.460928: step 5536, loss 0.162733, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:57.543588: step 5537, loss 0.276041, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:57.625005: step 5538, loss 0.341798, acc 0.890625, learning_rate 0.0001
2017-10-11T11:21:57.706863: step 5539, loss 0.205751, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:57.791056: step 5540, loss 0.197592, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:57.871579: step 5541, loss 0.157027, acc 0.984375, learning_rate 0.0001
2017-10-11T11:21:57.953239: step 5542, loss 0.177369, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:58.034753: step 5543, loss 0.213222, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:58.116259: step 5544, loss 0.22893, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:58.200938: step 5545, loss 0.252555, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:58.281909: step 5546, loss 0.195559, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:58.362530: step 5547, loss 0.20437, acc 0.953125, learning_rate 0.0001
2017-10-11T11:21:58.450016: step 5548, loss 0.203209, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:58.532634: step 5549, loss 0.339393, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:58.615629: step 5550, loss 0.366122, acc 0.859375, learning_rate 0.0001
2017-10-11T11:21:58.699158: step 5551, loss 0.245909, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:58.781280: step 5552, loss 0.24939, acc 0.9375, learning_rate 0.0001
2017-10-11T11:21:58.863574: step 5553, loss 0.204519, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:58.947821: step 5554, loss 0.244483, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:59.031845: step 5555, loss 0.236512, acc 0.921875, learning_rate 0.0001
2017-10-11T11:21:59.117394: step 5556, loss 0.165491, acc 0.96875, learning_rate 0.0001
2017-10-11T11:21:59.200211: step 5557, loss 0.250552, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:59.284659: step 5558, loss 0.259234, acc 0.875, learning_rate 0.0001
2017-10-11T11:21:59.367570: step 5559, loss 0.401338, acc 0.90625, learning_rate 0.0001
2017-10-11T11:21:59.449675: step 5560, loss 0.129139, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:21:59.644068: step 5560, loss 0.218581, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5560

2017-10-11T11:22:00.148749: step 5561, loss 0.162257, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:00.232161: step 5562, loss 0.321947, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:00.313826: step 5563, loss 0.230767, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:00.398698: step 5564, loss 0.156582, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:00.485075: step 5565, loss 0.200318, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:00.565732: step 5566, loss 0.187906, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:00.649296: step 5567, loss 0.290106, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:00.732425: step 5568, loss 0.315718, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:00.817971: step 5569, loss 0.309084, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:00.908619: step 5570, loss 0.145204, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:00.990805: step 5571, loss 0.344152, acc 0.84375, learning_rate 0.0001
2017-10-11T11:22:01.072505: step 5572, loss 0.187985, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:01.153427: step 5573, loss 0.156991, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:01.236132: step 5574, loss 0.327254, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:01.318868: step 5575, loss 0.260239, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:01.401963: step 5576, loss 0.228364, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:01.484056: step 5577, loss 0.159744, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:01.566399: step 5578, loss 0.292302, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:01.648372: step 5579, loss 0.164201, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:01.731139: step 5580, loss 0.13014, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:01.814731: step 5581, loss 0.163905, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:01.898796: step 5582, loss 0.209338, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:01.981112: step 5583, loss 0.113493, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:02.063951: step 5584, loss 0.183992, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:02.146573: step 5585, loss 0.259507, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:02.217085: step 5586, loss 0.276536, acc 0.882353, learning_rate 0.0001
2017-10-11T11:22:02.300833: step 5587, loss 0.145363, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:02.382080: step 5588, loss 0.292322, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:02.464927: step 5589, loss 0.220252, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:02.549164: step 5590, loss 0.23599, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:02.633040: step 5591, loss 0.299135, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:02.714974: step 5592, loss 0.114574, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:02.802290: step 5593, loss 0.302896, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:02.886060: step 5594, loss 0.197623, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:02.970189: step 5595, loss 0.380969, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:03.053458: step 5596, loss 0.262958, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:03.139350: step 5597, loss 0.113127, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:03.220808: step 5598, loss 0.204646, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:03.303838: step 5599, loss 0.172718, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:03.387751: step 5600, loss 0.397411, acc 0.828125, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:03.585305: step 5600, loss 0.218506, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5600

2017-10-11T11:22:04.090275: step 5601, loss 0.264212, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:04.170487: step 5602, loss 0.241917, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:04.251826: step 5603, loss 0.427301, acc 0.828125, learning_rate 0.0001
2017-10-11T11:22:04.334242: step 5604, loss 0.101592, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:04.416718: step 5605, loss 0.198337, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:04.498855: step 5606, loss 0.325545, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:04.581041: step 5607, loss 0.163088, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:04.665866: step 5608, loss 0.338688, acc 0.828125, learning_rate 0.0001
2017-10-11T11:22:04.748770: step 5609, loss 0.212133, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:04.830102: step 5610, loss 0.183044, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:04.913744: step 5611, loss 0.373727, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:04.996534: step 5612, loss 0.346641, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:05.081706: step 5613, loss 0.243496, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:05.163267: step 5614, loss 0.46063, acc 0.8125, learning_rate 0.0001
2017-10-11T11:22:05.247997: step 5615, loss 0.198433, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:05.331775: step 5616, loss 0.141746, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:05.413914: step 5617, loss 0.225522, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:05.497002: step 5618, loss 0.355128, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:05.578123: step 5619, loss 0.287035, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:05.660589: step 5620, loss 0.24372, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:05.740828: step 5621, loss 0.216693, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:05.823924: step 5622, loss 0.179074, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:05.907869: step 5623, loss 0.188716, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:05.990243: step 5624, loss 0.239681, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:06.072465: step 5625, loss 0.257271, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:06.156928: step 5626, loss 0.370636, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:06.242092: step 5627, loss 0.177758, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:06.324494: step 5628, loss 0.345998, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:06.408429: step 5629, loss 0.366678, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:06.492049: step 5630, loss 0.145783, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:06.572455: step 5631, loss 0.316413, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:06.655389: step 5632, loss 0.29603, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:06.738577: step 5633, loss 0.121224, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:06.821089: step 5634, loss 0.192245, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:06.903020: step 5635, loss 0.288384, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:06.987580: step 5636, loss 0.108005, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:07.071687: step 5637, loss 0.186126, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:07.154503: step 5638, loss 0.145799, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:07.237605: step 5639, loss 0.239494, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:07.321607: step 5640, loss 0.330294, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:07.517760: step 5640, loss 0.217636, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5640

2017-10-11T11:22:08.029712: step 5641, loss 0.254154, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:08.116036: step 5642, loss 0.179939, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:08.198437: step 5643, loss 0.178973, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:08.279985: step 5644, loss 0.276541, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:08.365286: step 5645, loss 0.212893, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:08.447608: step 5646, loss 0.243764, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:08.534646: step 5647, loss 0.264631, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:08.616974: step 5648, loss 0.224213, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:08.699268: step 5649, loss 0.123776, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:08.781300: step 5650, loss 0.224558, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:08.865120: step 5651, loss 0.215326, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:08.948152: step 5652, loss 0.314402, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:09.031682: step 5653, loss 0.236209, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:09.115286: step 5654, loss 0.195402, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:09.195217: step 5655, loss 0.195224, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:09.279376: step 5656, loss 0.210147, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:09.361855: step 5657, loss 0.185277, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:09.442291: step 5658, loss 0.3771, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:09.523690: step 5659, loss 0.418464, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:09.604277: step 5660, loss 0.256713, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:09.687207: step 5661, loss 0.25529, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:09.770554: step 5662, loss 0.162257, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:09.852042: step 5663, loss 0.25571, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:09.932503: step 5664, loss 0.37072, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:10.013396: step 5665, loss 0.205199, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:10.093618: step 5666, loss 0.223306, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:10.176735: step 5667, loss 0.326409, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:10.262715: step 5668, loss 0.288234, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:10.343120: step 5669, loss 0.360644, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:10.426019: step 5670, loss 0.317006, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:10.507283: step 5671, loss 0.279672, acc 0.84375, learning_rate 0.0001
2017-10-11T11:22:10.590264: step 5672, loss 0.224694, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:10.670688: step 5673, loss 0.251128, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:10.752955: step 5674, loss 0.112597, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:10.834885: step 5675, loss 0.155323, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:10.915777: step 5676, loss 0.205877, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:10.999377: step 5677, loss 0.279359, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:11.082403: step 5678, loss 0.219052, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:11.163795: step 5679, loss 0.208204, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:11.242866: step 5680, loss 0.260423, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:11.446340: step 5680, loss 0.21719, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5680

2017-10-11T11:22:11.955596: step 5681, loss 0.194532, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:12.037220: step 5682, loss 0.188166, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:12.119997: step 5683, loss 0.133126, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:12.191076: step 5684, loss 0.189451, acc 0.941176, learning_rate 0.0001
2017-10-11T11:22:12.276711: step 5685, loss 0.226384, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:12.356378: step 5686, loss 0.234647, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:12.438651: step 5687, loss 0.261396, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:12.521173: step 5688, loss 0.209331, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:12.604343: step 5689, loss 0.2023, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:12.688361: step 5690, loss 0.167623, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:12.776206: step 5691, loss 0.194763, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:12.857211: step 5692, loss 0.258063, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:12.945143: step 5693, loss 0.210501, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:13.028689: step 5694, loss 0.0730143, acc 1, learning_rate 0.0001
2017-10-11T11:22:13.111483: step 5695, loss 0.398976, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:13.194194: step 5696, loss 0.201089, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:13.277057: step 5697, loss 0.273049, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:13.359264: step 5698, loss 0.213667, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:13.444188: step 5699, loss 0.197162, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:13.529100: step 5700, loss 0.181588, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:13.610300: step 5701, loss 0.273976, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:13.691622: step 5702, loss 0.359567, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:13.775561: step 5703, loss 0.293685, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:13.859766: step 5704, loss 0.0986867, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:13.941780: step 5705, loss 0.23997, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:14.023501: step 5706, loss 0.232469, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:14.107707: step 5707, loss 0.285869, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:14.187270: step 5708, loss 0.210836, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:14.270935: step 5709, loss 0.165686, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:14.353337: step 5710, loss 0.188028, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:14.435396: step 5711, loss 0.352143, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:14.516677: step 5712, loss 0.261932, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:14.598942: step 5713, loss 0.251923, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:14.682079: step 5714, loss 0.214592, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:14.763786: step 5715, loss 0.129137, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:14.848138: step 5716, loss 0.266043, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:14.928235: step 5717, loss 0.166092, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:15.008374: step 5718, loss 0.197772, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:15.088945: step 5719, loss 0.224808, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:15.173704: step 5720, loss 0.304867, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:15.373828: step 5720, loss 0.217221, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5720

2017-10-11T11:22:15.813212: step 5721, loss 0.156504, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:15.896111: step 5722, loss 0.182926, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:15.979189: step 5723, loss 0.259739, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:16.061072: step 5724, loss 0.313631, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:16.148760: step 5725, loss 0.330153, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:16.232192: step 5726, loss 0.170866, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:16.315605: step 5727, loss 0.107009, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:16.395796: step 5728, loss 0.108546, acc 1, learning_rate 0.0001
2017-10-11T11:22:16.480833: step 5729, loss 0.194772, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:16.559720: step 5730, loss 0.213879, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:16.643005: step 5731, loss 0.26983, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:16.726882: step 5732, loss 0.236491, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:16.809685: step 5733, loss 0.184477, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:16.891101: step 5734, loss 0.150343, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:16.972884: step 5735, loss 0.264516, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:17.057257: step 5736, loss 0.234513, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:17.142538: step 5737, loss 0.107025, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:17.225884: step 5738, loss 0.232424, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:17.308697: step 5739, loss 0.260834, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:17.392738: step 5740, loss 0.331237, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:17.473839: step 5741, loss 0.179734, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:17.561594: step 5742, loss 0.220733, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:17.646895: step 5743, loss 0.192417, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:17.727342: step 5744, loss 0.1882, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:17.810136: step 5745, loss 0.204792, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:17.899416: step 5746, loss 0.0750973, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:17.993753: step 5747, loss 0.317568, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:18.079656: step 5748, loss 0.272234, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:18.166409: step 5749, loss 0.178087, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:18.248692: step 5750, loss 0.305841, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:18.326420: step 5751, loss 0.271277, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:18.409121: step 5752, loss 0.294897, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:18.491959: step 5753, loss 0.275467, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:18.574974: step 5754, loss 0.215375, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:18.656035: step 5755, loss 0.225232, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:18.738368: step 5756, loss 0.22213, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:18.820175: step 5757, loss 0.360551, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:18.904463: step 5758, loss 0.199139, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:18.986615: step 5759, loss 0.240432, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:19.068252: step 5760, loss 0.109703, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:19.267426: step 5760, loss 0.216951, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5760

2017-10-11T11:22:19.769100: step 5761, loss 0.18587, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:19.851382: step 5762, loss 0.214355, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:19.932825: step 5763, loss 0.378819, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:20.015454: step 5764, loss 0.304386, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:20.097983: step 5765, loss 0.253018, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:20.183369: step 5766, loss 0.125544, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:20.267568: step 5767, loss 0.127866, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:20.350194: step 5768, loss 0.355477, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:20.432714: step 5769, loss 0.10611, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:20.513770: step 5770, loss 0.293864, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:20.596581: step 5771, loss 0.177056, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:20.682539: step 5772, loss 0.203752, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:20.763798: step 5773, loss 0.343402, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:20.848632: step 5774, loss 0.267877, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:20.933549: step 5775, loss 0.229928, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:21.016895: step 5776, loss 0.139172, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:21.099915: step 5777, loss 0.32484, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:21.180872: step 5778, loss 0.188161, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:21.263212: step 5779, loss 0.303023, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:21.346875: step 5780, loss 0.188541, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:21.430802: step 5781, loss 0.212662, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:21.502252: step 5782, loss 0.0944243, acc 1, learning_rate 0.0001
2017-10-11T11:22:21.585290: step 5783, loss 0.162902, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:21.670044: step 5784, loss 0.220072, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:21.754433: step 5785, loss 0.201166, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:21.836106: step 5786, loss 0.388142, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:21.918058: step 5787, loss 0.126127, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:22.002908: step 5788, loss 0.277357, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:22.086629: step 5789, loss 0.147355, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:22.170558: step 5790, loss 0.149905, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:22.252381: step 5791, loss 0.201868, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:22.337239: step 5792, loss 0.219921, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:22.419611: step 5793, loss 0.144202, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:22.504564: step 5794, loss 0.172021, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:22.585070: step 5795, loss 0.294044, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:22.668462: step 5796, loss 0.153113, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:22.751059: step 5797, loss 0.159635, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:22.832386: step 5798, loss 0.271674, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:22.915489: step 5799, loss 0.142326, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:22.999477: step 5800, loss 0.201133, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:23.201919: step 5800, loss 0.217515, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5800

2017-10-11T11:22:23.703735: step 5801, loss 0.306568, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:23.787330: step 5802, loss 0.250572, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:23.872784: step 5803, loss 0.352925, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:23.957536: step 5804, loss 0.16967, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:24.040484: step 5805, loss 0.192697, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:24.123769: step 5806, loss 0.199261, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:24.205700: step 5807, loss 0.330607, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:24.290861: step 5808, loss 0.353919, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:24.374021: step 5809, loss 0.254371, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:24.457794: step 5810, loss 0.287616, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:24.538484: step 5811, loss 0.211285, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:24.621526: step 5812, loss 0.152392, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:24.702243: step 5813, loss 0.103729, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:24.782568: step 5814, loss 0.157062, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:24.866184: step 5815, loss 0.265565, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:24.947145: step 5816, loss 0.299613, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:25.028142: step 5817, loss 0.166909, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:25.109719: step 5818, loss 0.209668, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:25.188934: step 5819, loss 0.143299, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:25.272051: step 5820, loss 0.198994, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:25.355374: step 5821, loss 0.228747, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:25.442818: step 5822, loss 0.203191, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:25.525657: step 5823, loss 0.243872, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:25.609567: step 5824, loss 0.185679, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:25.695086: step 5825, loss 0.405902, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:25.783393: step 5826, loss 0.219914, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:25.868393: step 5827, loss 0.175013, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:25.953194: step 5828, loss 0.259942, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:26.035650: step 5829, loss 0.2365, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:26.119170: step 5830, loss 0.221465, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:26.201445: step 5831, loss 0.192421, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:26.282754: step 5832, loss 0.166136, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:26.369799: step 5833, loss 0.344215, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:26.452835: step 5834, loss 0.310743, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:26.536000: step 5835, loss 0.238365, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:26.621862: step 5836, loss 0.409316, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:26.703938: step 5837, loss 0.331921, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:26.790241: step 5838, loss 0.149544, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:26.872926: step 5839, loss 0.130994, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:26.956310: step 5840, loss 0.20928, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:27.141591: step 5840, loss 0.217657, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5840

2017-10-11T11:22:27.639384: step 5841, loss 0.177193, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:27.724517: step 5842, loss 0.198023, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:27.808990: step 5843, loss 0.302105, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:27.892462: step 5844, loss 0.21557, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:27.975105: step 5845, loss 0.187529, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:28.057301: step 5846, loss 0.174075, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:28.139499: step 5847, loss 0.16385, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:28.220036: step 5848, loss 0.192633, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:28.301944: step 5849, loss 0.118611, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:28.386201: step 5850, loss 0.287898, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:28.469606: step 5851, loss 0.281627, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:28.550595: step 5852, loss 0.259366, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:28.631433: step 5853, loss 0.223605, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:28.715660: step 5854, loss 0.252983, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:28.797922: step 5855, loss 0.22718, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:28.879725: step 5856, loss 0.101138, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:28.960420: step 5857, loss 0.229714, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:29.042688: step 5858, loss 0.192034, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:29.122931: step 5859, loss 0.143823, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:29.206571: step 5860, loss 0.423838, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:29.290035: step 5861, loss 0.290079, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:29.371789: step 5862, loss 0.221982, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:29.454192: step 5863, loss 0.180134, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:29.536704: step 5864, loss 0.165069, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:29.619294: step 5865, loss 0.178787, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:29.702127: step 5866, loss 0.261701, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:29.782981: step 5867, loss 0.262706, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:29.865446: step 5868, loss 0.316226, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:29.948597: step 5869, loss 0.135083, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:30.029972: step 5870, loss 0.184584, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:30.112045: step 5871, loss 0.246058, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:30.194709: step 5872, loss 0.36134, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:30.278905: step 5873, loss 0.279837, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:30.359499: step 5874, loss 0.190417, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:30.440233: step 5875, loss 0.223569, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:30.525214: step 5876, loss 0.289566, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:30.609498: step 5877, loss 0.159004, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:30.691886: step 5878, loss 0.201757, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:30.774143: step 5879, loss 0.235428, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:30.846581: step 5880, loss 0.183636, acc 0.941176, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:31.033790: step 5880, loss 0.216998, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5880

2017-10-11T11:22:31.541244: step 5881, loss 0.233179, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:31.623339: step 5882, loss 0.223925, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:31.703925: step 5883, loss 0.281659, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:31.788330: step 5884, loss 0.364799, acc 0.84375, learning_rate 0.0001
2017-10-11T11:22:31.870973: step 5885, loss 0.141427, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:31.952939: step 5886, loss 0.211271, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:32.037597: step 5887, loss 0.175048, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:32.120335: step 5888, loss 0.190669, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:32.201239: step 5889, loss 0.184165, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:32.285885: step 5890, loss 0.203916, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:32.367405: step 5891, loss 0.155902, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:32.448454: step 5892, loss 0.151037, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:32.533326: step 5893, loss 0.259114, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:32.616405: step 5894, loss 0.162112, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:32.703423: step 5895, loss 0.127248, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:32.786661: step 5896, loss 0.16403, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:32.869855: step 5897, loss 0.351842, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:32.951914: step 5898, loss 0.163697, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:33.031536: step 5899, loss 0.19227, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:33.114215: step 5900, loss 0.15186, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:33.196851: step 5901, loss 0.246479, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:33.281041: step 5902, loss 0.324523, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:33.364612: step 5903, loss 0.37756, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:33.446643: step 5904, loss 0.310386, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:33.529524: step 5905, loss 0.223979, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:33.612331: step 5906, loss 0.218571, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:33.698298: step 5907, loss 0.196184, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:33.784349: step 5908, loss 0.335224, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:33.868222: step 5909, loss 0.15093, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:33.951283: step 5910, loss 0.144185, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:34.030532: step 5911, loss 0.310571, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:34.111420: step 5912, loss 0.113691, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:34.192789: step 5913, loss 0.195816, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:34.275964: step 5914, loss 0.228711, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:34.359224: step 5915, loss 0.255785, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:34.442703: step 5916, loss 0.257242, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:34.526344: step 5917, loss 0.240462, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:34.607394: step 5918, loss 0.304297, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:34.688018: step 5919, loss 0.174663, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:34.770527: step 5920, loss 0.179193, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:34.976861: step 5920, loss 0.216662, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5920

2017-10-11T11:22:35.479951: step 5921, loss 0.224645, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:35.561815: step 5922, loss 0.237474, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:35.645474: step 5923, loss 0.188247, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:35.728426: step 5924, loss 0.203908, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:35.811068: step 5925, loss 0.246068, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:35.895261: step 5926, loss 0.233431, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:35.977658: step 5927, loss 0.24068, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:36.060102: step 5928, loss 0.228978, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:36.139548: step 5929, loss 0.232076, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:36.219837: step 5930, loss 0.321944, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:36.300792: step 5931, loss 0.160319, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:36.384831: step 5932, loss 0.358089, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:36.467918: step 5933, loss 0.299299, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:36.552125: step 5934, loss 0.0999434, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:36.636907: step 5935, loss 0.173298, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:36.719093: step 5936, loss 0.227785, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:36.802007: step 5937, loss 0.21899, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:36.885767: step 5938, loss 0.379514, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:36.968582: step 5939, loss 0.2589, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:37.050854: step 5940, loss 0.177925, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:37.133257: step 5941, loss 0.180031, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:37.215269: step 5942, loss 0.158723, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:37.301969: step 5943, loss 0.263664, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:37.384159: step 5944, loss 0.171198, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:37.466400: step 5945, loss 0.385007, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:37.550864: step 5946, loss 0.177531, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:37.634827: step 5947, loss 0.144023, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:37.718935: step 5948, loss 0.291456, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:37.802628: step 5949, loss 0.192053, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:37.887929: step 5950, loss 0.261588, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:37.971355: step 5951, loss 0.188861, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:38.055558: step 5952, loss 0.252777, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:38.138373: step 5953, loss 0.314696, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:38.222648: step 5954, loss 0.212853, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:38.305749: step 5955, loss 0.18222, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:38.387436: step 5956, loss 0.227018, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:38.468868: step 5957, loss 0.157534, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:38.551527: step 5958, loss 0.287456, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:38.636171: step 5959, loss 0.165231, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:38.717666: step 5960, loss 0.12364, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:38.922816: step 5960, loss 0.215625, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-5960

2017-10-11T11:22:39.568832: step 5961, loss 0.186438, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:39.648741: step 5962, loss 0.288152, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:39.729971: step 5963, loss 0.283914, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:39.812406: step 5964, loss 0.404519, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:39.895824: step 5965, loss 0.296392, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:39.979866: step 5966, loss 0.187863, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:40.062832: step 5967, loss 0.323484, acc 0.84375, learning_rate 0.0001
2017-10-11T11:22:40.146297: step 5968, loss 0.242154, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:40.229514: step 5969, loss 0.151765, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:40.310427: step 5970, loss 0.206274, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:40.391594: step 5971, loss 0.19863, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:40.475835: step 5972, loss 0.356562, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:40.560897: step 5973, loss 0.236758, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:40.645384: step 5974, loss 0.170416, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:40.730202: step 5975, loss 0.284136, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:40.813004: step 5976, loss 0.342918, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:40.898445: step 5977, loss 0.222039, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:40.969616: step 5978, loss 0.232184, acc 0.901961, learning_rate 0.0001
2017-10-11T11:22:41.055646: step 5979, loss 0.332181, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:41.139731: step 5980, loss 0.193723, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:41.223675: step 5981, loss 0.334426, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:41.304254: step 5982, loss 0.239005, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:41.386024: step 5983, loss 0.207314, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:41.467870: step 5984, loss 0.351985, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:41.548402: step 5985, loss 0.324702, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:41.631512: step 5986, loss 0.237639, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:41.711893: step 5987, loss 0.192829, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:41.796883: step 5988, loss 0.209726, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:41.879735: step 5989, loss 0.309025, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:41.960839: step 5990, loss 0.191429, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:42.046436: step 5991, loss 0.190496, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:42.127712: step 5992, loss 0.210365, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:42.211386: step 5993, loss 0.11463, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:42.294265: step 5994, loss 0.125787, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:42.377517: step 5995, loss 0.33301, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:42.459701: step 5996, loss 0.222006, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:42.544226: step 5997, loss 0.203448, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:42.627816: step 5998, loss 0.190033, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:42.707844: step 5999, loss 0.237403, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:42.789681: step 6000, loss 0.122536, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:42.987776: step 6000, loss 0.216775, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6000

2017-10-11T11:22:43.495060: step 6001, loss 0.210421, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:43.578217: step 6002, loss 0.212459, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:43.661119: step 6003, loss 0.194864, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:43.743114: step 6004, loss 0.322418, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:43.828481: step 6005, loss 0.290774, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:43.909526: step 6006, loss 0.166623, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:43.994386: step 6007, loss 0.243371, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:44.075082: step 6008, loss 0.288524, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:44.156348: step 6009, loss 0.239013, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:44.238951: step 6010, loss 0.263784, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:44.322116: step 6011, loss 0.282992, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:44.404208: step 6012, loss 0.268175, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:44.484143: step 6013, loss 0.279243, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:44.566852: step 6014, loss 0.254814, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:44.650243: step 6015, loss 0.262075, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:44.732976: step 6016, loss 0.380153, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:44.816063: step 6017, loss 0.276972, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:44.899538: step 6018, loss 0.104494, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:44.981813: step 6019, loss 0.178327, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:45.064835: step 6020, loss 0.277742, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:45.145890: step 6021, loss 0.171693, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:45.228255: step 6022, loss 0.209406, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:45.317781: step 6023, loss 0.205166, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:45.400614: step 6024, loss 0.155058, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:45.482648: step 6025, loss 0.34097, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:45.566577: step 6026, loss 0.262328, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:45.646805: step 6027, loss 0.277538, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:45.728961: step 6028, loss 0.242158, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:45.811896: step 6029, loss 0.0976271, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:45.893804: step 6030, loss 0.12095, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:45.977655: step 6031, loss 0.191919, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:46.059085: step 6032, loss 0.207227, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:46.141791: step 6033, loss 0.411012, acc 0.84375, learning_rate 0.0001
2017-10-11T11:22:46.222408: step 6034, loss 0.274066, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:46.304281: step 6035, loss 0.161393, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:46.386445: step 6036, loss 0.205511, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:46.469987: step 6037, loss 0.188527, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:46.554281: step 6038, loss 0.262555, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:46.637916: step 6039, loss 0.356475, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:46.721046: step 6040, loss 0.225437, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:46.910870: step 6040, loss 0.215652, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6040

2017-10-11T11:22:47.345001: step 6041, loss 0.231131, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:47.432007: step 6042, loss 0.148035, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:47.517011: step 6043, loss 0.230261, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:47.603849: step 6044, loss 0.278409, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:47.687151: step 6045, loss 0.145244, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:47.771751: step 6046, loss 0.235056, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:47.857197: step 6047, loss 0.111699, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:47.940597: step 6048, loss 0.19947, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:48.022487: step 6049, loss 0.208779, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:48.103280: step 6050, loss 0.22423, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:48.183626: step 6051, loss 0.336656, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:48.269226: step 6052, loss 0.230863, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:48.350828: step 6053, loss 0.382292, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:48.432500: step 6054, loss 0.348754, acc 0.84375, learning_rate 0.0001
2017-10-11T11:22:48.517973: step 6055, loss 0.261681, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:48.599288: step 6056, loss 0.248356, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:48.681356: step 6057, loss 0.175458, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:48.767250: step 6058, loss 0.185883, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:48.849548: step 6059, loss 0.183843, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:48.933359: step 6060, loss 0.204791, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:49.016161: step 6061, loss 0.235354, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:49.100318: step 6062, loss 0.237571, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:49.184789: step 6063, loss 0.23473, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:49.268070: step 6064, loss 0.322676, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:49.350873: step 6065, loss 0.373749, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:49.435596: step 6066, loss 0.141714, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:49.518741: step 6067, loss 0.243271, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:49.605502: step 6068, loss 0.257172, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:49.686505: step 6069, loss 0.118222, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:49.766597: step 6070, loss 0.257137, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:49.851858: step 6071, loss 0.159142, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:49.938486: step 6072, loss 0.251229, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:50.022297: step 6073, loss 0.165907, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:50.103497: step 6074, loss 0.234188, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:50.187177: step 6075, loss 0.220555, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:50.257195: step 6076, loss 0.158221, acc 0.980392, learning_rate 0.0001
2017-10-11T11:22:50.339790: step 6077, loss 0.269597, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:50.419861: step 6078, loss 0.171717, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:50.500797: step 6079, loss 0.23682, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:50.584550: step 6080, loss 0.138422, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:50.790996: step 6080, loss 0.215746, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6080

2017-10-11T11:22:51.289710: step 6081, loss 0.120764, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:51.371252: step 6082, loss 0.307156, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:51.454349: step 6083, loss 0.233564, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:51.536286: step 6084, loss 0.205167, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:51.621362: step 6085, loss 0.256983, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:51.702477: step 6086, loss 0.176519, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:51.783928: step 6087, loss 0.183982, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:51.870360: step 6088, loss 0.293055, acc 0.859375, learning_rate 0.0001
2017-10-11T11:22:51.952710: step 6089, loss 0.10296, acc 0.984375, learning_rate 0.0001
2017-10-11T11:22:52.034059: step 6090, loss 0.247557, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:52.120216: step 6091, loss 0.246129, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:52.202298: step 6092, loss 0.199006, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:52.290318: step 6093, loss 0.255642, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:52.373798: step 6094, loss 0.32516, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:52.454034: step 6095, loss 0.191322, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:52.537065: step 6096, loss 0.256662, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:52.621305: step 6097, loss 0.295712, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:52.702597: step 6098, loss 0.221741, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:52.783498: step 6099, loss 0.238908, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:52.867214: step 6100, loss 0.102176, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:52.949297: step 6101, loss 0.250653, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:53.035015: step 6102, loss 0.239861, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:53.116936: step 6103, loss 0.187041, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:53.199335: step 6104, loss 0.159696, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:53.283823: step 6105, loss 0.250447, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:53.367814: step 6106, loss 0.201005, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:53.447656: step 6107, loss 0.275924, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:53.535460: step 6108, loss 0.17227, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:53.621127: step 6109, loss 0.248578, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:53.700272: step 6110, loss 0.258649, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:53.783552: step 6111, loss 0.102608, acc 1, learning_rate 0.0001
2017-10-11T11:22:53.867800: step 6112, loss 0.216351, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:53.952422: step 6113, loss 0.156607, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:54.038686: step 6114, loss 0.213019, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:54.119746: step 6115, loss 0.263358, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:54.202207: step 6116, loss 0.164557, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:54.280675: step 6117, loss 0.216248, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:54.365458: step 6118, loss 0.319444, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:54.449112: step 6119, loss 0.22336, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:54.530464: step 6120, loss 0.321883, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:54.735784: step 6120, loss 0.216163, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6120

2017-10-11T11:22:55.234528: step 6121, loss 0.26065, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:55.316039: step 6122, loss 0.182124, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:55.395494: step 6123, loss 0.242254, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:55.475598: step 6124, loss 0.218308, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:55.558577: step 6125, loss 0.344234, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:55.639850: step 6126, loss 0.187758, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:55.724903: step 6127, loss 0.220134, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:55.803962: step 6128, loss 0.149064, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:55.888233: step 6129, loss 0.132615, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:55.970315: step 6130, loss 0.244497, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:56.053618: step 6131, loss 0.325325, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:56.136749: step 6132, loss 0.365297, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:56.219359: step 6133, loss 0.205423, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:56.300363: step 6134, loss 0.254408, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:56.382228: step 6135, loss 0.267734, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:56.464205: step 6136, loss 0.161627, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:56.547344: step 6137, loss 0.260061, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:56.630974: step 6138, loss 0.266097, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:56.713913: step 6139, loss 0.322461, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:56.796188: step 6140, loss 0.255473, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:56.879178: step 6141, loss 0.176208, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:56.961209: step 6142, loss 0.291242, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:57.046280: step 6143, loss 0.101212, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:57.127461: step 6144, loss 0.450157, acc 0.84375, learning_rate 0.0001
2017-10-11T11:22:57.212055: step 6145, loss 0.27525, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:57.291696: step 6146, loss 0.2708, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:57.375566: step 6147, loss 0.227016, acc 0.890625, learning_rate 0.0001
2017-10-11T11:22:57.459333: step 6148, loss 0.271771, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:57.541554: step 6149, loss 0.274757, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:57.623653: step 6150, loss 0.249878, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:57.706714: step 6151, loss 0.233997, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:57.790172: step 6152, loss 0.305436, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:57.873460: step 6153, loss 0.158835, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:57.954386: step 6154, loss 0.129722, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:58.033228: step 6155, loss 0.188958, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:58.111271: step 6156, loss 0.175533, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:58.195446: step 6157, loss 0.247868, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:58.279323: step 6158, loss 0.275467, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:58.361306: step 6159, loss 0.223524, acc 0.921875, learning_rate 0.0001
2017-10-11T11:22:58.443544: step 6160, loss 0.174649, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:22:58.637058: step 6160, loss 0.215389, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6160

2017-10-11T11:22:59.134648: step 6161, loss 0.287991, acc 0.875, learning_rate 0.0001
2017-10-11T11:22:59.215820: step 6162, loss 0.226939, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:59.298423: step 6163, loss 0.0902512, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:59.380595: step 6164, loss 0.18758, acc 0.9375, learning_rate 0.0001
2017-10-11T11:22:59.462926: step 6165, loss 0.191463, acc 0.96875, learning_rate 0.0001
2017-10-11T11:22:59.547105: step 6166, loss 0.358723, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:59.630146: step 6167, loss 0.261772, acc 0.90625, learning_rate 0.0001
2017-10-11T11:22:59.714208: step 6168, loss 0.237764, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:59.795878: step 6169, loss 0.419756, acc 0.84375, learning_rate 0.0001
2017-10-11T11:22:59.880283: step 6170, loss 0.234071, acc 0.953125, learning_rate 0.0001
2017-10-11T11:22:59.964646: step 6171, loss 0.273823, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:00.050061: step 6172, loss 0.206444, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:00.136205: step 6173, loss 0.325259, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:00.206117: step 6174, loss 0.131551, acc 0.941176, learning_rate 0.0001
2017-10-11T11:23:00.290962: step 6175, loss 0.21846, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:00.371786: step 6176, loss 0.236884, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:00.453845: step 6177, loss 0.129015, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:00.539122: step 6178, loss 0.272234, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:00.621067: step 6179, loss 0.209228, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:00.704060: step 6180, loss 0.216616, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:00.784328: step 6181, loss 0.194461, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:00.866899: step 6182, loss 0.209954, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:00.947745: step 6183, loss 0.243038, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:01.030310: step 6184, loss 0.211553, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:01.113122: step 6185, loss 0.294374, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:01.195297: step 6186, loss 0.101646, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:01.279389: step 6187, loss 0.413734, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:01.359605: step 6188, loss 0.325918, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:01.443396: step 6189, loss 0.264907, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:01.526593: step 6190, loss 0.303729, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:01.606775: step 6191, loss 0.124474, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:01.685227: step 6192, loss 0.155178, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:01.765887: step 6193, loss 0.170895, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:01.848500: step 6194, loss 0.353522, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:01.930530: step 6195, loss 0.168079, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:02.014019: step 6196, loss 0.157937, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:02.099584: step 6197, loss 0.193857, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:02.182348: step 6198, loss 0.318962, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:02.267188: step 6199, loss 0.135774, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:02.348350: step 6200, loss 0.146133, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:02.540638: step 6200, loss 0.215685, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6200

2017-10-11T11:23:03.041075: step 6201, loss 0.182178, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:03.125828: step 6202, loss 0.230983, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:03.207114: step 6203, loss 0.512244, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:03.291420: step 6204, loss 0.250815, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:03.377221: step 6205, loss 0.13384, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:03.458251: step 6206, loss 0.251577, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:03.544752: step 6207, loss 0.308296, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:03.627909: step 6208, loss 0.255072, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:03.711602: step 6209, loss 0.315046, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:03.793067: step 6210, loss 0.166587, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:03.877209: step 6211, loss 0.24688, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:03.960536: step 6212, loss 0.161591, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:04.044150: step 6213, loss 0.14415, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:04.127051: step 6214, loss 0.285591, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:04.209512: step 6215, loss 0.157742, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:04.293506: step 6216, loss 0.192091, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:04.375278: step 6217, loss 0.271209, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:04.456866: step 6218, loss 0.0853062, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:04.540580: step 6219, loss 0.239633, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:04.625107: step 6220, loss 0.30285, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:04.705374: step 6221, loss 0.167471, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:04.788807: step 6222, loss 0.141169, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:04.870994: step 6223, loss 0.383911, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:04.952946: step 6224, loss 0.435222, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:05.034824: step 6225, loss 0.244692, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:05.118138: step 6226, loss 0.111089, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:05.201184: step 6227, loss 0.258725, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:05.286211: step 6228, loss 0.311882, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:05.368065: step 6229, loss 0.350581, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:05.450496: step 6230, loss 0.131382, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:05.535555: step 6231, loss 0.354936, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:05.621670: step 6232, loss 0.214937, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:05.706298: step 6233, loss 0.230781, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:05.789704: step 6234, loss 0.248636, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:05.873488: step 6235, loss 0.145959, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:05.956637: step 6236, loss 0.180689, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:06.043685: step 6237, loss 0.14204, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:06.124262: step 6238, loss 0.416383, acc 0.828125, learning_rate 0.0001
2017-10-11T11:23:06.205007: step 6239, loss 0.149624, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:06.289712: step 6240, loss 0.308579, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:06.491236: step 6240, loss 0.215802, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6240

2017-10-11T11:23:06.992282: step 6241, loss 0.120243, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:07.075960: step 6242, loss 0.139182, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:07.158141: step 6243, loss 0.212966, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:07.240197: step 6244, loss 0.270488, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:07.320850: step 6245, loss 0.215638, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:07.403411: step 6246, loss 0.286602, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:07.486035: step 6247, loss 0.0911181, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:07.567514: step 6248, loss 0.140669, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:07.650831: step 6249, loss 0.209895, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:07.732522: step 6250, loss 0.389545, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:07.818363: step 6251, loss 0.289717, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:07.900619: step 6252, loss 0.197937, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:07.981951: step 6253, loss 0.179793, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:08.064894: step 6254, loss 0.278183, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:08.146273: step 6255, loss 0.223746, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:08.228191: step 6256, loss 0.360825, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:08.309830: step 6257, loss 0.211749, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:08.391220: step 6258, loss 0.248886, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:08.474553: step 6259, loss 0.130845, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:08.557393: step 6260, loss 0.152353, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:08.638967: step 6261, loss 0.217419, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:08.721914: step 6262, loss 0.309224, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:08.803732: step 6263, loss 0.2412, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:08.886558: step 6264, loss 0.262813, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:08.969448: step 6265, loss 0.140597, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:09.052771: step 6266, loss 0.124675, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:09.135152: step 6267, loss 0.217829, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:09.215573: step 6268, loss 0.258599, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:09.299474: step 6269, loss 0.252461, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:09.380968: step 6270, loss 0.316108, acc 0.84375, learning_rate 0.0001
2017-10-11T11:23:09.460973: step 6271, loss 0.161455, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:09.531484: step 6272, loss 0.365578, acc 0.862745, learning_rate 0.0001
2017-10-11T11:23:09.616571: step 6273, loss 0.258056, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:09.696552: step 6274, loss 0.238765, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:09.776951: step 6275, loss 0.250051, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:09.859869: step 6276, loss 0.328061, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:09.941785: step 6277, loss 0.191216, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:10.024364: step 6278, loss 0.238873, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:10.105954: step 6279, loss 0.159482, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:10.188984: step 6280, loss 0.274097, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:10.384908: step 6280, loss 0.215688, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6280

2017-10-11T11:23:10.886961: step 6281, loss 0.267643, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:10.970352: step 6282, loss 0.233133, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:11.053691: step 6283, loss 0.198846, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:11.136708: step 6284, loss 0.133708, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:11.218325: step 6285, loss 0.270455, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:11.301638: step 6286, loss 0.151519, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:11.381299: step 6287, loss 0.213409, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:11.466432: step 6288, loss 0.23535, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:11.550894: step 6289, loss 0.322467, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:11.634844: step 6290, loss 0.205536, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:11.721143: step 6291, loss 0.209373, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:11.802849: step 6292, loss 0.189344, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:11.886797: step 6293, loss 0.0986238, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:11.966446: step 6294, loss 0.336071, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:12.048122: step 6295, loss 0.189511, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:12.131023: step 6296, loss 0.247391, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:12.214097: step 6297, loss 0.289394, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:12.296633: step 6298, loss 0.252641, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:12.378642: step 6299, loss 0.27722, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:12.460515: step 6300, loss 0.195897, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:12.543514: step 6301, loss 0.234374, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:12.623319: step 6302, loss 0.15206, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:12.707596: step 6303, loss 0.132901, acc 1, learning_rate 0.0001
2017-10-11T11:23:12.792298: step 6304, loss 0.173471, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:12.874531: step 6305, loss 0.13754, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:12.958799: step 6306, loss 0.258073, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:13.039962: step 6307, loss 0.201227, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:13.123042: step 6308, loss 0.233343, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:13.205777: step 6309, loss 0.143901, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:13.286505: step 6310, loss 0.287004, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:13.368374: step 6311, loss 0.308499, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:13.454508: step 6312, loss 0.256993, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:13.536982: step 6313, loss 0.13896, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:13.622504: step 6314, loss 0.238518, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:13.707675: step 6315, loss 0.27958, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:13.792076: step 6316, loss 0.215781, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:13.874356: step 6317, loss 0.138925, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:13.959421: step 6318, loss 0.161458, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:14.043728: step 6319, loss 0.135699, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:14.127717: step 6320, loss 0.165964, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:14.329028: step 6320, loss 0.214099, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6320

2017-10-11T11:23:14.836095: step 6321, loss 0.224426, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:14.918413: step 6322, loss 0.157839, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:14.998096: step 6323, loss 0.207492, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:15.079099: step 6324, loss 0.17111, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:15.163236: step 6325, loss 0.177338, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:15.246864: step 6326, loss 0.137411, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:15.332399: step 6327, loss 0.234206, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:15.416709: step 6328, loss 0.090885, acc 1, learning_rate 0.0001
2017-10-11T11:23:15.498881: step 6329, loss 0.139978, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:15.581869: step 6330, loss 0.202882, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:15.664980: step 6331, loss 0.237392, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:15.746516: step 6332, loss 0.194434, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:15.833181: step 6333, loss 0.240033, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:15.916323: step 6334, loss 0.141234, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:15.998698: step 6335, loss 0.212652, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:16.080253: step 6336, loss 0.22921, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:16.161359: step 6337, loss 0.325927, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:16.245767: step 6338, loss 0.129155, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:16.326561: step 6339, loss 0.167218, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:16.408990: step 6340, loss 0.286596, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:16.490088: step 6341, loss 0.151608, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:16.575077: step 6342, loss 0.287037, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:16.658610: step 6343, loss 0.241168, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:16.740329: step 6344, loss 0.219713, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:16.823019: step 6345, loss 0.181415, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:16.909777: step 6346, loss 0.247909, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:16.990821: step 6347, loss 0.172076, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:17.070850: step 6348, loss 0.191695, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:17.151588: step 6349, loss 0.176038, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:17.229837: step 6350, loss 0.327464, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:17.312429: step 6351, loss 0.194349, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:17.395216: step 6352, loss 0.327557, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:17.479415: step 6353, loss 0.252526, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:17.560965: step 6354, loss 0.254582, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:17.644034: step 6355, loss 0.259009, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:17.726091: step 6356, loss 0.37043, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:17.808625: step 6357, loss 0.0878823, acc 1, learning_rate 0.0001
2017-10-11T11:23:17.888777: step 6358, loss 0.281879, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:17.972078: step 6359, loss 0.219082, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:18.055551: step 6360, loss 0.121083, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:18.261462: step 6360, loss 0.214605, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6360

2017-10-11T11:23:18.764432: step 6361, loss 0.380764, acc 0.84375, learning_rate 0.0001
2017-10-11T11:23:18.848798: step 6362, loss 0.207753, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:18.932819: step 6363, loss 0.300153, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:19.016177: step 6364, loss 0.249572, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:19.097410: step 6365, loss 0.331972, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:19.178903: step 6366, loss 0.154331, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:19.260356: step 6367, loss 0.208361, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:19.342130: step 6368, loss 0.173673, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:19.424537: step 6369, loss 0.261649, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:19.499002: step 6370, loss 0.161523, acc 0.941176, learning_rate 0.0001
2017-10-11T11:23:19.583553: step 6371, loss 0.204226, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:19.665259: step 6372, loss 0.228102, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:19.752241: step 6373, loss 0.193219, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:19.835941: step 6374, loss 0.285306, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:19.920284: step 6375, loss 0.155153, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:20.004272: step 6376, loss 0.257059, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:20.085619: step 6377, loss 0.327183, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:20.166519: step 6378, loss 0.17538, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:20.250218: step 6379, loss 0.287769, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:20.332984: step 6380, loss 0.193776, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:20.418833: step 6381, loss 0.4104, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:20.499999: step 6382, loss 0.277349, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:20.582503: step 6383, loss 0.212422, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:20.666544: step 6384, loss 0.215466, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:20.749553: step 6385, loss 0.225497, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:20.831447: step 6386, loss 0.302471, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:20.915973: step 6387, loss 0.266283, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:20.998646: step 6388, loss 0.256396, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:21.080231: step 6389, loss 0.255008, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:21.170504: step 6390, loss 0.209509, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:21.254046: step 6391, loss 0.131539, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:21.336435: step 6392, loss 0.228836, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:21.421894: step 6393, loss 0.174033, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:21.505346: step 6394, loss 0.268918, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:21.587619: step 6395, loss 0.174969, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:21.674043: step 6396, loss 0.330469, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:21.758933: step 6397, loss 0.219844, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:21.842564: step 6398, loss 0.262143, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:21.922846: step 6399, loss 0.165319, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:22.003942: step 6400, loss 0.347576, acc 0.859375, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:22.210396: step 6400, loss 0.214716, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6400

2017-10-11T11:23:22.641055: step 6401, loss 0.298095, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:22.723041: step 6402, loss 0.201751, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:22.806086: step 6403, loss 0.225217, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:22.888905: step 6404, loss 0.210866, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:22.970265: step 6405, loss 0.300898, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:23.056305: step 6406, loss 0.114283, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:23.138723: step 6407, loss 0.481168, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:23.222065: step 6408, loss 0.257476, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:23.306101: step 6409, loss 0.198021, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:23.391224: step 6410, loss 0.257561, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:23.475069: step 6411, loss 0.184344, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:23.554913: step 6412, loss 0.134073, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:23.636011: step 6413, loss 0.236097, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:23.717722: step 6414, loss 0.0814405, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:23.802518: step 6415, loss 0.261094, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:23.888689: step 6416, loss 0.224626, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:23.973885: step 6417, loss 0.112651, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:24.055024: step 6418, loss 0.452163, acc 0.84375, learning_rate 0.0001
2017-10-11T11:23:24.136234: step 6419, loss 0.210113, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:24.217402: step 6420, loss 0.221299, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:24.299992: step 6421, loss 0.134695, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:24.383118: step 6422, loss 0.155105, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:24.467438: step 6423, loss 0.280279, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:24.548956: step 6424, loss 0.191598, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:24.631525: step 6425, loss 0.377964, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:24.713891: step 6426, loss 0.204732, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:24.792299: step 6427, loss 0.11596, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:24.874771: step 6428, loss 0.112498, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:24.960195: step 6429, loss 0.137006, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:25.043677: step 6430, loss 0.241853, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:25.126584: step 6431, loss 0.263086, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:25.207633: step 6432, loss 0.135745, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:25.291668: step 6433, loss 0.149214, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:25.373401: step 6434, loss 0.181671, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:25.461132: step 6435, loss 0.414, acc 0.84375, learning_rate 0.0001
2017-10-11T11:23:25.547055: step 6436, loss 0.212335, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:25.631036: step 6437, loss 0.261046, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:25.712519: step 6438, loss 0.196697, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:25.797365: step 6439, loss 0.0765358, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:25.881967: step 6440, loss 0.340725, acc 0.84375, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:26.080908: step 6440, loss 0.214572, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6440

2017-10-11T11:23:26.572058: step 6441, loss 0.145597, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:26.654349: step 6442, loss 0.254794, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:26.736556: step 6443, loss 0.121488, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:26.816852: step 6444, loss 0.21336, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:26.900923: step 6445, loss 0.210625, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:26.980680: step 6446, loss 0.23117, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:27.063659: step 6447, loss 0.154457, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:27.146894: step 6448, loss 0.138856, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:27.228042: step 6449, loss 0.238894, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:27.311773: step 6450, loss 0.22383, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:27.394359: step 6451, loss 0.266823, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:27.478312: step 6452, loss 0.196099, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:27.561132: step 6453, loss 0.220048, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:27.643690: step 6454, loss 0.156151, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:27.727081: step 6455, loss 0.308683, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:27.810799: step 6456, loss 0.229866, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:27.892604: step 6457, loss 0.244087, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:27.977014: step 6458, loss 0.119166, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:28.058641: step 6459, loss 0.176666, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:28.144290: step 6460, loss 0.266705, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:28.226097: step 6461, loss 0.281979, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:28.307215: step 6462, loss 0.35359, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:28.391281: step 6463, loss 0.381065, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:28.472789: step 6464, loss 0.28838, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:28.557075: step 6465, loss 0.129514, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:28.637576: step 6466, loss 0.230213, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:28.719824: step 6467, loss 0.230347, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:28.790218: step 6468, loss 0.193097, acc 0.960784, learning_rate 0.0001
2017-10-11T11:23:28.872143: step 6469, loss 0.19443, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:28.956367: step 6470, loss 0.33697, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:29.038088: step 6471, loss 0.144511, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:29.120109: step 6472, loss 0.37944, acc 0.84375, learning_rate 0.0001
2017-10-11T11:23:29.204223: step 6473, loss 0.118476, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:29.288263: step 6474, loss 0.265335, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:29.370373: step 6475, loss 0.195969, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:29.453954: step 6476, loss 0.250232, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:29.538100: step 6477, loss 0.283157, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:29.621052: step 6478, loss 0.358589, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:29.706120: step 6479, loss 0.16519, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:29.787869: step 6480, loss 0.194194, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:29.993338: step 6480, loss 0.214098, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6480

2017-10-11T11:23:30.486062: step 6481, loss 0.261641, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:30.568295: step 6482, loss 0.281109, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:30.651212: step 6483, loss 0.281139, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:30.732985: step 6484, loss 0.210974, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:30.817253: step 6485, loss 0.198464, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:30.901693: step 6486, loss 0.155528, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:30.984543: step 6487, loss 0.159539, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:31.065178: step 6488, loss 0.357548, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:31.145747: step 6489, loss 0.253342, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:31.228044: step 6490, loss 0.246547, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:31.310606: step 6491, loss 0.336977, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:31.394500: step 6492, loss 0.185125, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:31.476330: step 6493, loss 0.169298, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:31.556210: step 6494, loss 0.371619, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:31.638795: step 6495, loss 0.267815, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:31.720614: step 6496, loss 0.375854, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:31.800795: step 6497, loss 0.220643, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:31.889041: step 6498, loss 0.210829, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:31.972359: step 6499, loss 0.277689, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:32.054959: step 6500, loss 0.207408, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:32.135795: step 6501, loss 0.160202, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:32.218126: step 6502, loss 0.0799803, acc 1, learning_rate 0.0001
2017-10-11T11:23:32.300315: step 6503, loss 0.210235, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:32.385586: step 6504, loss 0.157246, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:32.468358: step 6505, loss 0.207834, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:32.554220: step 6506, loss 0.334673, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:32.637621: step 6507, loss 0.283831, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:32.722263: step 6508, loss 0.248455, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:32.805079: step 6509, loss 0.222807, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:32.889205: step 6510, loss 0.262651, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:32.971858: step 6511, loss 0.197315, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:33.055980: step 6512, loss 0.315386, acc 0.84375, learning_rate 0.0001
2017-10-11T11:23:33.139646: step 6513, loss 0.209556, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:33.220108: step 6514, loss 0.231378, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:33.302241: step 6515, loss 0.344801, acc 0.84375, learning_rate 0.0001
2017-10-11T11:23:33.383118: step 6516, loss 0.179501, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:33.464639: step 6517, loss 0.420979, acc 0.84375, learning_rate 0.0001
2017-10-11T11:23:33.547802: step 6518, loss 0.289074, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:33.631880: step 6519, loss 0.121156, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:33.713228: step 6520, loss 0.23924, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:33.920256: step 6520, loss 0.213134, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6520

2017-10-11T11:23:34.417847: step 6521, loss 0.241065, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:34.504141: step 6522, loss 0.17405, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:34.587281: step 6523, loss 0.223442, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:34.673633: step 6524, loss 0.229141, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:34.754974: step 6525, loss 0.368586, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:34.835523: step 6526, loss 0.257943, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:34.917156: step 6527, loss 0.180967, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:34.998946: step 6528, loss 0.267902, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:35.080233: step 6529, loss 0.297227, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:35.160911: step 6530, loss 0.169895, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:35.241920: step 6531, loss 0.277139, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:35.325681: step 6532, loss 0.191525, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:35.407341: step 6533, loss 0.195593, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:35.489742: step 6534, loss 0.233293, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:35.571855: step 6535, loss 0.21787, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:35.653584: step 6536, loss 0.239853, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:35.736127: step 6537, loss 0.215671, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:35.821065: step 6538, loss 0.171972, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:35.903898: step 6539, loss 0.173504, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:35.983751: step 6540, loss 0.256981, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:36.065878: step 6541, loss 0.15282, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:36.146777: step 6542, loss 0.180406, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:36.226452: step 6543, loss 0.311183, acc 0.796875, learning_rate 0.0001
2017-10-11T11:23:36.308477: step 6544, loss 0.201674, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:36.389748: step 6545, loss 0.22967, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:36.472856: step 6546, loss 0.233644, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:36.555842: step 6547, loss 0.20497, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:36.639003: step 6548, loss 0.140437, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:36.720898: step 6549, loss 0.0881808, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:36.804916: step 6550, loss 0.231897, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:36.891786: step 6551, loss 0.160907, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:36.974017: step 6552, loss 0.31329, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:37.057343: step 6553, loss 0.229377, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:37.139124: step 6554, loss 0.189852, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:37.234783: step 6555, loss 0.269606, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:37.318099: step 6556, loss 0.195814, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:37.401212: step 6557, loss 0.176988, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:37.482871: step 6558, loss 0.136548, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:37.562937: step 6559, loss 0.126587, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:37.646948: step 6560, loss 0.228967, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:37.854595: step 6560, loss 0.212857, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6560

2017-10-11T11:23:38.348594: step 6561, loss 0.27452, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:38.431754: step 6562, loss 0.307149, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:38.514842: step 6563, loss 0.387036, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:38.596046: step 6564, loss 0.182799, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:38.678764: step 6565, loss 0.358527, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:38.749467: step 6566, loss 0.281114, acc 0.901961, learning_rate 0.0001
2017-10-11T11:23:38.835853: step 6567, loss 0.246379, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:38.918778: step 6568, loss 0.194438, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:39.001030: step 6569, loss 0.19491, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:39.084077: step 6570, loss 0.221419, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:39.166017: step 6571, loss 0.185657, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:39.250678: step 6572, loss 0.26616, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:39.336524: step 6573, loss 0.273719, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:39.419858: step 6574, loss 0.157942, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:39.502901: step 6575, loss 0.149842, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:39.584010: step 6576, loss 0.21459, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:39.668149: step 6577, loss 0.210919, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:39.748718: step 6578, loss 0.161367, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:39.831315: step 6579, loss 0.193575, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:39.915598: step 6580, loss 0.216414, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:39.999608: step 6581, loss 0.135013, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:40.086669: step 6582, loss 0.273099, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:40.169226: step 6583, loss 0.190113, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:40.255615: step 6584, loss 0.174535, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:40.339169: step 6585, loss 0.167827, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:40.423613: step 6586, loss 0.12864, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:40.507345: step 6587, loss 0.148477, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:40.588577: step 6588, loss 0.223067, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:40.669986: step 6589, loss 0.194398, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:40.750673: step 6590, loss 0.254297, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:40.834658: step 6591, loss 0.28696, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:40.918765: step 6592, loss 0.20304, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:41.000395: step 6593, loss 0.120913, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:41.083623: step 6594, loss 0.245592, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:41.167780: step 6595, loss 0.480965, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:41.252380: step 6596, loss 0.16576, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:41.333434: step 6597, loss 0.174323, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:41.414579: step 6598, loss 0.335318, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:41.496665: step 6599, loss 0.333169, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:41.578787: step 6600, loss 0.209921, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:41.783208: step 6600, loss 0.212139, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6600

2017-10-11T11:23:42.280338: step 6601, loss 0.167382, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:42.364219: step 6602, loss 0.175556, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:42.444087: step 6603, loss 0.158109, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:42.525879: step 6604, loss 0.179365, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:42.606610: step 6605, loss 0.161576, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:42.691035: step 6606, loss 0.251788, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:42.772920: step 6607, loss 0.228257, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:42.856303: step 6608, loss 0.245966, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:42.939613: step 6609, loss 0.18083, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:43.019250: step 6610, loss 0.264023, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:43.103650: step 6611, loss 0.201784, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:43.188463: step 6612, loss 0.169531, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:43.272594: step 6613, loss 0.25798, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:43.353898: step 6614, loss 0.211676, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:43.434125: step 6615, loss 0.44768, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:43.516657: step 6616, loss 0.24057, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:43.597213: step 6617, loss 0.334095, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:43.679524: step 6618, loss 0.130261, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:43.761103: step 6619, loss 0.164247, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:43.844780: step 6620, loss 0.179864, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:43.926638: step 6621, loss 0.230207, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:44.006989: step 6622, loss 0.179341, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:44.089997: step 6623, loss 0.200569, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:44.171893: step 6624, loss 0.173373, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:44.254318: step 6625, loss 0.225147, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:44.338211: step 6626, loss 0.162562, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:44.418363: step 6627, loss 0.280455, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:44.499962: step 6628, loss 0.25145, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:44.579516: step 6629, loss 0.125491, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:44.660602: step 6630, loss 0.299616, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:44.740547: step 6631, loss 0.262755, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:44.822596: step 6632, loss 0.174513, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:44.905655: step 6633, loss 0.208573, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:44.987623: step 6634, loss 0.289527, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:45.070018: step 6635, loss 0.268175, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:45.151838: step 6636, loss 0.298917, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:45.235961: step 6637, loss 0.177836, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:45.321523: step 6638, loss 0.191012, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:45.403454: step 6639, loss 0.0598593, acc 1, learning_rate 0.0001
2017-10-11T11:23:45.485430: step 6640, loss 0.173861, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:45.694194: step 6640, loss 0.211588, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6640

2017-10-11T11:23:46.197458: step 6641, loss 0.15291, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:46.282049: step 6642, loss 0.324835, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:46.366632: step 6643, loss 0.285174, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:46.448303: step 6644, loss 0.199561, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:46.531124: step 6645, loss 0.263266, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:46.613693: step 6646, loss 0.0796873, acc 1, learning_rate 0.0001
2017-10-11T11:23:46.696693: step 6647, loss 0.301131, acc 0.859375, learning_rate 0.0001
2017-10-11T11:23:46.781550: step 6648, loss 0.13334, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:46.865064: step 6649, loss 0.285189, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:46.947951: step 6650, loss 0.241573, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:47.030540: step 6651, loss 0.193098, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:47.113088: step 6652, loss 0.144066, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:47.200305: step 6653, loss 0.236762, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:47.284147: step 6654, loss 0.216269, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:47.365563: step 6655, loss 0.192991, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:47.449971: step 6656, loss 0.247426, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:47.532018: step 6657, loss 0.109091, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:47.613985: step 6658, loss 0.217583, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:47.698991: step 6659, loss 0.183177, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:47.779945: step 6660, loss 0.201074, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:47.865601: step 6661, loss 0.252633, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:47.950295: step 6662, loss 0.0980096, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:48.028566: step 6663, loss 0.275745, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:48.100281: step 6664, loss 0.340804, acc 0.901961, learning_rate 0.0001
2017-10-11T11:23:48.185721: step 6665, loss 0.360813, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:48.269872: step 6666, loss 0.195995, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:48.351115: step 6667, loss 0.178993, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:48.432282: step 6668, loss 0.344423, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:48.517019: step 6669, loss 0.0946331, acc 1, learning_rate 0.0001
2017-10-11T11:23:48.599551: step 6670, loss 0.214416, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:48.681164: step 6671, loss 0.133036, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:48.762683: step 6672, loss 0.211164, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:48.847935: step 6673, loss 0.187803, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:48.930638: step 6674, loss 0.294477, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:49.014303: step 6675, loss 0.211094, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:49.093920: step 6676, loss 0.124464, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:49.176968: step 6677, loss 0.235005, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:49.260509: step 6678, loss 0.234734, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:49.340649: step 6679, loss 0.303951, acc 0.828125, learning_rate 0.0001
2017-10-11T11:23:49.423383: step 6680, loss 0.252305, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:49.629179: step 6680, loss 0.212125, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6680

2017-10-11T11:23:50.139807: step 6681, loss 0.199385, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:50.222395: step 6682, loss 0.311455, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:50.302021: step 6683, loss 0.249317, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:50.383131: step 6684, loss 0.468694, acc 0.78125, learning_rate 0.0001
2017-10-11T11:23:50.466493: step 6685, loss 0.221353, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:50.548663: step 6686, loss 0.38708, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:50.630735: step 6687, loss 0.240327, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:50.710874: step 6688, loss 0.238628, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:50.792994: step 6689, loss 0.1997, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:50.877225: step 6690, loss 0.136813, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:50.961442: step 6691, loss 0.230446, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:51.042994: step 6692, loss 0.24427, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:51.126197: step 6693, loss 0.128683, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:51.207972: step 6694, loss 0.162718, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:51.290926: step 6695, loss 0.216823, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:51.374553: step 6696, loss 0.259511, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:51.457187: step 6697, loss 0.19111, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:51.540623: step 6698, loss 0.240917, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:51.625989: step 6699, loss 0.209612, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:51.708807: step 6700, loss 0.239234, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:51.792277: step 6701, loss 0.154739, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:51.876112: step 6702, loss 0.174209, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:51.957237: step 6703, loss 0.228784, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:52.042591: step 6704, loss 0.20543, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:52.123591: step 6705, loss 0.155313, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:52.204742: step 6706, loss 0.168341, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:52.288214: step 6707, loss 0.124209, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:52.375594: step 6708, loss 0.237048, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:52.458474: step 6709, loss 0.287972, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:52.540672: step 6710, loss 0.095814, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:52.626886: step 6711, loss 0.16301, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:52.709099: step 6712, loss 0.138735, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:52.792519: step 6713, loss 0.175581, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:52.875221: step 6714, loss 0.263102, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:52.958602: step 6715, loss 0.177696, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:53.041881: step 6716, loss 0.202056, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:53.127126: step 6717, loss 0.388985, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:53.212948: step 6718, loss 0.186254, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:53.294519: step 6719, loss 0.192324, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:53.379532: step 6720, loss 0.247397, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:53.586216: step 6720, loss 0.212492, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6720

2017-10-11T11:23:54.020206: step 6721, loss 0.0986459, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:54.104751: step 6722, loss 0.199151, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:54.187602: step 6723, loss 0.233951, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:54.268186: step 6724, loss 0.415361, acc 0.84375, learning_rate 0.0001
2017-10-11T11:23:54.349676: step 6725, loss 0.0810488, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:54.431493: step 6726, loss 0.105616, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:54.515977: step 6727, loss 0.164859, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:54.598912: step 6728, loss 0.152183, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:54.681814: step 6729, loss 0.204462, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:54.764529: step 6730, loss 0.259483, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:54.845027: step 6731, loss 0.243794, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:54.929769: step 6732, loss 0.18239, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:55.013633: step 6733, loss 0.275862, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:55.098978: step 6734, loss 0.234944, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:55.182671: step 6735, loss 0.209894, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:55.265621: step 6736, loss 0.399822, acc 0.828125, learning_rate 0.0001
2017-10-11T11:23:55.347271: step 6737, loss 0.247506, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:55.428955: step 6738, loss 0.181216, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:55.511552: step 6739, loss 0.207664, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:55.595352: step 6740, loss 0.201947, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:55.675594: step 6741, loss 0.113125, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:55.757597: step 6742, loss 0.210735, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:55.840295: step 6743, loss 0.201489, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:55.923048: step 6744, loss 0.245691, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:56.004791: step 6745, loss 0.268329, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:56.089413: step 6746, loss 0.21203, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:56.170609: step 6747, loss 0.345488, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:56.253761: step 6748, loss 0.204209, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:56.334881: step 6749, loss 0.224019, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:56.416388: step 6750, loss 0.241106, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:56.499918: step 6751, loss 0.130315, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:56.582572: step 6752, loss 0.210143, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:56.664266: step 6753, loss 0.157857, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:56.746512: step 6754, loss 0.195581, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:56.827372: step 6755, loss 0.110593, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:56.910265: step 6756, loss 0.226068, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:56.993431: step 6757, loss 0.266184, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:57.077149: step 6758, loss 0.170645, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:57.160014: step 6759, loss 0.274863, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:57.238333: step 6760, loss 0.256361, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:23:57.444005: step 6760, loss 0.211416, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6760

2017-10-11T11:23:57.935900: step 6761, loss 0.247337, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:58.006334: step 6762, loss 0.307197, acc 0.901961, learning_rate 0.0001
2017-10-11T11:23:58.091505: step 6763, loss 0.181602, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:58.175589: step 6764, loss 0.368153, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:58.260788: step 6765, loss 0.179379, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:58.341192: step 6766, loss 0.237029, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:58.423893: step 6767, loss 0.258631, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:58.507817: step 6768, loss 0.26713, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:58.593423: step 6769, loss 0.318196, acc 0.875, learning_rate 0.0001
2017-10-11T11:23:58.676609: step 6770, loss 0.225361, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:58.759083: step 6771, loss 0.118023, acc 0.984375, learning_rate 0.0001
2017-10-11T11:23:58.840868: step 6772, loss 0.263435, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:58.923936: step 6773, loss 0.227047, acc 0.90625, learning_rate 0.0001
2017-10-11T11:23:59.005966: step 6774, loss 0.284858, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:59.087621: step 6775, loss 0.202488, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:59.169787: step 6776, loss 0.223658, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:59.249558: step 6777, loss 0.230096, acc 0.921875, learning_rate 0.0001
2017-10-11T11:23:59.330417: step 6778, loss 0.159893, acc 0.953125, learning_rate 0.0001
2017-10-11T11:23:59.412262: step 6779, loss 0.280504, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:59.492580: step 6780, loss 0.311294, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:59.575589: step 6781, loss 0.182438, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:59.658834: step 6782, loss 0.192816, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:59.740968: step 6783, loss 0.248123, acc 0.890625, learning_rate 0.0001
2017-10-11T11:23:59.825213: step 6784, loss 0.167441, acc 0.9375, learning_rate 0.0001
2017-10-11T11:23:59.907083: step 6785, loss 0.096266, acc 0.96875, learning_rate 0.0001
2017-10-11T11:23:59.990260: step 6786, loss 0.154989, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:00.072553: step 6787, loss 0.0841581, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:00.156604: step 6788, loss 0.15233, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:00.240185: step 6789, loss 0.174398, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:00.324117: step 6790, loss 0.222315, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:00.406276: step 6791, loss 0.3594, acc 0.859375, learning_rate 0.0001
2017-10-11T11:24:00.487174: step 6792, loss 0.341213, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:00.569569: step 6793, loss 0.254213, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:00.653881: step 6794, loss 0.320866, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:00.735629: step 6795, loss 0.317645, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:00.817586: step 6796, loss 0.194685, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:00.901116: step 6797, loss 0.257627, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:00.984430: step 6798, loss 0.163108, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:01.069191: step 6799, loss 0.155478, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:01.153785: step 6800, loss 0.121919, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:01.360569: step 6800, loss 0.211286, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6800

2017-10-11T11:24:01.863007: step 6801, loss 0.138992, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:01.947264: step 6802, loss 0.186582, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:02.029156: step 6803, loss 0.235413, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:02.111847: step 6804, loss 0.219144, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:02.195265: step 6805, loss 0.168263, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:02.278334: step 6806, loss 0.190061, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:02.362394: step 6807, loss 0.158195, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:02.444049: step 6808, loss 0.260914, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:02.525087: step 6809, loss 0.223807, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:02.609760: step 6810, loss 0.247518, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:02.691126: step 6811, loss 0.170155, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:02.776833: step 6812, loss 0.241685, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:02.860461: step 6813, loss 0.173734, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:02.943426: step 6814, loss 0.177852, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:03.026600: step 6815, loss 0.124114, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:03.107270: step 6816, loss 0.289107, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:03.188760: step 6817, loss 0.417143, acc 0.859375, learning_rate 0.0001
2017-10-11T11:24:03.272448: step 6818, loss 0.255591, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:03.354250: step 6819, loss 0.213193, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:03.437277: step 6820, loss 0.0650587, acc 1, learning_rate 0.0001
2017-10-11T11:24:03.517689: step 6821, loss 0.247904, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:03.601912: step 6822, loss 0.207208, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:03.683365: step 6823, loss 0.302705, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:03.765756: step 6824, loss 0.248281, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:03.849849: step 6825, loss 0.148848, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:03.935482: step 6826, loss 0.263287, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:04.021158: step 6827, loss 0.367366, acc 0.859375, learning_rate 0.0001
2017-10-11T11:24:04.103648: step 6828, loss 0.0937063, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:04.185850: step 6829, loss 0.177557, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:04.267147: step 6830, loss 0.119546, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:04.350640: step 6831, loss 0.153165, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:04.432563: step 6832, loss 0.308505, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:04.516654: step 6833, loss 0.308596, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:04.596005: step 6834, loss 0.164558, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:04.677860: step 6835, loss 0.237381, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:04.760495: step 6836, loss 0.314191, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:04.843570: step 6837, loss 0.233695, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:04.927720: step 6838, loss 0.240141, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:05.008213: step 6839, loss 0.217549, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:05.088795: step 6840, loss 0.227319, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:05.304894: step 6840, loss 0.210418, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6840

2017-10-11T11:24:05.801575: step 6841, loss 0.185314, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:05.886517: step 6842, loss 0.156521, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:05.968592: step 6843, loss 0.188268, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:06.053098: step 6844, loss 0.152604, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:06.135301: step 6845, loss 0.367594, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:06.218949: step 6846, loss 0.319308, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:06.298681: step 6847, loss 0.22094, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:06.386683: step 6848, loss 0.199267, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:06.468241: step 6849, loss 0.241256, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:06.550055: step 6850, loss 0.325112, acc 0.859375, learning_rate 0.0001
2017-10-11T11:24:06.630128: step 6851, loss 0.408407, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:06.711058: step 6852, loss 0.104613, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:06.793570: step 6853, loss 0.183583, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:06.877029: step 6854, loss 0.302404, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:06.958153: step 6855, loss 0.232986, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:07.039668: step 6856, loss 0.344519, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:07.125634: step 6857, loss 0.222351, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:07.207628: step 6858, loss 0.142854, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:07.288184: step 6859, loss 0.10499, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:07.358610: step 6860, loss 0.341433, acc 0.862745, learning_rate 0.0001
2017-10-11T11:24:07.443370: step 6861, loss 0.173237, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:07.525814: step 6862, loss 0.255573, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:07.610948: step 6863, loss 0.163246, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:07.694973: step 6864, loss 0.358425, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:07.777254: step 6865, loss 0.170653, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:07.860968: step 6866, loss 0.156787, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:07.944411: step 6867, loss 0.303241, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:08.025252: step 6868, loss 0.163862, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:08.106267: step 6869, loss 0.292422, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:08.189645: step 6870, loss 0.243037, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:08.272991: step 6871, loss 0.198046, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:08.357099: step 6872, loss 0.243107, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:08.445784: step 6873, loss 0.255593, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:08.528359: step 6874, loss 0.296423, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:08.612941: step 6875, loss 0.283774, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:08.693684: step 6876, loss 0.176531, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:08.774804: step 6877, loss 0.21838, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:08.859140: step 6878, loss 0.304723, acc 0.859375, learning_rate 0.0001
2017-10-11T11:24:08.941019: step 6879, loss 0.171121, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:09.023353: step 6880, loss 0.218795, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:09.231033: step 6880, loss 0.210902, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6880

2017-10-11T11:24:09.726513: step 6881, loss 0.186435, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:09.810047: step 6882, loss 0.346724, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:09.894217: step 6883, loss 0.17809, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:09.977532: step 6884, loss 0.396156, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:10.059970: step 6885, loss 0.228544, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:10.144391: step 6886, loss 0.346651, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:10.226875: step 6887, loss 0.289678, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:10.305752: step 6888, loss 0.193364, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:10.390847: step 6889, loss 0.244244, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:10.471991: step 6890, loss 0.261349, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:10.552615: step 6891, loss 0.195988, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:10.634432: step 6892, loss 0.171087, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:10.719236: step 6893, loss 0.178693, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:10.802699: step 6894, loss 0.283072, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:10.886717: step 6895, loss 0.202196, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:10.971378: step 6896, loss 0.0842137, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:11.053577: step 6897, loss 0.206357, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:11.136610: step 6898, loss 0.196732, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:11.220432: step 6899, loss 0.119954, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:11.302291: step 6900, loss 0.207148, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:11.384290: step 6901, loss 0.149934, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:11.465918: step 6902, loss 0.216508, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:11.549378: step 6903, loss 0.233662, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:11.631803: step 6904, loss 0.198973, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:11.714213: step 6905, loss 0.281491, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:11.795980: step 6906, loss 0.23477, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:11.880210: step 6907, loss 0.179582, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:11.959455: step 6908, loss 0.233015, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:12.040704: step 6909, loss 0.229088, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:12.124692: step 6910, loss 0.230578, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:12.207581: step 6911, loss 0.125618, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:12.293230: step 6912, loss 0.288849, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:12.378039: step 6913, loss 0.196075, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:12.462940: step 6914, loss 0.108139, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:12.546627: step 6915, loss 0.209453, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:12.631728: step 6916, loss 0.200553, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:12.711632: step 6917, loss 0.220871, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:12.796095: step 6918, loss 0.181323, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:12.876016: step 6919, loss 0.185085, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:12.958171: step 6920, loss 0.242244, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:13.175760: step 6920, loss 0.210938, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6920

2017-10-11T11:24:13.675166: step 6921, loss 0.223108, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:13.757847: step 6922, loss 0.264099, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:13.840807: step 6923, loss 0.174488, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:13.923362: step 6924, loss 0.232172, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:14.005576: step 6925, loss 0.195205, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:14.088923: step 6926, loss 0.230609, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:14.170203: step 6927, loss 0.188231, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:14.253150: step 6928, loss 0.254748, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:14.338148: step 6929, loss 0.326675, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:14.423127: step 6930, loss 0.250597, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:14.506980: step 6931, loss 0.241194, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:14.588514: step 6932, loss 0.230802, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:14.672183: step 6933, loss 0.0930604, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:14.754640: step 6934, loss 0.301904, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:14.839085: step 6935, loss 0.255254, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:14.924085: step 6936, loss 0.269798, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:15.009981: step 6937, loss 0.218117, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:15.091030: step 6938, loss 0.185208, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:15.174056: step 6939, loss 0.203443, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:15.259260: step 6940, loss 0.274378, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:15.343764: step 6941, loss 0.151303, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:15.425717: step 6942, loss 0.224449, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:15.509662: step 6943, loss 0.244326, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:15.593031: step 6944, loss 0.23444, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:15.677341: step 6945, loss 0.243591, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:15.763588: step 6946, loss 0.170713, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:15.848666: step 6947, loss 0.243855, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:15.931116: step 6948, loss 0.0884027, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:16.014122: step 6949, loss 0.380032, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:16.096351: step 6950, loss 0.293702, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:16.181187: step 6951, loss 0.288088, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:16.266217: step 6952, loss 0.178153, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:16.348992: step 6953, loss 0.204679, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:16.432464: step 6954, loss 0.254781, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:16.514567: step 6955, loss 0.182634, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:16.596344: step 6956, loss 0.110191, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:16.679758: step 6957, loss 0.181967, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:16.747492: step 6958, loss 0.234991, acc 0.941176, learning_rate 0.0001
2017-10-11T11:24:16.830183: step 6959, loss 0.179372, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:16.915507: step 6960, loss 0.3257, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:17.132722: step 6960, loss 0.210374, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-6960

2017-10-11T11:24:17.629782: step 6961, loss 0.175277, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:17.713336: step 6962, loss 0.106729, acc 1, learning_rate 0.0001
2017-10-11T11:24:17.795241: step 6963, loss 0.190976, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:17.880466: step 6964, loss 0.377525, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:17.961835: step 6965, loss 0.189826, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:18.044849: step 6966, loss 0.277675, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:18.129490: step 6967, loss 0.348466, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:18.210992: step 6968, loss 0.11775, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:18.291710: step 6969, loss 0.272578, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:18.373872: step 6970, loss 0.294077, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:18.455969: step 6971, loss 0.242139, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:18.538862: step 6972, loss 0.308107, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:18.622652: step 6973, loss 0.269218, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:18.703942: step 6974, loss 0.246377, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:18.787511: step 6975, loss 0.139958, acc 1, learning_rate 0.0001
2017-10-11T11:24:18.869146: step 6976, loss 0.117979, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:18.950380: step 6977, loss 0.253584, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:19.030233: step 6978, loss 0.0745262, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:19.117269: step 6979, loss 0.343135, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:19.198247: step 6980, loss 0.241218, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:19.278555: step 6981, loss 0.14588, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:19.362719: step 6982, loss 0.167583, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:19.443919: step 6983, loss 0.192506, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:19.530343: step 6984, loss 0.194969, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:19.613684: step 6985, loss 0.161703, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:19.698398: step 6986, loss 0.189671, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:19.782545: step 6987, loss 0.164214, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:19.867209: step 6988, loss 0.22491, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:19.948419: step 6989, loss 0.262684, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:20.032035: step 6990, loss 0.226694, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:20.115289: step 6991, loss 0.151576, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:20.194384: step 6992, loss 0.143039, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:20.277700: step 6993, loss 0.118565, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:20.361365: step 6994, loss 0.189001, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:20.443035: step 6995, loss 0.0938897, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:20.522442: step 6996, loss 0.168109, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:20.603758: step 6997, loss 0.307189, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:20.686653: step 6998, loss 0.202153, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:20.766607: step 6999, loss 0.253321, acc 0.859375, learning_rate 0.0001
2017-10-11T11:24:20.847685: step 7000, loss 0.187007, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:21.058672: step 7000, loss 0.209957, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7000

2017-10-11T11:24:21.564157: step 7001, loss 0.188703, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:21.649694: step 7002, loss 0.141303, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:21.730771: step 7003, loss 0.212472, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:21.815072: step 7004, loss 0.306841, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:21.899609: step 7005, loss 0.11642, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:21.986428: step 7006, loss 0.200856, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:22.069325: step 7007, loss 0.182492, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:22.151053: step 7008, loss 0.174673, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:22.234105: step 7009, loss 0.286259, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:22.316560: step 7010, loss 0.261094, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:22.400254: step 7011, loss 0.219232, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:22.482161: step 7012, loss 0.28859, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:22.565084: step 7013, loss 0.269671, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:22.651322: step 7014, loss 0.29303, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:22.732802: step 7015, loss 0.135339, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:22.814639: step 7016, loss 0.145613, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:22.899270: step 7017, loss 0.156466, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:22.979646: step 7018, loss 0.198807, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:23.060373: step 7019, loss 0.174688, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:23.141632: step 7020, loss 0.282728, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:23.222980: step 7021, loss 0.171791, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:23.304348: step 7022, loss 0.239377, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:23.386054: step 7023, loss 0.249075, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:23.470600: step 7024, loss 0.243395, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:23.549997: step 7025, loss 0.282904, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:23.634043: step 7026, loss 0.185514, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:23.716028: step 7027, loss 0.328354, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:23.796597: step 7028, loss 0.298554, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:23.879720: step 7029, loss 0.31547, acc 0.84375, learning_rate 0.0001
2017-10-11T11:24:23.962383: step 7030, loss 0.233663, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:24.046100: step 7031, loss 0.153677, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:24.133904: step 7032, loss 0.130044, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:24.217878: step 7033, loss 0.233711, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:24.299099: step 7034, loss 0.303604, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:24.379341: step 7035, loss 0.114881, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:24.462029: step 7036, loss 0.166561, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:24.546434: step 7037, loss 0.180017, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:24.629410: step 7038, loss 0.278357, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:24.711019: step 7039, loss 0.217228, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:24.794138: step 7040, loss 0.116087, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:25.012275: step 7040, loss 0.209337, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7040

2017-10-11T11:24:25.514557: step 7041, loss 0.172948, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:25.595482: step 7042, loss 0.181734, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:25.678046: step 7043, loss 0.166372, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:25.765955: step 7044, loss 0.18512, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:25.850934: step 7045, loss 0.304196, acc 0.828125, learning_rate 0.0001
2017-10-11T11:24:25.931118: step 7046, loss 0.164902, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:26.016462: step 7047, loss 0.339563, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:26.101577: step 7048, loss 0.161149, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:26.180455: step 7049, loss 0.234325, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:26.262420: step 7050, loss 0.139875, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:26.343994: step 7051, loss 0.301297, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:26.425507: step 7052, loss 0.115516, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:26.508812: step 7053, loss 0.187311, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:26.591277: step 7054, loss 0.227864, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:26.674133: step 7055, loss 0.41473, acc 0.84375, learning_rate 0.0001
2017-10-11T11:24:26.748026: step 7056, loss 0.189494, acc 0.921569, learning_rate 0.0001
2017-10-11T11:24:26.834667: step 7057, loss 0.0807984, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:26.916349: step 7058, loss 0.314184, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:26.998965: step 7059, loss 0.240917, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:27.082640: step 7060, loss 0.22483, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:27.164536: step 7061, loss 0.160421, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:27.247528: step 7062, loss 0.208792, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:27.330582: step 7063, loss 0.107228, acc 1, learning_rate 0.0001
2017-10-11T11:24:27.409068: step 7064, loss 0.249023, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:27.489818: step 7065, loss 0.175103, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:27.574351: step 7066, loss 0.226276, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:27.655496: step 7067, loss 0.268282, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:27.736663: step 7068, loss 0.235445, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:27.819945: step 7069, loss 0.200111, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:27.901095: step 7070, loss 0.166795, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:27.982841: step 7071, loss 0.278603, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:28.064738: step 7072, loss 0.0879032, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:28.148711: step 7073, loss 0.219264, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:28.229738: step 7074, loss 0.210512, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:28.312881: step 7075, loss 0.167946, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:28.393401: step 7076, loss 0.192902, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:28.475905: step 7077, loss 0.165517, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:28.560819: step 7078, loss 0.282107, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:28.644232: step 7079, loss 0.226748, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:28.726650: step 7080, loss 0.390795, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:28.938996: step 7080, loss 0.208926, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7080

2017-10-11T11:24:29.370379: step 7081, loss 0.285269, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:29.456013: step 7082, loss 0.236071, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:29.537612: step 7083, loss 0.237305, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:29.619937: step 7084, loss 0.140357, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:29.703234: step 7085, loss 0.159068, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:29.784808: step 7086, loss 0.215431, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:29.869822: step 7087, loss 0.231737, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:29.953229: step 7088, loss 0.162997, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:30.033931: step 7089, loss 0.285081, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:30.114381: step 7090, loss 0.167862, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:30.195922: step 7091, loss 0.227314, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:30.275583: step 7092, loss 0.266984, acc 0.859375, learning_rate 0.0001
2017-10-11T11:24:30.358264: step 7093, loss 0.195188, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:30.439422: step 7094, loss 0.18902, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:30.518645: step 7095, loss 0.182861, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:30.601647: step 7096, loss 0.216168, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:30.683384: step 7097, loss 0.1523, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:30.767926: step 7098, loss 0.249822, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:30.852171: step 7099, loss 0.153396, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:30.933705: step 7100, loss 0.155313, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:31.014954: step 7101, loss 0.231329, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:31.098150: step 7102, loss 0.175875, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:31.181306: step 7103, loss 0.230135, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:31.265339: step 7104, loss 0.378207, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:31.348362: step 7105, loss 0.10594, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:31.431077: step 7106, loss 0.314512, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:31.512205: step 7107, loss 0.274443, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:31.594273: step 7108, loss 0.137372, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:31.677087: step 7109, loss 0.272915, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:31.757923: step 7110, loss 0.198682, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:31.840231: step 7111, loss 0.229587, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:31.922452: step 7112, loss 0.231894, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:32.004265: step 7113, loss 0.212368, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:32.086666: step 7114, loss 0.26966, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:32.169454: step 7115, loss 0.273196, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:32.252881: step 7116, loss 0.20892, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:32.335235: step 7117, loss 0.123629, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:32.416486: step 7118, loss 0.175721, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:32.501983: step 7119, loss 0.174319, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:32.587018: step 7120, loss 0.204692, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:32.804515: step 7120, loss 0.208362, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7120

2017-10-11T11:24:33.301120: step 7121, loss 0.199816, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:33.385790: step 7122, loss 0.221023, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:33.467497: step 7123, loss 0.294142, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:33.547357: step 7124, loss 0.185703, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:33.629764: step 7125, loss 0.237984, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:33.711804: step 7126, loss 0.211674, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:33.795101: step 7127, loss 0.136925, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:33.877877: step 7128, loss 0.174896, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:33.961541: step 7129, loss 0.146165, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:34.044338: step 7130, loss 0.162946, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:34.124257: step 7131, loss 0.178439, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:34.209968: step 7132, loss 0.333119, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:34.295221: step 7133, loss 0.161511, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:34.375864: step 7134, loss 0.222304, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:34.458173: step 7135, loss 0.213292, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:34.537187: step 7136, loss 0.244538, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:34.620749: step 7137, loss 0.247147, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:34.703807: step 7138, loss 0.244384, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:34.782910: step 7139, loss 0.113433, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:34.865931: step 7140, loss 0.255921, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:34.948219: step 7141, loss 0.218152, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:35.032213: step 7142, loss 0.223596, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:35.115986: step 7143, loss 0.227083, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:35.200623: step 7144, loss 0.21357, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:35.283451: step 7145, loss 0.127218, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:35.367228: step 7146, loss 0.255795, acc 0.859375, learning_rate 0.0001
2017-10-11T11:24:35.450683: step 7147, loss 0.230972, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:35.532566: step 7148, loss 0.274189, acc 0.859375, learning_rate 0.0001
2017-10-11T11:24:35.614849: step 7149, loss 0.213943, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:35.697612: step 7150, loss 0.142887, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:35.778381: step 7151, loss 0.229664, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:35.861604: step 7152, loss 0.301744, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:35.942691: step 7153, loss 0.207593, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:36.014346: step 7154, loss 0.17081, acc 0.921569, learning_rate 0.0001
2017-10-11T11:24:36.097881: step 7155, loss 0.232036, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:36.180098: step 7156, loss 0.124691, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:36.264594: step 7157, loss 0.21614, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:36.349634: step 7158, loss 0.100413, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:36.432742: step 7159, loss 0.144983, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:36.516268: step 7160, loss 0.324933, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:36.729605: step 7160, loss 0.208325, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7160

2017-10-11T11:24:37.225160: step 7161, loss 0.185406, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:37.305624: step 7162, loss 0.250371, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:37.390054: step 7163, loss 0.192828, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:37.472959: step 7164, loss 0.125272, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:37.555530: step 7165, loss 0.161723, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:37.638485: step 7166, loss 0.270051, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:37.721288: step 7167, loss 0.261783, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:37.807068: step 7168, loss 0.219836, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:37.894181: step 7169, loss 0.342324, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:37.977278: step 7170, loss 0.178294, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:38.060158: step 7171, loss 0.396971, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:38.144291: step 7172, loss 0.231143, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:38.228131: step 7173, loss 0.147961, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:38.314984: step 7174, loss 0.180294, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:38.396971: step 7175, loss 0.126835, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:38.481948: step 7176, loss 0.238759, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:38.564983: step 7177, loss 0.253401, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:38.646346: step 7178, loss 0.193881, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:38.729527: step 7179, loss 0.235754, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:38.812633: step 7180, loss 0.268913, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:38.902457: step 7181, loss 0.262106, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:38.985426: step 7182, loss 0.197311, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:39.068484: step 7183, loss 0.154282, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:39.152604: step 7184, loss 0.232713, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:39.235408: step 7185, loss 0.329284, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:39.316069: step 7186, loss 0.271133, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:39.400059: step 7187, loss 0.215198, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:39.483979: step 7188, loss 0.171832, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:39.569637: step 7189, loss 0.160234, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:39.649103: step 7190, loss 0.200034, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:39.732661: step 7191, loss 0.127058, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:39.815468: step 7192, loss 0.218802, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:39.900905: step 7193, loss 0.273205, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:39.982147: step 7194, loss 0.133681, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:40.062719: step 7195, loss 0.169848, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:40.143986: step 7196, loss 0.18701, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:40.226427: step 7197, loss 0.18499, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:40.307337: step 7198, loss 0.194311, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:40.389795: step 7199, loss 0.155997, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:40.473113: step 7200, loss 0.211263, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:40.685538: step 7200, loss 0.208143, acc 0.916547

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7200

2017-10-11T11:24:41.184930: step 7201, loss 0.240695, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:41.266763: step 7202, loss 0.239902, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:41.353231: step 7203, loss 0.262163, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:41.434748: step 7204, loss 0.233855, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:41.516800: step 7205, loss 0.216025, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:41.598064: step 7206, loss 0.317196, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:41.682441: step 7207, loss 0.219548, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:41.764791: step 7208, loss 0.207941, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:41.847796: step 7209, loss 0.180661, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:41.929112: step 7210, loss 0.343731, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:42.010156: step 7211, loss 0.192349, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:42.092327: step 7212, loss 0.145994, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:42.177975: step 7213, loss 0.271926, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:42.257639: step 7214, loss 0.207707, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:42.339748: step 7215, loss 0.150466, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:42.419880: step 7216, loss 0.247393, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:42.502960: step 7217, loss 0.279777, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:42.583619: step 7218, loss 0.214698, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:42.664196: step 7219, loss 0.27287, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:42.745801: step 7220, loss 0.327605, acc 0.859375, learning_rate 0.0001
2017-10-11T11:24:42.827760: step 7221, loss 0.240432, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:42.909719: step 7222, loss 0.164011, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:42.992827: step 7223, loss 0.171737, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:43.077822: step 7224, loss 0.285184, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:43.160036: step 7225, loss 0.221233, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:43.242193: step 7226, loss 0.181964, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:43.325930: step 7227, loss 0.239615, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:43.411481: step 7228, loss 0.181888, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:43.494646: step 7229, loss 0.238546, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:43.576193: step 7230, loss 0.205372, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:43.657147: step 7231, loss 0.140569, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:43.738798: step 7232, loss 0.184194, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:43.820390: step 7233, loss 0.222225, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:43.904514: step 7234, loss 0.168956, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:43.986507: step 7235, loss 0.222935, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:44.070378: step 7236, loss 0.318132, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:44.151331: step 7237, loss 0.247518, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:44.231664: step 7238, loss 0.216751, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:44.316736: step 7239, loss 0.333181, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:44.397167: step 7240, loss 0.118692, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:44.611028: step 7240, loss 0.208433, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7240

2017-10-11T11:24:45.112767: step 7241, loss 0.0958687, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:45.196702: step 7242, loss 0.180068, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:45.276430: step 7243, loss 0.21736, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:45.358780: step 7244, loss 0.254142, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:45.443208: step 7245, loss 0.200585, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:45.525108: step 7246, loss 0.24198, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:45.610606: step 7247, loss 0.156905, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:45.691837: step 7248, loss 0.178064, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:45.772604: step 7249, loss 0.208801, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:45.855287: step 7250, loss 0.167645, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:45.936372: step 7251, loss 0.0922751, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:46.008414: step 7252, loss 0.174526, acc 0.941176, learning_rate 0.0001
2017-10-11T11:24:46.090513: step 7253, loss 0.21848, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:46.173745: step 7254, loss 0.341579, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:46.253886: step 7255, loss 0.175477, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:46.336965: step 7256, loss 0.161324, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:46.418066: step 7257, loss 0.201221, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:46.499559: step 7258, loss 0.111599, acc 1, learning_rate 0.0001
2017-10-11T11:24:46.581048: step 7259, loss 0.152315, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:46.658991: step 7260, loss 0.179436, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:46.742259: step 7261, loss 0.154778, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:46.824184: step 7262, loss 0.230504, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:46.906251: step 7263, loss 0.268599, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:46.989317: step 7264, loss 0.131572, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:47.072636: step 7265, loss 0.206207, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:47.154620: step 7266, loss 0.169639, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:47.237541: step 7267, loss 0.150507, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:47.322710: step 7268, loss 0.240173, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:47.404683: step 7269, loss 0.208635, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:47.487543: step 7270, loss 0.174434, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:47.568014: step 7271, loss 0.20726, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:47.652069: step 7272, loss 0.14597, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:47.735143: step 7273, loss 0.188653, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:47.818368: step 7274, loss 0.291213, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:47.902377: step 7275, loss 0.159876, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:47.984535: step 7276, loss 0.152622, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:48.067171: step 7277, loss 0.180974, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:48.152788: step 7278, loss 0.221678, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:48.236753: step 7279, loss 0.0760227, acc 1, learning_rate 0.0001
2017-10-11T11:24:48.320038: step 7280, loss 0.235441, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:48.536611: step 7280, loss 0.207426, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7280

2017-10-11T11:24:49.036049: step 7281, loss 0.166288, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:49.120497: step 7282, loss 0.273677, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:49.204844: step 7283, loss 0.242051, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:49.284278: step 7284, loss 0.186605, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:49.366729: step 7285, loss 0.142516, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:49.450192: step 7286, loss 0.39006, acc 0.84375, learning_rate 0.0001
2017-10-11T11:24:49.532767: step 7287, loss 0.161085, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:49.621996: step 7288, loss 0.243019, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:49.701608: step 7289, loss 0.20389, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:49.782240: step 7290, loss 0.339238, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:49.866048: step 7291, loss 0.143007, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:49.946351: step 7292, loss 0.184459, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:50.025237: step 7293, loss 0.276343, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:50.107202: step 7294, loss 0.0790404, acc 1, learning_rate 0.0001
2017-10-11T11:24:50.190936: step 7295, loss 0.250602, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:50.274356: step 7296, loss 0.104664, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:50.360172: step 7297, loss 0.150988, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:50.442669: step 7298, loss 0.168426, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:50.527740: step 7299, loss 0.240343, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:50.614105: step 7300, loss 0.264805, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:50.693908: step 7301, loss 0.244122, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:50.777632: step 7302, loss 0.18568, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:50.862679: step 7303, loss 0.152425, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:50.942514: step 7304, loss 0.122465, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:51.024667: step 7305, loss 0.275121, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:51.107972: step 7306, loss 0.201107, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:51.192813: step 7307, loss 0.12169, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:51.276940: step 7308, loss 0.227177, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:51.356854: step 7309, loss 0.172498, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:51.438735: step 7310, loss 0.18004, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:51.522756: step 7311, loss 0.304736, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:51.605661: step 7312, loss 0.12577, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:51.686883: step 7313, loss 0.193859, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:51.768290: step 7314, loss 0.157847, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:51.853068: step 7315, loss 0.224087, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:51.936931: step 7316, loss 0.121085, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:52.016123: step 7317, loss 0.321418, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:52.099778: step 7318, loss 0.128164, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:52.181128: step 7319, loss 0.155306, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:52.264534: step 7320, loss 0.126899, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:52.474747: step 7320, loss 0.207525, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7320

2017-10-11T11:24:52.975085: step 7321, loss 0.155412, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:53.057994: step 7322, loss 0.177228, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:53.139508: step 7323, loss 0.191397, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:53.218863: step 7324, loss 0.176951, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:53.300199: step 7325, loss 0.183771, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:53.382932: step 7326, loss 0.102826, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:53.470448: step 7327, loss 0.177056, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:53.557932: step 7328, loss 0.185947, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:53.642682: step 7329, loss 0.173994, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:53.726167: step 7330, loss 0.188386, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:53.808449: step 7331, loss 0.231943, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:53.891592: step 7332, loss 0.231, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:53.971245: step 7333, loss 0.163732, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:54.049578: step 7334, loss 0.169128, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:54.129819: step 7335, loss 0.19236, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:54.213672: step 7336, loss 0.259129, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:54.294603: step 7337, loss 0.175184, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:54.376131: step 7338, loss 0.262023, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:54.457407: step 7339, loss 0.312937, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:54.540580: step 7340, loss 0.192408, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:54.624525: step 7341, loss 0.220358, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:54.707100: step 7342, loss 0.372289, acc 0.84375, learning_rate 0.0001
2017-10-11T11:24:54.788236: step 7343, loss 0.174328, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:54.876769: step 7344, loss 0.171689, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:54.957508: step 7345, loss 0.152575, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:55.036787: step 7346, loss 0.185117, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:55.118773: step 7347, loss 0.476748, acc 0.859375, learning_rate 0.0001
2017-10-11T11:24:55.204003: step 7348, loss 0.157453, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:55.285794: step 7349, loss 0.0886962, acc 1, learning_rate 0.0001
2017-10-11T11:24:55.357558: step 7350, loss 0.153229, acc 0.960784, learning_rate 0.0001
2017-10-11T11:24:55.442145: step 7351, loss 0.167694, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:55.522393: step 7352, loss 0.34801, acc 0.84375, learning_rate 0.0001
2017-10-11T11:24:55.604141: step 7353, loss 0.109754, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:55.686285: step 7354, loss 0.188028, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:55.765795: step 7355, loss 0.173287, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:55.845927: step 7356, loss 0.197826, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:55.927556: step 7357, loss 0.282682, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:56.008411: step 7358, loss 0.197953, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:56.087864: step 7359, loss 0.307109, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:56.171734: step 7360, loss 0.205678, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:24:56.387903: step 7360, loss 0.207633, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7360

2017-10-11T11:24:56.885933: step 7361, loss 0.247814, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:56.968738: step 7362, loss 0.140767, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:57.050706: step 7363, loss 0.155182, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:57.131996: step 7364, loss 0.300921, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:57.214637: step 7365, loss 0.154642, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:57.298789: step 7366, loss 0.14552, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:57.381339: step 7367, loss 0.232605, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:57.463717: step 7368, loss 0.0801709, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:57.547606: step 7369, loss 0.0873617, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:57.630898: step 7370, loss 0.137065, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:57.713896: step 7371, loss 0.18205, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:57.795778: step 7372, loss 0.224313, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:57.876742: step 7373, loss 0.222296, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:57.959331: step 7374, loss 0.292302, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:58.042417: step 7375, loss 0.141008, acc 0.96875, learning_rate 0.0001
2017-10-11T11:24:58.127251: step 7376, loss 0.183511, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:58.207812: step 7377, loss 0.207365, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:58.287860: step 7378, loss 0.21247, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:58.374010: step 7379, loss 0.199551, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:58.455698: step 7380, loss 0.17817, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:58.537660: step 7381, loss 0.245781, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:58.619486: step 7382, loss 0.112412, acc 0.984375, learning_rate 0.0001
2017-10-11T11:24:58.702812: step 7383, loss 0.241176, acc 0.890625, learning_rate 0.0001
2017-10-11T11:24:58.785516: step 7384, loss 0.181379, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:58.867174: step 7385, loss 0.176803, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:58.947889: step 7386, loss 0.115472, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:59.032558: step 7387, loss 0.179272, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:59.117245: step 7388, loss 0.155182, acc 0.9375, learning_rate 0.0001
2017-10-11T11:24:59.199888: step 7389, loss 0.126768, acc 1, learning_rate 0.0001
2017-10-11T11:24:59.279905: step 7390, loss 0.28213, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:59.364169: step 7391, loss 0.227386, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:59.444493: step 7392, loss 0.427745, acc 0.8125, learning_rate 0.0001
2017-10-11T11:24:59.529806: step 7393, loss 0.341469, acc 0.875, learning_rate 0.0001
2017-10-11T11:24:59.611309: step 7394, loss 0.227251, acc 0.921875, learning_rate 0.0001
2017-10-11T11:24:59.693516: step 7395, loss 0.156351, acc 0.953125, learning_rate 0.0001
2017-10-11T11:24:59.776534: step 7396, loss 0.285167, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:59.866697: step 7397, loss 0.245928, acc 0.90625, learning_rate 0.0001
2017-10-11T11:24:59.946080: step 7398, loss 0.350248, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:00.027643: step 7399, loss 0.267399, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:00.108601: step 7400, loss 0.217059, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:25:00.329884: step 7400, loss 0.207345, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7400

2017-10-11T11:25:00.757133: step 7401, loss 0.175755, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:00.846422: step 7402, loss 0.211718, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:00.926835: step 7403, loss 0.222017, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:01.007715: step 7404, loss 0.194866, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:01.091451: step 7405, loss 0.16859, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:01.173380: step 7406, loss 0.154613, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:01.255517: step 7407, loss 0.178917, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:01.340850: step 7408, loss 0.17653, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:01.422472: step 7409, loss 0.119741, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:01.506424: step 7410, loss 0.15322, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:01.587628: step 7411, loss 0.242848, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:01.673975: step 7412, loss 0.307376, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:01.758344: step 7413, loss 0.164181, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:01.841683: step 7414, loss 0.214487, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:01.924777: step 7415, loss 0.133586, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:02.006368: step 7416, loss 0.221789, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:02.089152: step 7417, loss 0.175768, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:02.172086: step 7418, loss 0.164546, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:02.256165: step 7419, loss 0.171623, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:02.338908: step 7420, loss 0.0899841, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:02.420382: step 7421, loss 0.24706, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:02.503429: step 7422, loss 0.163632, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:02.588960: step 7423, loss 0.321938, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:02.673878: step 7424, loss 0.172338, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:02.756341: step 7425, loss 0.16744, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:02.841286: step 7426, loss 0.248555, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:02.922112: step 7427, loss 0.281701, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:03.003122: step 7428, loss 0.130278, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:03.087303: step 7429, loss 0.318502, acc 0.859375, learning_rate 0.0001
2017-10-11T11:25:03.170019: step 7430, loss 0.102581, acc 1, learning_rate 0.0001
2017-10-11T11:25:03.250928: step 7431, loss 0.166533, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:03.333512: step 7432, loss 0.156187, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:03.419047: step 7433, loss 0.187413, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:03.499875: step 7434, loss 0.299013, acc 0.859375, learning_rate 0.0001
2017-10-11T11:25:03.582863: step 7435, loss 0.200598, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:03.665291: step 7436, loss 0.339432, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:03.749017: step 7437, loss 0.256194, acc 0.859375, learning_rate 0.0001
2017-10-11T11:25:03.831089: step 7438, loss 0.201897, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:03.913771: step 7439, loss 0.227033, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:03.995674: step 7440, loss 0.210054, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:25:04.208500: step 7440, loss 0.207193, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7440

2017-10-11T11:25:04.705977: step 7441, loss 0.105849, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:04.788943: step 7442, loss 0.102207, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:04.870262: step 7443, loss 0.190415, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:04.954431: step 7444, loss 0.172005, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:05.040061: step 7445, loss 0.0877799, acc 1, learning_rate 0.0001
2017-10-11T11:25:05.119484: step 7446, loss 0.179326, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:05.201556: step 7447, loss 0.222825, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:05.272477: step 7448, loss 0.177915, acc 0.960784, learning_rate 0.0001
2017-10-11T11:25:05.361320: step 7449, loss 0.164361, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:05.440750: step 7450, loss 0.256555, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:05.522465: step 7451, loss 0.100796, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:05.605402: step 7452, loss 0.129153, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:05.686785: step 7453, loss 0.243193, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:05.769870: step 7454, loss 0.169418, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:05.853497: step 7455, loss 0.297066, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:05.938019: step 7456, loss 0.205167, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:06.020878: step 7457, loss 0.284884, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:06.102238: step 7458, loss 0.197284, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:06.186499: step 7459, loss 0.199766, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:06.268804: step 7460, loss 0.104906, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:06.352387: step 7461, loss 0.235728, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:06.436141: step 7462, loss 0.207649, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:06.519257: step 7463, loss 0.26751, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:06.600894: step 7464, loss 0.163917, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:06.682458: step 7465, loss 0.175688, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:06.764779: step 7466, loss 0.193185, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:06.844862: step 7467, loss 0.298214, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:06.928099: step 7468, loss 0.190682, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:07.013599: step 7469, loss 0.210203, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:07.098710: step 7470, loss 0.158946, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:07.184848: step 7471, loss 0.148882, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:07.267347: step 7472, loss 0.118085, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:07.348779: step 7473, loss 0.166305, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:07.433580: step 7474, loss 0.246587, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:07.515916: step 7475, loss 0.150602, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:07.597576: step 7476, loss 0.104166, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:07.679627: step 7477, loss 0.216445, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:07.763211: step 7478, loss 0.255689, acc 0.859375, learning_rate 0.0001
2017-10-11T11:25:07.843675: step 7479, loss 0.153986, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:07.928974: step 7480, loss 0.180623, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:25:08.142000: step 7480, loss 0.207083, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7480

2017-10-11T11:25:08.637374: step 7481, loss 0.183229, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:08.718687: step 7482, loss 0.235918, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:08.802726: step 7483, loss 0.135279, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:08.886170: step 7484, loss 0.165021, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:08.967919: step 7485, loss 0.0931745, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:09.053121: step 7486, loss 0.172708, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:09.135802: step 7487, loss 0.207092, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:09.218207: step 7488, loss 0.220734, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:09.301546: step 7489, loss 0.224632, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:09.385459: step 7490, loss 0.161553, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:09.468528: step 7491, loss 0.325454, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:09.553117: step 7492, loss 0.266267, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:09.635553: step 7493, loss 0.133073, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:09.719717: step 7494, loss 0.184438, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:09.802507: step 7495, loss 0.186591, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:09.885288: step 7496, loss 0.233438, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:09.968402: step 7497, loss 0.145515, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:10.051336: step 7498, loss 0.21502, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:10.132813: step 7499, loss 0.149942, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:10.217306: step 7500, loss 0.243171, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:10.298402: step 7501, loss 0.246799, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:10.379654: step 7502, loss 0.35296, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:10.463538: step 7503, loss 0.305594, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:10.545251: step 7504, loss 0.203337, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:10.627009: step 7505, loss 0.192938, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:10.709648: step 7506, loss 0.331527, acc 0.859375, learning_rate 0.0001
2017-10-11T11:25:10.789593: step 7507, loss 0.314898, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:10.872377: step 7508, loss 0.222375, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:10.956344: step 7509, loss 0.240899, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:11.042376: step 7510, loss 0.282462, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:11.124773: step 7511, loss 0.182857, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:11.207444: step 7512, loss 0.191469, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:11.290783: step 7513, loss 0.220619, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:11.374605: step 7514, loss 0.319133, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:11.458707: step 7515, loss 0.149904, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:11.541779: step 7516, loss 0.161227, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:11.623670: step 7517, loss 0.208439, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:11.704528: step 7518, loss 0.20062, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:11.786482: step 7519, loss 0.110948, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:11.870229: step 7520, loss 0.166275, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-11T11:25:12.089218: step 7520, loss 0.206631, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7520

2017-10-11T11:25:12.584382: step 7521, loss 0.107256, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:12.668801: step 7522, loss 0.252562, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:12.750033: step 7523, loss 0.291742, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:12.832688: step 7524, loss 0.272968, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:12.917375: step 7525, loss 0.147737, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:13.000246: step 7526, loss 0.136953, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:13.082234: step 7527, loss 0.17385, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:13.164337: step 7528, loss 0.279253, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:13.248415: step 7529, loss 0.301885, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:13.330842: step 7530, loss 0.231109, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:13.415535: step 7531, loss 0.205687, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:13.497762: step 7532, loss 0.228088, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:13.577999: step 7533, loss 0.216274, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:13.656682: step 7534, loss 0.226335, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:13.737683: step 7535, loss 0.337938, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:13.819393: step 7536, loss 0.168958, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:13.900193: step 7537, loss 0.241813, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:13.983032: step 7538, loss 0.258717, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:14.062746: step 7539, loss 0.238935, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:14.145061: step 7540, loss 0.263378, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:14.227095: step 7541, loss 0.284054, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:14.314245: step 7542, loss 0.192773, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:14.397015: step 7543, loss 0.172798, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:14.475278: step 7544, loss 0.20242, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:14.556552: step 7545, loss 0.204949, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:14.627119: step 7546, loss 0.170193, acc 0.960784, learning_rate 0.0001
2017-10-11T11:25:14.709729: step 7547, loss 0.1459, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:14.790340: step 7548, loss 0.176531, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:14.872131: step 7549, loss 0.383313, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:14.953306: step 7550, loss 0.151583, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:15.032295: step 7551, loss 0.157631, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:15.115057: step 7552, loss 0.176067, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:15.197598: step 7553, loss 0.25012, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:15.282907: step 7554, loss 0.172456, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:15.366784: step 7555, loss 0.305209, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:15.448991: step 7556, loss 0.281418, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:15.532874: step 7557, loss 0.337189, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:15.613653: step 7558, loss 0.209301, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:15.694103: step 7559, loss 0.119763, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:15.777486: step 7560, loss 0.195613, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-11T11:25:15.992385: step 7560, loss 0.206901, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7560

2017-10-11T11:25:16.491935: step 7561, loss 0.332919, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:16.576716: step 7562, loss 0.202975, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:16.660878: step 7563, loss 0.198703, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:16.743848: step 7564, loss 0.198932, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:16.825269: step 7565, loss 0.30344, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:16.907847: step 7566, loss 0.278536, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:16.989737: step 7567, loss 0.197971, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:17.072960: step 7568, loss 0.216718, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:17.155130: step 7569, loss 0.216502, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:17.237564: step 7570, loss 0.186454, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:17.316280: step 7571, loss 0.209628, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:17.399256: step 7572, loss 0.174069, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:17.481538: step 7573, loss 0.132514, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:17.563645: step 7574, loss 0.141834, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:17.642043: step 7575, loss 0.128343, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:17.725225: step 7576, loss 0.208892, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:17.807270: step 7577, loss 0.146304, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:17.891687: step 7578, loss 0.297031, acc 0.859375, learning_rate 0.0001
2017-10-11T11:25:17.971069: step 7579, loss 0.142544, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:18.054890: step 7580, loss 0.22693, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:18.137811: step 7581, loss 0.10987, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:18.221188: step 7582, loss 0.0693625, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:18.301805: step 7583, loss 0.191452, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:18.385532: step 7584, loss 0.280579, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:18.468166: step 7585, loss 0.139093, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:18.553142: step 7586, loss 0.272215, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:18.635396: step 7587, loss 0.236091, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:18.714834: step 7588, loss 0.332688, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:18.795535: step 7589, loss 0.223253, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:18.879242: step 7590, loss 0.243995, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:18.963080: step 7591, loss 0.148954, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:19.047146: step 7592, loss 0.14642, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:19.128812: step 7593, loss 0.16931, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:19.212087: step 7594, loss 0.232657, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:19.293725: step 7595, loss 0.292288, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:19.374183: step 7596, loss 0.292744, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:19.456441: step 7597, loss 0.138783, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:19.537882: step 7598, loss 0.246327, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:19.622971: step 7599, loss 0.13205, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:19.704201: step 7600, loss 0.200025, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:25:19.924392: step 7600, loss 0.206228, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7600

2017-10-11T11:25:20.432336: step 7601, loss 0.154428, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:20.515442: step 7602, loss 0.273634, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:20.598968: step 7603, loss 0.271922, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:20.681650: step 7604, loss 0.318356, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:20.763450: step 7605, loss 0.13031, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:20.845627: step 7606, loss 0.252287, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:20.929078: step 7607, loss 0.222099, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:21.012855: step 7608, loss 0.133542, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:21.091920: step 7609, loss 0.162603, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:21.174933: step 7610, loss 0.245872, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:21.256638: step 7611, loss 0.111984, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:21.339036: step 7612, loss 0.113864, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:21.420701: step 7613, loss 0.180026, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:21.504134: step 7614, loss 0.265563, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:21.585162: step 7615, loss 0.233116, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:21.666156: step 7616, loss 0.313259, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:21.748333: step 7617, loss 0.193367, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:21.834862: step 7618, loss 0.283479, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:21.917266: step 7619, loss 0.117303, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:21.999224: step 7620, loss 0.185542, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:22.079937: step 7621, loss 0.284071, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:22.162946: step 7622, loss 0.35631, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:22.244582: step 7623, loss 0.286437, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:22.326907: step 7624, loss 0.143973, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:22.408656: step 7625, loss 0.105671, acc 1, learning_rate 0.0001
2017-10-11T11:25:22.493009: step 7626, loss 0.181719, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:22.575331: step 7627, loss 0.275602, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:22.655317: step 7628, loss 0.186056, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:22.738500: step 7629, loss 0.240757, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:22.820526: step 7630, loss 0.148637, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:22.905493: step 7631, loss 0.222626, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:22.987757: step 7632, loss 0.174637, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:23.070504: step 7633, loss 0.279632, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:23.150402: step 7634, loss 0.218889, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:23.234768: step 7635, loss 0.171607, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:23.316943: step 7636, loss 0.285013, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:23.399360: step 7637, loss 0.177669, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:23.480770: step 7638, loss 0.172803, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:23.566639: step 7639, loss 0.241219, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:23.648001: step 7640, loss 0.219994, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-11T11:25:23.868952: step 7640, loss 0.206802, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7640

2017-10-11T11:25:24.371339: step 7641, loss 0.209446, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:24.450380: step 7642, loss 0.0981726, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:24.534512: step 7643, loss 0.227845, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:24.606688: step 7644, loss 0.174305, acc 0.941176, learning_rate 0.0001
2017-10-11T11:25:24.690771: step 7645, loss 0.249085, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:24.772147: step 7646, loss 0.226508, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:24.853660: step 7647, loss 0.198841, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:24.934147: step 7648, loss 0.117157, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:25.013565: step 7649, loss 0.236247, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:25.095960: step 7650, loss 0.199353, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:25.179884: step 7651, loss 0.199272, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:25.261642: step 7652, loss 0.0981019, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:25.346327: step 7653, loss 0.119701, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:25.432099: step 7654, loss 0.263625, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:25.513551: step 7655, loss 0.168376, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:25.594586: step 7656, loss 0.148818, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:25.677532: step 7657, loss 0.126889, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:25.761694: step 7658, loss 0.266349, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:25.845769: step 7659, loss 0.177806, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:25.926320: step 7660, loss 0.138655, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:26.012301: step 7661, loss 0.196522, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:26.094753: step 7662, loss 0.197888, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:26.181267: step 7663, loss 0.260279, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:26.265467: step 7664, loss 0.0709814, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:26.348434: step 7665, loss 0.380168, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:26.430137: step 7666, loss 0.202844, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:26.511770: step 7667, loss 0.163858, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:26.594952: step 7668, loss 0.165217, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:26.677851: step 7669, loss 0.283766, acc 0.859375, learning_rate 0.0001
2017-10-11T11:25:26.762025: step 7670, loss 0.13182, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:26.846594: step 7671, loss 0.215362, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:26.930067: step 7672, loss 0.225307, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:27.014168: step 7673, loss 0.183096, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:27.097307: step 7674, loss 0.178345, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:27.176983: step 7675, loss 0.0767544, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:27.258062: step 7676, loss 0.307954, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:27.341264: step 7677, loss 0.146734, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:27.424608: step 7678, loss 0.190157, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:27.507407: step 7679, loss 0.340096, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:27.591067: step 7680, loss 0.196481, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-11T11:25:27.811180: step 7680, loss 0.207036, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7680

2017-10-11T11:25:28.312736: step 7681, loss 0.161674, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:28.394713: step 7682, loss 0.226124, acc 0.859375, learning_rate 0.0001
2017-10-11T11:25:28.475739: step 7683, loss 0.126786, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:28.559444: step 7684, loss 0.278029, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:28.640235: step 7685, loss 0.125677, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:28.726550: step 7686, loss 0.153959, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:28.810570: step 7687, loss 0.110968, acc 1, learning_rate 0.0001
2017-10-11T11:25:28.894378: step 7688, loss 0.134346, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:28.977639: step 7689, loss 0.187295, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:29.060175: step 7690, loss 0.254628, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:29.146371: step 7691, loss 0.204342, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:29.226643: step 7692, loss 0.177347, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:29.308366: step 7693, loss 0.367669, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:29.391702: step 7694, loss 0.344243, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:29.474097: step 7695, loss 0.261702, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:29.557232: step 7696, loss 0.163032, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:29.641247: step 7697, loss 0.10519, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:29.724051: step 7698, loss 0.189052, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:29.806049: step 7699, loss 0.25628, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:29.887220: step 7700, loss 0.189953, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:29.969333: step 7701, loss 0.207399, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:30.052748: step 7702, loss 0.169301, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:30.134256: step 7703, loss 0.207059, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:30.213903: step 7704, loss 0.240113, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:30.298469: step 7705, loss 0.341087, acc 0.859375, learning_rate 0.0001
2017-10-11T11:25:30.380059: step 7706, loss 0.186541, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:30.460239: step 7707, loss 0.224364, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:30.540465: step 7708, loss 0.349883, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:30.625294: step 7709, loss 0.176459, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:30.708664: step 7710, loss 0.435238, acc 0.859375, learning_rate 0.0001
2017-10-11T11:25:30.789930: step 7711, loss 0.14865, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:30.872520: step 7712, loss 0.187444, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:30.954326: step 7713, loss 0.137797, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:31.035344: step 7714, loss 0.153076, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:31.118467: step 7715, loss 0.226123, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:31.198651: step 7716, loss 0.129899, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:31.279362: step 7717, loss 0.17745, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:31.362522: step 7718, loss 0.132849, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:31.444048: step 7719, loss 0.353204, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:31.523912: step 7720, loss 0.173439, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-11T11:25:31.746464: step 7720, loss 0.206573, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7720

2017-10-11T11:25:32.252213: step 7721, loss 0.174865, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:32.334985: step 7722, loss 0.113939, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:32.420412: step 7723, loss 0.151358, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:32.503309: step 7724, loss 0.418623, acc 0.859375, learning_rate 0.0001
2017-10-11T11:25:32.582412: step 7725, loss 0.0952043, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:32.664586: step 7726, loss 0.148247, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:32.747615: step 7727, loss 0.271594, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:32.827963: step 7728, loss 0.171979, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:32.908042: step 7729, loss 0.168238, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:32.990046: step 7730, loss 0.189276, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:33.072121: step 7731, loss 0.1782, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:33.155076: step 7732, loss 0.199549, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:33.239280: step 7733, loss 0.277629, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:33.322716: step 7734, loss 0.337493, acc 0.8125, learning_rate 0.0001
2017-10-11T11:25:33.403115: step 7735, loss 0.0898267, acc 1, learning_rate 0.0001
2017-10-11T11:25:33.486758: step 7736, loss 0.228299, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:33.570729: step 7737, loss 0.0476515, acc 1, learning_rate 0.0001
2017-10-11T11:25:33.649668: step 7738, loss 0.112326, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:33.731267: step 7739, loss 0.101219, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:33.812045: step 7740, loss 0.228384, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:33.892828: step 7741, loss 0.12931, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:33.964013: step 7742, loss 0.169677, acc 0.941176, learning_rate 0.0001
2017-10-11T11:25:34.046812: step 7743, loss 0.324342, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:34.128706: step 7744, loss 0.109524, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:34.212092: step 7745, loss 0.287263, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:34.291870: step 7746, loss 0.268539, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:34.374556: step 7747, loss 0.166231, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:34.456761: step 7748, loss 0.192173, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:34.539286: step 7749, loss 0.199827, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:34.621324: step 7750, loss 0.17666, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:34.704768: step 7751, loss 0.338011, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:34.787809: step 7752, loss 0.0634453, acc 1, learning_rate 0.0001
2017-10-11T11:25:34.867779: step 7753, loss 0.168296, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:34.957474: step 7754, loss 0.0791063, acc 1, learning_rate 0.0001
2017-10-11T11:25:35.040763: step 7755, loss 0.164205, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:35.124838: step 7756, loss 0.180594, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:35.209153: step 7757, loss 0.320796, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:35.293514: step 7758, loss 0.205151, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:35.374531: step 7759, loss 0.227749, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:35.455531: step 7760, loss 0.290562, acc 0.859375, learning_rate 0.0001

Evaluation:
2017-10-11T11:25:35.671295: step 7760, loss 0.206088, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7760

2017-10-11T11:25:36.102145: step 7761, loss 0.23198, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:36.182660: step 7762, loss 0.254405, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:36.266380: step 7763, loss 0.0908366, acc 1, learning_rate 0.0001
2017-10-11T11:25:36.349119: step 7764, loss 0.299972, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:36.432584: step 7765, loss 0.235466, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:36.515659: step 7766, loss 0.209654, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:36.597954: step 7767, loss 0.227416, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:36.680038: step 7768, loss 0.317357, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:36.765620: step 7769, loss 0.159635, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:36.847831: step 7770, loss 0.143812, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:36.933309: step 7771, loss 0.141773, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:37.015784: step 7772, loss 0.148113, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:37.097596: step 7773, loss 0.190836, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:37.182432: step 7774, loss 0.257398, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:37.263783: step 7775, loss 0.131722, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:37.347866: step 7776, loss 0.193499, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:37.430342: step 7777, loss 0.28219, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:37.512067: step 7778, loss 0.293458, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:37.595689: step 7779, loss 0.21164, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:37.679122: step 7780, loss 0.178828, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:37.762639: step 7781, loss 0.31012, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:37.848359: step 7782, loss 0.178836, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:37.931999: step 7783, loss 0.171725, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:38.013844: step 7784, loss 0.205756, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:38.093569: step 7785, loss 0.118778, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:38.177127: step 7786, loss 0.234101, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:38.258284: step 7787, loss 0.182522, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:38.338549: step 7788, loss 0.294428, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:38.421156: step 7789, loss 0.127197, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:38.504421: step 7790, loss 0.120256, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:38.585564: step 7791, loss 0.204636, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:38.665152: step 7792, loss 0.234636, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:38.746206: step 7793, loss 0.143857, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:38.828031: step 7794, loss 0.246699, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:38.908731: step 7795, loss 0.118629, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:38.990879: step 7796, loss 0.21607, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:39.070601: step 7797, loss 0.154179, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:39.155499: step 7798, loss 0.180097, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:39.238769: step 7799, loss 0.14998, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:39.319601: step 7800, loss 0.235523, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-11T11:25:39.545368: step 7800, loss 0.205397, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7800

2017-10-11T11:25:40.044560: step 7801, loss 0.110104, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:40.127054: step 7802, loss 0.211401, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:40.210650: step 7803, loss 0.148702, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:40.290534: step 7804, loss 0.220891, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:40.371112: step 7805, loss 0.206547, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:40.453701: step 7806, loss 0.17727, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:40.534680: step 7807, loss 0.169449, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:40.618147: step 7808, loss 0.152293, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:40.696844: step 7809, loss 0.155224, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:40.774100: step 7810, loss 0.202276, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:40.857140: step 7811, loss 0.250964, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:40.940633: step 7812, loss 0.174526, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:41.022274: step 7813, loss 0.177012, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:41.105209: step 7814, loss 0.173607, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:41.186708: step 7815, loss 0.0928425, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:41.272806: step 7816, loss 0.233019, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:41.352740: step 7817, loss 0.216013, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:41.435611: step 7818, loss 0.182501, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:41.517047: step 7819, loss 0.250467, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:41.600792: step 7820, loss 0.210898, acc 0.90625, learning_rate 0.0001
2017-10-11T11:25:41.685466: step 7821, loss 0.184569, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:41.765564: step 7822, loss 0.203315, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:41.849685: step 7823, loss 0.218737, acc 0.921875, learning_rate 0.0001
2017-10-11T11:25:41.933152: step 7824, loss 0.323311, acc 0.859375, learning_rate 0.0001
2017-10-11T11:25:42.016718: step 7825, loss 0.0796852, acc 1, learning_rate 0.0001
2017-10-11T11:25:42.098447: step 7826, loss 0.196882, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:42.181188: step 7827, loss 0.195171, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:42.261936: step 7828, loss 0.212219, acc 0.953125, learning_rate 0.0001
2017-10-11T11:25:42.344179: step 7829, loss 0.0922585, acc 1, learning_rate 0.0001
2017-10-11T11:25:42.423946: step 7830, loss 0.277526, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:42.511317: step 7831, loss 0.143247, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:42.593685: step 7832, loss 0.122076, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:42.677613: step 7833, loss 0.105392, acc 0.984375, learning_rate 0.0001
2017-10-11T11:25:42.757421: step 7834, loss 0.244078, acc 0.890625, learning_rate 0.0001
2017-10-11T11:25:42.840894: step 7835, loss 0.137973, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:42.922276: step 7836, loss 0.245331, acc 0.9375, learning_rate 0.0001
2017-10-11T11:25:43.004104: step 7837, loss 0.274245, acc 0.875, learning_rate 0.0001
2017-10-11T11:25:43.086675: step 7838, loss 0.185935, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:43.168187: step 7839, loss 0.0970907, acc 0.96875, learning_rate 0.0001
2017-10-11T11:25:43.238937: step 7840, loss 0.276881, acc 0.882353, learning_rate 0.0001

Evaluation:
2017-10-11T11:25:43.461576: step 7840, loss 0.20539, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507738210/checkpoints/model-7840

