
Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=40
DECAY_COEFFICIENT=2.5
DEV_SAMPLE_PERCENTAGE=0.1
DROPOUT_KEEP_PROB=0.1
EMBEDDING_DIM=25
ENABLE_WORD_EMBEDDINGS=True
EVALUATE_EVERY=40
FILTER_SIZES=3,4,5
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_CHECKPOINTS=5
NUM_EPOCHS=80
NUM_FILTERS=128

Loading data...
6259
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
695
[[1 0 0 0 0]
 [1 0 0 0 0]
 [1 0 0 0 0]
 ..., 
 [0 0 0 0 1]
 [0 0 0 0 1]
 [0 0 0 0 1]]
6259
695
6954
Writing to /home/sheep/bigdata/runs/1507657518

Load glove file /home/sheep/bigdata/vec25.txt
glove file has been loaded

2017-10-10T12:45:26.748357: step 1, loss 20.8353, acc 0.203125, learning_rate 0.005
2017-10-10T12:45:27.372415: step 2, loss 16.739, acc 0.21875, learning_rate 0.00498
2017-10-10T12:45:27.829079: step 3, loss 15.0426, acc 0.296875, learning_rate 0.00496008
2017-10-10T12:45:28.354256: step 4, loss 14.0888, acc 0.296875, learning_rate 0.00494024
2017-10-10T12:45:28.916914: step 5, loss 16.9083, acc 0.296875, learning_rate 0.00492049
2017-10-10T12:45:29.413398: step 6, loss 15.9282, acc 0.296875, learning_rate 0.00490081
2017-10-10T12:45:29.963002: step 7, loss 15.8084, acc 0.359375, learning_rate 0.00488121
2017-10-10T12:45:30.540998: step 8, loss 17.0919, acc 0.25, learning_rate 0.0048617
2017-10-10T12:45:31.029913: step 9, loss 13.8574, acc 0.3125, learning_rate 0.00484226
2017-10-10T12:45:31.566279: step 10, loss 13.6485, acc 0.328125, learning_rate 0.00482291
2017-10-10T12:45:32.080879: step 11, loss 12.8337, acc 0.40625, learning_rate 0.00480363
2017-10-10T12:45:32.608881: step 12, loss 11.9073, acc 0.34375, learning_rate 0.00478443
2017-10-10T12:45:33.183971: step 13, loss 12.5473, acc 0.3125, learning_rate 0.00476531
2017-10-10T12:45:33.801085: step 14, loss 11.0093, acc 0.359375, learning_rate 0.00474627
2017-10-10T12:45:34.233200: step 15, loss 12.9166, acc 0.21875, learning_rate 0.0047273
2017-10-10T12:45:34.629153: step 16, loss 8.2553, acc 0.359375, learning_rate 0.00470841
2017-10-10T12:45:35.032874: step 17, loss 9.97559, acc 0.34375, learning_rate 0.0046896
2017-10-10T12:45:35.464892: step 18, loss 9.45966, acc 0.328125, learning_rate 0.00467087
2017-10-10T12:45:35.883901: step 19, loss 11.9588, acc 0.28125, learning_rate 0.00465221
2017-10-10T12:45:36.439568: step 20, loss 11.0468, acc 0.1875, learning_rate 0.00463363
2017-10-10T12:45:36.966417: step 21, loss 9.38305, acc 0.421875, learning_rate 0.00461513
2017-10-10T12:45:37.476472: step 22, loss 6.33939, acc 0.421875, learning_rate 0.0045967
2017-10-10T12:45:38.037069: step 23, loss 7.48992, acc 0.3125, learning_rate 0.00457834
2017-10-10T12:45:38.653003: step 24, loss 7.22467, acc 0.375, learning_rate 0.00456006
2017-10-10T12:45:39.190821: step 25, loss 8.41442, acc 0.390625, learning_rate 0.00454186
2017-10-10T12:45:39.768820: step 26, loss 6.70067, acc 0.421875, learning_rate 0.00452373
2017-10-10T12:45:40.311562: step 27, loss 8.65615, acc 0.328125, learning_rate 0.00450567
2017-10-10T12:45:40.918391: step 28, loss 6.65608, acc 0.4375, learning_rate 0.00448769
2017-10-10T12:45:41.474609: step 29, loss 6.34891, acc 0.453125, learning_rate 0.00446978
2017-10-10T12:45:41.975219: step 30, loss 9.5101, acc 0.3125, learning_rate 0.00445194
2017-10-10T12:45:42.461544: step 31, loss 8.74339, acc 0.40625, learning_rate 0.00443418
2017-10-10T12:45:42.984867: step 32, loss 7.0627, acc 0.46875, learning_rate 0.00441649
2017-10-10T12:45:43.460378: step 33, loss 7.79072, acc 0.375, learning_rate 0.00439887
2017-10-10T12:45:44.028923: step 34, loss 5.21191, acc 0.453125, learning_rate 0.00438132
2017-10-10T12:45:44.574427: step 35, loss 4.61776, acc 0.546875, learning_rate 0.00436385
2017-10-10T12:45:45.087792: step 36, loss 5.68301, acc 0.453125, learning_rate 0.00434644
2017-10-10T12:45:45.538573: step 37, loss 6.78257, acc 0.390625, learning_rate 0.00432911
2017-10-10T12:45:46.069007: step 38, loss 5.51963, acc 0.390625, learning_rate 0.00431185
2017-10-10T12:45:46.506173: step 39, loss 5.25804, acc 0.453125, learning_rate 0.00429465
2017-10-10T12:45:47.104164: step 40, loss 5.18052, acc 0.515625, learning_rate 0.00427753

Evaluation:
2017-10-10T12:45:49.769288: step 40, loss 1.04965, acc 0.753957

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-40

2017-10-10T12:45:52.336269: step 41, loss 4.5471, acc 0.5625, learning_rate 0.00426048
2017-10-10T12:45:52.812081: step 42, loss 4.43887, acc 0.484375, learning_rate 0.0042435
2017-10-10T12:45:53.388557: step 43, loss 4.09182, acc 0.46875, learning_rate 0.00422659
2017-10-10T12:45:53.856276: step 44, loss 4.08246, acc 0.515625, learning_rate 0.00420974
2017-10-10T12:45:54.316895: step 45, loss 5.05378, acc 0.40625, learning_rate 0.00419297
2017-10-10T12:45:54.854166: step 46, loss 4.58074, acc 0.453125, learning_rate 0.00417626
2017-10-10T12:45:55.361015: step 47, loss 3.06423, acc 0.546875, learning_rate 0.00415962
2017-10-10T12:45:55.889331: step 48, loss 4.84175, acc 0.5, learning_rate 0.00414305
2017-10-10T12:45:56.412858: step 49, loss 2.97553, acc 0.5625, learning_rate 0.00412655
2017-10-10T12:45:56.951467: step 50, loss 5.05478, acc 0.46875, learning_rate 0.00411011
2017-10-10T12:45:57.546815: step 51, loss 3.00343, acc 0.515625, learning_rate 0.00409375
2017-10-10T12:45:58.056767: step 52, loss 3.62209, acc 0.46875, learning_rate 0.00407744
2017-10-10T12:45:58.486817: step 53, loss 4.48082, acc 0.390625, learning_rate 0.00406121
2017-10-10T12:45:58.933120: step 54, loss 2.84768, acc 0.515625, learning_rate 0.00404504
2017-10-10T12:45:59.476121: step 55, loss 3.54604, acc 0.578125, learning_rate 0.00402894
2017-10-10T12:46:00.005102: step 56, loss 2.87941, acc 0.59375, learning_rate 0.0040129
2017-10-10T12:46:00.549050: step 57, loss 3.47586, acc 0.515625, learning_rate 0.00399693
2017-10-10T12:46:01.077186: step 58, loss 3.46164, acc 0.5, learning_rate 0.00398102
2017-10-10T12:46:01.573006: step 59, loss 3.04124, acc 0.5625, learning_rate 0.00396518
2017-10-10T12:46:02.057161: step 60, loss 4.74062, acc 0.4375, learning_rate 0.00394941
2017-10-10T12:46:02.577143: step 61, loss 3.74327, acc 0.578125, learning_rate 0.00393369
2017-10-10T12:46:03.058268: step 62, loss 2.44821, acc 0.625, learning_rate 0.00391804
2017-10-10T12:46:03.525913: step 63, loss 2.70602, acc 0.640625, learning_rate 0.00390246
2017-10-10T12:46:04.032853: step 64, loss 3.33521, acc 0.53125, learning_rate 0.00388694
2017-10-10T12:46:04.613696: step 65, loss 2.0338, acc 0.65625, learning_rate 0.00387148
2017-10-10T12:46:05.156867: step 66, loss 2.40846, acc 0.578125, learning_rate 0.00385609
2017-10-10T12:46:05.645163: step 67, loss 1.71673, acc 0.65625, learning_rate 0.00384076
2017-10-10T12:46:06.157035: step 68, loss 2.61864, acc 0.546875, learning_rate 0.00382549
2017-10-10T12:46:06.829651: step 69, loss 3.51469, acc 0.546875, learning_rate 0.00381028
2017-10-10T12:46:07.300479: step 70, loss 2.41801, acc 0.5, learning_rate 0.00379514
2017-10-10T12:46:07.795256: step 71, loss 3.67589, acc 0.453125, learning_rate 0.00378005
2017-10-10T12:46:08.268812: step 72, loss 3.01433, acc 0.4375, learning_rate 0.00376503
2017-10-10T12:46:08.726329: step 73, loss 2.80591, acc 0.546875, learning_rate 0.00375007
2017-10-10T12:46:09.245661: step 74, loss 2.20802, acc 0.546875, learning_rate 0.00373517
2017-10-10T12:46:09.828211: step 75, loss 1.87093, acc 0.609375, learning_rate 0.00372034
2017-10-10T12:46:10.476964: step 76, loss 1.90248, acc 0.578125, learning_rate 0.00370556
2017-10-10T12:46:10.942331: step 77, loss 2.07369, acc 0.59375, learning_rate 0.00369084
2017-10-10T12:46:11.400513: step 78, loss 2.11399, acc 0.5625, learning_rate 0.00367619
2017-10-10T12:46:11.921027: step 79, loss 2.77367, acc 0.59375, learning_rate 0.00366159
2017-10-10T12:46:12.528404: step 80, loss 2.80328, acc 0.59375, learning_rate 0.00364705

Evaluation:
2017-10-10T12:46:13.816417: step 80, loss 0.494272, acc 0.847482

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-80

2017-10-10T12:46:15.430294: step 81, loss 2.72044, acc 0.46875, learning_rate 0.00363257
2017-10-10T12:46:15.928850: step 82, loss 1.75411, acc 0.609375, learning_rate 0.00361815
2017-10-10T12:46:16.445015: step 83, loss 1.80589, acc 0.578125, learning_rate 0.00360379
2017-10-10T12:46:16.996737: step 84, loss 1.59643, acc 0.703125, learning_rate 0.00358949
2017-10-10T12:46:17.528569: step 85, loss 2.6097, acc 0.46875, learning_rate 0.00357525
2017-10-10T12:46:18.051520: step 86, loss 1.37167, acc 0.625, learning_rate 0.00356106
2017-10-10T12:46:18.572897: step 87, loss 2.69668, acc 0.53125, learning_rate 0.00354694
2017-10-10T12:46:19.136864: step 88, loss 1.49489, acc 0.65625, learning_rate 0.00353287
2017-10-10T12:46:19.752949: step 89, loss 2.38487, acc 0.53125, learning_rate 0.00351885
2017-10-10T12:46:20.363225: step 90, loss 2.05495, acc 0.59375, learning_rate 0.0035049
2017-10-10T12:46:20.800722: step 91, loss 1.4316, acc 0.65625, learning_rate 0.003491
2017-10-10T12:46:21.589808: step 92, loss 1.60593, acc 0.5625, learning_rate 0.00347716
2017-10-10T12:46:22.038595: step 93, loss 2.50714, acc 0.515625, learning_rate 0.00346338
2017-10-10T12:46:22.491177: step 94, loss 1.95997, acc 0.609375, learning_rate 0.00344965
2017-10-10T12:46:22.971197: step 95, loss 1.85175, acc 0.578125, learning_rate 0.00343597
2017-10-10T12:46:23.460120: step 96, loss 1.64572, acc 0.6875, learning_rate 0.00342236
2017-10-10T12:46:24.030081: step 97, loss 1.94478, acc 0.609375, learning_rate 0.0034088
2017-10-10T12:46:24.588564: step 98, loss 1.76647, acc 0.568627, learning_rate 0.00339529
2017-10-10T12:46:25.429043: step 99, loss 1.91669, acc 0.625, learning_rate 0.00338184
2017-10-10T12:46:25.926237: step 100, loss 1.66305, acc 0.59375, learning_rate 0.00336844
2017-10-10T12:46:26.421169: step 101, loss 1.84672, acc 0.625, learning_rate 0.0033551
2017-10-10T12:46:26.989155: step 102, loss 2.14762, acc 0.59375, learning_rate 0.00334182
2017-10-10T12:46:27.660955: step 103, loss 1.99851, acc 0.578125, learning_rate 0.00332858
2017-10-10T12:46:28.180922: step 104, loss 1.34703, acc 0.6875, learning_rate 0.00331541
2017-10-10T12:46:28.712533: step 105, loss 1.21443, acc 0.703125, learning_rate 0.00330228
2017-10-10T12:46:29.224882: step 106, loss 1.06632, acc 0.78125, learning_rate 0.00328921
2017-10-10T12:46:29.740961: step 107, loss 1.1303, acc 0.75, learning_rate 0.00327619
2017-10-10T12:46:30.385076: step 108, loss 1.27295, acc 0.6875, learning_rate 0.00326323
2017-10-10T12:46:30.846774: step 109, loss 1.31389, acc 0.6875, learning_rate 0.00325032
2017-10-10T12:46:31.290173: step 110, loss 1.25417, acc 0.75, learning_rate 0.00323746
2017-10-10T12:46:31.708845: step 111, loss 1.33858, acc 0.703125, learning_rate 0.00322465
2017-10-10T12:46:32.236877: step 112, loss 0.982632, acc 0.71875, learning_rate 0.0032119
2017-10-10T12:46:32.700507: step 113, loss 0.856284, acc 0.71875, learning_rate 0.0031992
2017-10-10T12:46:33.349175: step 114, loss 1.25683, acc 0.625, learning_rate 0.00318655
2017-10-10T12:46:33.936632: step 115, loss 1.22597, acc 0.75, learning_rate 0.00317395
2017-10-10T12:46:34.398825: step 116, loss 1.50198, acc 0.640625, learning_rate 0.0031614
2017-10-10T12:46:34.804889: step 117, loss 2.02414, acc 0.609375, learning_rate 0.0031489
2017-10-10T12:46:35.319782: step 118, loss 1.21737, acc 0.625, learning_rate 0.00313646
2017-10-10T12:46:35.857841: step 119, loss 1.51928, acc 0.65625, learning_rate 0.00312407
2017-10-10T12:46:36.432862: step 120, loss 1.6476, acc 0.640625, learning_rate 0.00311172

Evaluation:
2017-10-10T12:46:37.534321: step 120, loss 0.382847, acc 0.861871

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-120

2017-10-10T12:46:39.136832: step 121, loss 1.27966, acc 0.6875, learning_rate 0.00309943
2017-10-10T12:46:39.642547: step 122, loss 0.761484, acc 0.796875, learning_rate 0.00308719
2017-10-10T12:46:40.168145: step 123, loss 0.879337, acc 0.765625, learning_rate 0.00307499
2017-10-10T12:46:40.712475: step 124, loss 1.58618, acc 0.59375, learning_rate 0.00306285
2017-10-10T12:46:41.263321: step 125, loss 0.94962, acc 0.75, learning_rate 0.00305076
2017-10-10T12:46:41.796636: step 126, loss 0.942716, acc 0.671875, learning_rate 0.00303871
2017-10-10T12:46:42.364040: step 127, loss 1.1706, acc 0.734375, learning_rate 0.00302672
2017-10-10T12:46:42.904986: step 128, loss 0.957545, acc 0.75, learning_rate 0.00301477
2017-10-10T12:46:43.454206: step 129, loss 1.15359, acc 0.609375, learning_rate 0.00300287
2017-10-10T12:46:44.011215: step 130, loss 1.30338, acc 0.75, learning_rate 0.00299102
2017-10-10T12:46:44.541570: step 131, loss 1.09742, acc 0.734375, learning_rate 0.00297922
2017-10-10T12:46:45.016832: step 132, loss 1.34417, acc 0.640625, learning_rate 0.00296747
2017-10-10T12:46:45.489134: step 133, loss 0.999259, acc 0.765625, learning_rate 0.00295577
2017-10-10T12:46:46.071339: step 134, loss 0.939568, acc 0.765625, learning_rate 0.00294411
2017-10-10T12:46:46.611831: step 135, loss 0.96174, acc 0.75, learning_rate 0.0029325
2017-10-10T12:46:47.050088: step 136, loss 1.12915, acc 0.65625, learning_rate 0.00292094
2017-10-10T12:46:47.565082: step 137, loss 0.763191, acc 0.71875, learning_rate 0.00290943
2017-10-10T12:46:48.068950: step 138, loss 1.73839, acc 0.65625, learning_rate 0.00289796
2017-10-10T12:46:48.595803: step 139, loss 0.971576, acc 0.703125, learning_rate 0.00288654
2017-10-10T12:46:49.180844: step 140, loss 0.833154, acc 0.765625, learning_rate 0.00287516
2017-10-10T12:46:49.758820: step 141, loss 1.00152, acc 0.734375, learning_rate 0.00286384
2017-10-10T12:46:50.276875: step 142, loss 1.08174, acc 0.71875, learning_rate 0.00285256
2017-10-10T12:46:50.820819: step 143, loss 1.52284, acc 0.578125, learning_rate 0.00284132
2017-10-10T12:46:51.439712: step 144, loss 1.40389, acc 0.6875, learning_rate 0.00283013
2017-10-10T12:46:51.961127: step 145, loss 0.517645, acc 0.828125, learning_rate 0.00281899
2017-10-10T12:46:52.576936: step 146, loss 1.46247, acc 0.65625, learning_rate 0.00280789
2017-10-10T12:46:53.159205: step 147, loss 1.52025, acc 0.65625, learning_rate 0.00279684
2017-10-10T12:46:53.715298: step 148, loss 1.35061, acc 0.703125, learning_rate 0.00278583
2017-10-10T12:46:54.144436: step 149, loss 0.577973, acc 0.828125, learning_rate 0.00277486
2017-10-10T12:46:54.561721: step 150, loss 1.32009, acc 0.640625, learning_rate 0.00276395
2017-10-10T12:46:55.056977: step 151, loss 1.26902, acc 0.59375, learning_rate 0.00275307
2017-10-10T12:46:55.539679: step 152, loss 1.15462, acc 0.703125, learning_rate 0.00274224
2017-10-10T12:46:56.148862: step 153, loss 0.6747, acc 0.78125, learning_rate 0.00273146
2017-10-10T12:46:56.708947: step 154, loss 0.847807, acc 0.75, learning_rate 0.00272072
2017-10-10T12:46:57.287818: step 155, loss 0.912285, acc 0.6875, learning_rate 0.00271002
2017-10-10T12:46:57.745976: step 156, loss 1.26268, acc 0.640625, learning_rate 0.00269937
2017-10-10T12:46:58.241159: step 157, loss 0.598011, acc 0.828125, learning_rate 0.00268876
2017-10-10T12:46:58.750050: step 158, loss 0.517168, acc 0.828125, learning_rate 0.00267819
2017-10-10T12:46:59.287386: step 159, loss 1.65892, acc 0.6875, learning_rate 0.00266767
2017-10-10T12:46:59.800177: step 160, loss 0.762883, acc 0.734375, learning_rate 0.00265719

Evaluation:
2017-10-10T12:47:00.979919: step 160, loss 0.368692, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-160

2017-10-10T12:47:02.539523: step 161, loss 0.556098, acc 0.78125, learning_rate 0.00264675
2017-10-10T12:47:03.109117: step 162, loss 1.19209, acc 0.65625, learning_rate 0.00263635
2017-10-10T12:47:03.637038: step 163, loss 1.28013, acc 0.609375, learning_rate 0.002626
2017-10-10T12:47:04.207641: step 164, loss 1.12762, acc 0.640625, learning_rate 0.00261569
2017-10-10T12:47:04.676704: step 165, loss 0.776699, acc 0.734375, learning_rate 0.00260542
2017-10-10T12:47:05.182576: step 166, loss 0.770573, acc 0.75, learning_rate 0.0025952
2017-10-10T12:47:05.748312: step 167, loss 0.590244, acc 0.734375, learning_rate 0.00258501
2017-10-10T12:47:06.416325: step 168, loss 0.666132, acc 0.765625, learning_rate 0.00257487
2017-10-10T12:47:06.905415: step 169, loss 0.544026, acc 0.84375, learning_rate 0.00256477
2017-10-10T12:47:07.363322: step 170, loss 0.960364, acc 0.71875, learning_rate 0.0025547
2017-10-10T12:47:07.924313: step 171, loss 0.942863, acc 0.71875, learning_rate 0.00254469
2017-10-10T12:47:08.393478: step 172, loss 1.08916, acc 0.640625, learning_rate 0.00253471
2017-10-10T12:47:08.919561: step 173, loss 0.835843, acc 0.65625, learning_rate 0.00252477
2017-10-10T12:47:09.528726: step 174, loss 0.924442, acc 0.734375, learning_rate 0.00251487
2017-10-10T12:47:10.060842: step 175, loss 0.865224, acc 0.71875, learning_rate 0.00250501
2017-10-10T12:47:10.496261: step 176, loss 0.97259, acc 0.765625, learning_rate 0.0024952
2017-10-10T12:47:10.917327: step 177, loss 0.907664, acc 0.71875, learning_rate 0.00248542
2017-10-10T12:47:11.379674: step 178, loss 0.824017, acc 0.75, learning_rate 0.00247568
2017-10-10T12:47:11.870212: step 179, loss 1.15452, acc 0.640625, learning_rate 0.00246599
2017-10-10T12:47:12.409008: step 180, loss 0.792685, acc 0.765625, learning_rate 0.00245633
2017-10-10T12:47:12.946342: step 181, loss 0.937097, acc 0.734375, learning_rate 0.00244671
2017-10-10T12:47:13.505093: step 182, loss 1.3256, acc 0.75, learning_rate 0.00243713
2017-10-10T12:47:14.089125: step 183, loss 0.860442, acc 0.8125, learning_rate 0.00242759
2017-10-10T12:47:14.736920: step 184, loss 1.22671, acc 0.71875, learning_rate 0.00241809
2017-10-10T12:47:15.213053: step 185, loss 0.997579, acc 0.65625, learning_rate 0.00240863
2017-10-10T12:47:15.758258: step 186, loss 0.652613, acc 0.828125, learning_rate 0.00239921
2017-10-10T12:47:16.324946: step 187, loss 0.853818, acc 0.703125, learning_rate 0.00238982
2017-10-10T12:47:16.920627: step 188, loss 0.842324, acc 0.796875, learning_rate 0.00238048
2017-10-10T12:47:17.401272: step 189, loss 0.75546, acc 0.734375, learning_rate 0.00237117
2017-10-10T12:47:17.812841: step 190, loss 0.725373, acc 0.75, learning_rate 0.0023619
2017-10-10T12:47:18.258898: step 191, loss 0.820481, acc 0.8125, learning_rate 0.00235267
2017-10-10T12:47:18.839586: step 192, loss 1.26256, acc 0.59375, learning_rate 0.00234347
2017-10-10T12:47:19.381183: step 193, loss 0.816585, acc 0.765625, learning_rate 0.00233431
2017-10-10T12:47:19.980320: step 194, loss 0.710058, acc 0.796875, learning_rate 0.00232519
2017-10-10T12:47:20.409780: step 195, loss 0.757931, acc 0.765625, learning_rate 0.00231611
2017-10-10T12:47:20.781043: step 196, loss 0.852506, acc 0.745098, learning_rate 0.00230707
2017-10-10T12:47:21.276960: step 197, loss 0.771039, acc 0.75, learning_rate 0.00229806
2017-10-10T12:47:21.850052: step 198, loss 0.708493, acc 0.765625, learning_rate 0.00228908
2017-10-10T12:47:22.440483: step 199, loss 0.754806, acc 0.734375, learning_rate 0.00228015
2017-10-10T12:47:23.004832: step 200, loss 0.825419, acc 0.703125, learning_rate 0.00227125

Evaluation:
2017-10-10T12:47:24.169091: step 200, loss 0.346446, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-200

2017-10-10T12:47:25.759242: step 201, loss 0.755021, acc 0.71875, learning_rate 0.00226239
2017-10-10T12:47:26.240909: step 202, loss 0.723248, acc 0.71875, learning_rate 0.00225356
2017-10-10T12:47:26.785207: step 203, loss 0.821819, acc 0.78125, learning_rate 0.00224477
2017-10-10T12:47:27.395637: step 204, loss 0.889384, acc 0.6875, learning_rate 0.00223602
2017-10-10T12:47:27.915914: step 205, loss 0.716673, acc 0.734375, learning_rate 0.0022273
2017-10-10T12:47:28.368866: step 206, loss 0.64406, acc 0.78125, learning_rate 0.00221862
2017-10-10T12:47:28.968912: step 207, loss 0.638731, acc 0.8125, learning_rate 0.00220997
2017-10-10T12:47:29.523261: step 208, loss 0.400641, acc 0.828125, learning_rate 0.00220136
2017-10-10T12:47:29.964472: step 209, loss 0.62518, acc 0.765625, learning_rate 0.00219278
2017-10-10T12:47:30.415029: step 210, loss 1.16033, acc 0.703125, learning_rate 0.00218424
2017-10-10T12:47:30.964059: step 211, loss 0.946552, acc 0.6875, learning_rate 0.00217573
2017-10-10T12:47:31.524957: step 212, loss 0.747052, acc 0.78125, learning_rate 0.00216726
2017-10-10T12:47:32.096865: step 213, loss 0.717376, acc 0.78125, learning_rate 0.00215882
2017-10-10T12:47:32.619263: step 214, loss 0.720179, acc 0.734375, learning_rate 0.00215041
2017-10-10T12:47:33.153436: step 215, loss 0.506066, acc 0.78125, learning_rate 0.00214204
2017-10-10T12:47:33.729171: step 216, loss 0.90739, acc 0.671875, learning_rate 0.00213371
2017-10-10T12:47:34.236552: step 217, loss 0.728898, acc 0.78125, learning_rate 0.00212541
2017-10-10T12:47:34.751881: step 218, loss 0.758001, acc 0.765625, learning_rate 0.00211714
2017-10-10T12:47:35.240819: step 219, loss 0.912736, acc 0.765625, learning_rate 0.00210891
2017-10-10T12:47:35.721104: step 220, loss 0.751364, acc 0.703125, learning_rate 0.00210071
2017-10-10T12:47:36.245135: step 221, loss 0.780245, acc 0.734375, learning_rate 0.00209254
2017-10-10T12:47:36.802020: step 222, loss 0.771556, acc 0.765625, learning_rate 0.00208441
2017-10-10T12:47:37.297810: step 223, loss 1.01245, acc 0.703125, learning_rate 0.00207631
2017-10-10T12:47:37.873200: step 224, loss 0.666242, acc 0.734375, learning_rate 0.00206824
2017-10-10T12:47:38.414606: step 225, loss 0.744005, acc 0.765625, learning_rate 0.00206021
2017-10-10T12:47:38.881085: step 226, loss 0.549018, acc 0.8125, learning_rate 0.00205221
2017-10-10T12:47:39.436886: step 227, loss 0.736848, acc 0.8125, learning_rate 0.00204424
2017-10-10T12:47:40.033655: step 228, loss 0.683639, acc 0.796875, learning_rate 0.0020363
2017-10-10T12:47:40.441000: step 229, loss 0.683696, acc 0.734375, learning_rate 0.0020284
2017-10-10T12:47:40.888394: step 230, loss 1.09974, acc 0.640625, learning_rate 0.00202053
2017-10-10T12:47:41.344137: step 231, loss 0.964218, acc 0.703125, learning_rate 0.00201269
2017-10-10T12:47:41.966546: step 232, loss 0.901997, acc 0.734375, learning_rate 0.00200488
2017-10-10T12:47:42.438121: step 233, loss 0.580635, acc 0.78125, learning_rate 0.00199711
2017-10-10T12:47:42.861608: step 234, loss 0.558676, acc 0.890625, learning_rate 0.00198936
2017-10-10T12:47:43.288898: step 235, loss 0.651405, acc 0.78125, learning_rate 0.00198165
2017-10-10T12:47:43.769964: step 236, loss 0.667412, acc 0.765625, learning_rate 0.00197397
2017-10-10T12:47:44.338124: step 237, loss 0.672562, acc 0.78125, learning_rate 0.00196632
2017-10-10T12:47:44.868114: step 238, loss 0.636831, acc 0.765625, learning_rate 0.0019587
2017-10-10T12:47:45.421162: step 239, loss 0.977468, acc 0.703125, learning_rate 0.00195112
2017-10-10T12:47:45.964675: step 240, loss 0.589567, acc 0.765625, learning_rate 0.00194356

Evaluation:
2017-10-10T12:47:47.210452: step 240, loss 0.333362, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-240

2017-10-10T12:47:48.932991: step 241, loss 0.66163, acc 0.796875, learning_rate 0.00193604
2017-10-10T12:47:49.461094: step 242, loss 0.742047, acc 0.6875, learning_rate 0.00192854
2017-10-10T12:47:49.952990: step 243, loss 0.953563, acc 0.78125, learning_rate 0.00192108
2017-10-10T12:47:50.363953: step 244, loss 0.651789, acc 0.78125, learning_rate 0.00191364
2017-10-10T12:47:50.891647: step 245, loss 0.950426, acc 0.671875, learning_rate 0.00190624
2017-10-10T12:47:51.373367: step 246, loss 0.684587, acc 0.78125, learning_rate 0.00189887
2017-10-10T12:47:51.885689: step 247, loss 0.88319, acc 0.703125, learning_rate 0.00189153
2017-10-10T12:47:52.446967: step 248, loss 0.560865, acc 0.84375, learning_rate 0.00188421
2017-10-10T12:47:52.968293: step 249, loss 0.585336, acc 0.828125, learning_rate 0.00187693
2017-10-10T12:47:53.417128: step 250, loss 0.710133, acc 0.765625, learning_rate 0.00186968
2017-10-10T12:47:53.789081: step 251, loss 0.589527, acc 0.78125, learning_rate 0.00186245
2017-10-10T12:47:54.320865: step 252, loss 0.504352, acc 0.828125, learning_rate 0.00185526
2017-10-10T12:47:54.841441: step 253, loss 0.780187, acc 0.734375, learning_rate 0.0018481
2017-10-10T12:47:55.372574: step 254, loss 0.647191, acc 0.796875, learning_rate 0.00184096
2017-10-10T12:47:55.912895: step 255, loss 0.523798, acc 0.78125, learning_rate 0.00183385
2017-10-10T12:47:56.480875: step 256, loss 0.705246, acc 0.71875, learning_rate 0.00182678
2017-10-10T12:47:57.088929: step 257, loss 0.615917, acc 0.828125, learning_rate 0.00181973
2017-10-10T12:47:57.592855: step 258, loss 0.421908, acc 0.921875, learning_rate 0.00181271
2017-10-10T12:47:58.168963: step 259, loss 0.592449, acc 0.765625, learning_rate 0.00180572
2017-10-10T12:47:58.684926: step 260, loss 0.579428, acc 0.8125, learning_rate 0.00179876
2017-10-10T12:47:59.149085: step 261, loss 0.845033, acc 0.796875, learning_rate 0.00179182
2017-10-10T12:47:59.626499: step 262, loss 0.641511, acc 0.78125, learning_rate 0.00178492
2017-10-10T12:48:00.197269: step 263, loss 0.601321, acc 0.75, learning_rate 0.00177804
2017-10-10T12:48:00.633834: step 264, loss 0.695119, acc 0.796875, learning_rate 0.00177119
2017-10-10T12:48:01.181043: step 265, loss 0.59951, acc 0.84375, learning_rate 0.00176437
2017-10-10T12:48:01.721477: step 266, loss 0.505131, acc 0.796875, learning_rate 0.00175758
2017-10-10T12:48:02.335990: step 267, loss 0.845288, acc 0.75, learning_rate 0.00175081
2017-10-10T12:48:02.921530: step 268, loss 0.519346, acc 0.859375, learning_rate 0.00174407
2017-10-10T12:48:03.366484: step 269, loss 0.948343, acc 0.765625, learning_rate 0.00173736
2017-10-10T12:48:03.756834: step 270, loss 0.630336, acc 0.8125, learning_rate 0.00173068
2017-10-10T12:48:04.282286: step 271, loss 0.633803, acc 0.78125, learning_rate 0.00172402
2017-10-10T12:48:04.904998: step 272, loss 0.506464, acc 0.84375, learning_rate 0.00171739
2017-10-10T12:48:05.510274: step 273, loss 0.510775, acc 0.828125, learning_rate 0.00171079
2017-10-10T12:48:05.954449: step 274, loss 1.00406, acc 0.6875, learning_rate 0.00170422
2017-10-10T12:48:06.416459: step 275, loss 0.596716, acc 0.859375, learning_rate 0.00169767
2017-10-10T12:48:06.906316: step 276, loss 0.791753, acc 0.78125, learning_rate 0.00169115
2017-10-10T12:48:07.345161: step 277, loss 1.16403, acc 0.734375, learning_rate 0.00168465
2017-10-10T12:48:07.829669: step 278, loss 0.869474, acc 0.75, learning_rate 0.00167818
2017-10-10T12:48:08.325202: step 279, loss 0.733958, acc 0.765625, learning_rate 0.00167174
2017-10-10T12:48:08.901095: step 280, loss 0.485969, acc 0.859375, learning_rate 0.00166533

Evaluation:
2017-10-10T12:48:10.144918: step 280, loss 0.341186, acc 0.879137

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-280

2017-10-10T12:48:11.837884: step 281, loss 0.835003, acc 0.71875, learning_rate 0.00165894
2017-10-10T12:48:12.357849: step 282, loss 0.843967, acc 0.765625, learning_rate 0.00165257
2017-10-10T12:48:12.915677: step 283, loss 0.667228, acc 0.8125, learning_rate 0.00164624
2017-10-10T12:48:13.465186: step 284, loss 0.708445, acc 0.84375, learning_rate 0.00163993
2017-10-10T12:48:14.045001: step 285, loss 0.901736, acc 0.765625, learning_rate 0.00163364
2017-10-10T12:48:14.466670: step 286, loss 0.456175, acc 0.8125, learning_rate 0.00162738
2017-10-10T12:48:15.013000: step 287, loss 0.401019, acc 0.828125, learning_rate 0.00162115
2017-10-10T12:48:15.518753: step 288, loss 0.818704, acc 0.78125, learning_rate 0.00161494
2017-10-10T12:48:16.181201: step 289, loss 0.569211, acc 0.828125, learning_rate 0.00160875
2017-10-10T12:48:16.601012: step 290, loss 0.696225, acc 0.734375, learning_rate 0.00160259
2017-10-10T12:48:17.052920: step 291, loss 0.649825, acc 0.8125, learning_rate 0.00159646
2017-10-10T12:48:17.537998: step 292, loss 0.528254, acc 0.796875, learning_rate 0.00159035
2017-10-10T12:48:18.064283: step 293, loss 0.512722, acc 0.859375, learning_rate 0.00158427
2017-10-10T12:48:18.556848: step 294, loss 0.788586, acc 0.784314, learning_rate 0.00157821
2017-10-10T12:48:19.164880: step 295, loss 0.620418, acc 0.71875, learning_rate 0.00157218
2017-10-10T12:48:19.748940: step 296, loss 0.790021, acc 0.796875, learning_rate 0.00156617
2017-10-10T12:48:20.285179: step 297, loss 0.545124, acc 0.796875, learning_rate 0.00156018
2017-10-10T12:48:20.775145: step 298, loss 0.399354, acc 0.84375, learning_rate 0.00155422
2017-10-10T12:48:21.334043: step 299, loss 0.63716, acc 0.765625, learning_rate 0.00154829
2017-10-10T12:48:21.859502: step 300, loss 0.517485, acc 0.796875, learning_rate 0.00154238
2017-10-10T12:48:22.373498: step 301, loss 0.962964, acc 0.671875, learning_rate 0.00153649
2017-10-10T12:48:22.905176: step 302, loss 0.69455, acc 0.734375, learning_rate 0.00153063
2017-10-10T12:48:23.378338: step 303, loss 0.738055, acc 0.75, learning_rate 0.00152479
2017-10-10T12:48:23.925339: step 304, loss 0.681438, acc 0.8125, learning_rate 0.00151897
2017-10-10T12:48:24.470710: step 305, loss 0.537798, acc 0.859375, learning_rate 0.00151318
2017-10-10T12:48:24.967192: step 306, loss 0.699776, acc 0.734375, learning_rate 0.00150741
2017-10-10T12:48:25.523706: step 307, loss 0.890939, acc 0.703125, learning_rate 0.00150167
2017-10-10T12:48:26.153402: step 308, loss 0.625319, acc 0.8125, learning_rate 0.00149594
2017-10-10T12:48:26.658610: step 309, loss 0.567868, acc 0.796875, learning_rate 0.00149025
2017-10-10T12:48:27.152883: step 310, loss 0.360471, acc 0.875, learning_rate 0.00148457
2017-10-10T12:48:27.785148: step 311, loss 0.582381, acc 0.8125, learning_rate 0.00147892
2017-10-10T12:48:28.265560: step 312, loss 0.831296, acc 0.71875, learning_rate 0.00147329
2017-10-10T12:48:28.696376: step 313, loss 0.653265, acc 0.8125, learning_rate 0.00146769
2017-10-10T12:48:29.165674: step 314, loss 0.683041, acc 0.75, learning_rate 0.0014621
2017-10-10T12:48:29.692972: step 315, loss 0.5154, acc 0.859375, learning_rate 0.00145654
2017-10-10T12:48:30.242586: step 316, loss 0.813563, acc 0.828125, learning_rate 0.00145101
2017-10-10T12:48:30.752919: step 317, loss 0.768996, acc 0.765625, learning_rate 0.00144549
2017-10-10T12:48:31.273451: step 318, loss 0.635939, acc 0.796875, learning_rate 0.00144
2017-10-10T12:48:31.755557: step 319, loss 0.551153, acc 0.734375, learning_rate 0.00143453
2017-10-10T12:48:32.336938: step 320, loss 0.863304, acc 0.75, learning_rate 0.00142908

Evaluation:
2017-10-10T12:48:33.508828: step 320, loss 0.341104, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-320

2017-10-10T12:48:35.179553: step 321, loss 0.746684, acc 0.796875, learning_rate 0.00142366
2017-10-10T12:48:35.696830: step 322, loss 0.500937, acc 0.78125, learning_rate 0.00141826
2017-10-10T12:48:36.260688: step 323, loss 0.775655, acc 0.78125, learning_rate 0.00141288
2017-10-10T12:48:36.798457: step 324, loss 0.698192, acc 0.703125, learning_rate 0.00140752
2017-10-10T12:48:37.348244: step 325, loss 0.849249, acc 0.75, learning_rate 0.00140218
2017-10-10T12:48:37.857020: step 326, loss 0.419623, acc 0.84375, learning_rate 0.00139686
2017-10-10T12:48:38.421099: step 327, loss 0.491546, acc 0.828125, learning_rate 0.00139157
2017-10-10T12:48:38.989193: step 328, loss 0.686346, acc 0.734375, learning_rate 0.0013863
2017-10-10T12:48:39.402517: step 329, loss 0.473697, acc 0.796875, learning_rate 0.00138105
2017-10-10T12:48:39.824525: step 330, loss 0.696128, acc 0.796875, learning_rate 0.00137582
2017-10-10T12:48:40.239725: step 331, loss 0.767139, acc 0.75, learning_rate 0.00137061
2017-10-10T12:48:40.781200: step 332, loss 0.431745, acc 0.796875, learning_rate 0.00136543
2017-10-10T12:48:41.327679: step 333, loss 0.522747, acc 0.796875, learning_rate 0.00136026
2017-10-10T12:48:41.836875: step 334, loss 0.758245, acc 0.75, learning_rate 0.00135512
2017-10-10T12:48:42.401757: step 335, loss 0.5183, acc 0.8125, learning_rate 0.00134999
2017-10-10T12:48:42.955206: step 336, loss 0.658404, acc 0.8125, learning_rate 0.00134489
2017-10-10T12:48:43.504561: step 337, loss 0.730102, acc 0.75, learning_rate 0.00133981
2017-10-10T12:48:44.060692: step 338, loss 0.74017, acc 0.78125, learning_rate 0.00133475
2017-10-10T12:48:44.590469: step 339, loss 0.69073, acc 0.78125, learning_rate 0.00132971
2017-10-10T12:48:45.104811: step 340, loss 0.706568, acc 0.765625, learning_rate 0.00132469
2017-10-10T12:48:45.675514: step 341, loss 0.606723, acc 0.78125, learning_rate 0.00131969
2017-10-10T12:48:46.208936: step 342, loss 0.468374, acc 0.828125, learning_rate 0.00131471
2017-10-10T12:48:46.775114: step 343, loss 0.400095, acc 0.84375, learning_rate 0.00130975
2017-10-10T12:48:47.325336: step 344, loss 0.526666, acc 0.875, learning_rate 0.00130482
2017-10-10T12:48:47.885022: step 345, loss 0.846373, acc 0.734375, learning_rate 0.0012999
2017-10-10T12:48:48.533103: step 346, loss 0.503129, acc 0.71875, learning_rate 0.001295
2017-10-10T12:48:48.981679: step 347, loss 0.699091, acc 0.765625, learning_rate 0.00129012
2017-10-10T12:48:49.424237: step 348, loss 0.828444, acc 0.78125, learning_rate 0.00128527
2017-10-10T12:48:49.886158: step 349, loss 0.491489, acc 0.828125, learning_rate 0.00128043
2017-10-10T12:48:50.475151: step 350, loss 0.407515, acc 0.859375, learning_rate 0.00127561
2017-10-10T12:48:51.056838: step 351, loss 0.410646, acc 0.796875, learning_rate 0.00127081
2017-10-10T12:48:51.533630: step 352, loss 0.632312, acc 0.75, learning_rate 0.00126603
2017-10-10T12:48:52.048829: step 353, loss 0.927682, acc 0.6875, learning_rate 0.00126127
2017-10-10T12:48:52.550857: step 354, loss 0.468244, acc 0.78125, learning_rate 0.00125653
2017-10-10T12:48:52.964889: step 355, loss 0.417352, acc 0.890625, learning_rate 0.00125181
2017-10-10T12:48:53.485304: step 356, loss 0.523757, acc 0.8125, learning_rate 0.00124711
2017-10-10T12:48:53.989075: step 357, loss 0.498592, acc 0.828125, learning_rate 0.00124243
2017-10-10T12:48:54.548243: step 358, loss 0.568538, acc 0.828125, learning_rate 0.00123777
2017-10-10T12:48:55.056858: step 359, loss 0.526137, acc 0.8125, learning_rate 0.00123312
2017-10-10T12:48:55.609736: step 360, loss 0.594191, acc 0.828125, learning_rate 0.0012285

Evaluation:
2017-10-10T12:48:56.752094: step 360, loss 0.328949, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-360

2017-10-10T12:48:58.364825: step 361, loss 0.537041, acc 0.8125, learning_rate 0.00122389
2017-10-10T12:48:58.958866: step 362, loss 0.544148, acc 0.765625, learning_rate 0.0012193
2017-10-10T12:48:59.480866: step 363, loss 0.620765, acc 0.78125, learning_rate 0.00121473
2017-10-10T12:48:59.979741: step 364, loss 0.635088, acc 0.828125, learning_rate 0.00121018
2017-10-10T12:49:00.500851: step 365, loss 0.428409, acc 0.875, learning_rate 0.00120565
2017-10-10T12:49:01.045841: step 366, loss 0.420626, acc 0.84375, learning_rate 0.00120114
2017-10-10T12:49:01.612855: step 367, loss 0.560835, acc 0.765625, learning_rate 0.00119664
2017-10-10T12:49:02.174810: step 368, loss 0.509363, acc 0.84375, learning_rate 0.00119217
2017-10-10T12:49:02.622140: step 369, loss 0.516852, acc 0.84375, learning_rate 0.00118771
2017-10-10T12:49:03.094965: step 370, loss 0.590495, acc 0.8125, learning_rate 0.00118327
2017-10-10T12:49:03.687561: step 371, loss 0.450172, acc 0.84375, learning_rate 0.00117885
2017-10-10T12:49:04.220168: step 372, loss 0.522413, acc 0.8125, learning_rate 0.00117445
2017-10-10T12:49:04.745151: step 373, loss 0.470769, acc 0.84375, learning_rate 0.00117006
2017-10-10T12:49:05.297176: step 374, loss 0.729293, acc 0.6875, learning_rate 0.00116569
2017-10-10T12:49:05.819776: step 375, loss 0.627972, acc 0.8125, learning_rate 0.00116134
2017-10-10T12:49:06.317018: step 376, loss 0.473317, acc 0.859375, learning_rate 0.00115701
2017-10-10T12:49:06.796875: step 377, loss 0.596663, acc 0.78125, learning_rate 0.0011527
2017-10-10T12:49:07.322608: step 378, loss 0.500327, acc 0.828125, learning_rate 0.0011484
2017-10-10T12:49:07.827446: step 379, loss 0.48189, acc 0.828125, learning_rate 0.00114412
2017-10-10T12:49:08.338537: step 380, loss 0.474785, acc 0.78125, learning_rate 0.00113986
2017-10-10T12:49:08.872984: step 381, loss 0.605693, acc 0.84375, learning_rate 0.00113561
2017-10-10T12:49:09.360912: step 382, loss 0.580469, acc 0.8125, learning_rate 0.00113139
2017-10-10T12:49:09.904942: step 383, loss 0.595709, acc 0.859375, learning_rate 0.00112718
2017-10-10T12:49:10.488966: step 384, loss 0.473577, acc 0.875, learning_rate 0.00112298
2017-10-10T12:49:10.964917: step 385, loss 0.51719, acc 0.796875, learning_rate 0.00111881
2017-10-10T12:49:11.508292: step 386, loss 0.663815, acc 0.8125, learning_rate 0.00111465
2017-10-10T12:49:11.988099: step 387, loss 0.471527, acc 0.828125, learning_rate 0.00111051
2017-10-10T12:49:12.408861: step 388, loss 0.624304, acc 0.78125, learning_rate 0.00110638
2017-10-10T12:49:12.929159: step 389, loss 0.541184, acc 0.765625, learning_rate 0.00110228
2017-10-10T12:49:13.504872: step 390, loss 0.589642, acc 0.796875, learning_rate 0.00109818
2017-10-10T12:49:14.145188: step 391, loss 0.424339, acc 0.84375, learning_rate 0.00109411
2017-10-10T12:49:14.557612: step 392, loss 0.419964, acc 0.862745, learning_rate 0.00109005
2017-10-10T12:49:14.962978: step 393, loss 0.595917, acc 0.84375, learning_rate 0.00108601
2017-10-10T12:49:15.377096: step 394, loss 0.480266, acc 0.921875, learning_rate 0.00108199
2017-10-10T12:49:15.783645: step 395, loss 0.621876, acc 0.8125, learning_rate 0.00107798
2017-10-10T12:49:16.296931: step 396, loss 0.415825, acc 0.84375, learning_rate 0.00107399
2017-10-10T12:49:16.793059: step 397, loss 0.607463, acc 0.765625, learning_rate 0.00107001
2017-10-10T12:49:17.381155: step 398, loss 0.450425, acc 0.875, learning_rate 0.00106605
2017-10-10T12:49:17.850286: step 399, loss 0.567453, acc 0.859375, learning_rate 0.00106211
2017-10-10T12:49:18.433538: step 400, loss 0.56514, acc 0.859375, learning_rate 0.00105818

Evaluation:
2017-10-10T12:49:19.608307: step 400, loss 0.325894, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-400

2017-10-10T12:49:21.020575: step 401, loss 0.374171, acc 0.84375, learning_rate 0.00105427
2017-10-10T12:49:21.437398: step 402, loss 0.765731, acc 0.75, learning_rate 0.00105037
2017-10-10T12:49:21.972415: step 403, loss 0.618005, acc 0.78125, learning_rate 0.0010465
2017-10-10T12:49:22.520867: step 404, loss 0.705239, acc 0.828125, learning_rate 0.00104263
2017-10-10T12:49:23.060841: step 405, loss 0.483551, acc 0.84375, learning_rate 0.00103878
2017-10-10T12:49:23.599146: step 406, loss 0.542124, acc 0.828125, learning_rate 0.00103495
2017-10-10T12:49:24.149006: step 407, loss 0.886413, acc 0.703125, learning_rate 0.00103114
2017-10-10T12:49:24.753654: step 408, loss 0.373136, acc 0.875, learning_rate 0.00102734
2017-10-10T12:49:25.288975: step 409, loss 0.444604, acc 0.84375, learning_rate 0.00102355
2017-10-10T12:49:25.744874: step 410, loss 0.615749, acc 0.828125, learning_rate 0.00101978
2017-10-10T12:49:26.190485: step 411, loss 0.611569, acc 0.78125, learning_rate 0.00101603
2017-10-10T12:49:26.632294: step 412, loss 0.628579, acc 0.796875, learning_rate 0.00101229
2017-10-10T12:49:27.188848: step 413, loss 0.812878, acc 0.75, learning_rate 0.00100856
2017-10-10T12:49:27.768925: step 414, loss 0.540505, acc 0.796875, learning_rate 0.00100486
2017-10-10T12:49:28.264996: step 415, loss 0.533777, acc 0.84375, learning_rate 0.00100116
2017-10-10T12:49:28.814103: step 416, loss 0.554533, acc 0.765625, learning_rate 0.000997483
2017-10-10T12:49:29.351219: step 417, loss 0.544352, acc 0.828125, learning_rate 0.00099382
2017-10-10T12:49:29.931005: step 418, loss 0.549328, acc 0.859375, learning_rate 0.000990172
2017-10-10T12:49:30.447514: step 419, loss 0.471713, acc 0.8125, learning_rate 0.000986538
2017-10-10T12:49:30.817253: step 420, loss 0.645804, acc 0.796875, learning_rate 0.00098292
2017-10-10T12:49:31.389369: step 421, loss 0.456163, acc 0.890625, learning_rate 0.000979316
2017-10-10T12:49:31.933257: step 422, loss 0.698986, acc 0.796875, learning_rate 0.000975727
2017-10-10T12:49:32.425143: step 423, loss 0.54821, acc 0.78125, learning_rate 0.000972152
2017-10-10T12:49:32.920916: step 424, loss 0.528529, acc 0.8125, learning_rate 0.000968592
2017-10-10T12:49:33.372858: step 425, loss 0.555131, acc 0.796875, learning_rate 0.000965047
2017-10-10T12:49:34.005035: step 426, loss 0.611921, acc 0.828125, learning_rate 0.000961516
2017-10-10T12:49:34.605155: step 427, loss 0.341041, acc 0.90625, learning_rate 0.000958
2017-10-10T12:49:35.050828: step 428, loss 0.411012, acc 0.890625, learning_rate 0.000954497
2017-10-10T12:49:35.495503: step 429, loss 0.886784, acc 0.6875, learning_rate 0.00095101
2017-10-10T12:49:36.012896: step 430, loss 0.660204, acc 0.78125, learning_rate 0.000947536
2017-10-10T12:49:36.643234: step 431, loss 0.492382, acc 0.890625, learning_rate 0.000944076
2017-10-10T12:49:37.247264: step 432, loss 0.415583, acc 0.875, learning_rate 0.000940631
2017-10-10T12:49:37.677027: step 433, loss 0.408558, acc 0.859375, learning_rate 0.0009372
2017-10-10T12:49:38.076807: step 434, loss 0.455655, acc 0.828125, learning_rate 0.000933783
2017-10-10T12:49:38.565296: step 435, loss 0.622644, acc 0.8125, learning_rate 0.000930379
2017-10-10T12:49:39.133276: step 436, loss 0.625653, acc 0.84375, learning_rate 0.00092699
2017-10-10T12:49:39.668976: step 437, loss 0.302309, acc 0.90625, learning_rate 0.000923614
2017-10-10T12:49:40.224920: step 438, loss 0.591001, acc 0.8125, learning_rate 0.000920253
2017-10-10T12:49:40.794967: step 439, loss 0.433255, acc 0.84375, learning_rate 0.000916905
2017-10-10T12:49:41.315270: step 440, loss 0.611504, acc 0.796875, learning_rate 0.00091357

Evaluation:
2017-10-10T12:49:42.365009: step 440, loss 0.328012, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-440

2017-10-10T12:49:43.960731: step 441, loss 0.460168, acc 0.859375, learning_rate 0.000910249
2017-10-10T12:49:44.499283: step 442, loss 0.361643, acc 0.875, learning_rate 0.000906942
2017-10-10T12:49:45.092253: step 443, loss 0.471692, acc 0.859375, learning_rate 0.000903648
2017-10-10T12:49:45.648832: step 444, loss 0.599858, acc 0.8125, learning_rate 0.000900368
2017-10-10T12:49:46.208444: step 445, loss 0.795788, acc 0.78125, learning_rate 0.000897101
2017-10-10T12:49:46.764173: step 446, loss 0.393396, acc 0.875, learning_rate 0.000893848
2017-10-10T12:49:47.352999: step 447, loss 0.33545, acc 0.84375, learning_rate 0.000890607
2017-10-10T12:49:47.911625: step 448, loss 0.785803, acc 0.78125, learning_rate 0.00088738
2017-10-10T12:49:48.651998: step 449, loss 0.694151, acc 0.796875, learning_rate 0.000884166
2017-10-10T12:49:49.097007: step 450, loss 0.573919, acc 0.875, learning_rate 0.000880966
2017-10-10T12:49:49.516538: step 451, loss 0.348154, acc 0.890625, learning_rate 0.000877778
2017-10-10T12:49:50.020852: step 452, loss 0.617788, acc 0.8125, learning_rate 0.000874603
2017-10-10T12:49:50.545567: step 453, loss 0.527972, acc 0.875, learning_rate 0.000871441
2017-10-10T12:49:51.086074: step 454, loss 0.534212, acc 0.875, learning_rate 0.000868293
2017-10-10T12:49:51.621057: step 455, loss 0.836767, acc 0.71875, learning_rate 0.000865157
2017-10-10T12:49:52.224316: step 456, loss 0.542621, acc 0.78125, learning_rate 0.000862033
2017-10-10T12:49:52.787157: step 457, loss 0.527843, acc 0.875, learning_rate 0.000858923
2017-10-10T12:49:53.376841: step 458, loss 0.548971, acc 0.796875, learning_rate 0.000855825
2017-10-10T12:49:53.972305: step 459, loss 0.540176, acc 0.828125, learning_rate 0.00085274
2017-10-10T12:49:54.532367: step 460, loss 0.318214, acc 0.890625, learning_rate 0.000849668
2017-10-10T12:49:55.128936: step 461, loss 0.464039, acc 0.890625, learning_rate 0.000846608
2017-10-10T12:49:55.676978: step 462, loss 0.780854, acc 0.703125, learning_rate 0.00084356
2017-10-10T12:49:56.297096: step 463, loss 0.639473, acc 0.734375, learning_rate 0.000840525
2017-10-10T12:49:56.815696: step 464, loss 0.393324, acc 0.84375, learning_rate 0.000837502
2017-10-10T12:49:57.416016: step 465, loss 0.530239, acc 0.84375, learning_rate 0.000834492
2017-10-10T12:49:57.869008: step 466, loss 0.414981, acc 0.84375, learning_rate 0.000831494
2017-10-10T12:49:58.328767: step 467, loss 0.610487, acc 0.78125, learning_rate 0.000828508
2017-10-10T12:49:58.720824: step 468, loss 0.477513, acc 0.796875, learning_rate 0.000825535
2017-10-10T12:49:59.291744: step 469, loss 0.724108, acc 0.734375, learning_rate 0.000822573
2017-10-10T12:49:59.726583: step 470, loss 0.483747, acc 0.84375, learning_rate 0.000819624
2017-10-10T12:50:00.303533: step 471, loss 0.442428, acc 0.875, learning_rate 0.000816687
2017-10-10T12:50:00.829951: step 472, loss 0.378178, acc 0.84375, learning_rate 0.000813761
2017-10-10T12:50:01.317950: step 473, loss 0.408984, acc 0.84375, learning_rate 0.000810848
2017-10-10T12:50:01.716838: step 474, loss 0.476206, acc 0.875, learning_rate 0.000807946
2017-10-10T12:50:02.288076: step 475, loss 0.694929, acc 0.75, learning_rate 0.000805057
2017-10-10T12:50:02.832820: step 476, loss 0.481473, acc 0.8125, learning_rate 0.000802179
2017-10-10T12:50:03.426014: step 477, loss 0.343875, acc 0.875, learning_rate 0.000799313
2017-10-10T12:50:03.995984: step 478, loss 0.532409, acc 0.828125, learning_rate 0.000796458
2017-10-10T12:50:04.555849: step 479, loss 0.760362, acc 0.6875, learning_rate 0.000793616
2017-10-10T12:50:05.049251: step 480, loss 0.588927, acc 0.8125, learning_rate 0.000790784

Evaluation:
2017-10-10T12:50:06.192978: step 480, loss 0.320965, acc 0.879137

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-480

2017-10-10T12:50:08.069203: step 481, loss 0.629969, acc 0.8125, learning_rate 0.000787965
2017-10-10T12:50:08.597091: step 482, loss 0.590814, acc 0.796875, learning_rate 0.000785157
2017-10-10T12:50:09.098977: step 483, loss 0.534481, acc 0.78125, learning_rate 0.00078236
2017-10-10T12:50:09.671403: step 484, loss 0.350747, acc 0.921875, learning_rate 0.000779575
2017-10-10T12:50:10.167912: step 485, loss 0.285174, acc 0.953125, learning_rate 0.000776801
2017-10-10T12:50:10.671502: step 486, loss 0.544832, acc 0.828125, learning_rate 0.000774038
2017-10-10T12:50:11.137027: step 487, loss 0.536348, acc 0.8125, learning_rate 0.000771287
2017-10-10T12:50:11.572819: step 488, loss 0.413037, acc 0.859375, learning_rate 0.000768547
2017-10-10T12:50:12.024853: step 489, loss 0.562208, acc 0.78125, learning_rate 0.000765818
2017-10-10T12:50:12.472872: step 490, loss 0.662228, acc 0.823529, learning_rate 0.000763101
2017-10-10T12:50:12.945055: step 491, loss 0.525591, acc 0.859375, learning_rate 0.000760394
2017-10-10T12:50:13.473072: step 492, loss 0.492726, acc 0.84375, learning_rate 0.000757698
2017-10-10T12:50:14.082409: step 493, loss 0.461923, acc 0.875, learning_rate 0.000755014
2017-10-10T12:50:14.653955: step 494, loss 0.526192, acc 0.84375, learning_rate 0.00075234
2017-10-10T12:50:15.199797: step 495, loss 0.476195, acc 0.859375, learning_rate 0.000749677
2017-10-10T12:50:15.705793: step 496, loss 0.536682, acc 0.796875, learning_rate 0.000747026
2017-10-10T12:50:16.237116: step 497, loss 0.825648, acc 0.71875, learning_rate 0.000744385
2017-10-10T12:50:16.732937: step 498, loss 0.40679, acc 0.859375, learning_rate 0.000741754
2017-10-10T12:50:17.284829: step 499, loss 0.437684, acc 0.875, learning_rate 0.000739135
2017-10-10T12:50:17.813166: step 500, loss 0.335478, acc 0.90625, learning_rate 0.000736526
2017-10-10T12:50:18.311862: step 501, loss 0.397641, acc 0.890625, learning_rate 0.000733928
2017-10-10T12:50:18.872855: step 502, loss 0.747277, acc 0.828125, learning_rate 0.00073134
2017-10-10T12:50:19.338435: step 503, loss 0.557766, acc 0.828125, learning_rate 0.000728763
2017-10-10T12:50:19.949463: step 504, loss 0.380933, acc 0.859375, learning_rate 0.000726197
2017-10-10T12:50:20.519153: step 505, loss 0.578053, acc 0.828125, learning_rate 0.000723641
2017-10-10T12:50:21.013675: step 506, loss 0.55988, acc 0.84375, learning_rate 0.000721095
2017-10-10T12:50:21.488837: step 507, loss 0.566325, acc 0.78125, learning_rate 0.00071856
2017-10-10T12:50:21.943365: step 508, loss 0.45933, acc 0.828125, learning_rate 0.000716036
2017-10-10T12:50:22.552933: step 509, loss 0.744906, acc 0.8125, learning_rate 0.000713521
2017-10-10T12:50:23.148937: step 510, loss 0.451219, acc 0.828125, learning_rate 0.000711017
2017-10-10T12:50:23.650754: step 511, loss 0.495677, acc 0.859375, learning_rate 0.000708523
2017-10-10T12:50:24.072820: step 512, loss 0.591544, acc 0.78125, learning_rate 0.000706039
2017-10-10T12:50:24.472997: step 513, loss 0.826069, acc 0.703125, learning_rate 0.000703565
2017-10-10T12:50:25.024853: step 514, loss 0.657637, acc 0.8125, learning_rate 0.000701102
2017-10-10T12:50:25.503826: step 515, loss 0.619891, acc 0.8125, learning_rate 0.000698648
2017-10-10T12:50:26.041054: step 516, loss 0.688372, acc 0.78125, learning_rate 0.000696204
2017-10-10T12:50:26.592854: step 517, loss 0.380716, acc 0.84375, learning_rate 0.000693771
2017-10-10T12:50:27.168819: step 518, loss 0.633913, acc 0.8125, learning_rate 0.000691347
2017-10-10T12:50:27.679336: step 519, loss 0.414083, acc 0.875, learning_rate 0.000688934
2017-10-10T12:50:28.252818: step 520, loss 0.47375, acc 0.84375, learning_rate 0.00068653

Evaluation:
2017-10-10T12:50:29.499521: step 520, loss 0.314829, acc 0.879137

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-520

2017-10-10T12:50:31.033042: step 521, loss 0.659638, acc 0.75, learning_rate 0.000684136
2017-10-10T12:50:31.519835: step 522, loss 0.739197, acc 0.796875, learning_rate 0.000681751
2017-10-10T12:50:32.009216: step 523, loss 0.553658, acc 0.828125, learning_rate 0.000679377
2017-10-10T12:50:32.520402: step 524, loss 0.457609, acc 0.859375, learning_rate 0.000677012
2017-10-10T12:50:33.005005: step 525, loss 0.67091, acc 0.8125, learning_rate 0.000674657
2017-10-10T12:50:33.618632: step 526, loss 0.645277, acc 0.796875, learning_rate 0.000672311
2017-10-10T12:50:34.217261: step 527, loss 0.430697, acc 0.90625, learning_rate 0.000669975
2017-10-10T12:50:34.678953: step 528, loss 0.52514, acc 0.84375, learning_rate 0.000667648
2017-10-10T12:50:35.144845: step 529, loss 0.498259, acc 0.828125, learning_rate 0.000665331
2017-10-10T12:50:35.644955: step 530, loss 0.417481, acc 0.90625, learning_rate 0.000663024
2017-10-10T12:50:36.236710: step 531, loss 0.747452, acc 0.765625, learning_rate 0.000660726
2017-10-10T12:50:36.763956: step 532, loss 0.678326, acc 0.75, learning_rate 0.000658437
2017-10-10T12:50:37.356919: step 533, loss 0.511094, acc 0.828125, learning_rate 0.000656158
2017-10-10T12:50:37.869035: step 534, loss 0.558957, acc 0.75, learning_rate 0.000653888
2017-10-10T12:50:38.488899: step 535, loss 0.602641, acc 0.8125, learning_rate 0.000651627
2017-10-10T12:50:39.011709: step 536, loss 0.546349, acc 0.796875, learning_rate 0.000649375
2017-10-10T12:50:39.540927: step 537, loss 0.658503, acc 0.734375, learning_rate 0.000647133
2017-10-10T12:50:40.072542: step 538, loss 0.342152, acc 0.875, learning_rate 0.000644899
2017-10-10T12:50:40.560825: step 539, loss 0.576399, acc 0.84375, learning_rate 0.000642675
2017-10-10T12:50:41.094328: step 540, loss 0.431747, acc 0.875, learning_rate 0.00064046
2017-10-10T12:50:41.632913: step 541, loss 0.797323, acc 0.75, learning_rate 0.000638254
2017-10-10T12:50:42.136992: step 542, loss 0.616956, acc 0.8125, learning_rate 0.000636057
2017-10-10T12:50:42.724901: step 543, loss 0.341131, acc 0.921875, learning_rate 0.000633869
2017-10-10T12:50:43.325157: step 544, loss 0.537695, acc 0.859375, learning_rate 0.00063169
2017-10-10T12:50:43.809000: step 545, loss 0.441352, acc 0.828125, learning_rate 0.00062952
2017-10-10T12:50:44.240164: step 546, loss 0.305989, acc 0.84375, learning_rate 0.000627358
2017-10-10T12:50:44.780894: step 547, loss 0.391727, acc 0.84375, learning_rate 0.000625206
2017-10-10T12:50:45.269057: step 548, loss 0.687881, acc 0.75, learning_rate 0.000623062
2017-10-10T12:50:45.889149: step 549, loss 0.610609, acc 0.765625, learning_rate 0.000620927
2017-10-10T12:50:46.313021: step 550, loss 0.414572, acc 0.859375, learning_rate 0.000618801
2017-10-10T12:50:46.677383: step 551, loss 0.409655, acc 0.921875, learning_rate 0.000616683
2017-10-10T12:50:47.062393: step 552, loss 0.386171, acc 0.90625, learning_rate 0.000614574
2017-10-10T12:50:47.608998: step 553, loss 0.650209, acc 0.828125, learning_rate 0.000612474
2017-10-10T12:50:48.176176: step 554, loss 0.484053, acc 0.84375, learning_rate 0.000610382
2017-10-10T12:50:48.723130: step 555, loss 0.349548, acc 0.875, learning_rate 0.000608299
2017-10-10T12:50:49.289137: step 556, loss 0.41267, acc 0.84375, learning_rate 0.000606224
2017-10-10T12:50:49.817139: step 557, loss 0.327829, acc 0.890625, learning_rate 0.000604158
2017-10-10T12:50:50.284867: step 558, loss 0.420529, acc 0.859375, learning_rate 0.0006021
2017-10-10T12:50:50.826234: step 559, loss 0.535968, acc 0.796875, learning_rate 0.00060005
2017-10-10T12:50:51.415638: step 560, loss 0.326834, acc 0.859375, learning_rate 0.000598009

Evaluation:
2017-10-10T12:50:52.514158: step 560, loss 0.317742, acc 0.876259

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-560

2017-10-10T12:50:54.037431: step 561, loss 0.516309, acc 0.828125, learning_rate 0.000595977
2017-10-10T12:50:54.580248: step 562, loss 0.703581, acc 0.78125, learning_rate 0.000593952
2017-10-10T12:50:55.124854: step 563, loss 0.298533, acc 0.921875, learning_rate 0.000591936
2017-10-10T12:50:55.702625: step 564, loss 0.340246, acc 0.9375, learning_rate 0.000589928
2017-10-10T12:50:56.199793: step 565, loss 0.561737, acc 0.828125, learning_rate 0.000587928
2017-10-10T12:50:56.744865: step 566, loss 0.552276, acc 0.796875, learning_rate 0.000585937
2017-10-10T12:50:57.391511: step 567, loss 0.608963, acc 0.8125, learning_rate 0.000583953
2017-10-10T12:50:57.877169: step 568, loss 0.563074, acc 0.796875, learning_rate 0.000581978
2017-10-10T12:50:58.276901: step 569, loss 0.483914, acc 0.78125, learning_rate 0.00058001
2017-10-10T12:50:58.866307: step 570, loss 0.527586, acc 0.859375, learning_rate 0.000578051
2017-10-10T12:50:59.428921: step 571, loss 0.313158, acc 0.953125, learning_rate 0.0005761
2017-10-10T12:50:59.976945: step 572, loss 0.665698, acc 0.828125, learning_rate 0.000574157
2017-10-10T12:51:00.473008: step 573, loss 0.320264, acc 0.921875, learning_rate 0.000572221
2017-10-10T12:51:01.024956: step 574, loss 0.369307, acc 0.90625, learning_rate 0.000570294
2017-10-10T12:51:01.572944: step 575, loss 0.447604, acc 0.828125, learning_rate 0.000568374
2017-10-10T12:51:02.153858: step 576, loss 0.58668, acc 0.828125, learning_rate 0.000566462
2017-10-10T12:51:02.594702: step 577, loss 0.635104, acc 0.796875, learning_rate 0.000564558
2017-10-10T12:51:03.040259: step 578, loss 0.634855, acc 0.75, learning_rate 0.000562662
2017-10-10T12:51:03.498707: step 579, loss 0.530939, acc 0.8125, learning_rate 0.000560774
2017-10-10T12:51:03.989926: step 580, loss 0.51317, acc 0.765625, learning_rate 0.000558893
2017-10-10T12:51:04.564488: step 581, loss 0.522502, acc 0.859375, learning_rate 0.00055702
2017-10-10T12:51:05.101410: step 582, loss 0.348339, acc 0.8125, learning_rate 0.000555154
2017-10-10T12:51:05.747337: step 583, loss 0.460637, acc 0.859375, learning_rate 0.000553296
2017-10-10T12:51:06.348999: step 584, loss 0.362776, acc 0.84375, learning_rate 0.000551446
2017-10-10T12:51:06.769026: step 585, loss 0.595494, acc 0.78125, learning_rate 0.000549604
2017-10-10T12:51:07.212147: step 586, loss 0.49937, acc 0.8125, learning_rate 0.000547768
2017-10-10T12:51:07.721113: step 587, loss 0.449045, acc 0.875, learning_rate 0.000545941
2017-10-10T12:51:08.077191: step 588, loss 0.771186, acc 0.764706, learning_rate 0.00054412
2017-10-10T12:51:08.643248: step 589, loss 0.600862, acc 0.8125, learning_rate 0.000542308
2017-10-10T12:51:09.252896: step 590, loss 0.494312, acc 0.828125, learning_rate 0.000540502
2017-10-10T12:51:09.670142: step 591, loss 0.416567, acc 0.828125, learning_rate 0.000538704
2017-10-10T12:51:10.083957: step 592, loss 0.475784, acc 0.796875, learning_rate 0.000536914
2017-10-10T12:51:10.606748: step 593, loss 0.653921, acc 0.78125, learning_rate 0.00053513
2017-10-10T12:51:11.135434: step 594, loss 0.462619, acc 0.875, learning_rate 0.000533354
2017-10-10T12:51:11.668951: step 595, loss 0.382008, acc 0.890625, learning_rate 0.000531585
2017-10-10T12:51:12.151598: step 596, loss 0.556548, acc 0.8125, learning_rate 0.000529824
2017-10-10T12:51:12.676862: step 597, loss 0.524523, acc 0.8125, learning_rate 0.000528069
2017-10-10T12:51:13.200884: step 598, loss 0.427755, acc 0.8125, learning_rate 0.000526322
2017-10-10T12:51:13.764794: step 599, loss 0.498494, acc 0.796875, learning_rate 0.000524582
2017-10-10T12:51:14.296990: step 600, loss 0.422648, acc 0.84375, learning_rate 0.000522849

Evaluation:
2017-10-10T12:51:15.380950: step 600, loss 0.311562, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-600

2017-10-10T12:51:17.225716: step 601, loss 0.496864, acc 0.8125, learning_rate 0.000521123
2017-10-10T12:51:17.748851: step 602, loss 0.542238, acc 0.796875, learning_rate 0.000519404
2017-10-10T12:51:18.304921: step 603, loss 0.350399, acc 0.875, learning_rate 0.000517692
2017-10-10T12:51:18.783746: step 604, loss 0.524576, acc 0.875, learning_rate 0.000515987
2017-10-10T12:51:19.240984: step 605, loss 0.54226, acc 0.8125, learning_rate 0.000514289
2017-10-10T12:51:19.740952: step 606, loss 0.526977, acc 0.78125, learning_rate 0.000512598
2017-10-10T12:51:20.395819: step 607, loss 0.489897, acc 0.84375, learning_rate 0.000510914
2017-10-10T12:51:20.783853: step 608, loss 0.50128, acc 0.859375, learning_rate 0.000509237
2017-10-10T12:51:21.220061: step 609, loss 0.511603, acc 0.765625, learning_rate 0.000507566
2017-10-10T12:51:21.657156: step 610, loss 0.338109, acc 0.890625, learning_rate 0.000505903
2017-10-10T12:51:22.125098: step 611, loss 0.392387, acc 0.84375, learning_rate 0.000504246
2017-10-10T12:51:22.732874: step 612, loss 0.60428, acc 0.78125, learning_rate 0.000502596
2017-10-10T12:51:23.241763: step 613, loss 0.557597, acc 0.84375, learning_rate 0.000500953
2017-10-10T12:51:23.771747: step 614, loss 0.646417, acc 0.796875, learning_rate 0.000499316
2017-10-10T12:51:24.296953: step 615, loss 0.434726, acc 0.859375, learning_rate 0.000497686
2017-10-10T12:51:24.815863: step 616, loss 0.520806, acc 0.859375, learning_rate 0.000496063
2017-10-10T12:51:25.281948: step 617, loss 0.456394, acc 0.84375, learning_rate 0.000494446
2017-10-10T12:51:25.765319: step 618, loss 0.492108, acc 0.875, learning_rate 0.000492836
2017-10-10T12:51:26.324832: step 619, loss 0.368325, acc 0.90625, learning_rate 0.000491233
2017-10-10T12:51:26.862613: step 620, loss 0.495415, acc 0.859375, learning_rate 0.000489636
2017-10-10T12:51:27.379454: step 621, loss 0.403253, acc 0.84375, learning_rate 0.000488045
2017-10-10T12:51:27.948955: step 622, loss 0.526597, acc 0.84375, learning_rate 0.000486461
2017-10-10T12:51:28.500902: step 623, loss 0.610052, acc 0.84375, learning_rate 0.000484884
2017-10-10T12:51:28.912899: step 624, loss 0.482001, acc 0.875, learning_rate 0.000483313
2017-10-10T12:51:29.401175: step 625, loss 0.382982, acc 0.875, learning_rate 0.000481748
2017-10-10T12:51:29.905033: step 626, loss 0.458773, acc 0.84375, learning_rate 0.00048019
2017-10-10T12:51:30.441432: step 627, loss 0.513592, acc 0.859375, learning_rate 0.000478638
2017-10-10T12:51:30.934856: step 628, loss 0.754053, acc 0.8125, learning_rate 0.000477093
2017-10-10T12:51:31.479734: step 629, loss 0.400877, acc 0.8125, learning_rate 0.000475554
2017-10-10T12:51:31.964967: step 630, loss 0.357105, acc 0.875, learning_rate 0.000474021
2017-10-10T12:51:32.408852: step 631, loss 0.479609, acc 0.859375, learning_rate 0.000472494
2017-10-10T12:51:32.864438: step 632, loss 0.462662, acc 0.859375, learning_rate 0.000470974
2017-10-10T12:51:33.125005: step 633, loss 0.385226, acc 0.859375, learning_rate 0.000469459
2017-10-10T12:51:33.665086: step 634, loss 0.38793, acc 0.890625, learning_rate 0.000467951
2017-10-10T12:51:34.190762: step 635, loss 0.660478, acc 0.78125, learning_rate 0.000466449
2017-10-10T12:51:34.756924: step 636, loss 0.351976, acc 0.875, learning_rate 0.000464954
2017-10-10T12:51:35.320879: step 637, loss 0.475694, acc 0.84375, learning_rate 0.000463464
2017-10-10T12:51:35.805408: step 638, loss 0.466989, acc 0.859375, learning_rate 0.00046198
2017-10-10T12:51:36.312821: step 639, loss 0.59344, acc 0.84375, learning_rate 0.000460503
2017-10-10T12:51:36.852852: step 640, loss 0.418275, acc 0.859375, learning_rate 0.000459031

Evaluation:
2017-10-10T12:51:38.022710: step 640, loss 0.311707, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-640

2017-10-10T12:51:39.476675: step 641, loss 0.356141, acc 0.890625, learning_rate 0.000457566
2017-10-10T12:51:40.013162: step 642, loss 0.432026, acc 0.828125, learning_rate 0.000456106
2017-10-10T12:51:40.556870: step 643, loss 0.555169, acc 0.8125, learning_rate 0.000454653
2017-10-10T12:51:41.026225: step 644, loss 0.610409, acc 0.78125, learning_rate 0.000453205
2017-10-10T12:51:41.545471: step 645, loss 0.464404, acc 0.890625, learning_rate 0.000451764
2017-10-10T12:51:42.063960: step 646, loss 0.809498, acc 0.671875, learning_rate 0.000450328
2017-10-10T12:51:42.591896: step 647, loss 0.331765, acc 0.875, learning_rate 0.000448898
2017-10-10T12:51:43.194965: step 648, loss 0.576633, acc 0.765625, learning_rate 0.000447474
2017-10-10T12:51:43.676878: step 649, loss 0.543216, acc 0.78125, learning_rate 0.000446055
2017-10-10T12:51:44.164876: step 650, loss 0.45303, acc 0.84375, learning_rate 0.000444643
2017-10-10T12:51:44.608222: step 651, loss 0.553076, acc 0.828125, learning_rate 0.000443236
2017-10-10T12:51:45.182336: step 652, loss 0.57661, acc 0.796875, learning_rate 0.000441835
2017-10-10T12:51:45.700899: step 653, loss 0.489487, acc 0.78125, learning_rate 0.00044044
2017-10-10T12:51:46.167839: step 654, loss 0.375775, acc 0.890625, learning_rate 0.00043905
2017-10-10T12:51:46.790637: step 655, loss 0.373005, acc 0.8125, learning_rate 0.000437666
2017-10-10T12:51:47.305139: step 656, loss 0.457918, acc 0.859375, learning_rate 0.000436288
2017-10-10T12:51:47.846507: step 657, loss 0.431595, acc 0.8125, learning_rate 0.000434915
2017-10-10T12:51:48.352824: step 658, loss 0.560806, acc 0.84375, learning_rate 0.000433548
2017-10-10T12:51:48.948928: step 659, loss 0.341884, acc 0.875, learning_rate 0.000432187
2017-10-10T12:51:49.486726: step 660, loss 0.597277, acc 0.8125, learning_rate 0.000430831
2017-10-10T12:51:50.068123: step 661, loss 0.526247, acc 0.84375, learning_rate 0.000429481
2017-10-10T12:51:50.597628: step 662, loss 0.502664, acc 0.78125, learning_rate 0.000428136
2017-10-10T12:51:51.193006: step 663, loss 0.444458, acc 0.859375, learning_rate 0.000426796
2017-10-10T12:51:51.700650: step 664, loss 0.384526, acc 0.828125, learning_rate 0.000425463
2017-10-10T12:51:52.177052: step 665, loss 0.405718, acc 0.84375, learning_rate 0.000424134
2017-10-10T12:51:52.666222: step 666, loss 0.499692, acc 0.828125, learning_rate 0.000422811
2017-10-10T12:51:53.175929: step 667, loss 0.504393, acc 0.75, learning_rate 0.000421493
2017-10-10T12:51:53.660857: step 668, loss 0.732844, acc 0.765625, learning_rate 0.000420181
2017-10-10T12:51:54.252872: step 669, loss 0.511236, acc 0.828125, learning_rate 0.000418874
2017-10-10T12:51:54.856189: step 670, loss 0.504483, acc 0.8125, learning_rate 0.000417573
2017-10-10T12:51:55.341168: step 671, loss 0.643176, acc 0.8125, learning_rate 0.000416276
2017-10-10T12:51:55.877118: step 672, loss 0.440993, acc 0.859375, learning_rate 0.000414985
2017-10-10T12:51:56.536991: step 673, loss 0.629621, acc 0.859375, learning_rate 0.0004137
2017-10-10T12:51:57.049633: step 674, loss 0.303528, acc 0.953125, learning_rate 0.000412419
2017-10-10T12:51:57.545021: step 675, loss 0.434032, acc 0.8125, learning_rate 0.000411144
2017-10-10T12:51:58.092051: step 676, loss 0.402943, acc 0.8125, learning_rate 0.000409874
2017-10-10T12:51:58.636313: step 677, loss 0.508396, acc 0.78125, learning_rate 0.000408609
2017-10-10T12:51:59.136855: step 678, loss 0.501183, acc 0.859375, learning_rate 0.00040735
2017-10-10T12:51:59.698814: step 679, loss 0.562214, acc 0.8125, learning_rate 0.000406095
2017-10-10T12:52:00.224856: step 680, loss 0.543589, acc 0.8125, learning_rate 0.000404846

Evaluation:
2017-10-10T12:52:01.287554: step 680, loss 0.307154, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-680

2017-10-10T12:52:02.962720: step 681, loss 0.44781, acc 0.84375, learning_rate 0.000403601
2017-10-10T12:52:03.473398: step 682, loss 0.67543, acc 0.8125, learning_rate 0.000402362
2017-10-10T12:52:03.992948: step 683, loss 0.310929, acc 0.890625, learning_rate 0.000401128
2017-10-10T12:52:04.456813: step 684, loss 0.324228, acc 0.859375, learning_rate 0.000399899
2017-10-10T12:52:05.044593: step 685, loss 0.648187, acc 0.78125, learning_rate 0.000398675
2017-10-10T12:52:05.465096: step 686, loss 0.445174, acc 0.843137, learning_rate 0.000397456
2017-10-10T12:52:06.071597: step 687, loss 0.405614, acc 0.859375, learning_rate 0.000396241
2017-10-10T12:52:06.576982: step 688, loss 0.517287, acc 0.84375, learning_rate 0.000395032
2017-10-10T12:52:06.996849: step 689, loss 0.51676, acc 0.84375, learning_rate 0.000393828
2017-10-10T12:52:07.520938: step 690, loss 0.343318, acc 0.921875, learning_rate 0.000392629
2017-10-10T12:52:08.077692: step 691, loss 0.650061, acc 0.75, learning_rate 0.000391434
2017-10-10T12:52:08.641816: step 692, loss 0.425, acc 0.890625, learning_rate 0.000390245
2017-10-10T12:52:09.167473: step 693, loss 0.419263, acc 0.84375, learning_rate 0.00038906
2017-10-10T12:52:09.684092: step 694, loss 0.491502, acc 0.84375, learning_rate 0.00038788
2017-10-10T12:52:10.200906: step 695, loss 0.407066, acc 0.828125, learning_rate 0.000386705
2017-10-10T12:52:10.708845: step 696, loss 0.56694, acc 0.75, learning_rate 0.000385535
2017-10-10T12:52:11.226116: step 697, loss 0.373731, acc 0.859375, learning_rate 0.000384369
2017-10-10T12:52:11.761035: step 698, loss 0.514325, acc 0.859375, learning_rate 0.000383209
2017-10-10T12:52:12.265100: step 699, loss 0.485688, acc 0.828125, learning_rate 0.000382053
2017-10-10T12:52:12.710201: step 700, loss 0.652665, acc 0.765625, learning_rate 0.000380901
2017-10-10T12:52:13.203833: step 701, loss 0.384262, acc 0.84375, learning_rate 0.000379755
2017-10-10T12:52:13.678681: step 702, loss 0.477043, acc 0.828125, learning_rate 0.000378613
2017-10-10T12:52:14.226028: step 703, loss 0.473817, acc 0.828125, learning_rate 0.000377476
2017-10-10T12:52:14.802806: step 704, loss 0.464435, acc 0.859375, learning_rate 0.000376343
2017-10-10T12:52:15.200897: step 705, loss 0.425409, acc 0.859375, learning_rate 0.000375215
2017-10-10T12:52:15.644865: step 706, loss 0.567401, acc 0.90625, learning_rate 0.000374092
2017-10-10T12:52:16.100932: step 707, loss 0.531694, acc 0.796875, learning_rate 0.000372973
2017-10-10T12:52:16.672871: step 708, loss 0.555323, acc 0.859375, learning_rate 0.000371859
2017-10-10T12:52:17.259820: step 709, loss 0.483656, acc 0.796875, learning_rate 0.000370749
2017-10-10T12:52:17.716873: step 710, loss 0.343843, acc 0.90625, learning_rate 0.000369644
2017-10-10T12:52:18.211392: step 711, loss 0.460032, acc 0.78125, learning_rate 0.000368543
2017-10-10T12:52:18.683284: step 712, loss 0.48712, acc 0.8125, learning_rate 0.000367447
2017-10-10T12:52:19.284881: step 713, loss 0.372706, acc 0.90625, learning_rate 0.000366356
2017-10-10T12:52:19.816864: step 714, loss 0.494318, acc 0.84375, learning_rate 0.000365268
2017-10-10T12:52:20.352257: step 715, loss 0.354496, acc 0.859375, learning_rate 0.000364186
2017-10-10T12:52:20.883333: step 716, loss 0.797975, acc 0.75, learning_rate 0.000363107
2017-10-10T12:52:21.415782: step 717, loss 0.309092, acc 0.90625, learning_rate 0.000362033
2017-10-10T12:52:21.971812: step 718, loss 0.70601, acc 0.796875, learning_rate 0.000360964
2017-10-10T12:52:22.520826: step 719, loss 0.442302, acc 0.859375, learning_rate 0.000359899
2017-10-10T12:52:23.125233: step 720, loss 0.484901, acc 0.828125, learning_rate 0.000358838

Evaluation:
2017-10-10T12:52:24.298047: step 720, loss 0.306953, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-720

2017-10-10T12:52:26.101578: step 721, loss 0.789102, acc 0.734375, learning_rate 0.000357781
2017-10-10T12:52:26.665735: step 722, loss 0.454568, acc 0.859375, learning_rate 0.000356729
2017-10-10T12:52:27.233871: step 723, loss 0.537503, acc 0.828125, learning_rate 0.000355681
2017-10-10T12:52:27.692928: step 724, loss 0.391651, acc 0.828125, learning_rate 0.000354637
2017-10-10T12:52:28.241944: step 725, loss 0.510405, acc 0.8125, learning_rate 0.000353598
2017-10-10T12:52:28.748739: step 726, loss 0.657428, acc 0.8125, learning_rate 0.000352563
2017-10-10T12:52:29.164842: step 727, loss 0.513839, acc 0.859375, learning_rate 0.000351532
2017-10-10T12:52:29.562658: step 728, loss 0.512891, acc 0.859375, learning_rate 0.000350505
2017-10-10T12:52:29.987356: step 729, loss 0.476119, acc 0.84375, learning_rate 0.000349483
2017-10-10T12:52:30.524985: step 730, loss 0.476132, acc 0.84375, learning_rate 0.000348465
2017-10-10T12:52:31.094352: step 731, loss 0.591329, acc 0.78125, learning_rate 0.00034745
2017-10-10T12:52:31.591765: step 732, loss 0.290212, acc 0.921875, learning_rate 0.00034644
2017-10-10T12:52:32.099057: step 733, loss 0.397538, acc 0.828125, learning_rate 0.000345434
2017-10-10T12:52:32.613215: step 734, loss 0.482895, acc 0.796875, learning_rate 0.000344433
2017-10-10T12:52:33.161282: step 735, loss 0.309389, acc 0.921875, learning_rate 0.000343435
2017-10-10T12:52:33.695145: step 736, loss 0.38292, acc 0.875, learning_rate 0.000342441
2017-10-10T12:52:34.237146: step 737, loss 0.443033, acc 0.859375, learning_rate 0.000341452
2017-10-10T12:52:34.794552: step 738, loss 0.534276, acc 0.84375, learning_rate 0.000340466
2017-10-10T12:52:35.315806: step 739, loss 0.401249, acc 0.875, learning_rate 0.000339485
2017-10-10T12:52:35.859206: step 740, loss 0.469173, acc 0.890625, learning_rate 0.000338507
2017-10-10T12:52:36.330419: step 741, loss 0.431809, acc 0.828125, learning_rate 0.000337534
2017-10-10T12:52:36.805898: step 742, loss 0.465008, acc 0.84375, learning_rate 0.000336564
2017-10-10T12:52:37.375569: step 743, loss 0.532756, acc 0.8125, learning_rate 0.000335598
2017-10-10T12:52:37.897146: step 744, loss 0.32622, acc 0.890625, learning_rate 0.000334637
2017-10-10T12:52:38.376853: step 745, loss 0.662251, acc 0.796875, learning_rate 0.000333679
2017-10-10T12:52:38.804165: step 746, loss 0.401955, acc 0.859375, learning_rate 0.000332725
2017-10-10T12:52:39.257023: step 747, loss 0.312503, acc 0.890625, learning_rate 0.000331775
2017-10-10T12:52:39.864886: step 748, loss 0.642186, acc 0.75, learning_rate 0.000330829
2017-10-10T12:52:40.444857: step 749, loss 0.408765, acc 0.890625, learning_rate 0.000329887
2017-10-10T12:52:40.836972: step 750, loss 0.36669, acc 0.84375, learning_rate 0.000328949
2017-10-10T12:52:41.256177: step 751, loss 0.581697, acc 0.78125, learning_rate 0.000328014
2017-10-10T12:52:41.744409: step 752, loss 0.405309, acc 0.90625, learning_rate 0.000327083
2017-10-10T12:52:42.320884: step 753, loss 0.450593, acc 0.84375, learning_rate 0.000326157
2017-10-10T12:52:42.853128: step 754, loss 0.630004, acc 0.8125, learning_rate 0.000325233
2017-10-10T12:52:43.439822: step 755, loss 0.456855, acc 0.84375, learning_rate 0.000324314
2017-10-10T12:52:44.007294: step 756, loss 0.34484, acc 0.859375, learning_rate 0.000323399
2017-10-10T12:52:44.548026: step 757, loss 0.49699, acc 0.859375, learning_rate 0.000322487
2017-10-10T12:52:45.105705: step 758, loss 0.465945, acc 0.796875, learning_rate 0.000321579
2017-10-10T12:52:45.656822: step 759, loss 0.523324, acc 0.84375, learning_rate 0.000320674
2017-10-10T12:52:46.193821: step 760, loss 0.303788, acc 0.90625, learning_rate 0.000319773

Evaluation:
2017-10-10T12:52:47.364876: step 760, loss 0.306992, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-760

2017-10-10T12:52:48.772187: step 761, loss 0.509337, acc 0.84375, learning_rate 0.000318876
2017-10-10T12:52:49.309753: step 762, loss 0.45242, acc 0.8125, learning_rate 0.000317983
2017-10-10T12:52:49.853431: step 763, loss 0.45563, acc 0.84375, learning_rate 0.000317093
2017-10-10T12:52:50.400708: step 764, loss 0.262771, acc 0.953125, learning_rate 0.000316207
2017-10-10T12:52:50.950416: step 765, loss 0.440205, acc 0.875, learning_rate 0.000315325
2017-10-10T12:52:51.575662: step 766, loss 0.548353, acc 0.828125, learning_rate 0.000314446
2017-10-10T12:52:52.076142: step 767, loss 0.377666, acc 0.859375, learning_rate 0.00031357
2017-10-10T12:52:52.563173: step 768, loss 0.505786, acc 0.796875, learning_rate 0.000312699
2017-10-10T12:52:53.036381: step 769, loss 0.647389, acc 0.734375, learning_rate 0.00031183
2017-10-10T12:52:53.488830: step 770, loss 0.531656, acc 0.796875, learning_rate 0.000310966
2017-10-10T12:52:53.992919: step 771, loss 0.465552, acc 0.84375, learning_rate 0.000310105
2017-10-10T12:52:54.496845: step 772, loss 0.576333, acc 0.828125, learning_rate 0.000309247
2017-10-10T12:52:55.001836: step 773, loss 0.559359, acc 0.8125, learning_rate 0.000308393
2017-10-10T12:52:55.533811: step 774, loss 0.347755, acc 0.890625, learning_rate 0.000307542
2017-10-10T12:52:56.076935: step 775, loss 0.697036, acc 0.84375, learning_rate 0.000306695
2017-10-10T12:52:56.597068: step 776, loss 0.35418, acc 0.875, learning_rate 0.000305852
2017-10-10T12:52:57.116902: step 777, loss 0.406795, acc 0.890625, learning_rate 0.000305011
2017-10-10T12:52:57.583771: step 778, loss 0.601874, acc 0.78125, learning_rate 0.000304174
2017-10-10T12:52:58.152945: step 779, loss 0.554025, acc 0.8125, learning_rate 0.000303341
2017-10-10T12:52:58.744223: step 780, loss 0.474947, acc 0.796875, learning_rate 0.000302511
2017-10-10T12:52:59.300871: step 781, loss 0.470401, acc 0.828125, learning_rate 0.000301684
2017-10-10T12:52:59.812987: step 782, loss 0.523357, acc 0.765625, learning_rate 0.000300861
2017-10-10T12:53:00.398952: step 783, loss 0.349158, acc 0.859375, learning_rate 0.000300041
2017-10-10T12:53:00.961147: step 784, loss 0.334516, acc 0.843137, learning_rate 0.000299225
2017-10-10T12:53:01.458737: step 785, loss 0.445449, acc 0.828125, learning_rate 0.000298412
2017-10-10T12:53:01.919308: step 786, loss 0.341981, acc 0.828125, learning_rate 0.000297602
2017-10-10T12:53:02.460981: step 787, loss 0.315792, acc 0.921875, learning_rate 0.000296795
2017-10-10T12:53:03.000870: step 788, loss 0.560741, acc 0.8125, learning_rate 0.000295992
2017-10-10T12:53:03.521228: step 789, loss 0.262643, acc 0.9375, learning_rate 0.000295192
2017-10-10T12:53:03.980984: step 790, loss 0.437467, acc 0.859375, learning_rate 0.000294395
2017-10-10T12:53:04.449040: step 791, loss 0.469031, acc 0.875, learning_rate 0.000293602
2017-10-10T12:53:04.936988: step 792, loss 0.52233, acc 0.796875, learning_rate 0.000292812
2017-10-10T12:53:05.488485: step 793, loss 0.504448, acc 0.84375, learning_rate 0.000292025
2017-10-10T12:53:05.982138: step 794, loss 0.529456, acc 0.84375, learning_rate 0.000291241
2017-10-10T12:53:06.536892: step 795, loss 0.369896, acc 0.921875, learning_rate 0.00029046
2017-10-10T12:53:07.072852: step 796, loss 0.425607, acc 0.890625, learning_rate 0.000289683
2017-10-10T12:53:07.622527: step 797, loss 0.453903, acc 0.796875, learning_rate 0.000288908
2017-10-10T12:53:08.177003: step 798, loss 0.359731, acc 0.890625, learning_rate 0.000288137
2017-10-10T12:53:08.722672: step 799, loss 0.401957, acc 0.84375, learning_rate 0.000287369
2017-10-10T12:53:09.304859: step 800, loss 0.534261, acc 0.8125, learning_rate 0.000286605

Evaluation:
2017-10-10T12:53:10.507735: step 800, loss 0.305183, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-800

2017-10-10T12:53:12.033193: step 801, loss 0.646646, acc 0.765625, learning_rate 0.000285843
2017-10-10T12:53:12.528967: step 802, loss 0.281995, acc 0.90625, learning_rate 0.000285084
2017-10-10T12:53:13.027502: step 803, loss 0.321706, acc 0.90625, learning_rate 0.000284329
2017-10-10T12:53:13.496616: step 804, loss 0.648537, acc 0.84375, learning_rate 0.000283577
2017-10-10T12:53:14.025114: step 805, loss 0.63199, acc 0.765625, learning_rate 0.000282827
2017-10-10T12:53:14.594590: step 806, loss 0.341401, acc 0.90625, learning_rate 0.000282081
2017-10-10T12:53:15.209105: step 807, loss 0.482684, acc 0.84375, learning_rate 0.000281338
2017-10-10T12:53:15.772869: step 808, loss 0.472075, acc 0.84375, learning_rate 0.000280598
2017-10-10T12:53:16.224863: step 809, loss 0.46447, acc 0.90625, learning_rate 0.00027986
2017-10-10T12:53:16.656913: step 810, loss 0.652941, acc 0.84375, learning_rate 0.000279126
2017-10-10T12:53:17.094399: step 811, loss 0.459083, acc 0.875, learning_rate 0.000278395
2017-10-10T12:53:17.617121: step 812, loss 0.464198, acc 0.828125, learning_rate 0.000277667
2017-10-10T12:53:18.256993: step 813, loss 0.63194, acc 0.765625, learning_rate 0.000276942
2017-10-10T12:53:18.838241: step 814, loss 0.373671, acc 0.859375, learning_rate 0.00027622
2017-10-10T12:53:19.384841: step 815, loss 0.352401, acc 0.90625, learning_rate 0.0002755
2017-10-10T12:53:19.940900: step 816, loss 0.42548, acc 0.875, learning_rate 0.000274784
2017-10-10T12:53:20.448890: step 817, loss 0.391089, acc 0.859375, learning_rate 0.000274071
2017-10-10T12:53:20.992895: step 818, loss 0.385533, acc 0.828125, learning_rate 0.00027336
2017-10-10T12:53:21.409634: step 819, loss 0.576966, acc 0.78125, learning_rate 0.000272652
2017-10-10T12:53:22.039545: step 820, loss 0.62053, acc 0.765625, learning_rate 0.000271948
2017-10-10T12:53:22.592825: step 821, loss 0.351602, acc 0.890625, learning_rate 0.000271246
2017-10-10T12:53:23.172531: step 822, loss 0.41177, acc 0.890625, learning_rate 0.000270547
2017-10-10T12:53:23.766486: step 823, loss 0.527366, acc 0.84375, learning_rate 0.000269851
2017-10-10T12:53:24.180873: step 824, loss 0.52402, acc 0.859375, learning_rate 0.000269157
2017-10-10T12:53:24.607216: step 825, loss 0.528691, acc 0.8125, learning_rate 0.000268467
2017-10-10T12:53:24.988939: step 826, loss 0.430849, acc 0.828125, learning_rate 0.000267779
2017-10-10T12:53:25.508774: step 827, loss 0.567636, acc 0.828125, learning_rate 0.000267094
2017-10-10T12:53:26.152617: step 828, loss 0.301779, acc 0.890625, learning_rate 0.000266412
2017-10-10T12:53:26.565065: step 829, loss 0.443381, acc 0.859375, learning_rate 0.000265733
2017-10-10T12:53:27.038062: step 830, loss 0.316844, acc 0.875, learning_rate 0.000265057
2017-10-10T12:53:27.434316: step 831, loss 0.63245, acc 0.765625, learning_rate 0.000264383
2017-10-10T12:53:27.963764: step 832, loss 0.284291, acc 0.921875, learning_rate 0.000263712
2017-10-10T12:53:28.497804: step 833, loss 0.435656, acc 0.84375, learning_rate 0.000263044
2017-10-10T12:53:29.050891: step 834, loss 0.335601, acc 0.859375, learning_rate 0.000262378
2017-10-10T12:53:29.601098: step 835, loss 0.311507, acc 0.890625, learning_rate 0.000261715
2017-10-10T12:53:30.129818: step 836, loss 0.641224, acc 0.765625, learning_rate 0.000261055
2017-10-10T12:53:30.602769: step 837, loss 0.629006, acc 0.75, learning_rate 0.000260398
2017-10-10T12:53:31.117084: step 838, loss 0.599779, acc 0.8125, learning_rate 0.000259743
2017-10-10T12:53:31.692943: step 839, loss 0.383032, acc 0.859375, learning_rate 0.000259091
2017-10-10T12:53:32.267888: step 840, loss 0.519437, acc 0.8125, learning_rate 0.000258442

Evaluation:
2017-10-10T12:53:33.401547: step 840, loss 0.302907, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-840

2017-10-10T12:53:35.199361: step 841, loss 0.3468, acc 0.875, learning_rate 0.000257795
2017-10-10T12:53:35.752539: step 842, loss 0.572997, acc 0.78125, learning_rate 0.000257151
2017-10-10T12:53:36.295238: step 843, loss 0.688965, acc 0.8125, learning_rate 0.00025651
2017-10-10T12:53:36.861292: step 844, loss 0.384395, acc 0.828125, learning_rate 0.000255871
2017-10-10T12:53:37.393040: step 845, loss 0.601253, acc 0.890625, learning_rate 0.000255235
2017-10-10T12:53:37.956854: step 846, loss 0.391027, acc 0.875, learning_rate 0.000254601
2017-10-10T12:53:38.560540: step 847, loss 0.449058, acc 0.828125, learning_rate 0.00025397
2017-10-10T12:53:39.096802: step 848, loss 0.544024, acc 0.8125, learning_rate 0.000253341
2017-10-10T12:53:39.552737: step 849, loss 0.4463, acc 0.90625, learning_rate 0.000252716
2017-10-10T12:53:40.072294: step 850, loss 0.328507, acc 0.84375, learning_rate 0.000252092
2017-10-10T12:53:40.626901: step 851, loss 0.435445, acc 0.84375, learning_rate 0.000251471
2017-10-10T12:53:41.163305: step 852, loss 0.472119, acc 0.875, learning_rate 0.000250853
2017-10-10T12:53:41.705697: step 853, loss 0.537389, acc 0.796875, learning_rate 0.000250237
2017-10-10T12:53:42.246243: step 854, loss 0.334567, acc 0.90625, learning_rate 0.000249624
2017-10-10T12:53:42.811241: step 855, loss 0.347785, acc 0.84375, learning_rate 0.000249013
2017-10-10T12:53:43.388990: step 856, loss 0.461839, acc 0.828125, learning_rate 0.000248405
2017-10-10T12:53:43.909039: step 857, loss 0.483306, acc 0.796875, learning_rate 0.000247799
2017-10-10T12:53:44.458660: step 858, loss 0.551112, acc 0.8125, learning_rate 0.000247196
2017-10-10T12:53:45.024991: step 859, loss 0.214958, acc 0.921875, learning_rate 0.000246595
2017-10-10T12:53:45.644840: step 860, loss 0.431017, acc 0.84375, learning_rate 0.000245997
2017-10-10T12:53:46.153240: step 861, loss 0.425645, acc 0.859375, learning_rate 0.000245401
2017-10-10T12:53:46.816870: step 862, loss 0.523667, acc 0.796875, learning_rate 0.000244808
2017-10-10T12:53:47.465211: step 863, loss 0.537379, acc 0.8125, learning_rate 0.000244216
2017-10-10T12:53:47.937693: step 864, loss 0.381623, acc 0.859375, learning_rate 0.000243628
2017-10-10T12:53:48.360020: step 865, loss 0.381432, acc 0.890625, learning_rate 0.000243042
2017-10-10T12:53:48.900648: step 866, loss 0.486328, acc 0.84375, learning_rate 0.000242458
2017-10-10T12:53:49.518682: step 867, loss 0.38199, acc 0.875, learning_rate 0.000241876
2017-10-10T12:53:49.977002: step 868, loss 0.318569, acc 0.890625, learning_rate 0.000241297
2017-10-10T12:53:50.431975: step 869, loss 0.245009, acc 0.953125, learning_rate 0.00024072
2017-10-10T12:53:50.844899: step 870, loss 0.497817, acc 0.875, learning_rate 0.000240146
2017-10-10T12:53:51.447919: step 871, loss 0.481978, acc 0.828125, learning_rate 0.000239574
2017-10-10T12:53:51.959984: step 872, loss 0.372348, acc 0.890625, learning_rate 0.000239004
2017-10-10T12:53:52.478609: step 873, loss 0.474177, acc 0.859375, learning_rate 0.000238437
2017-10-10T12:53:52.987801: step 874, loss 0.262174, acc 0.90625, learning_rate 0.000237872
2017-10-10T12:53:53.503933: step 875, loss 0.50462, acc 0.859375, learning_rate 0.000237309
2017-10-10T12:53:54.045683: step 876, loss 0.349568, acc 0.828125, learning_rate 0.000236749
2017-10-10T12:53:54.592936: step 877, loss 0.452126, acc 0.796875, learning_rate 0.00023619
2017-10-10T12:53:55.130702: step 878, loss 0.438207, acc 0.890625, learning_rate 0.000235635
2017-10-10T12:53:55.676854: step 879, loss 0.478828, acc 0.84375, learning_rate 0.000235081
2017-10-10T12:53:56.262601: step 880, loss 0.550919, acc 0.796875, learning_rate 0.00023453

Evaluation:
2017-10-10T12:53:57.380261: step 880, loss 0.302531, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-880

2017-10-10T12:53:58.852922: step 881, loss 0.379261, acc 0.859375, learning_rate 0.00023398
2017-10-10T12:53:59.349223: step 882, loss 0.355374, acc 0.901961, learning_rate 0.000233434
2017-10-10T12:53:59.921354: step 883, loss 0.517301, acc 0.84375, learning_rate 0.000232889
2017-10-10T12:54:00.505546: step 884, loss 0.820104, acc 0.734375, learning_rate 0.000232346
2017-10-10T12:54:01.109075: step 885, loss 0.289473, acc 0.890625, learning_rate 0.000231806
2017-10-10T12:54:01.703848: step 886, loss 0.304957, acc 0.921875, learning_rate 0.000231268
2017-10-10T12:54:02.146573: step 887, loss 0.345342, acc 0.875, learning_rate 0.000230732
2017-10-10T12:54:02.557603: step 888, loss 0.599522, acc 0.796875, learning_rate 0.000230199
2017-10-10T12:54:02.969390: step 889, loss 0.579964, acc 0.8125, learning_rate 0.000229667
2017-10-10T12:54:03.500200: step 890, loss 0.375523, acc 0.875, learning_rate 0.000229138
2017-10-10T12:54:04.021167: step 891, loss 0.414575, acc 0.875, learning_rate 0.000228611
2017-10-10T12:54:04.520534: step 892, loss 0.648165, acc 0.78125, learning_rate 0.000228086
2017-10-10T12:54:05.007266: step 893, loss 0.529701, acc 0.8125, learning_rate 0.000227563
2017-10-10T12:54:05.545226: step 894, loss 0.525348, acc 0.84375, learning_rate 0.000227043
2017-10-10T12:54:06.108043: step 895, loss 0.436485, acc 0.84375, learning_rate 0.000226524
2017-10-10T12:54:06.559138: step 896, loss 0.491212, acc 0.84375, learning_rate 0.000226008
2017-10-10T12:54:07.030462: step 897, loss 0.359621, acc 0.828125, learning_rate 0.000225493
2017-10-10T12:54:07.596473: step 898, loss 0.36962, acc 0.8125, learning_rate 0.000224981
2017-10-10T12:54:08.158171: step 899, loss 0.502727, acc 0.84375, learning_rate 0.000224471
2017-10-10T12:54:08.680165: step 900, loss 0.632304, acc 0.796875, learning_rate 0.000223963
2017-10-10T12:54:09.230396: step 901, loss 0.421001, acc 0.90625, learning_rate 0.000223457
2017-10-10T12:54:09.708349: step 902, loss 0.386106, acc 0.828125, learning_rate 0.000222953
2017-10-10T12:54:10.232903: step 903, loss 0.336993, acc 0.859375, learning_rate 0.000222451
2017-10-10T12:54:10.832519: step 904, loss 0.387765, acc 0.875, learning_rate 0.000221951
2017-10-10T12:54:11.280635: step 905, loss 0.629642, acc 0.796875, learning_rate 0.000221453
2017-10-10T12:54:11.792878: step 906, loss 0.456527, acc 0.8125, learning_rate 0.000220958
2017-10-10T12:54:12.329063: step 907, loss 0.446927, acc 0.84375, learning_rate 0.000220464
2017-10-10T12:54:12.894010: step 908, loss 0.450565, acc 0.8125, learning_rate 0.000219972
2017-10-10T12:54:13.336995: step 909, loss 0.420179, acc 0.8125, learning_rate 0.000219483
2017-10-10T12:54:13.732012: step 910, loss 0.428528, acc 0.875, learning_rate 0.000218995
2017-10-10T12:54:14.237084: step 911, loss 0.581331, acc 0.8125, learning_rate 0.000218509
2017-10-10T12:54:14.781707: step 912, loss 0.325005, acc 0.890625, learning_rate 0.000218025
2017-10-10T12:54:15.348910: step 913, loss 0.324245, acc 0.890625, learning_rate 0.000217544
2017-10-10T12:54:15.956819: step 914, loss 0.33446, acc 0.921875, learning_rate 0.000217064
2017-10-10T12:54:16.513880: step 915, loss 0.497148, acc 0.796875, learning_rate 0.000216586
2017-10-10T12:54:17.071220: step 916, loss 0.417261, acc 0.859375, learning_rate 0.00021611
2017-10-10T12:54:17.616848: step 917, loss 0.446364, acc 0.875, learning_rate 0.000215636
2017-10-10T12:54:18.181720: step 918, loss 0.362169, acc 0.875, learning_rate 0.000215164
2017-10-10T12:54:18.768873: step 919, loss 0.341778, acc 0.90625, learning_rate 0.000214694
2017-10-10T12:54:19.307345: step 920, loss 0.599458, acc 0.765625, learning_rate 0.000214226

Evaluation:
2017-10-10T12:54:20.492245: step 920, loss 0.299808, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-920

2017-10-10T12:54:22.089129: step 921, loss 0.362084, acc 0.84375, learning_rate 0.00021376
2017-10-10T12:54:22.592842: step 922, loss 0.555198, acc 0.78125, learning_rate 0.000213295
2017-10-10T12:54:23.084961: step 923, loss 0.351856, acc 0.875, learning_rate 0.000212833
2017-10-10T12:54:23.668848: step 924, loss 0.368198, acc 0.90625, learning_rate 0.000212372
2017-10-10T12:54:24.289688: step 925, loss 0.381108, acc 0.859375, learning_rate 0.000211914
2017-10-10T12:54:24.858760: step 926, loss 0.244006, acc 0.9375, learning_rate 0.000211457
2017-10-10T12:54:25.314777: step 927, loss 0.391958, acc 0.8125, learning_rate 0.000211002
2017-10-10T12:54:25.766540: step 928, loss 0.4403, acc 0.859375, learning_rate 0.000210549
2017-10-10T12:54:26.176625: step 929, loss 0.310212, acc 0.90625, learning_rate 0.000210098
2017-10-10T12:54:26.691560: step 930, loss 0.425624, acc 0.921875, learning_rate 0.000209648
2017-10-10T12:54:27.183782: step 931, loss 0.657925, acc 0.828125, learning_rate 0.000209201
2017-10-10T12:54:27.689601: step 932, loss 0.406362, acc 0.859375, learning_rate 0.000208755
2017-10-10T12:54:28.200855: step 933, loss 0.411838, acc 0.84375, learning_rate 0.000208311
2017-10-10T12:54:28.746227: step 934, loss 0.346784, acc 0.875, learning_rate 0.000207869
2017-10-10T12:54:29.283023: step 935, loss 0.426005, acc 0.828125, learning_rate 0.000207429
2017-10-10T12:54:29.837146: step 936, loss 0.610897, acc 0.734375, learning_rate 0.00020699
2017-10-10T12:54:30.381032: step 937, loss 0.250519, acc 0.90625, learning_rate 0.000206554
2017-10-10T12:54:30.960840: step 938, loss 0.247814, acc 0.921875, learning_rate 0.000206119
2017-10-10T12:54:31.476858: step 939, loss 0.502944, acc 0.859375, learning_rate 0.000205685
2017-10-10T12:54:32.021524: step 940, loss 0.466735, acc 0.78125, learning_rate 0.000205254
2017-10-10T12:54:32.533071: step 941, loss 0.507944, acc 0.8125, learning_rate 0.000204824
2017-10-10T12:54:33.093570: step 942, loss 0.587419, acc 0.8125, learning_rate 0.000204397
2017-10-10T12:54:33.713736: step 943, loss 0.449196, acc 0.84375, learning_rate 0.00020397
2017-10-10T12:54:34.173708: step 944, loss 0.434415, acc 0.875, learning_rate 0.000203546
2017-10-10T12:54:34.700883: step 945, loss 0.399154, acc 0.90625, learning_rate 0.000203123
2017-10-10T12:54:35.389035: step 946, loss 0.636418, acc 0.84375, learning_rate 0.000202702
2017-10-10T12:54:35.807511: step 947, loss 0.517601, acc 0.796875, learning_rate 0.000202283
2017-10-10T12:54:36.193598: step 948, loss 0.366652, acc 0.90625, learning_rate 0.000201866
2017-10-10T12:54:36.743600: step 949, loss 0.326248, acc 0.953125, learning_rate 0.00020145
2017-10-10T12:54:37.228863: step 950, loss 0.465032, acc 0.84375, learning_rate 0.000201036
2017-10-10T12:54:37.751589: step 951, loss 0.40623, acc 0.875, learning_rate 0.000200623
2017-10-10T12:54:38.296828: step 952, loss 0.358844, acc 0.890625, learning_rate 0.000200213
2017-10-10T12:54:38.823140: step 953, loss 0.385809, acc 0.890625, learning_rate 0.000199804
2017-10-10T12:54:39.346795: step 954, loss 0.47198, acc 0.796875, learning_rate 0.000199396
2017-10-10T12:54:39.917154: step 955, loss 0.606439, acc 0.75, learning_rate 0.000198991
2017-10-10T12:54:40.446846: step 956, loss 0.302342, acc 0.953125, learning_rate 0.000198587
2017-10-10T12:54:40.973555: step 957, loss 0.659426, acc 0.78125, learning_rate 0.000198184
2017-10-10T12:54:41.502768: step 958, loss 0.429961, acc 0.859375, learning_rate 0.000197783
2017-10-10T12:54:42.024922: step 959, loss 0.496101, acc 0.828125, learning_rate 0.000197384
2017-10-10T12:54:42.620878: step 960, loss 0.558458, acc 0.796875, learning_rate 0.000196987

Evaluation:
2017-10-10T12:54:43.824891: step 960, loss 0.3011, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-960

2017-10-10T12:54:45.358330: step 961, loss 0.403292, acc 0.890625, learning_rate 0.000196591
2017-10-10T12:54:45.933133: step 962, loss 0.386262, acc 0.8125, learning_rate 0.000196197
2017-10-10T12:54:46.401591: step 963, loss 0.314731, acc 0.90625, learning_rate 0.000195804
2017-10-10T12:54:46.921312: step 964, loss 0.500447, acc 0.8125, learning_rate 0.000195413
2017-10-10T12:54:47.493295: step 965, loss 0.521008, acc 0.828125, learning_rate 0.000195023
2017-10-10T12:54:48.098993: step 966, loss 0.349359, acc 0.890625, learning_rate 0.000194636
2017-10-10T12:54:48.588614: step 967, loss 0.496276, acc 0.796875, learning_rate 0.000194249
2017-10-10T12:54:49.085019: step 968, loss 0.553958, acc 0.75, learning_rate 0.000193865
2017-10-10T12:54:49.612255: step 969, loss 0.540601, acc 0.78125, learning_rate 0.000193482
2017-10-10T12:54:50.136846: step 970, loss 0.501381, acc 0.890625, learning_rate 0.0001931
2017-10-10T12:54:50.692935: step 971, loss 0.325926, acc 0.890625, learning_rate 0.00019272
2017-10-10T12:54:51.213802: step 972, loss 0.372366, acc 0.875, learning_rate 0.000192341
2017-10-10T12:54:51.715930: step 973, loss 0.343198, acc 0.859375, learning_rate 0.000191965
2017-10-10T12:54:52.237569: step 974, loss 0.379732, acc 0.90625, learning_rate 0.000191589
2017-10-10T12:54:52.772912: step 975, loss 0.529735, acc 0.828125, learning_rate 0.000191215
2017-10-10T12:54:53.272901: step 976, loss 0.367796, acc 0.890625, learning_rate 0.000190843
2017-10-10T12:54:53.870314: step 977, loss 0.339345, acc 0.859375, learning_rate 0.000190472
2017-10-10T12:54:54.386976: step 978, loss 0.442853, acc 0.859375, learning_rate 0.000190103
2017-10-10T12:54:54.912704: step 979, loss 0.419416, acc 0.890625, learning_rate 0.000189735
2017-10-10T12:54:55.376845: step 980, loss 0.720616, acc 0.803922, learning_rate 0.000189369
2017-10-10T12:54:55.896105: step 981, loss 0.709944, acc 0.796875, learning_rate 0.000189004
2017-10-10T12:54:56.476915: step 982, loss 0.506539, acc 0.84375, learning_rate 0.000188641
2017-10-10T12:54:57.048149: step 983, loss 0.345497, acc 0.90625, learning_rate 0.000188279
2017-10-10T12:54:57.560212: step 984, loss 0.38171, acc 0.875, learning_rate 0.000187919
2017-10-10T12:54:58.061073: step 985, loss 0.360638, acc 0.921875, learning_rate 0.00018756
2017-10-10T12:54:58.472824: step 986, loss 0.600527, acc 0.796875, learning_rate 0.000187202
2017-10-10T12:54:58.908839: step 987, loss 0.31249, acc 0.859375, learning_rate 0.000186846
2017-10-10T12:54:59.337396: step 988, loss 0.444533, acc 0.859375, learning_rate 0.000186492
2017-10-10T12:54:59.915258: step 989, loss 0.34369, acc 0.875, learning_rate 0.000186139
2017-10-10T12:55:00.492343: step 990, loss 0.397335, acc 0.890625, learning_rate 0.000185787
2017-10-10T12:55:01.029062: step 991, loss 0.600171, acc 0.8125, learning_rate 0.000185437
2017-10-10T12:55:01.538581: step 992, loss 0.429793, acc 0.859375, learning_rate 0.000185088
2017-10-10T12:55:02.098423: step 993, loss 0.38022, acc 0.875, learning_rate 0.000184741
2017-10-10T12:55:02.661294: step 994, loss 0.326827, acc 0.90625, learning_rate 0.000184395
2017-10-10T12:55:03.169924: step 995, loss 0.59842, acc 0.796875, learning_rate 0.000184051
2017-10-10T12:55:03.693039: step 996, loss 0.510575, acc 0.859375, learning_rate 0.000183708
2017-10-10T12:55:04.161179: step 997, loss 0.455122, acc 0.78125, learning_rate 0.000183366
2017-10-10T12:55:04.712018: step 998, loss 0.466075, acc 0.859375, learning_rate 0.000183026
2017-10-10T12:55:05.213058: step 999, loss 0.557443, acc 0.8125, learning_rate 0.000182687
2017-10-10T12:55:05.788829: step 1000, loss 0.500582, acc 0.859375, learning_rate 0.000182349

Evaluation:
2017-10-10T12:55:06.912863: step 1000, loss 0.302019, acc 0.883453

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1000

2017-10-10T12:55:08.647074: step 1001, loss 0.516906, acc 0.8125, learning_rate 0.000182013
2017-10-10T12:55:09.200948: step 1002, loss 0.369071, acc 0.859375, learning_rate 0.000181678
2017-10-10T12:55:09.785134: step 1003, loss 0.366365, acc 0.890625, learning_rate 0.000181345
2017-10-10T12:55:10.408858: step 1004, loss 0.678481, acc 0.8125, learning_rate 0.000181013
2017-10-10T12:55:10.956855: step 1005, loss 0.345182, acc 0.875, learning_rate 0.000180682
2017-10-10T12:55:11.323300: step 1006, loss 0.520829, acc 0.859375, learning_rate 0.000180353
2017-10-10T12:55:11.745011: step 1007, loss 0.554718, acc 0.84375, learning_rate 0.000180025
2017-10-10T12:55:12.261066: step 1008, loss 0.359305, acc 0.875, learning_rate 0.000179698
2017-10-10T12:55:12.739067: step 1009, loss 0.514629, acc 0.828125, learning_rate 0.000179373
2017-10-10T12:55:13.164477: step 1010, loss 0.550074, acc 0.828125, learning_rate 0.000179049
2017-10-10T12:55:13.757049: step 1011, loss 0.500087, acc 0.828125, learning_rate 0.000178726
2017-10-10T12:55:14.213433: step 1012, loss 0.239413, acc 0.921875, learning_rate 0.000178405
2017-10-10T12:55:14.724455: step 1013, loss 0.544572, acc 0.8125, learning_rate 0.000178085
2017-10-10T12:55:15.184043: step 1014, loss 0.409491, acc 0.875, learning_rate 0.000177766
2017-10-10T12:55:15.678017: step 1015, loss 0.512756, acc 0.828125, learning_rate 0.000177449
2017-10-10T12:55:16.237135: step 1016, loss 0.490141, acc 0.859375, learning_rate 0.000177133
2017-10-10T12:55:16.753115: step 1017, loss 0.33246, acc 0.875, learning_rate 0.000176818
2017-10-10T12:55:17.322633: step 1018, loss 0.355651, acc 0.84375, learning_rate 0.000176504
2017-10-10T12:55:17.832799: step 1019, loss 0.360005, acc 0.859375, learning_rate 0.000176192
2017-10-10T12:55:18.409119: step 1020, loss 0.305427, acc 0.9375, learning_rate 0.000175881
2017-10-10T12:55:18.916846: step 1021, loss 0.294545, acc 0.9375, learning_rate 0.000175571
2017-10-10T12:55:19.476431: step 1022, loss 0.43316, acc 0.828125, learning_rate 0.000175263
2017-10-10T12:55:19.925018: step 1023, loss 0.435122, acc 0.796875, learning_rate 0.000174956
2017-10-10T12:55:20.370176: step 1024, loss 0.394884, acc 0.828125, learning_rate 0.00017465
2017-10-10T12:55:20.786234: step 1025, loss 0.369839, acc 0.890625, learning_rate 0.000174345
2017-10-10T12:55:21.408805: step 1026, loss 0.287045, acc 0.921875, learning_rate 0.000174042
2017-10-10T12:55:21.880413: step 1027, loss 0.448104, acc 0.859375, learning_rate 0.000173739
2017-10-10T12:55:22.337480: step 1028, loss 0.343208, acc 0.859375, learning_rate 0.000173438
2017-10-10T12:55:22.851586: step 1029, loss 0.366216, acc 0.890625, learning_rate 0.000173139
2017-10-10T12:55:23.380694: step 1030, loss 0.463875, acc 0.875, learning_rate 0.00017284
2017-10-10T12:55:23.876876: step 1031, loss 0.362477, acc 0.875, learning_rate 0.000172543
2017-10-10T12:55:24.461139: step 1032, loss 0.406907, acc 0.890625, learning_rate 0.000172247
2017-10-10T12:55:25.034260: step 1033, loss 0.557773, acc 0.78125, learning_rate 0.000171952
2017-10-10T12:55:25.603137: step 1034, loss 0.414931, acc 0.84375, learning_rate 0.000171658
2017-10-10T12:55:26.154300: step 1035, loss 0.767348, acc 0.828125, learning_rate 0.000171366
2017-10-10T12:55:26.668969: step 1036, loss 0.468365, acc 0.875, learning_rate 0.000171074
2017-10-10T12:55:27.212835: step 1037, loss 0.434219, acc 0.84375, learning_rate 0.000170784
2017-10-10T12:55:27.737454: step 1038, loss 0.484904, acc 0.828125, learning_rate 0.000170495
2017-10-10T12:55:28.244842: step 1039, loss 0.500482, acc 0.765625, learning_rate 0.000170208
2017-10-10T12:55:28.788870: step 1040, loss 0.40198, acc 0.921875, learning_rate 0.000169921

Evaluation:
2017-10-10T12:55:30.005410: step 1040, loss 0.30122, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1040

2017-10-10T12:55:31.469022: step 1041, loss 0.37657, acc 0.921875, learning_rate 0.000169636
2017-10-10T12:55:31.964659: step 1042, loss 0.463786, acc 0.859375, learning_rate 0.000169351
2017-10-10T12:55:32.526227: step 1043, loss 0.428708, acc 0.828125, learning_rate 0.000169068
2017-10-10T12:55:33.062141: step 1044, loss 0.402987, acc 0.84375, learning_rate 0.000168786
2017-10-10T12:55:33.664982: step 1045, loss 0.453762, acc 0.84375, learning_rate 0.000168506
2017-10-10T12:55:34.192858: step 1046, loss 0.66319, acc 0.71875, learning_rate 0.000168226
2017-10-10T12:55:34.673214: step 1047, loss 0.521697, acc 0.828125, learning_rate 0.000167947
2017-10-10T12:55:35.132103: step 1048, loss 0.357033, acc 0.875, learning_rate 0.00016767
2017-10-10T12:55:35.618667: step 1049, loss 0.575023, acc 0.796875, learning_rate 0.000167394
2017-10-10T12:55:36.199759: step 1050, loss 0.394988, acc 0.90625, learning_rate 0.000167119
2017-10-10T12:55:36.741438: step 1051, loss 0.30306, acc 0.890625, learning_rate 0.000166845
2017-10-10T12:55:37.251482: step 1052, loss 0.41107, acc 0.90625, learning_rate 0.000166572
2017-10-10T12:55:37.797003: step 1053, loss 0.480258, acc 0.8125, learning_rate 0.0001663
2017-10-10T12:55:38.237001: step 1054, loss 0.35445, acc 0.859375, learning_rate 0.00016603
2017-10-10T12:55:38.757033: step 1055, loss 0.612716, acc 0.828125, learning_rate 0.00016576
2017-10-10T12:55:39.253074: step 1056, loss 0.474415, acc 0.875, learning_rate 0.000165492
2017-10-10T12:55:39.781533: step 1057, loss 0.359309, acc 0.828125, learning_rate 0.000165224
2017-10-10T12:55:40.340935: step 1058, loss 0.447275, acc 0.796875, learning_rate 0.000164958
2017-10-10T12:55:40.837015: step 1059, loss 0.453309, acc 0.8125, learning_rate 0.000164693
2017-10-10T12:55:41.301036: step 1060, loss 0.785134, acc 0.8125, learning_rate 0.000164429
2017-10-10T12:55:41.866315: step 1061, loss 0.53081, acc 0.875, learning_rate 0.000164166
2017-10-10T12:55:42.537740: step 1062, loss 0.286742, acc 0.9375, learning_rate 0.000163904
2017-10-10T12:55:43.159334: step 1063, loss 0.542136, acc 0.84375, learning_rate 0.000163643
2017-10-10T12:55:43.614754: step 1064, loss 0.327107, acc 0.90625, learning_rate 0.000163383
2017-10-10T12:55:44.120927: step 1065, loss 0.400455, acc 0.859375, learning_rate 0.000163125
2017-10-10T12:55:44.751824: step 1066, loss 0.437006, acc 0.890625, learning_rate 0.000162867
2017-10-10T12:55:45.240825: step 1067, loss 0.360845, acc 0.875, learning_rate 0.00016261
2017-10-10T12:55:45.693900: step 1068, loss 0.558936, acc 0.828125, learning_rate 0.000162355
2017-10-10T12:55:46.240227: step 1069, loss 0.262692, acc 0.953125, learning_rate 0.0001621
2017-10-10T12:55:46.796845: step 1070, loss 0.602301, acc 0.8125, learning_rate 0.000161847
2017-10-10T12:55:47.343899: step 1071, loss 0.485711, acc 0.796875, learning_rate 0.000161594
2017-10-10T12:55:47.912865: step 1072, loss 0.425967, acc 0.78125, learning_rate 0.000161343
2017-10-10T12:55:48.414219: step 1073, loss 0.449894, acc 0.859375, learning_rate 0.000161093
2017-10-10T12:55:48.927585: step 1074, loss 0.464469, acc 0.875, learning_rate 0.000160843
2017-10-10T12:55:49.457212: step 1075, loss 0.395238, acc 0.875, learning_rate 0.000160595
2017-10-10T12:55:49.968960: step 1076, loss 0.476623, acc 0.859375, learning_rate 0.000160348
2017-10-10T12:55:50.481035: step 1077, loss 0.474767, acc 0.828125, learning_rate 0.000160101
2017-10-10T12:55:50.943672: step 1078, loss 0.391329, acc 0.901961, learning_rate 0.000159856
2017-10-10T12:55:51.477051: step 1079, loss 0.440609, acc 0.859375, learning_rate 0.000159612
2017-10-10T12:55:52.080870: step 1080, loss 0.556698, acc 0.796875, learning_rate 0.000159368

Evaluation:
2017-10-10T12:55:53.340953: step 1080, loss 0.298467, acc 0.882014

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1080

2017-10-10T12:55:54.951370: step 1081, loss 0.439583, acc 0.828125, learning_rate 0.000159126
2017-10-10T12:55:55.504564: step 1082, loss 0.461945, acc 0.84375, learning_rate 0.000158885
2017-10-10T12:55:55.993976: step 1083, loss 0.389251, acc 0.859375, learning_rate 0.000158644
2017-10-10T12:55:56.575698: step 1084, loss 0.242663, acc 0.90625, learning_rate 0.000158405
2017-10-10T12:55:57.192859: step 1085, loss 0.313597, acc 0.9375, learning_rate 0.000158167
2017-10-10T12:55:57.627408: step 1086, loss 0.257283, acc 0.890625, learning_rate 0.000157929
2017-10-10T12:55:58.075403: step 1087, loss 0.524642, acc 0.84375, learning_rate 0.000157693
2017-10-10T12:55:58.591786: step 1088, loss 0.47697, acc 0.8125, learning_rate 0.000157457
2017-10-10T12:55:59.125569: step 1089, loss 0.281222, acc 0.875, learning_rate 0.000157223
2017-10-10T12:55:59.621096: step 1090, loss 0.487893, acc 0.828125, learning_rate 0.000156989
2017-10-10T12:56:00.154190: step 1091, loss 0.431172, acc 0.84375, learning_rate 0.000156757
2017-10-10T12:56:00.713646: step 1092, loss 0.487481, acc 0.84375, learning_rate 0.000156525
2017-10-10T12:56:01.281905: step 1093, loss 0.400903, acc 0.84375, learning_rate 0.000156294
2017-10-10T12:56:01.821571: step 1094, loss 0.565164, acc 0.8125, learning_rate 0.000156064
2017-10-10T12:56:02.361503: step 1095, loss 0.393765, acc 0.8125, learning_rate 0.000155836
2017-10-10T12:56:02.844579: step 1096, loss 0.642491, acc 0.78125, learning_rate 0.000155608
2017-10-10T12:56:03.348873: step 1097, loss 0.264258, acc 0.875, learning_rate 0.000155381
2017-10-10T12:56:03.902959: step 1098, loss 0.328204, acc 0.890625, learning_rate 0.000155155
2017-10-10T12:56:04.357971: step 1099, loss 0.445681, acc 0.859375, learning_rate 0.000154929
2017-10-10T12:56:04.976778: step 1100, loss 0.851727, acc 0.78125, learning_rate 0.000154705
2017-10-10T12:56:05.593229: step 1101, loss 0.399302, acc 0.8125, learning_rate 0.000154482
2017-10-10T12:56:06.001905: step 1102, loss 0.407961, acc 0.859375, learning_rate 0.00015426
2017-10-10T12:56:06.439652: step 1103, loss 0.790556, acc 0.796875, learning_rate 0.000154038
2017-10-10T12:56:06.916944: step 1104, loss 0.478977, acc 0.8125, learning_rate 0.000153818
2017-10-10T12:56:07.456819: step 1105, loss 0.483562, acc 0.8125, learning_rate 0.000153598
2017-10-10T12:56:07.976826: step 1106, loss 0.517718, acc 0.828125, learning_rate 0.000153379
2017-10-10T12:56:08.392559: step 1107, loss 0.545362, acc 0.8125, learning_rate 0.000153161
2017-10-10T12:56:08.815417: step 1108, loss 0.496914, acc 0.8125, learning_rate 0.000152944
2017-10-10T12:56:09.336953: step 1109, loss 0.230456, acc 0.9375, learning_rate 0.000152728
2017-10-10T12:56:09.832225: step 1110, loss 0.432909, acc 0.828125, learning_rate 0.000152513
2017-10-10T12:56:10.360185: step 1111, loss 0.389157, acc 0.875, learning_rate 0.000152299
2017-10-10T12:56:10.794512: step 1112, loss 0.547593, acc 0.8125, learning_rate 0.000152085
2017-10-10T12:56:11.294462: step 1113, loss 0.475111, acc 0.84375, learning_rate 0.000151872
2017-10-10T12:56:11.862717: step 1114, loss 0.8168, acc 0.8125, learning_rate 0.000151661
2017-10-10T12:56:12.369090: step 1115, loss 0.537562, acc 0.828125, learning_rate 0.00015145
2017-10-10T12:56:12.861046: step 1116, loss 0.448891, acc 0.859375, learning_rate 0.00015124
2017-10-10T12:56:13.391445: step 1117, loss 0.498445, acc 0.828125, learning_rate 0.000151031
2017-10-10T12:56:13.901053: step 1118, loss 0.370817, acc 0.859375, learning_rate 0.000150822
2017-10-10T12:56:14.429036: step 1119, loss 0.513367, acc 0.84375, learning_rate 0.000150615
2017-10-10T12:56:14.969100: step 1120, loss 0.482915, acc 0.84375, learning_rate 0.000150408

Evaluation:
2017-10-10T12:56:16.169022: step 1120, loss 0.297532, acc 0.880576

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1120

2017-10-10T12:56:17.956848: step 1121, loss 0.48618, acc 0.796875, learning_rate 0.000150203
2017-10-10T12:56:18.492418: step 1122, loss 0.441657, acc 0.875, learning_rate 0.000149998
2017-10-10T12:56:19.056978: step 1123, loss 0.448729, acc 0.828125, learning_rate 0.000149794
2017-10-10T12:56:19.552982: step 1124, loss 0.53201, acc 0.8125, learning_rate 0.00014959
2017-10-10T12:56:20.109409: step 1125, loss 0.307526, acc 0.875, learning_rate 0.000149388
2017-10-10T12:56:20.651877: step 1126, loss 0.403243, acc 0.875, learning_rate 0.000149186
2017-10-10T12:56:21.198916: step 1127, loss 0.324682, acc 0.90625, learning_rate 0.000148986
2017-10-10T12:56:21.759554: step 1128, loss 0.43579, acc 0.859375, learning_rate 0.000148786
2017-10-10T12:56:22.310619: step 1129, loss 0.463717, acc 0.875, learning_rate 0.000148587
2017-10-10T12:56:22.901409: step 1130, loss 0.506773, acc 0.859375, learning_rate 0.000148388
2017-10-10T12:56:23.521093: step 1131, loss 0.542765, acc 0.828125, learning_rate 0.000148191
2017-10-10T12:56:24.034965: step 1132, loss 0.415007, acc 0.828125, learning_rate 0.000147994
2017-10-10T12:56:24.581495: step 1133, loss 0.415119, acc 0.859375, learning_rate 0.000147798
2017-10-10T12:56:25.191003: step 1134, loss 0.546885, acc 0.84375, learning_rate 0.000147603
2017-10-10T12:56:25.684862: step 1135, loss 0.324558, acc 0.890625, learning_rate 0.000147409
2017-10-10T12:56:26.196934: step 1136, loss 0.269457, acc 0.890625, learning_rate 0.000147215
2017-10-10T12:56:26.665020: step 1137, loss 0.611019, acc 0.796875, learning_rate 0.000147022
2017-10-10T12:56:27.187511: step 1138, loss 0.329056, acc 0.890625, learning_rate 0.000146831
2017-10-10T12:56:27.721010: step 1139, loss 0.293131, acc 0.84375, learning_rate 0.000146639
2017-10-10T12:56:28.316118: step 1140, loss 0.495252, acc 0.84375, learning_rate 0.000146449
2017-10-10T12:56:28.783737: step 1141, loss 0.61008, acc 0.796875, learning_rate 0.000146259
2017-10-10T12:56:29.186765: step 1142, loss 0.270401, acc 0.953125, learning_rate 0.000146071
2017-10-10T12:56:29.608822: step 1143, loss 0.460675, acc 0.828125, learning_rate 0.000145883
2017-10-10T12:56:30.183579: step 1144, loss 0.403881, acc 0.890625, learning_rate 0.000145695
2017-10-10T12:56:30.784354: step 1145, loss 0.625871, acc 0.828125, learning_rate 0.000145509
2017-10-10T12:56:31.268344: step 1146, loss 0.380471, acc 0.859375, learning_rate 0.000145323
2017-10-10T12:56:31.708013: step 1147, loss 0.595781, acc 0.8125, learning_rate 0.000145138
2017-10-10T12:56:32.179055: step 1148, loss 0.326381, acc 0.90625, learning_rate 0.000144954
2017-10-10T12:56:32.595827: step 1149, loss 0.481729, acc 0.84375, learning_rate 0.00014477
2017-10-10T12:56:33.103723: step 1150, loss 0.430654, acc 0.84375, learning_rate 0.000144588
2017-10-10T12:56:33.613751: step 1151, loss 0.38077, acc 0.875, learning_rate 0.000144406
2017-10-10T12:56:34.125084: step 1152, loss 0.470595, acc 0.859375, learning_rate 0.000144224
2017-10-10T12:56:34.661189: step 1153, loss 0.480904, acc 0.859375, learning_rate 0.000144044
2017-10-10T12:56:35.238492: step 1154, loss 0.232771, acc 0.921875, learning_rate 0.000143864
2017-10-10T12:56:35.783402: step 1155, loss 0.412709, acc 0.84375, learning_rate 0.000143685
2017-10-10T12:56:36.327893: step 1156, loss 0.425377, acc 0.859375, learning_rate 0.000143507
2017-10-10T12:56:36.860852: step 1157, loss 0.431889, acc 0.84375, learning_rate 0.000143329
2017-10-10T12:56:37.413649: step 1158, loss 0.335566, acc 0.890625, learning_rate 0.000143152
2017-10-10T12:56:37.962711: step 1159, loss 0.195936, acc 0.9375, learning_rate 0.000142976
2017-10-10T12:56:38.504828: step 1160, loss 0.250613, acc 0.921875, learning_rate 0.000142801

Evaluation:
2017-10-10T12:56:39.698144: step 1160, loss 0.297475, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1160

2017-10-10T12:56:41.156923: step 1161, loss 0.364433, acc 0.84375, learning_rate 0.000142626
2017-10-10T12:56:41.649101: step 1162, loss 0.343297, acc 0.890625, learning_rate 0.000142452
2017-10-10T12:56:42.211886: step 1163, loss 0.340664, acc 0.921875, learning_rate 0.000142279
2017-10-10T12:56:42.821351: step 1164, loss 0.464928, acc 0.84375, learning_rate 0.000142106
2017-10-10T12:56:43.258958: step 1165, loss 0.317382, acc 0.859375, learning_rate 0.000141934
2017-10-10T12:56:43.680961: step 1166, loss 0.319819, acc 0.9375, learning_rate 0.000141763
2017-10-10T12:56:44.152884: step 1167, loss 0.402861, acc 0.890625, learning_rate 0.000141593
2017-10-10T12:56:44.720845: step 1168, loss 0.47484, acc 0.84375, learning_rate 0.000141423
2017-10-10T12:56:45.248896: step 1169, loss 0.268861, acc 0.890625, learning_rate 0.000141254
2017-10-10T12:56:45.795656: step 1170, loss 0.391119, acc 0.859375, learning_rate 0.000141085
2017-10-10T12:56:46.328803: step 1171, loss 0.248434, acc 0.96875, learning_rate 0.000140918
2017-10-10T12:56:46.870347: step 1172, loss 0.467093, acc 0.859375, learning_rate 0.000140751
2017-10-10T12:56:47.386967: step 1173, loss 0.453817, acc 0.859375, learning_rate 0.000140584
2017-10-10T12:56:47.887897: step 1174, loss 0.435912, acc 0.84375, learning_rate 0.000140419
2017-10-10T12:56:48.450986: step 1175, loss 0.40573, acc 0.875, learning_rate 0.000140254
2017-10-10T12:56:48.904972: step 1176, loss 0.496481, acc 0.843137, learning_rate 0.000140089
2017-10-10T12:56:49.474098: step 1177, loss 0.475746, acc 0.890625, learning_rate 0.000139926
2017-10-10T12:56:50.025028: step 1178, loss 0.591796, acc 0.78125, learning_rate 0.000139763
2017-10-10T12:56:50.576820: step 1179, loss 0.31197, acc 0.9375, learning_rate 0.0001396
2017-10-10T12:56:51.134150: step 1180, loss 0.40256, acc 0.890625, learning_rate 0.000139439
2017-10-10T12:56:51.804054: step 1181, loss 0.368254, acc 0.84375, learning_rate 0.000139278
2017-10-10T12:56:52.244183: step 1182, loss 0.273986, acc 0.890625, learning_rate 0.000139118
2017-10-10T12:56:52.700508: step 1183, loss 0.297739, acc 0.90625, learning_rate 0.000138958
2017-10-10T12:56:53.345106: step 1184, loss 0.187484, acc 0.9375, learning_rate 0.000138799
2017-10-10T12:56:53.894837: step 1185, loss 0.534561, acc 0.78125, learning_rate 0.00013864
2017-10-10T12:56:54.334381: step 1186, loss 0.239889, acc 0.921875, learning_rate 0.000138483
2017-10-10T12:56:54.781910: step 1187, loss 0.682324, acc 0.71875, learning_rate 0.000138326
2017-10-10T12:56:55.333641: step 1188, loss 0.440017, acc 0.875, learning_rate 0.000138169
2017-10-10T12:56:55.832949: step 1189, loss 0.262264, acc 0.890625, learning_rate 0.000138013
2017-10-10T12:56:56.331600: step 1190, loss 0.293718, acc 0.890625, learning_rate 0.000137858
2017-10-10T12:56:56.869834: step 1191, loss 0.455991, acc 0.859375, learning_rate 0.000137704
2017-10-10T12:56:57.383237: step 1192, loss 0.64367, acc 0.796875, learning_rate 0.00013755
2017-10-10T12:56:58.004847: step 1193, loss 0.789065, acc 0.765625, learning_rate 0.000137397
2017-10-10T12:56:58.543429: step 1194, loss 0.545386, acc 0.765625, learning_rate 0.000137244
2017-10-10T12:56:59.048965: step 1195, loss 0.49527, acc 0.8125, learning_rate 0.000137092
2017-10-10T12:56:59.528597: step 1196, loss 0.312932, acc 0.890625, learning_rate 0.000136941
2017-10-10T12:57:00.069076: step 1197, loss 0.422462, acc 0.859375, learning_rate 0.00013679
2017-10-10T12:57:00.563150: step 1198, loss 0.596796, acc 0.84375, learning_rate 0.00013664
2017-10-10T12:57:01.035055: step 1199, loss 0.727351, acc 0.71875, learning_rate 0.00013649
2017-10-10T12:57:01.560873: step 1200, loss 0.368295, acc 0.84375, learning_rate 0.000136341

Evaluation:
2017-10-10T12:57:02.734024: step 1200, loss 0.296364, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1200

2017-10-10T12:57:04.488986: step 1201, loss 0.45949, acc 0.84375, learning_rate 0.000136193
2017-10-10T12:57:05.020922: step 1202, loss 0.504547, acc 0.84375, learning_rate 0.000136045
2017-10-10T12:57:05.549585: step 1203, loss 0.395917, acc 0.84375, learning_rate 0.000135898
2017-10-10T12:57:06.072845: step 1204, loss 0.38575, acc 0.90625, learning_rate 0.000135751
2017-10-10T12:57:06.496884: step 1205, loss 0.495882, acc 0.796875, learning_rate 0.000135605
2017-10-10T12:57:06.913012: step 1206, loss 0.391031, acc 0.875, learning_rate 0.00013546
2017-10-10T12:57:07.416863: step 1207, loss 0.553531, acc 0.796875, learning_rate 0.000135315
2017-10-10T12:57:07.975153: step 1208, loss 0.428463, acc 0.875, learning_rate 0.000135171
2017-10-10T12:57:08.538200: step 1209, loss 0.472484, acc 0.828125, learning_rate 0.000135028
2017-10-10T12:57:09.121129: step 1210, loss 0.321896, acc 0.84375, learning_rate 0.000134885
2017-10-10T12:57:09.674478: step 1211, loss 0.458228, acc 0.84375, learning_rate 0.000134742
2017-10-10T12:57:10.203565: step 1212, loss 0.462711, acc 0.875, learning_rate 0.0001346
2017-10-10T12:57:10.768919: step 1213, loss 0.378913, acc 0.828125, learning_rate 0.000134459
2017-10-10T12:57:11.355791: step 1214, loss 0.369341, acc 0.890625, learning_rate 0.000134319
2017-10-10T12:57:11.906052: step 1215, loss 0.677752, acc 0.78125, learning_rate 0.000134178
2017-10-10T12:57:12.471501: step 1216, loss 0.312961, acc 0.90625, learning_rate 0.000134039
2017-10-10T12:57:13.023589: step 1217, loss 0.486041, acc 0.828125, learning_rate 0.0001339
2017-10-10T12:57:13.556554: step 1218, loss 0.605703, acc 0.796875, learning_rate 0.000133762
2017-10-10T12:57:14.156918: step 1219, loss 0.417311, acc 0.875, learning_rate 0.000133624
2017-10-10T12:57:14.761009: step 1220, loss 0.316487, acc 0.890625, learning_rate 0.000133487
2017-10-10T12:57:15.240103: step 1221, loss 0.371397, acc 0.890625, learning_rate 0.00013335
2017-10-10T12:57:15.668823: step 1222, loss 0.402472, acc 0.890625, learning_rate 0.000133214
2017-10-10T12:57:16.093442: step 1223, loss 0.507927, acc 0.734375, learning_rate 0.000133078
2017-10-10T12:57:16.757959: step 1224, loss 0.488117, acc 0.859375, learning_rate 0.000132943
2017-10-10T12:57:17.295443: step 1225, loss 0.412821, acc 0.828125, learning_rate 0.000132809
2017-10-10T12:57:17.742955: step 1226, loss 0.245512, acc 0.9375, learning_rate 0.000132675
2017-10-10T12:57:18.204384: step 1227, loss 0.484945, acc 0.8125, learning_rate 0.000132541
2017-10-10T12:57:18.780930: step 1228, loss 0.262472, acc 0.9375, learning_rate 0.000132409
2017-10-10T12:57:19.316830: step 1229, loss 0.304582, acc 0.90625, learning_rate 0.000132276
2017-10-10T12:57:19.852886: step 1230, loss 0.590742, acc 0.796875, learning_rate 0.000132145
2017-10-10T12:57:20.365799: step 1231, loss 0.254981, acc 0.921875, learning_rate 0.000132013
2017-10-10T12:57:20.876715: step 1232, loss 0.377059, acc 0.890625, learning_rate 0.000131883
2017-10-10T12:57:21.388870: step 1233, loss 0.722134, acc 0.734375, learning_rate 0.000131753
2017-10-10T12:57:21.828944: step 1234, loss 0.13318, acc 0.96875, learning_rate 0.000131623
2017-10-10T12:57:22.352990: step 1235, loss 0.436065, acc 0.859375, learning_rate 0.000131494
2017-10-10T12:57:22.800758: step 1236, loss 0.419506, acc 0.859375, learning_rate 0.000131365
2017-10-10T12:57:23.292843: step 1237, loss 0.420882, acc 0.84375, learning_rate 0.000131237
2017-10-10T12:57:23.822459: step 1238, loss 0.407274, acc 0.84375, learning_rate 0.00013111
2017-10-10T12:57:24.362536: step 1239, loss 0.4585, acc 0.828125, learning_rate 0.000130983
2017-10-10T12:57:24.899122: step 1240, loss 0.250688, acc 0.90625, learning_rate 0.000130856

Evaluation:
2017-10-10T12:57:26.119161: step 1240, loss 0.294436, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1240

2017-10-10T12:57:27.928533: step 1241, loss 0.328929, acc 0.875, learning_rate 0.00013073
2017-10-10T12:57:28.548915: step 1242, loss 0.512634, acc 0.8125, learning_rate 0.000130605
2017-10-10T12:57:29.165904: step 1243, loss 0.547435, acc 0.8125, learning_rate 0.00013048
2017-10-10T12:57:29.617004: step 1244, loss 0.364541, acc 0.890625, learning_rate 0.000130356
2017-10-10T12:57:30.055818: step 1245, loss 0.619709, acc 0.78125, learning_rate 0.000130232
2017-10-10T12:57:30.622041: step 1246, loss 0.380029, acc 0.875, learning_rate 0.000130108
2017-10-10T12:57:31.090932: step 1247, loss 0.279906, acc 0.921875, learning_rate 0.000129985
2017-10-10T12:57:31.620932: step 1248, loss 0.765032, acc 0.78125, learning_rate 0.000129863
2017-10-10T12:57:32.125808: step 1249, loss 0.359283, acc 0.859375, learning_rate 0.000129741
2017-10-10T12:57:32.636702: step 1250, loss 0.261104, acc 0.953125, learning_rate 0.00012962
2017-10-10T12:57:33.120908: step 1251, loss 0.461743, acc 0.890625, learning_rate 0.000129499
2017-10-10T12:57:33.588891: step 1252, loss 0.370567, acc 0.859375, learning_rate 0.000129378
2017-10-10T12:57:34.160936: step 1253, loss 0.29009, acc 0.921875, learning_rate 0.000129259
2017-10-10T12:57:34.747277: step 1254, loss 0.428903, acc 0.859375, learning_rate 0.000129139
2017-10-10T12:57:35.149057: step 1255, loss 0.22712, acc 0.953125, learning_rate 0.00012902
2017-10-10T12:57:35.725001: step 1256, loss 0.348347, acc 0.890625, learning_rate 0.000128902
2017-10-10T12:57:36.308876: step 1257, loss 0.355307, acc 0.859375, learning_rate 0.000128784
2017-10-10T12:57:36.769209: step 1258, loss 0.326161, acc 0.84375, learning_rate 0.000128666
2017-10-10T12:57:37.368958: step 1259, loss 0.494259, acc 0.859375, learning_rate 0.000128549
2017-10-10T12:57:37.829919: step 1260, loss 0.527496, acc 0.8125, learning_rate 0.000128433
2017-10-10T12:57:38.176958: step 1261, loss 0.46542, acc 0.875, learning_rate 0.000128317
2017-10-10T12:57:38.648822: step 1262, loss 0.478361, acc 0.828125, learning_rate 0.000128201
2017-10-10T12:57:39.291586: step 1263, loss 0.364987, acc 0.890625, learning_rate 0.000128086
2017-10-10T12:57:39.881557: step 1264, loss 0.561279, acc 0.796875, learning_rate 0.000127971
2017-10-10T12:57:40.312309: step 1265, loss 0.41394, acc 0.90625, learning_rate 0.000127857
2017-10-10T12:57:40.781574: step 1266, loss 0.602998, acc 0.765625, learning_rate 0.000127743
2017-10-10T12:57:41.228870: step 1267, loss 0.355228, acc 0.859375, learning_rate 0.00012763
2017-10-10T12:57:41.767389: step 1268, loss 0.486579, acc 0.84375, learning_rate 0.000127517
2017-10-10T12:57:42.323833: step 1269, loss 0.500656, acc 0.828125, learning_rate 0.000127405
2017-10-10T12:57:42.841969: step 1270, loss 0.429502, acc 0.859375, learning_rate 0.000127293
2017-10-10T12:57:43.379970: step 1271, loss 0.443513, acc 0.828125, learning_rate 0.000127182
2017-10-10T12:57:43.905043: step 1272, loss 0.550716, acc 0.859375, learning_rate 0.000127071
2017-10-10T12:57:44.437168: step 1273, loss 0.341264, acc 0.875, learning_rate 0.00012696
2017-10-10T12:57:44.961071: step 1274, loss 0.423633, acc 0.803922, learning_rate 0.00012685
2017-10-10T12:57:45.464916: step 1275, loss 0.386113, acc 0.890625, learning_rate 0.000126741
2017-10-10T12:57:45.939513: step 1276, loss 0.370903, acc 0.875, learning_rate 0.000126632
2017-10-10T12:57:46.524918: step 1277, loss 0.488966, acc 0.796875, learning_rate 0.000126523
2017-10-10T12:57:46.956551: step 1278, loss 0.446137, acc 0.8125, learning_rate 0.000126415
2017-10-10T12:57:47.399662: step 1279, loss 0.536016, acc 0.890625, learning_rate 0.000126307
2017-10-10T12:57:47.912445: step 1280, loss 0.29229, acc 0.90625, learning_rate 0.000126199

Evaluation:
2017-10-10T12:57:49.083396: step 1280, loss 0.294891, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1280

2017-10-10T12:57:50.461777: step 1281, loss 0.288766, acc 0.9375, learning_rate 0.000126093
2017-10-10T12:57:51.076871: step 1282, loss 0.469872, acc 0.8125, learning_rate 0.000125986
2017-10-10T12:57:51.612146: step 1283, loss 0.437072, acc 0.84375, learning_rate 0.00012588
2017-10-10T12:57:52.150835: step 1284, loss 0.392537, acc 0.828125, learning_rate 0.000125774
2017-10-10T12:57:52.597055: step 1285, loss 0.384491, acc 0.859375, learning_rate 0.000125669
2017-10-10T12:57:53.008830: step 1286, loss 0.337158, acc 0.84375, learning_rate 0.000125564
2017-10-10T12:57:53.388687: step 1287, loss 0.339917, acc 0.890625, learning_rate 0.00012546
2017-10-10T12:57:53.964805: step 1288, loss 0.85123, acc 0.734375, learning_rate 0.000125356
2017-10-10T12:57:54.520386: step 1289, loss 0.49455, acc 0.8125, learning_rate 0.000125253
2017-10-10T12:57:55.010252: step 1290, loss 0.303185, acc 0.890625, learning_rate 0.00012515
2017-10-10T12:57:55.504222: step 1291, loss 0.63166, acc 0.8125, learning_rate 0.000125047
2017-10-10T12:57:56.041779: step 1292, loss 0.586408, acc 0.78125, learning_rate 0.000124945
2017-10-10T12:57:56.516967: step 1293, loss 0.474705, acc 0.796875, learning_rate 0.000124843
2017-10-10T12:57:57.053132: step 1294, loss 0.493886, acc 0.828125, learning_rate 0.000124741
2017-10-10T12:57:57.581305: step 1295, loss 0.426469, acc 0.84375, learning_rate 0.00012464
2017-10-10T12:57:58.078981: step 1296, loss 0.599635, acc 0.8125, learning_rate 0.00012454
2017-10-10T12:57:58.647502: step 1297, loss 0.503895, acc 0.859375, learning_rate 0.00012444
2017-10-10T12:57:59.157870: step 1298, loss 0.790352, acc 0.734375, learning_rate 0.00012434
2017-10-10T12:57:59.668816: step 1299, loss 0.286415, acc 0.9375, learning_rate 0.000124241
2017-10-10T12:58:00.153127: step 1300, loss 0.275759, acc 0.9375, learning_rate 0.000124142
2017-10-10T12:58:00.704861: step 1301, loss 0.324654, acc 0.84375, learning_rate 0.000124043
2017-10-10T12:58:01.297072: step 1302, loss 0.432161, acc 0.84375, learning_rate 0.000123945
2017-10-10T12:58:01.716036: step 1303, loss 0.262193, acc 0.90625, learning_rate 0.000123847
2017-10-10T12:58:02.324340: step 1304, loss 0.423173, acc 0.84375, learning_rate 0.00012375
2017-10-10T12:58:02.828205: step 1305, loss 0.454284, acc 0.859375, learning_rate 0.000123653
2017-10-10T12:58:03.296898: step 1306, loss 0.189137, acc 0.9375, learning_rate 0.000123556
2017-10-10T12:58:03.677322: step 1307, loss 0.361815, acc 0.890625, learning_rate 0.00012346
2017-10-10T12:58:04.229155: step 1308, loss 0.313294, acc 0.890625, learning_rate 0.000123364
2017-10-10T12:58:04.828825: step 1309, loss 0.249309, acc 0.890625, learning_rate 0.000123269
2017-10-10T12:58:05.329296: step 1310, loss 0.509359, acc 0.875, learning_rate 0.000123174
2017-10-10T12:58:05.850726: step 1311, loss 0.403285, acc 0.84375, learning_rate 0.00012308
2017-10-10T12:58:06.404011: step 1312, loss 0.446173, acc 0.859375, learning_rate 0.000122985
2017-10-10T12:58:06.928861: step 1313, loss 0.584854, acc 0.84375, learning_rate 0.000122892
2017-10-10T12:58:07.498947: step 1314, loss 0.399926, acc 0.84375, learning_rate 0.000122798
2017-10-10T12:58:08.053965: step 1315, loss 0.419479, acc 0.84375, learning_rate 0.000122705
2017-10-10T12:58:08.585051: step 1316, loss 0.578483, acc 0.8125, learning_rate 0.000122612
2017-10-10T12:58:09.045994: step 1317, loss 0.513562, acc 0.875, learning_rate 0.00012252
2017-10-10T12:58:09.591317: step 1318, loss 0.375099, acc 0.828125, learning_rate 0.000122428
2017-10-10T12:58:10.122254: step 1319, loss 0.602267, acc 0.765625, learning_rate 0.000122337
2017-10-10T12:58:10.582517: step 1320, loss 0.44872, acc 0.875, learning_rate 0.000122245

Evaluation:
2017-10-10T12:58:11.757029: step 1320, loss 0.293896, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1320

2017-10-10T12:58:13.418951: step 1321, loss 0.468485, acc 0.90625, learning_rate 0.000122155
2017-10-10T12:58:14.008422: step 1322, loss 0.546516, acc 0.8125, learning_rate 0.000122064
2017-10-10T12:58:14.523251: step 1323, loss 0.547823, acc 0.859375, learning_rate 0.000121974
2017-10-10T12:58:15.044910: step 1324, loss 0.446065, acc 0.828125, learning_rate 0.000121884
2017-10-10T12:58:15.576959: step 1325, loss 0.309075, acc 0.90625, learning_rate 0.000121795
2017-10-10T12:58:16.056497: step 1326, loss 0.506049, acc 0.796875, learning_rate 0.000121706
2017-10-10T12:58:16.568969: step 1327, loss 0.35225, acc 0.890625, learning_rate 0.000121618
2017-10-10T12:58:17.145483: step 1328, loss 0.55236, acc 0.734375, learning_rate 0.000121529
2017-10-10T12:58:17.702176: step 1329, loss 0.455716, acc 0.828125, learning_rate 0.000121441
2017-10-10T12:58:18.214772: step 1330, loss 0.428504, acc 0.859375, learning_rate 0.000121354
2017-10-10T12:58:18.748873: step 1331, loss 0.386754, acc 0.921875, learning_rate 0.000121267
2017-10-10T12:58:19.295534: step 1332, loss 0.475497, acc 0.84375, learning_rate 0.00012118
2017-10-10T12:58:19.879107: step 1333, loss 0.330608, acc 0.875, learning_rate 0.000121093
2017-10-10T12:58:20.412851: step 1334, loss 0.596901, acc 0.8125, learning_rate 0.000121007
2017-10-10T12:58:20.973912: step 1335, loss 0.728059, acc 0.828125, learning_rate 0.000120922
2017-10-10T12:58:21.443570: step 1336, loss 0.454432, acc 0.828125, learning_rate 0.000120836
2017-10-10T12:58:22.037056: step 1337, loss 0.269892, acc 0.921875, learning_rate 0.000120751
2017-10-10T12:58:22.608318: step 1338, loss 0.494181, acc 0.78125, learning_rate 0.000120666
2017-10-10T12:58:23.176897: step 1339, loss 0.367447, acc 0.859375, learning_rate 0.000120582
2017-10-10T12:58:23.734829: step 1340, loss 0.624213, acc 0.78125, learning_rate 0.000120498
2017-10-10T12:58:24.246989: step 1341, loss 0.372328, acc 0.875, learning_rate 0.000120414
2017-10-10T12:58:24.769671: step 1342, loss 0.377878, acc 0.84375, learning_rate 0.000120331
2017-10-10T12:58:25.155228: step 1343, loss 0.277611, acc 0.921875, learning_rate 0.000120248
2017-10-10T12:58:25.596566: step 1344, loss 0.337203, acc 0.875, learning_rate 0.000120165
2017-10-10T12:58:26.056401: step 1345, loss 0.352165, acc 0.890625, learning_rate 0.000120083
2017-10-10T12:58:26.604959: step 1346, loss 0.42056, acc 0.828125, learning_rate 0.000120001
2017-10-10T12:58:27.139343: step 1347, loss 0.344007, acc 0.859375, learning_rate 0.00011992
2017-10-10T12:58:27.652844: step 1348, loss 0.439977, acc 0.828125, learning_rate 0.000119838
2017-10-10T12:58:28.241179: step 1349, loss 0.471298, acc 0.84375, learning_rate 0.000119757
2017-10-10T12:58:28.736884: step 1350, loss 0.32306, acc 0.890625, learning_rate 0.000119677
2017-10-10T12:58:29.286548: step 1351, loss 0.629926, acc 0.796875, learning_rate 0.000119596
2017-10-10T12:58:29.842383: step 1352, loss 0.378292, acc 0.875, learning_rate 0.000119516
2017-10-10T12:58:30.377557: step 1353, loss 0.315732, acc 0.921875, learning_rate 0.000119437
2017-10-10T12:58:30.910927: step 1354, loss 0.507165, acc 0.75, learning_rate 0.000119357
2017-10-10T12:58:31.465135: step 1355, loss 0.552048, acc 0.8125, learning_rate 0.000119278
2017-10-10T12:58:32.008323: step 1356, loss 0.400936, acc 0.890625, learning_rate 0.0001192
2017-10-10T12:58:32.469144: step 1357, loss 0.28832, acc 0.90625, learning_rate 0.000119121
2017-10-10T12:58:32.990540: step 1358, loss 0.471058, acc 0.8125, learning_rate 0.000119043
2017-10-10T12:58:33.513080: step 1359, loss 0.537201, acc 0.828125, learning_rate 0.000118965
2017-10-10T12:58:34.002918: step 1360, loss 0.404924, acc 0.828125, learning_rate 0.000118888

Evaluation:
2017-10-10T12:58:35.222769: step 1360, loss 0.294135, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1360

2017-10-10T12:58:36.701169: step 1361, loss 0.503012, acc 0.828125, learning_rate 0.000118811
2017-10-10T12:58:37.305099: step 1362, loss 0.414495, acc 0.84375, learning_rate 0.000118734
2017-10-10T12:58:37.821135: step 1363, loss 0.637911, acc 0.84375, learning_rate 0.000118658
2017-10-10T12:58:38.397163: step 1364, loss 0.509583, acc 0.796875, learning_rate 0.000118582
2017-10-10T12:58:38.832897: step 1365, loss 0.432, acc 0.890625, learning_rate 0.000118506
2017-10-10T12:58:39.298988: step 1366, loss 0.382255, acc 0.859375, learning_rate 0.00011843
2017-10-10T12:58:39.828910: step 1367, loss 0.434935, acc 0.890625, learning_rate 0.000118355
2017-10-10T12:58:40.351562: step 1368, loss 0.454542, acc 0.859375, learning_rate 0.00011828
2017-10-10T12:58:40.888934: step 1369, loss 0.743395, acc 0.734375, learning_rate 0.000118205
2017-10-10T12:58:41.376951: step 1370, loss 0.404654, acc 0.890625, learning_rate 0.000118131
2017-10-10T12:58:41.918957: step 1371, loss 0.276915, acc 0.921875, learning_rate 0.000118057
2017-10-10T12:58:42.305219: step 1372, loss 0.373628, acc 0.843137, learning_rate 0.000117983
2017-10-10T12:58:42.857025: step 1373, loss 0.453355, acc 0.859375, learning_rate 0.00011791
2017-10-10T12:58:43.353016: step 1374, loss 0.359203, acc 0.859375, learning_rate 0.000117837
2017-10-10T12:58:43.796846: step 1375, loss 0.453892, acc 0.796875, learning_rate 0.000117764
2017-10-10T12:58:44.420922: step 1376, loss 0.486866, acc 0.828125, learning_rate 0.000117692
2017-10-10T12:58:45.011288: step 1377, loss 0.27943, acc 0.90625, learning_rate 0.000117619
2017-10-10T12:58:45.506458: step 1378, loss 0.321316, acc 0.90625, learning_rate 0.000117547
2017-10-10T12:58:46.053001: step 1379, loss 0.32744, acc 0.890625, learning_rate 0.000117476
2017-10-10T12:58:46.617156: step 1380, loss 0.451873, acc 0.84375, learning_rate 0.000117404
2017-10-10T12:58:47.089232: step 1381, loss 0.301988, acc 0.875, learning_rate 0.000117333
2017-10-10T12:58:47.629096: step 1382, loss 0.359157, acc 0.875, learning_rate 0.000117263
2017-10-10T12:58:48.001093: step 1383, loss 0.319493, acc 0.90625, learning_rate 0.000117192
2017-10-10T12:58:48.315160: step 1384, loss 0.20615, acc 0.96875, learning_rate 0.000117122
2017-10-10T12:58:48.742144: step 1385, loss 0.308495, acc 0.921875, learning_rate 0.000117052
2017-10-10T12:58:49.177233: step 1386, loss 0.307795, acc 0.890625, learning_rate 0.000116983
2017-10-10T12:58:49.715637: step 1387, loss 0.539752, acc 0.828125, learning_rate 0.000116913
2017-10-10T12:58:50.217011: step 1388, loss 0.238114, acc 0.90625, learning_rate 0.000116844
2017-10-10T12:58:50.791967: step 1389, loss 0.332386, acc 0.875, learning_rate 0.000116775
2017-10-10T12:58:51.328907: step 1390, loss 0.432687, acc 0.8125, learning_rate 0.000116707
2017-10-10T12:58:51.788834: step 1391, loss 0.358916, acc 0.90625, learning_rate 0.000116639
2017-10-10T12:58:52.346885: step 1392, loss 0.50261, acc 0.8125, learning_rate 0.000116571
2017-10-10T12:58:52.865144: step 1393, loss 0.326344, acc 0.90625, learning_rate 0.000116503
2017-10-10T12:58:53.453002: step 1394, loss 0.523106, acc 0.859375, learning_rate 0.000116436
2017-10-10T12:58:53.988884: step 1395, loss 0.446492, acc 0.859375, learning_rate 0.000116369
2017-10-10T12:58:54.521610: step 1396, loss 0.34176, acc 0.890625, learning_rate 0.000116302
2017-10-10T12:58:54.995828: step 1397, loss 0.386815, acc 0.875, learning_rate 0.000116235
2017-10-10T12:58:55.507882: step 1398, loss 0.557292, acc 0.8125, learning_rate 0.000116169
2017-10-10T12:58:56.098580: step 1399, loss 0.417772, acc 0.859375, learning_rate 0.000116103
2017-10-10T12:58:56.640841: step 1400, loss 0.48615, acc 0.8125, learning_rate 0.000116037

Evaluation:
2017-10-10T12:58:57.863048: step 1400, loss 0.293972, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1400

2017-10-10T12:58:59.701924: step 1401, loss 0.561383, acc 0.8125, learning_rate 0.000115972
2017-10-10T12:59:00.277054: step 1402, loss 0.423381, acc 0.859375, learning_rate 0.000115907
2017-10-10T12:59:00.882671: step 1403, loss 0.370802, acc 0.796875, learning_rate 0.000115842
2017-10-10T12:59:01.335534: step 1404, loss 0.317916, acc 0.890625, learning_rate 0.000115777
2017-10-10T12:59:01.721071: step 1405, loss 0.378261, acc 0.890625, learning_rate 0.000115713
2017-10-10T12:59:02.177038: step 1406, loss 0.313578, acc 0.890625, learning_rate 0.000115649
2017-10-10T12:59:02.697166: step 1407, loss 0.319248, acc 0.921875, learning_rate 0.000115585
2017-10-10T12:59:03.208939: step 1408, loss 0.4362, acc 0.84375, learning_rate 0.000115521
2017-10-10T12:59:03.753070: step 1409, loss 0.340477, acc 0.90625, learning_rate 0.000115458
2017-10-10T12:59:04.296723: step 1410, loss 0.332633, acc 0.90625, learning_rate 0.000115395
2017-10-10T12:59:04.797076: step 1411, loss 0.591379, acc 0.78125, learning_rate 0.000115332
2017-10-10T12:59:05.285093: step 1412, loss 0.319645, acc 0.890625, learning_rate 0.000115269
2017-10-10T12:59:05.741969: step 1413, loss 0.296122, acc 0.90625, learning_rate 0.000115207
2017-10-10T12:59:06.275959: step 1414, loss 0.532932, acc 0.796875, learning_rate 0.000115145
2017-10-10T12:59:06.814302: step 1415, loss 0.316397, acc 0.90625, learning_rate 0.000115083
2017-10-10T12:59:07.349734: step 1416, loss 0.394819, acc 0.84375, learning_rate 0.000115022
2017-10-10T12:59:07.876889: step 1417, loss 0.576523, acc 0.8125, learning_rate 0.00011496
2017-10-10T12:59:08.428853: step 1418, loss 0.319783, acc 0.875, learning_rate 0.000114899
2017-10-10T12:59:08.997263: step 1419, loss 0.604339, acc 0.78125, learning_rate 0.000114838
2017-10-10T12:59:09.584854: step 1420, loss 0.385357, acc 0.859375, learning_rate 0.000114778
2017-10-10T12:59:10.189122: step 1421, loss 0.475971, acc 0.8125, learning_rate 0.000114717
2017-10-10T12:59:10.701191: step 1422, loss 0.301661, acc 0.890625, learning_rate 0.000114657
2017-10-10T12:59:11.089525: step 1423, loss 0.220432, acc 0.9375, learning_rate 0.000114598
2017-10-10T12:59:11.532921: step 1424, loss 0.26953, acc 0.890625, learning_rate 0.000114538
2017-10-10T12:59:11.992979: step 1425, loss 0.661492, acc 0.796875, learning_rate 0.000114479
2017-10-10T12:59:12.502716: step 1426, loss 0.417748, acc 0.859375, learning_rate 0.00011442
2017-10-10T12:59:13.053011: step 1427, loss 0.320232, acc 0.890625, learning_rate 0.000114361
2017-10-10T12:59:13.570725: step 1428, loss 0.290849, acc 0.875, learning_rate 0.000114302
2017-10-10T12:59:14.044351: step 1429, loss 0.47218, acc 0.84375, learning_rate 0.000114244
2017-10-10T12:59:14.575388: step 1430, loss 0.509822, acc 0.8125, learning_rate 0.000114186
2017-10-10T12:59:15.068868: step 1431, loss 0.420912, acc 0.890625, learning_rate 0.000114128
2017-10-10T12:59:15.625022: step 1432, loss 0.312094, acc 0.90625, learning_rate 0.00011407
2017-10-10T12:59:16.190202: step 1433, loss 0.502618, acc 0.84375, learning_rate 0.000114013
2017-10-10T12:59:16.756774: step 1434, loss 0.573672, acc 0.765625, learning_rate 0.000113955
2017-10-10T12:59:17.316880: step 1435, loss 0.460501, acc 0.8125, learning_rate 0.000113898
2017-10-10T12:59:17.867894: step 1436, loss 0.351391, acc 0.84375, learning_rate 0.000113842
2017-10-10T12:59:18.348938: step 1437, loss 0.533602, acc 0.765625, learning_rate 0.000113785
2017-10-10T12:59:18.903645: step 1438, loss 0.475985, acc 0.84375, learning_rate 0.000113729
2017-10-10T12:59:19.377030: step 1439, loss 0.662694, acc 0.84375, learning_rate 0.000113673
2017-10-10T12:59:19.985032: step 1440, loss 0.385341, acc 0.875, learning_rate 0.000113617

Evaluation:
2017-10-10T12:59:21.232470: step 1440, loss 0.292585, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1440

2017-10-10T12:59:22.804150: step 1441, loss 0.393247, acc 0.859375, learning_rate 0.000113561
2017-10-10T12:59:23.408859: step 1442, loss 0.498074, acc 0.84375, learning_rate 0.000113506
2017-10-10T12:59:23.803712: step 1443, loss 0.384154, acc 0.90625, learning_rate 0.000113451
2017-10-10T12:59:24.276843: step 1444, loss 0.501508, acc 0.859375, learning_rate 0.000113396
2017-10-10T12:59:24.701118: step 1445, loss 0.421873, acc 0.828125, learning_rate 0.000113341
2017-10-10T12:59:25.232947: step 1446, loss 0.467323, acc 0.859375, learning_rate 0.000113287
2017-10-10T12:59:25.780866: step 1447, loss 0.474308, acc 0.828125, learning_rate 0.000113233
2017-10-10T12:59:26.236927: step 1448, loss 0.475156, acc 0.828125, learning_rate 0.000113179
2017-10-10T12:59:26.768877: step 1449, loss 0.351115, acc 0.90625, learning_rate 0.000113125
2017-10-10T12:59:27.298678: step 1450, loss 0.451206, acc 0.84375, learning_rate 0.000113071
2017-10-10T12:59:27.836924: step 1451, loss 0.561125, acc 0.828125, learning_rate 0.000113018
2017-10-10T12:59:28.369742: step 1452, loss 0.511814, acc 0.859375, learning_rate 0.000112965
2017-10-10T12:59:28.928852: step 1453, loss 0.400997, acc 0.84375, learning_rate 0.000112912
2017-10-10T12:59:29.478836: step 1454, loss 0.545519, acc 0.828125, learning_rate 0.000112859
2017-10-10T12:59:30.021237: step 1455, loss 0.636386, acc 0.796875, learning_rate 0.000112807
2017-10-10T12:59:30.574059: step 1456, loss 0.323707, acc 0.890625, learning_rate 0.000112754
2017-10-10T12:59:31.120955: step 1457, loss 0.320165, acc 0.921875, learning_rate 0.000112702
2017-10-10T12:59:31.675280: step 1458, loss 0.390062, acc 0.890625, learning_rate 0.000112651
2017-10-10T12:59:32.241763: step 1459, loss 0.601368, acc 0.796875, learning_rate 0.000112599
2017-10-10T12:59:32.760850: step 1460, loss 0.429239, acc 0.828125, learning_rate 0.000112547
2017-10-10T12:59:46.483035: step 1461, loss 0.685782, acc 0.78125, learning_rate 0.000112496
2017-10-10T12:59:46.793279: step 1462, loss 0.449968, acc 0.84375, learning_rate 0.000112445
2017-10-10T12:59:47.051050: step 1463, loss 0.496344, acc 0.828125, learning_rate 0.000112394
2017-10-10T12:59:47.319281: step 1464, loss 0.559259, acc 0.8125, learning_rate 0.000112344
2017-10-10T12:59:47.645056: step 1465, loss 0.42866, acc 0.84375, learning_rate 0.000112293
2017-10-10T12:59:47.996812: step 1466, loss 0.51297, acc 0.828125, learning_rate 0.000112243
2017-10-10T12:59:48.453007: step 1467, loss 0.457704, acc 0.890625, learning_rate 0.000112193
2017-10-10T12:59:48.900620: step 1468, loss 0.307695, acc 0.90625, learning_rate 0.000112144
2017-10-10T12:59:49.437321: step 1469, loss 0.320033, acc 0.90625, learning_rate 0.000112094
2017-10-10T12:59:49.944964: step 1470, loss 0.285393, acc 0.941176, learning_rate 0.000112045
2017-10-10T12:59:50.779519: step 1471, loss 0.506422, acc 0.828125, learning_rate 0.000111995
2017-10-10T12:59:51.333076: step 1472, loss 0.494656, acc 0.84375, learning_rate 0.000111946
2017-10-10T12:59:51.885044: step 1473, loss 0.376545, acc 0.875, learning_rate 0.000111898
2017-10-10T12:59:52.387586: step 1474, loss 0.667854, acc 0.796875, learning_rate 0.000111849
2017-10-10T12:59:52.926148: step 1475, loss 0.325168, acc 0.84375, learning_rate 0.000111801
2017-10-10T12:59:53.497044: step 1476, loss 0.376435, acc 0.828125, learning_rate 0.000111753
2017-10-10T12:59:54.041261: step 1477, loss 0.43796, acc 0.859375, learning_rate 0.000111705
2017-10-10T12:59:54.572969: step 1478, loss 0.258175, acc 0.90625, learning_rate 0.000111657
2017-10-10T12:59:55.149090: step 1479, loss 0.399621, acc 0.859375, learning_rate 0.000111609
2017-10-10T12:59:55.689002: step 1480, loss 0.344753, acc 0.859375, learning_rate 0.000111562

Evaluation:
2017-10-10T12:59:56.821352: step 1480, loss 0.291205, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1480

2017-10-10T12:59:58.513627: step 1481, loss 0.523147, acc 0.890625, learning_rate 0.000111515
2017-10-10T12:59:58.972232: step 1482, loss 0.446667, acc 0.890625, learning_rate 0.000111468
2017-10-10T12:59:59.489371: step 1483, loss 0.528306, acc 0.8125, learning_rate 0.000111421
2017-10-10T13:00:00.034694: step 1484, loss 0.374914, acc 0.90625, learning_rate 0.000111374
2017-10-10T13:00:00.590344: step 1485, loss 0.496613, acc 0.859375, learning_rate 0.000111328
2017-10-10T13:00:01.143915: step 1486, loss 0.386897, acc 0.859375, learning_rate 0.000111282
2017-10-10T13:00:01.683670: step 1487, loss 0.314137, acc 0.890625, learning_rate 0.000111236
2017-10-10T13:00:02.243443: step 1488, loss 0.355614, acc 0.90625, learning_rate 0.00011119
2017-10-10T13:00:02.794696: step 1489, loss 0.362932, acc 0.84375, learning_rate 0.000111144
2017-10-10T13:00:03.294467: step 1490, loss 0.351628, acc 0.875, learning_rate 0.000111099
2017-10-10T13:00:03.820550: step 1491, loss 0.402959, acc 0.90625, learning_rate 0.000111053
2017-10-10T13:00:04.332823: step 1492, loss 0.39122, acc 0.796875, learning_rate 0.000111008
2017-10-10T13:00:04.876893: step 1493, loss 0.474348, acc 0.828125, learning_rate 0.000110963
2017-10-10T13:00:05.411584: step 1494, loss 0.620956, acc 0.828125, learning_rate 0.000110918
2017-10-10T13:00:06.288416: step 1495, loss 0.494352, acc 0.84375, learning_rate 0.000110874
2017-10-10T13:00:06.840892: step 1496, loss 0.288691, acc 0.921875, learning_rate 0.00011083
2017-10-10T13:00:07.401047: step 1497, loss 0.506456, acc 0.78125, learning_rate 0.000110785
2017-10-10T13:00:07.929897: step 1498, loss 0.24893, acc 0.890625, learning_rate 0.000110741
2017-10-10T13:00:08.478130: step 1499, loss 0.58486, acc 0.8125, learning_rate 0.000110697
2017-10-10T13:00:09.016553: step 1500, loss 0.358349, acc 0.890625, learning_rate 0.000110654
2017-10-10T13:00:09.573021: step 1501, loss 0.478231, acc 0.828125, learning_rate 0.00011061
2017-10-10T13:00:10.089852: step 1502, loss 0.331113, acc 0.90625, learning_rate 0.000110567
2017-10-10T13:00:10.709136: step 1503, loss 0.306712, acc 0.859375, learning_rate 0.000110524
2017-10-10T13:00:11.188417: step 1504, loss 0.301693, acc 0.890625, learning_rate 0.000110481
2017-10-10T13:00:11.548324: step 1505, loss 0.325223, acc 0.890625, learning_rate 0.000110438
2017-10-10T13:00:11.904840: step 1506, loss 0.405136, acc 0.90625, learning_rate 0.000110396
2017-10-10T13:00:12.242238: step 1507, loss 0.423068, acc 0.84375, learning_rate 0.000110353
2017-10-10T13:00:12.760878: step 1508, loss 0.266649, acc 0.90625, learning_rate 0.000110311
2017-10-10T13:00:13.288937: step 1509, loss 0.363631, acc 0.84375, learning_rate 0.000110269
2017-10-10T13:00:13.825027: step 1510, loss 0.365887, acc 0.84375, learning_rate 0.000110227
2017-10-10T13:00:14.285016: step 1511, loss 0.505765, acc 0.890625, learning_rate 0.000110185
2017-10-10T13:00:14.804904: step 1512, loss 0.411072, acc 0.84375, learning_rate 0.000110144
2017-10-10T13:00:15.281082: step 1513, loss 0.320855, acc 0.875, learning_rate 0.000110102
2017-10-10T13:00:15.849009: step 1514, loss 0.448852, acc 0.875, learning_rate 0.000110061
2017-10-10T13:00:16.336600: step 1515, loss 0.423328, acc 0.8125, learning_rate 0.00011002
2017-10-10T13:00:16.848309: step 1516, loss 0.574302, acc 0.8125, learning_rate 0.000109979
2017-10-10T13:00:17.417611: step 1517, loss 0.665419, acc 0.828125, learning_rate 0.000109938
2017-10-10T13:00:17.877060: step 1518, loss 0.379039, acc 0.828125, learning_rate 0.000109898
2017-10-10T13:00:18.333146: step 1519, loss 0.541652, acc 0.90625, learning_rate 0.000109857
2017-10-10T13:00:18.884954: step 1520, loss 0.619894, acc 0.78125, learning_rate 0.000109817

Evaluation:
2017-10-10T13:00:20.062705: step 1520, loss 0.292109, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1520

2017-10-10T13:00:21.735720: step 1521, loss 0.554077, acc 0.859375, learning_rate 0.000109777
2017-10-10T13:00:22.167666: step 1522, loss 0.432271, acc 0.890625, learning_rate 0.000109737
2017-10-10T13:00:22.759161: step 1523, loss 0.238, acc 0.921875, learning_rate 0.000109697
2017-10-10T13:00:23.316413: step 1524, loss 0.521124, acc 0.84375, learning_rate 0.000109658
2017-10-10T13:00:23.815340: step 1525, loss 0.413482, acc 0.859375, learning_rate 0.000109618
2017-10-10T13:00:24.348905: step 1526, loss 0.275614, acc 0.921875, learning_rate 0.000109579
2017-10-10T13:00:24.968038: step 1527, loss 0.530858, acc 0.84375, learning_rate 0.00010954
2017-10-10T13:00:25.520406: step 1528, loss 0.464841, acc 0.875, learning_rate 0.000109501
2017-10-10T13:00:26.079113: step 1529, loss 0.462441, acc 0.828125, learning_rate 0.000109462
2017-10-10T13:00:26.593064: step 1530, loss 0.535129, acc 0.859375, learning_rate 0.000109424
2017-10-10T13:00:27.101271: step 1531, loss 0.439418, acc 0.8125, learning_rate 0.000109385
2017-10-10T13:00:27.609118: step 1532, loss 0.454422, acc 0.859375, learning_rate 0.000109347
2017-10-10T13:00:28.130006: step 1533, loss 0.495917, acc 0.828125, learning_rate 0.000109309
2017-10-10T13:00:28.553261: step 1534, loss 0.477525, acc 0.859375, learning_rate 0.000109271
2017-10-10T13:00:29.017167: step 1535, loss 0.410072, acc 0.84375, learning_rate 0.000109233
2017-10-10T13:00:29.511123: step 1536, loss 0.610418, acc 0.703125, learning_rate 0.000109195
2017-10-10T13:00:30.028232: step 1537, loss 0.390292, acc 0.890625, learning_rate 0.000109158
2017-10-10T13:00:30.596026: step 1538, loss 0.373911, acc 0.8125, learning_rate 0.00010912
2017-10-10T13:00:31.172903: step 1539, loss 0.422402, acc 0.875, learning_rate 0.000109083
2017-10-10T13:00:31.785098: step 1540, loss 0.280598, acc 0.90625, learning_rate 0.000109046
2017-10-10T13:00:32.420307: step 1541, loss 0.782666, acc 0.734375, learning_rate 0.000109009
2017-10-10T13:00:32.964905: step 1542, loss 0.450118, acc 0.8125, learning_rate 0.000108972
2017-10-10T13:00:33.440913: step 1543, loss 0.483811, acc 0.78125, learning_rate 0.000108936
2017-10-10T13:00:34.034002: step 1544, loss 0.33074, acc 0.921875, learning_rate 0.000108899
2017-10-10T13:00:34.652965: step 1545, loss 0.355649, acc 0.921875, learning_rate 0.000108863
2017-10-10T13:00:35.021964: step 1546, loss 0.410454, acc 0.828125, learning_rate 0.000108827
2017-10-10T13:00:35.360853: step 1547, loss 0.648452, acc 0.828125, learning_rate 0.000108791
2017-10-10T13:00:35.896844: step 1548, loss 0.463907, acc 0.84375, learning_rate 0.000108755
2017-10-10T13:00:36.460679: step 1549, loss 0.423686, acc 0.875, learning_rate 0.000108719
2017-10-10T13:00:37.094736: step 1550, loss 0.374316, acc 0.859375, learning_rate 0.000108683
2017-10-10T13:00:37.618524: step 1551, loss 0.424902, acc 0.875, learning_rate 0.000108648
2017-10-10T13:00:38.154368: step 1552, loss 0.403703, acc 0.859375, learning_rate 0.000108613
2017-10-10T13:00:38.691245: step 1553, loss 0.315338, acc 0.859375, learning_rate 0.000108577
2017-10-10T13:00:39.227835: step 1554, loss 0.434117, acc 0.8125, learning_rate 0.000108542
2017-10-10T13:00:39.741797: step 1555, loss 0.311581, acc 0.890625, learning_rate 0.000108508
2017-10-10T13:00:40.348844: step 1556, loss 0.364898, acc 0.875, learning_rate 0.000108473
2017-10-10T13:00:40.834655: step 1557, loss 0.374372, acc 0.890625, learning_rate 0.000108438
2017-10-10T13:00:41.333214: step 1558, loss 0.421661, acc 0.84375, learning_rate 0.000108404
2017-10-10T13:00:41.855525: step 1559, loss 0.425495, acc 0.828125, learning_rate 0.00010837
2017-10-10T13:00:42.252611: step 1560, loss 0.156836, acc 0.984375, learning_rate 0.000108335

Evaluation:
2017-10-10T13:00:43.530210: step 1560, loss 0.291706, acc 0.884892

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1560

2017-10-10T13:00:44.805565: step 1561, loss 0.205136, acc 0.9375, learning_rate 0.000108301
2017-10-10T13:00:45.268914: step 1562, loss 0.359465, acc 0.828125, learning_rate 0.000108267
2017-10-10T13:00:45.820007: step 1563, loss 0.42944, acc 0.796875, learning_rate 0.000108234
2017-10-10T13:00:46.352968: step 1564, loss 0.465232, acc 0.84375, learning_rate 0.0001082
2017-10-10T13:00:46.846861: step 1565, loss 0.415541, acc 0.921875, learning_rate 0.000108167
2017-10-10T13:00:47.354776: step 1566, loss 0.32934, acc 0.921875, learning_rate 0.000108133
2017-10-10T13:00:47.820873: step 1567, loss 0.349948, acc 0.890625, learning_rate 0.0001081
2017-10-10T13:00:48.367707: step 1568, loss 0.327569, acc 0.901961, learning_rate 0.000108067
2017-10-10T13:00:48.927821: step 1569, loss 0.339562, acc 0.875, learning_rate 0.000108034
2017-10-10T13:00:49.917334: step 1570, loss 0.552355, acc 0.8125, learning_rate 0.000108001
2017-10-10T13:00:50.486198: step 1571, loss 0.382293, acc 0.828125, learning_rate 0.000107969
2017-10-10T13:00:51.056722: step 1572, loss 0.452982, acc 0.875, learning_rate 0.000107936
2017-10-10T13:00:51.601038: step 1573, loss 0.400073, acc 0.796875, learning_rate 0.000107904
2017-10-10T13:00:52.138683: step 1574, loss 0.370143, acc 0.875, learning_rate 0.000107871
2017-10-10T13:00:52.625813: step 1575, loss 0.395346, acc 0.875, learning_rate 0.000107839
2017-10-10T13:00:53.187315: step 1576, loss 0.551661, acc 0.8125, learning_rate 0.000107807
2017-10-10T13:00:53.741118: step 1577, loss 0.49078, acc 0.859375, learning_rate 0.000107775
2017-10-10T13:00:54.273554: step 1578, loss 0.334399, acc 0.875, learning_rate 0.000107744
2017-10-10T13:00:54.723333: step 1579, loss 0.517242, acc 0.828125, learning_rate 0.000107712
2017-10-10T13:00:55.232976: step 1580, loss 0.283868, acc 0.890625, learning_rate 0.000107681
2017-10-10T13:00:55.770844: step 1581, loss 0.350287, acc 0.90625, learning_rate 0.000107649
2017-10-10T13:00:56.296851: step 1582, loss 0.319876, acc 0.9375, learning_rate 0.000107618
2017-10-10T13:00:56.777570: step 1583, loss 0.621483, acc 0.8125, learning_rate 0.000107587
2017-10-10T13:00:57.473441: step 1584, loss 0.454697, acc 0.828125, learning_rate 0.000107556
2017-10-10T13:00:58.137201: step 1585, loss 0.63506, acc 0.796875, learning_rate 0.000107525
2017-10-10T13:00:58.470759: step 1586, loss 0.349041, acc 0.921875, learning_rate 0.000107494
2017-10-10T13:00:58.767363: step 1587, loss 0.357878, acc 0.90625, learning_rate 0.000107464
2017-10-10T13:00:59.094901: step 1588, loss 0.436921, acc 0.890625, learning_rate 0.000107433
2017-10-10T13:00:59.583485: step 1589, loss 0.515214, acc 0.859375, learning_rate 0.000107403
2017-10-10T13:01:00.148587: step 1590, loss 0.306652, acc 0.90625, learning_rate 0.000107373
2017-10-10T13:01:00.736896: step 1591, loss 0.615124, acc 0.796875, learning_rate 0.000107343
2017-10-10T13:01:01.256819: step 1592, loss 0.457546, acc 0.84375, learning_rate 0.000107313
2017-10-10T13:01:01.830989: step 1593, loss 0.348719, acc 0.90625, learning_rate 0.000107283
2017-10-10T13:01:02.401614: step 1594, loss 0.360035, acc 0.890625, learning_rate 0.000107253
2017-10-10T13:01:02.957187: step 1595, loss 0.299571, acc 0.921875, learning_rate 0.000107224
2017-10-10T13:01:03.513108: step 1596, loss 0.482772, acc 0.828125, learning_rate 0.000107194
2017-10-10T13:01:04.056997: step 1597, loss 0.385463, acc 0.890625, learning_rate 0.000107165
2017-10-10T13:01:04.557019: step 1598, loss 0.355975, acc 0.828125, learning_rate 0.000107136
2017-10-10T13:01:05.164351: step 1599, loss 0.5055, acc 0.875, learning_rate 0.000107106
2017-10-10T13:01:05.644855: step 1600, loss 0.422924, acc 0.859375, learning_rate 0.000107077

Evaluation:
2017-10-10T13:01:06.857057: step 1600, loss 0.290919, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1600

2017-10-10T13:01:08.447461: step 1601, loss 0.579298, acc 0.84375, learning_rate 0.000107048
2017-10-10T13:01:08.975529: step 1602, loss 0.279151, acc 0.90625, learning_rate 0.00010702
2017-10-10T13:01:09.540111: step 1603, loss 0.373681, acc 0.84375, learning_rate 0.000106991
2017-10-10T13:01:10.048916: step 1604, loss 0.520034, acc 0.796875, learning_rate 0.000106963
2017-10-10T13:01:10.612553: step 1605, loss 0.418234, acc 0.8125, learning_rate 0.000106934
2017-10-10T13:01:11.117614: step 1606, loss 0.341724, acc 0.90625, learning_rate 0.000106906
2017-10-10T13:01:11.601113: step 1607, loss 0.503739, acc 0.859375, learning_rate 0.000106878
2017-10-10T13:01:12.119346: step 1608, loss 0.345422, acc 0.890625, learning_rate 0.00010685
2017-10-10T13:01:12.589047: step 1609, loss 0.419862, acc 0.828125, learning_rate 0.000106822
2017-10-10T13:01:13.117129: step 1610, loss 0.510702, acc 0.84375, learning_rate 0.000106794
2017-10-10T13:01:13.738116: step 1611, loss 0.410771, acc 0.828125, learning_rate 0.000106766
2017-10-10T13:01:14.294982: step 1612, loss 0.577201, acc 0.796875, learning_rate 0.000106738
2017-10-10T13:01:14.712837: step 1613, loss 0.354535, acc 0.875, learning_rate 0.000106711
2017-10-10T13:01:15.208872: step 1614, loss 0.237401, acc 0.953125, learning_rate 0.000106684
2017-10-10T13:01:15.714112: step 1615, loss 0.383467, acc 0.875, learning_rate 0.000106656
2017-10-10T13:01:16.246146: step 1616, loss 0.411018, acc 0.875, learning_rate 0.000106629
2017-10-10T13:01:16.739204: step 1617, loss 0.365744, acc 0.84375, learning_rate 0.000106602
2017-10-10T13:01:17.234355: step 1618, loss 0.463845, acc 0.84375, learning_rate 0.000106575
2017-10-10T13:01:17.791538: step 1619, loss 0.481764, acc 0.8125, learning_rate 0.000106548
2017-10-10T13:01:18.345052: step 1620, loss 0.48279, acc 0.84375, learning_rate 0.000106521
2017-10-10T13:01:18.972863: step 1621, loss 0.271764, acc 0.953125, learning_rate 0.000106495
2017-10-10T13:01:19.527083: step 1622, loss 0.311318, acc 0.921875, learning_rate 0.000106468
2017-10-10T13:01:20.021174: step 1623, loss 0.471805, acc 0.875, learning_rate 0.000106442
2017-10-10T13:01:20.576981: step 1624, loss 0.520299, acc 0.8125, learning_rate 0.000106416
2017-10-10T13:01:21.101178: step 1625, loss 0.478444, acc 0.84375, learning_rate 0.000106389
2017-10-10T13:01:21.560869: step 1626, loss 0.340444, acc 0.90625, learning_rate 0.000106363
2017-10-10T13:01:21.908313: step 1627, loss 0.437889, acc 0.859375, learning_rate 0.000106337
2017-10-10T13:01:22.393112: step 1628, loss 0.425399, acc 0.78125, learning_rate 0.000106312
2017-10-10T13:01:22.896817: step 1629, loss 0.341336, acc 0.859375, learning_rate 0.000106286
2017-10-10T13:01:23.469235: step 1630, loss 0.272045, acc 0.9375, learning_rate 0.00010626
2017-10-10T13:01:24.032186: step 1631, loss 0.409625, acc 0.890625, learning_rate 0.000106235
2017-10-10T13:01:24.556910: step 1632, loss 0.380639, acc 0.890625, learning_rate 0.000106209
2017-10-10T13:01:25.113052: step 1633, loss 0.595651, acc 0.78125, learning_rate 0.000106184
2017-10-10T13:01:25.627241: step 1634, loss 0.299339, acc 0.9375, learning_rate 0.000106159
2017-10-10T13:01:26.123667: step 1635, loss 0.269395, acc 0.859375, learning_rate 0.000106133
2017-10-10T13:01:26.593593: step 1636, loss 0.297899, acc 0.921875, learning_rate 0.000106108
2017-10-10T13:01:27.082937: step 1637, loss 0.400104, acc 0.875, learning_rate 0.000106083
2017-10-10T13:01:27.564854: step 1638, loss 0.439187, acc 0.859375, learning_rate 0.000106059
2017-10-10T13:01:28.130244: step 1639, loss 0.327338, acc 0.890625, learning_rate 0.000106034
2017-10-10T13:01:28.632250: step 1640, loss 0.276036, acc 0.9375, learning_rate 0.000106009

Evaluation:
2017-10-10T13:01:29.688930: step 1640, loss 0.289157, acc 0.886331

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1640

2017-10-10T13:01:31.610346: step 1641, loss 0.437078, acc 0.890625, learning_rate 0.000105985
2017-10-10T13:01:32.164858: step 1642, loss 0.335406, acc 0.953125, learning_rate 0.00010596
2017-10-10T13:01:32.729137: step 1643, loss 0.622222, acc 0.796875, learning_rate 0.000105936
2017-10-10T13:01:33.265068: step 1644, loss 0.258403, acc 0.921875, learning_rate 0.000105912
2017-10-10T13:01:33.829849: step 1645, loss 0.574259, acc 0.765625, learning_rate 0.000105888
2017-10-10T13:01:34.292930: step 1646, loss 0.741833, acc 0.828125, learning_rate 0.000105864
2017-10-10T13:01:34.885292: step 1647, loss 0.361128, acc 0.890625, learning_rate 0.00010584
2017-10-10T13:01:35.471929: step 1648, loss 0.538087, acc 0.859375, learning_rate 0.000105816
2017-10-10T13:01:36.028872: step 1649, loss 0.42001, acc 0.890625, learning_rate 0.000105792
2017-10-10T13:01:36.577048: step 1650, loss 0.463834, acc 0.84375, learning_rate 0.000105768
2017-10-10T13:01:37.140086: step 1651, loss 0.301754, acc 0.90625, learning_rate 0.000105745
2017-10-10T13:01:37.678238: step 1652, loss 0.461705, acc 0.859375, learning_rate 0.000105721
2017-10-10T13:01:38.182058: step 1653, loss 0.363099, acc 0.921875, learning_rate 0.000105698
2017-10-10T13:01:38.690568: step 1654, loss 0.565378, acc 0.8125, learning_rate 0.000105675
2017-10-10T13:01:39.225059: step 1655, loss 0.537303, acc 0.8125, learning_rate 0.000105652
2017-10-10T13:01:39.772970: step 1656, loss 0.190805, acc 0.96875, learning_rate 0.000105629
2017-10-10T13:01:40.387039: step 1657, loss 0.376321, acc 0.890625, learning_rate 0.000105606
2017-10-10T13:01:40.973185: step 1658, loss 0.479858, acc 0.859375, learning_rate 0.000105583
2017-10-10T13:01:41.458515: step 1659, loss 0.555692, acc 0.8125, learning_rate 0.00010556
2017-10-10T13:01:41.913024: step 1660, loss 0.43394, acc 0.875, learning_rate 0.000105537
2017-10-10T13:01:42.453638: step 1661, loss 0.425229, acc 0.8125, learning_rate 0.000105515
2017-10-10T13:01:43.072996: step 1662, loss 0.278101, acc 0.875, learning_rate 0.000105492
2017-10-10T13:01:43.712971: step 1663, loss 0.336146, acc 0.859375, learning_rate 0.00010547
2017-10-10T13:01:43.996353: step 1664, loss 0.53112, acc 0.765625, learning_rate 0.000105447
2017-10-10T13:01:44.440765: step 1665, loss 0.354025, acc 0.859375, learning_rate 0.000105425
2017-10-10T13:01:44.783215: step 1666, loss 0.353468, acc 0.901961, learning_rate 0.000105403
2017-10-10T13:01:45.107189: step 1667, loss 0.289022, acc 0.90625, learning_rate 0.000105381
2017-10-10T13:01:45.592864: step 1668, loss 0.508473, acc 0.765625, learning_rate 0.000105359
2017-10-10T13:01:46.152885: step 1669, loss 0.393049, acc 0.859375, learning_rate 0.000105337
2017-10-10T13:01:46.704855: step 1670, loss 0.352071, acc 0.875, learning_rate 0.000105315
2017-10-10T13:01:47.265061: step 1671, loss 0.34453, acc 0.859375, learning_rate 0.000105294
2017-10-10T13:01:47.794170: step 1672, loss 0.411905, acc 0.8125, learning_rate 0.000105272
2017-10-10T13:01:48.387034: step 1673, loss 0.336757, acc 0.875, learning_rate 0.000105251
2017-10-10T13:01:48.908909: step 1674, loss 0.462176, acc 0.828125, learning_rate 0.000105229
2017-10-10T13:01:49.468767: step 1675, loss 0.590329, acc 0.8125, learning_rate 0.000105208
2017-10-10T13:01:50.041027: step 1676, loss 0.404942, acc 0.875, learning_rate 0.000105186
2017-10-10T13:01:50.548930: step 1677, loss 0.264507, acc 0.953125, learning_rate 0.000105165
2017-10-10T13:01:51.039012: step 1678, loss 0.437495, acc 0.859375, learning_rate 0.000105144
2017-10-10T13:01:51.612894: step 1679, loss 0.303361, acc 0.890625, learning_rate 0.000105123
2017-10-10T13:01:52.217362: step 1680, loss 0.315093, acc 0.875, learning_rate 0.000105102

Evaluation:
2017-10-10T13:01:53.192819: step 1680, loss 0.290316, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1680

2017-10-10T13:01:54.597854: step 1681, loss 0.551511, acc 0.8125, learning_rate 0.000105081
2017-10-10T13:01:55.177181: step 1682, loss 0.249892, acc 0.921875, learning_rate 0.000105061
2017-10-10T13:01:55.724951: step 1683, loss 0.320951, acc 0.90625, learning_rate 0.00010504
2017-10-10T13:01:56.292831: step 1684, loss 0.5156, acc 0.828125, learning_rate 0.00010502
2017-10-10T13:01:56.834272: step 1685, loss 0.364728, acc 0.84375, learning_rate 0.000104999
2017-10-10T13:01:57.384872: step 1686, loss 0.54072, acc 0.828125, learning_rate 0.000104979
2017-10-10T13:01:57.972563: step 1687, loss 0.474702, acc 0.734375, learning_rate 0.000104958
2017-10-10T13:01:58.577134: step 1688, loss 0.432564, acc 0.8125, learning_rate 0.000104938
2017-10-10T13:01:59.044869: step 1689, loss 0.52461, acc 0.8125, learning_rate 0.000104918
2017-10-10T13:01:59.661081: step 1690, loss 0.353279, acc 0.921875, learning_rate 0.000104898
2017-10-10T13:02:00.154205: step 1691, loss 0.225317, acc 0.921875, learning_rate 0.000104878
2017-10-10T13:02:00.621139: step 1692, loss 0.539871, acc 0.84375, learning_rate 0.000104858
2017-10-10T13:02:01.144889: step 1693, loss 0.488979, acc 0.875, learning_rate 0.000104838
2017-10-10T13:02:01.707512: step 1694, loss 0.372968, acc 0.859375, learning_rate 0.000104818
2017-10-10T13:02:02.285099: step 1695, loss 0.297499, acc 0.921875, learning_rate 0.000104799
2017-10-10T13:02:02.850539: step 1696, loss 0.371355, acc 0.875, learning_rate 0.000104779
2017-10-10T13:02:03.399259: step 1697, loss 0.486356, acc 0.84375, learning_rate 0.00010476
2017-10-10T13:02:03.968854: step 1698, loss 0.326102, acc 0.921875, learning_rate 0.00010474
2017-10-10T13:02:04.509360: step 1699, loss 0.250486, acc 0.984375, learning_rate 0.000104721
2017-10-10T13:02:05.089044: step 1700, loss 0.29123, acc 0.921875, learning_rate 0.000104702
2017-10-10T13:02:05.601939: step 1701, loss 0.664245, acc 0.78125, learning_rate 0.000104682
2017-10-10T13:02:06.162823: step 1702, loss 0.434367, acc 0.859375, learning_rate 0.000104663
2017-10-10T13:02:06.848892: step 1703, loss 0.387016, acc 0.859375, learning_rate 0.000104644
2017-10-10T13:02:07.434825: step 1704, loss 0.395303, acc 0.84375, learning_rate 0.000104625
2017-10-10T13:02:07.780195: step 1705, loss 0.515192, acc 0.8125, learning_rate 0.000104606
2017-10-10T13:02:08.125187: step 1706, loss 0.234795, acc 0.953125, learning_rate 0.000104588
2017-10-10T13:02:08.672850: step 1707, loss 0.433383, acc 0.875, learning_rate 0.000104569
2017-10-10T13:02:09.201465: step 1708, loss 0.438866, acc 0.875, learning_rate 0.00010455
2017-10-10T13:02:09.755913: step 1709, loss 0.543891, acc 0.8125, learning_rate 0.000104532
2017-10-10T13:02:10.276871: step 1710, loss 0.265169, acc 0.890625, learning_rate 0.000104513
2017-10-10T13:02:10.834244: step 1711, loss 0.397978, acc 0.859375, learning_rate 0.000104495
2017-10-10T13:02:11.387911: step 1712, loss 0.50791, acc 0.84375, learning_rate 0.000104476
2017-10-10T13:02:11.884337: step 1713, loss 0.566821, acc 0.8125, learning_rate 0.000104458
2017-10-10T13:02:12.385095: step 1714, loss 0.290444, acc 0.890625, learning_rate 0.00010444
2017-10-10T13:02:12.809005: step 1715, loss 0.500696, acc 0.8125, learning_rate 0.000104422
2017-10-10T13:02:13.295067: step 1716, loss 0.309991, acc 0.890625, learning_rate 0.000104404
2017-10-10T13:02:13.797093: step 1717, loss 0.466965, acc 0.84375, learning_rate 0.000104386
2017-10-10T13:02:14.238527: step 1718, loss 0.475014, acc 0.8125, learning_rate 0.000104368
2017-10-10T13:02:14.805503: step 1719, loss 0.543361, acc 0.8125, learning_rate 0.00010435
2017-10-10T13:02:15.386999: step 1720, loss 0.356756, acc 0.90625, learning_rate 0.000104332

Evaluation:
2017-10-10T13:02:16.408860: step 1720, loss 0.290842, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1720

2017-10-10T13:02:18.043097: step 1721, loss 0.558362, acc 0.84375, learning_rate 0.000104315
2017-10-10T13:02:18.550962: step 1722, loss 0.526134, acc 0.875, learning_rate 0.000104297
2017-10-10T13:02:19.148972: step 1723, loss 0.572891, acc 0.796875, learning_rate 0.000104279
2017-10-10T13:02:19.709787: step 1724, loss 0.405238, acc 0.859375, learning_rate 0.000104262
2017-10-10T13:02:20.252905: step 1725, loss 0.533308, acc 0.859375, learning_rate 0.000104245
2017-10-10T13:02:20.769204: step 1726, loss 0.263485, acc 0.921875, learning_rate 0.000104227
2017-10-10T13:02:21.333141: step 1727, loss 0.301453, acc 0.90625, learning_rate 0.00010421
2017-10-10T13:02:21.893478: step 1728, loss 0.375869, acc 0.828125, learning_rate 0.000104193
2017-10-10T13:02:22.445174: step 1729, loss 0.489271, acc 0.796875, learning_rate 0.000104176
2017-10-10T13:02:22.976609: step 1730, loss 0.462067, acc 0.875, learning_rate 0.000104159
2017-10-10T13:02:23.544940: step 1731, loss 0.291101, acc 0.90625, learning_rate 0.000104142
2017-10-10T13:02:24.105055: step 1732, loss 0.51395, acc 0.84375, learning_rate 0.000104125
2017-10-10T13:02:24.669201: step 1733, loss 0.129013, acc 0.96875, learning_rate 0.000104108
2017-10-10T13:02:25.228837: step 1734, loss 0.599495, acc 0.796875, learning_rate 0.000104091
2017-10-10T13:02:25.740845: step 1735, loss 0.595201, acc 0.796875, learning_rate 0.000104074
2017-10-10T13:02:26.304934: step 1736, loss 0.393074, acc 0.859375, learning_rate 0.000104058
2017-10-10T13:02:26.794877: step 1737, loss 0.27564, acc 0.921875, learning_rate 0.000104041
2017-10-10T13:02:27.352465: step 1738, loss 0.582444, acc 0.78125, learning_rate 0.000104025
2017-10-10T13:02:27.872849: step 1739, loss 0.481201, acc 0.890625, learning_rate 0.000104008
2017-10-10T13:02:28.425682: step 1740, loss 0.638292, acc 0.765625, learning_rate 0.000103992
2017-10-10T13:02:29.004867: step 1741, loss 0.24267, acc 0.921875, learning_rate 0.000103976
2017-10-10T13:02:29.595163: step 1742, loss 0.398873, acc 0.859375, learning_rate 0.000103959
2017-10-10T13:02:30.232969: step 1743, loss 0.363297, acc 0.875, learning_rate 0.000103943
2017-10-10T13:02:30.728775: step 1744, loss 0.651934, acc 0.84375, learning_rate 0.000103927
2017-10-10T13:02:31.100862: step 1745, loss 0.505493, acc 0.84375, learning_rate 0.000103911
2017-10-10T13:02:31.554080: step 1746, loss 0.406094, acc 0.890625, learning_rate 0.000103895
2017-10-10T13:02:32.036902: step 1747, loss 0.324111, acc 0.84375, learning_rate 0.000103879
2017-10-10T13:02:32.560923: step 1748, loss 0.443798, acc 0.859375, learning_rate 0.000103863
2017-10-10T13:02:33.120997: step 1749, loss 0.325866, acc 0.90625, learning_rate 0.000103848
2017-10-10T13:02:33.688971: step 1750, loss 0.432666, acc 0.84375, learning_rate 0.000103832
2017-10-10T13:02:34.190110: step 1751, loss 0.47576, acc 0.78125, learning_rate 0.000103816
2017-10-10T13:02:34.721040: step 1752, loss 0.392448, acc 0.890625, learning_rate 0.000103801
2017-10-10T13:02:35.222799: step 1753, loss 0.419065, acc 0.875, learning_rate 0.000103785
2017-10-10T13:02:35.790568: step 1754, loss 0.327174, acc 0.859375, learning_rate 0.00010377
2017-10-10T13:02:36.259646: step 1755, loss 0.249324, acc 0.953125, learning_rate 0.000103754
2017-10-10T13:02:36.852953: step 1756, loss 0.255061, acc 0.90625, learning_rate 0.000103739
2017-10-10T13:02:37.512673: step 1757, loss 0.42322, acc 0.859375, learning_rate 0.000103724
2017-10-10T13:02:37.948897: step 1758, loss 0.388063, acc 0.90625, learning_rate 0.000103709
2017-10-10T13:02:38.376948: step 1759, loss 0.589168, acc 0.8125, learning_rate 0.000103694
2017-10-10T13:02:38.847483: step 1760, loss 0.282237, acc 0.921875, learning_rate 0.000103678

Evaluation:
2017-10-10T13:02:39.998055: step 1760, loss 0.289621, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1760

2017-10-10T13:02:41.682076: step 1761, loss 0.397858, acc 0.859375, learning_rate 0.000103663
2017-10-10T13:02:42.213843: step 1762, loss 0.214395, acc 0.96875, learning_rate 0.000103648
2017-10-10T13:02:42.744107: step 1763, loss 0.323588, acc 0.875, learning_rate 0.000103634
2017-10-10T13:02:43.212897: step 1764, loss 0.512605, acc 0.823529, learning_rate 0.000103619
2017-10-10T13:02:43.677325: step 1765, loss 0.337922, acc 0.84375, learning_rate 0.000103604
2017-10-10T13:02:44.240868: step 1766, loss 0.524083, acc 0.828125, learning_rate 0.000103589
2017-10-10T13:02:44.792310: step 1767, loss 0.554855, acc 0.84375, learning_rate 0.000103575
2017-10-10T13:02:45.385211: step 1768, loss 0.339029, acc 0.90625, learning_rate 0.00010356
2017-10-10T13:02:45.980837: step 1769, loss 0.510025, acc 0.8125, learning_rate 0.000103545
2017-10-10T13:02:46.484881: step 1770, loss 0.49596, acc 0.84375, learning_rate 0.000103531
2017-10-10T13:02:47.066940: step 1771, loss 0.442149, acc 0.859375, learning_rate 0.000103517
2017-10-10T13:02:47.551659: step 1772, loss 0.445464, acc 0.84375, learning_rate 0.000103502
2017-10-10T13:02:48.050132: step 1773, loss 0.355397, acc 0.84375, learning_rate 0.000103488
2017-10-10T13:02:48.565998: step 1774, loss 0.34372, acc 0.90625, learning_rate 0.000103474
2017-10-10T13:02:49.152927: step 1775, loss 0.401963, acc 0.828125, learning_rate 0.00010346
2017-10-10T13:02:49.697125: step 1776, loss 0.397143, acc 0.84375, learning_rate 0.000103445
2017-10-10T13:02:50.200947: step 1777, loss 0.267188, acc 0.890625, learning_rate 0.000103431
2017-10-10T13:02:50.725086: step 1778, loss 0.479541, acc 0.828125, learning_rate 0.000103417
2017-10-10T13:02:51.297804: step 1779, loss 0.328269, acc 0.859375, learning_rate 0.000103403
2017-10-10T13:02:51.838358: step 1780, loss 0.437641, acc 0.828125, learning_rate 0.00010339
2017-10-10T13:02:52.456875: step 1781, loss 0.505484, acc 0.84375, learning_rate 0.000103376
2017-10-10T13:02:52.944958: step 1782, loss 0.255823, acc 0.921875, learning_rate 0.000103362
2017-10-10T13:02:53.447653: step 1783, loss 0.324153, acc 0.859375, learning_rate 0.000103348
2017-10-10T13:02:53.764594: step 1784, loss 0.42626, acc 0.875, learning_rate 0.000103335
2017-10-10T13:02:54.100860: step 1785, loss 0.348604, acc 0.859375, learning_rate 0.000103321
2017-10-10T13:02:54.556978: step 1786, loss 0.460487, acc 0.828125, learning_rate 0.000103307
2017-10-10T13:02:55.053041: step 1787, loss 0.550869, acc 0.78125, learning_rate 0.000103294
2017-10-10T13:02:55.638821: step 1788, loss 0.375843, acc 0.84375, learning_rate 0.00010328
2017-10-10T13:02:56.120959: step 1789, loss 0.329973, acc 0.890625, learning_rate 0.000103267
2017-10-10T13:02:56.591304: step 1790, loss 0.277898, acc 0.875, learning_rate 0.000103254
2017-10-10T13:02:57.120964: step 1791, loss 0.423135, acc 0.859375, learning_rate 0.00010324
2017-10-10T13:02:57.676870: step 1792, loss 0.376055, acc 0.84375, learning_rate 0.000103227
2017-10-10T13:02:58.241006: step 1793, loss 0.323043, acc 0.90625, learning_rate 0.000103214
2017-10-10T13:02:58.810177: step 1794, loss 0.532364, acc 0.859375, learning_rate 0.000103201
2017-10-10T13:02:59.339797: step 1795, loss 0.456932, acc 0.84375, learning_rate 0.000103188
2017-10-10T13:02:59.964854: step 1796, loss 0.500051, acc 0.859375, learning_rate 0.000103175
2017-10-10T13:03:00.437345: step 1797, loss 0.327969, acc 0.875, learning_rate 0.000103162
2017-10-10T13:03:00.870990: step 1798, loss 0.684729, acc 0.875, learning_rate 0.000103149
2017-10-10T13:03:01.392840: step 1799, loss 0.455147, acc 0.84375, learning_rate 0.000103136
2017-10-10T13:03:01.958815: step 1800, loss 0.259471, acc 0.90625, learning_rate 0.000103123

Evaluation:
2017-10-10T13:03:03.102476: step 1800, loss 0.289722, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1800

2017-10-10T13:03:04.596863: step 1801, loss 0.516065, acc 0.859375, learning_rate 0.000103111
2017-10-10T13:03:05.147327: step 1802, loss 0.634373, acc 0.796875, learning_rate 0.000103098
2017-10-10T13:03:05.674836: step 1803, loss 0.422594, acc 0.875, learning_rate 0.000103085
2017-10-10T13:03:06.216847: step 1804, loss 0.367214, acc 0.859375, learning_rate 0.000103073
2017-10-10T13:03:06.771794: step 1805, loss 0.453723, acc 0.890625, learning_rate 0.00010306
2017-10-10T13:03:07.278719: step 1806, loss 0.327903, acc 0.859375, learning_rate 0.000103048
2017-10-10T13:03:07.840565: step 1807, loss 0.523438, acc 0.828125, learning_rate 0.000103035
2017-10-10T13:03:08.429936: step 1808, loss 0.263882, acc 0.921875, learning_rate 0.000103023
2017-10-10T13:03:08.965813: step 1809, loss 0.398716, acc 0.84375, learning_rate 0.00010301
2017-10-10T13:03:09.516128: step 1810, loss 0.508525, acc 0.8125, learning_rate 0.000102998
2017-10-10T13:03:09.966146: step 1811, loss 0.324418, acc 0.890625, learning_rate 0.000102986
2017-10-10T13:03:10.468863: step 1812, loss 0.467973, acc 0.875, learning_rate 0.000102974
2017-10-10T13:03:10.972910: step 1813, loss 0.438553, acc 0.859375, learning_rate 0.000102962
2017-10-10T13:03:11.488881: step 1814, loss 0.35344, acc 0.859375, learning_rate 0.000102949
2017-10-10T13:03:11.956930: step 1815, loss 0.251325, acc 0.90625, learning_rate 0.000102937
2017-10-10T13:03:12.469034: step 1816, loss 0.237823, acc 0.90625, learning_rate 0.000102925
2017-10-10T13:03:13.021604: step 1817, loss 0.406668, acc 0.84375, learning_rate 0.000102913
2017-10-10T13:03:13.597066: step 1818, loss 0.337581, acc 0.90625, learning_rate 0.000102902
2017-10-10T13:03:14.049083: step 1819, loss 0.687992, acc 0.84375, learning_rate 0.00010289
2017-10-10T13:03:14.589233: step 1820, loss 0.549726, acc 0.8125, learning_rate 0.000102878
2017-10-10T13:03:15.013088: step 1821, loss 0.416643, acc 0.8125, learning_rate 0.000102866
2017-10-10T13:03:15.656882: step 1822, loss 0.387017, acc 0.859375, learning_rate 0.000102855
2017-10-10T13:03:16.304988: step 1823, loss 0.583086, acc 0.796875, learning_rate 0.000102843
2017-10-10T13:03:16.672811: step 1824, loss 0.491882, acc 0.875, learning_rate 0.000102831
2017-10-10T13:03:17.016630: step 1825, loss 0.519563, acc 0.8125, learning_rate 0.00010282
2017-10-10T13:03:17.524903: step 1826, loss 0.558063, acc 0.828125, learning_rate 0.000102808
2017-10-10T13:03:17.957106: step 1827, loss 0.350135, acc 0.90625, learning_rate 0.000102797
2017-10-10T13:03:18.528320: step 1828, loss 0.526447, acc 0.875, learning_rate 0.000102785
2017-10-10T13:03:19.021124: step 1829, loss 0.409172, acc 0.921875, learning_rate 0.000102774
2017-10-10T13:03:19.546777: step 1830, loss 0.344999, acc 0.875, learning_rate 0.000102763
2017-10-10T13:03:20.119890: step 1831, loss 0.309094, acc 0.890625, learning_rate 0.000102751
2017-10-10T13:03:20.624250: step 1832, loss 0.284156, acc 0.921875, learning_rate 0.00010274
2017-10-10T13:03:21.057140: step 1833, loss 0.37412, acc 0.859375, learning_rate 0.000102729
2017-10-10T13:03:21.547538: step 1834, loss 0.304196, acc 0.921875, learning_rate 0.000102718
2017-10-10T13:03:22.088204: step 1835, loss 0.254437, acc 0.921875, learning_rate 0.000102707
2017-10-10T13:03:22.661087: step 1836, loss 0.667718, acc 0.796875, learning_rate 0.000102696
2017-10-10T13:03:23.147151: step 1837, loss 0.511825, acc 0.859375, learning_rate 0.000102685
2017-10-10T13:03:23.760352: step 1838, loss 0.327749, acc 0.875, learning_rate 0.000102674
2017-10-10T13:03:24.217227: step 1839, loss 0.434488, acc 0.828125, learning_rate 0.000102663
2017-10-10T13:03:24.692687: step 1840, loss 0.407303, acc 0.859375, learning_rate 0.000102652

Evaluation:
2017-10-10T13:03:25.934403: step 1840, loss 0.287745, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1840

2017-10-10T13:03:27.556991: step 1841, loss 0.319372, acc 0.90625, learning_rate 0.000102641
2017-10-10T13:03:27.976683: step 1842, loss 0.459813, acc 0.859375, learning_rate 0.00010263
2017-10-10T13:03:28.461069: step 1843, loss 0.394827, acc 0.890625, learning_rate 0.00010262
2017-10-10T13:03:28.965311: step 1844, loss 0.302452, acc 0.875, learning_rate 0.000102609
2017-10-10T13:03:29.506131: step 1845, loss 0.423751, acc 0.84375, learning_rate 0.000102598
2017-10-10T13:03:30.012049: step 1846, loss 0.470507, acc 0.8125, learning_rate 0.000102588
2017-10-10T13:03:30.484921: step 1847, loss 0.554925, acc 0.828125, learning_rate 0.000102577
2017-10-10T13:03:31.053099: step 1848, loss 0.359547, acc 0.84375, learning_rate 0.000102567
2017-10-10T13:03:31.560571: step 1849, loss 0.378544, acc 0.859375, learning_rate 0.000102556
2017-10-10T13:03:32.032861: step 1850, loss 0.258733, acc 0.921875, learning_rate 0.000102546
2017-10-10T13:03:32.569172: step 1851, loss 0.450374, acc 0.875, learning_rate 0.000102535
2017-10-10T13:03:33.133078: step 1852, loss 0.439949, acc 0.796875, learning_rate 0.000102525
2017-10-10T13:03:33.708871: step 1853, loss 0.501186, acc 0.8125, learning_rate 0.000102515
2017-10-10T13:03:34.241228: step 1854, loss 0.424287, acc 0.859375, learning_rate 0.000102504
2017-10-10T13:03:34.802655: step 1855, loss 0.428715, acc 0.890625, learning_rate 0.000102494
2017-10-10T13:03:35.373161: step 1856, loss 0.275258, acc 0.890625, learning_rate 0.000102484
2017-10-10T13:03:36.032989: step 1857, loss 0.484806, acc 0.84375, learning_rate 0.000102474
2017-10-10T13:03:36.560526: step 1858, loss 0.309179, acc 0.890625, learning_rate 0.000102464
2017-10-10T13:03:37.073038: step 1859, loss 0.276078, acc 0.9375, learning_rate 0.000102454
2017-10-10T13:03:37.632136: step 1860, loss 0.331132, acc 0.859375, learning_rate 0.000102444
2017-10-10T13:03:38.109072: step 1861, loss 0.307109, acc 0.890625, learning_rate 0.000102434
2017-10-10T13:03:38.603760: step 1862, loss 0.415458, acc 0.862745, learning_rate 0.000102424
2017-10-10T13:03:39.237197: step 1863, loss 0.297842, acc 0.90625, learning_rate 0.000102414
2017-10-10T13:03:39.676987: step 1864, loss 0.443037, acc 0.875, learning_rate 0.000102404
2017-10-10T13:03:40.032820: step 1865, loss 0.311555, acc 0.90625, learning_rate 0.000102394
2017-10-10T13:03:40.348549: step 1866, loss 0.429412, acc 0.84375, learning_rate 0.000102384
2017-10-10T13:03:40.853098: step 1867, loss 0.311111, acc 0.90625, learning_rate 0.000102375
2017-10-10T13:03:41.394856: step 1868, loss 0.487691, acc 0.796875, learning_rate 0.000102365
2017-10-10T13:03:41.920896: step 1869, loss 0.575907, acc 0.84375, learning_rate 0.000102355
2017-10-10T13:03:42.500896: step 1870, loss 0.285462, acc 0.9375, learning_rate 0.000102346
2017-10-10T13:03:43.000903: step 1871, loss 0.559014, acc 0.828125, learning_rate 0.000102336
2017-10-10T13:03:43.554696: step 1872, loss 0.414274, acc 0.84375, learning_rate 0.000102327
2017-10-10T13:03:44.107788: step 1873, loss 0.388917, acc 0.84375, learning_rate 0.000102317
2017-10-10T13:03:44.643081: step 1874, loss 0.697191, acc 0.8125, learning_rate 0.000102308
2017-10-10T13:03:45.199926: step 1875, loss 0.332706, acc 0.875, learning_rate 0.000102298
2017-10-10T13:03:45.768850: step 1876, loss 0.425514, acc 0.875, learning_rate 0.000102289
2017-10-10T13:03:46.381562: step 1877, loss 0.376274, acc 0.890625, learning_rate 0.000102279
2017-10-10T13:03:46.799414: step 1878, loss 0.442357, acc 0.859375, learning_rate 0.00010227
2017-10-10T13:03:47.242770: step 1879, loss 0.269574, acc 0.90625, learning_rate 0.000102261
2017-10-10T13:03:47.776881: step 1880, loss 0.302118, acc 0.890625, learning_rate 0.000102252

Evaluation:
2017-10-10T13:03:49.016994: step 1880, loss 0.28798, acc 0.889209

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1880

2017-10-10T13:03:50.843207: step 1881, loss 0.251129, acc 0.9375, learning_rate 0.000102242
2017-10-10T13:03:51.282745: step 1882, loss 0.401205, acc 0.859375, learning_rate 0.000102233
2017-10-10T13:03:51.798733: step 1883, loss 0.319893, acc 0.90625, learning_rate 0.000102224
2017-10-10T13:03:52.280885: step 1884, loss 0.514441, acc 0.8125, learning_rate 0.000102215
2017-10-10T13:03:52.811255: step 1885, loss 0.283305, acc 0.890625, learning_rate 0.000102206
2017-10-10T13:03:53.328060: step 1886, loss 0.445122, acc 0.8125, learning_rate 0.000102197
2017-10-10T13:03:53.832420: step 1887, loss 0.347415, acc 0.859375, learning_rate 0.000102188
2017-10-10T13:03:54.369117: step 1888, loss 0.585201, acc 0.765625, learning_rate 0.000102179
2017-10-10T13:03:54.912834: step 1889, loss 0.367792, acc 0.953125, learning_rate 0.00010217
2017-10-10T13:03:55.484970: step 1890, loss 0.457564, acc 0.875, learning_rate 0.000102161
2017-10-10T13:03:56.041055: step 1891, loss 0.409406, acc 0.859375, learning_rate 0.000102153
2017-10-10T13:03:56.624919: step 1892, loss 0.318504, acc 0.90625, learning_rate 0.000102144
2017-10-10T13:03:57.153042: step 1893, loss 0.367025, acc 0.890625, learning_rate 0.000102135
2017-10-10T13:03:57.717087: step 1894, loss 0.328407, acc 0.921875, learning_rate 0.000102126
2017-10-10T13:03:58.246829: step 1895, loss 0.268945, acc 0.9375, learning_rate 0.000102118
2017-10-10T13:03:58.840960: step 1896, loss 0.278621, acc 0.921875, learning_rate 0.000102109
2017-10-10T13:03:59.448970: step 1897, loss 0.458537, acc 0.84375, learning_rate 0.0001021
2017-10-10T13:03:59.996832: step 1898, loss 0.481511, acc 0.796875, learning_rate 0.000102092
2017-10-10T13:04:00.538853: step 1899, loss 0.48353, acc 0.8125, learning_rate 0.000102083
2017-10-10T13:04:01.144943: step 1900, loss 0.358638, acc 0.859375, learning_rate 0.000102075
2017-10-10T13:04:01.759605: step 1901, loss 0.460946, acc 0.8125, learning_rate 0.000102066
2017-10-10T13:04:02.361203: step 1902, loss 0.25049, acc 0.9375, learning_rate 0.000102058
2017-10-10T13:04:02.666716: step 1903, loss 0.429348, acc 0.8125, learning_rate 0.00010205
2017-10-10T13:04:02.965553: step 1904, loss 0.442003, acc 0.84375, learning_rate 0.000102041
2017-10-10T13:04:03.300006: step 1905, loss 0.561487, acc 0.765625, learning_rate 0.000102033
2017-10-10T13:04:03.644910: step 1906, loss 0.293072, acc 0.9375, learning_rate 0.000102025
2017-10-10T13:04:03.988863: step 1907, loss 0.340782, acc 0.890625, learning_rate 0.000102016
2017-10-10T13:04:04.530668: step 1908, loss 0.347045, acc 0.875, learning_rate 0.000102008
2017-10-10T13:04:05.110463: step 1909, loss 0.327538, acc 0.90625, learning_rate 0.000102
2017-10-10T13:04:05.609451: step 1910, loss 0.434409, acc 0.859375, learning_rate 0.000101992
2017-10-10T13:04:06.119650: step 1911, loss 0.504694, acc 0.75, learning_rate 0.000101984
2017-10-10T13:04:06.648874: step 1912, loss 0.31775, acc 0.875, learning_rate 0.000101975
2017-10-10T13:04:07.178889: step 1913, loss 0.498159, acc 0.84375, learning_rate 0.000101967
2017-10-10T13:04:07.767258: step 1914, loss 0.315597, acc 0.875, learning_rate 0.000101959
2017-10-10T13:04:08.416872: step 1915, loss 0.396019, acc 0.84375, learning_rate 0.000101951
2017-10-10T13:04:08.872865: step 1916, loss 0.515343, acc 0.828125, learning_rate 0.000101943
2017-10-10T13:04:09.280958: step 1917, loss 0.27525, acc 0.921875, learning_rate 0.000101935
2017-10-10T13:04:09.741086: step 1918, loss 0.359923, acc 0.875, learning_rate 0.000101928
2017-10-10T13:04:10.288867: step 1919, loss 0.311713, acc 0.90625, learning_rate 0.00010192
2017-10-10T13:04:10.828970: step 1920, loss 0.508858, acc 0.828125, learning_rate 0.000101912

Evaluation:
2017-10-10T13:04:11.985115: step 1920, loss 0.287053, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1920

2017-10-10T13:04:13.405010: step 1921, loss 0.438746, acc 0.859375, learning_rate 0.000101904
2017-10-10T13:04:13.895823: step 1922, loss 0.498306, acc 0.765625, learning_rate 0.000101896
2017-10-10T13:04:14.407524: step 1923, loss 0.614817, acc 0.75, learning_rate 0.000101889
2017-10-10T13:04:14.914877: step 1924, loss 0.515427, acc 0.828125, learning_rate 0.000101881
2017-10-10T13:04:15.417820: step 1925, loss 0.324145, acc 0.890625, learning_rate 0.000101873
2017-10-10T13:04:15.903946: step 1926, loss 0.476911, acc 0.859375, learning_rate 0.000101865
2017-10-10T13:04:16.461142: step 1927, loss 0.56894, acc 0.765625, learning_rate 0.000101858
2017-10-10T13:04:16.964906: step 1928, loss 0.331531, acc 0.890625, learning_rate 0.00010185
2017-10-10T13:04:17.508921: step 1929, loss 0.313228, acc 0.890625, learning_rate 0.000101843
2017-10-10T13:04:18.099576: step 1930, loss 0.281504, acc 0.90625, learning_rate 0.000101835
2017-10-10T13:04:18.649096: step 1931, loss 0.343704, acc 0.90625, learning_rate 0.000101828
2017-10-10T13:04:19.172473: step 1932, loss 0.392923, acc 0.84375, learning_rate 0.00010182
2017-10-10T13:04:19.642482: step 1933, loss 0.620459, acc 0.796875, learning_rate 0.000101813
2017-10-10T13:04:20.178974: step 1934, loss 0.608373, acc 0.796875, learning_rate 0.000101805
2017-10-10T13:04:20.633190: step 1935, loss 0.465575, acc 0.875, learning_rate 0.000101798
2017-10-10T13:04:21.121198: step 1936, loss 0.343035, acc 0.875, learning_rate 0.000101791
2017-10-10T13:04:21.612897: step 1937, loss 0.242972, acc 0.90625, learning_rate 0.000101783
2017-10-10T13:04:22.184831: step 1938, loss 0.335413, acc 0.84375, learning_rate 0.000101776
2017-10-10T13:04:22.676282: step 1939, loss 0.406214, acc 0.828125, learning_rate 0.000101769
2017-10-10T13:04:23.239363: step 1940, loss 0.407969, acc 0.84375, learning_rate 0.000101762
2017-10-10T13:04:23.739385: step 1941, loss 0.362015, acc 0.890625, learning_rate 0.000101754
2017-10-10T13:04:24.260382: step 1942, loss 0.5631, acc 0.828125, learning_rate 0.000101747
2017-10-10T13:04:24.760845: step 1943, loss 0.266681, acc 0.90625, learning_rate 0.00010174
2017-10-10T13:04:25.296987: step 1944, loss 0.451325, acc 0.875, learning_rate 0.000101733
2017-10-10T13:04:25.989210: step 1945, loss 0.313412, acc 0.90625, learning_rate 0.000101726
2017-10-10T13:04:26.349818: step 1946, loss 0.481315, acc 0.828125, learning_rate 0.000101719
2017-10-10T13:04:26.660287: step 1947, loss 0.517916, acc 0.828125, learning_rate 0.000101712
2017-10-10T13:04:26.991375: step 1948, loss 0.573592, acc 0.796875, learning_rate 0.000101705
2017-10-10T13:04:27.514134: step 1949, loss 0.404711, acc 0.84375, learning_rate 0.000101698
2017-10-10T13:04:28.035792: step 1950, loss 0.394419, acc 0.859375, learning_rate 0.000101691
2017-10-10T13:04:28.567005: step 1951, loss 0.306491, acc 0.890625, learning_rate 0.000101684
2017-10-10T13:04:29.116179: step 1952, loss 0.515639, acc 0.859375, learning_rate 0.000101677
2017-10-10T13:04:29.638717: step 1953, loss 0.330844, acc 0.875, learning_rate 0.00010167
2017-10-10T13:04:30.179373: step 1954, loss 0.454867, acc 0.875, learning_rate 0.000101664
2017-10-10T13:04:30.724891: step 1955, loss 0.467984, acc 0.875, learning_rate 0.000101657
2017-10-10T13:04:31.344022: step 1956, loss 0.286979, acc 0.90625, learning_rate 0.00010165
2017-10-10T13:04:31.712830: step 1957, loss 0.316321, acc 0.84375, learning_rate 0.000101643
2017-10-10T13:04:32.054178: step 1958, loss 0.451667, acc 0.84375, learning_rate 0.000101637
2017-10-10T13:04:32.493135: step 1959, loss 0.533472, acc 0.796875, learning_rate 0.00010163
2017-10-10T13:04:32.886081: step 1960, loss 0.215561, acc 0.980392, learning_rate 0.000101623

Evaluation:
2017-10-10T13:04:34.015123: step 1960, loss 0.287275, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-1960

2017-10-10T13:04:35.608817: step 1961, loss 0.144875, acc 0.984375, learning_rate 0.000101617
2017-10-10T13:04:36.164882: step 1962, loss 0.456828, acc 0.84375, learning_rate 0.00010161
2017-10-10T13:04:36.676916: step 1963, loss 0.40949, acc 0.859375, learning_rate 0.000101604
2017-10-10T13:04:37.234519: step 1964, loss 0.570509, acc 0.8125, learning_rate 0.000101597
2017-10-10T13:04:37.736906: step 1965, loss 0.46927, acc 0.859375, learning_rate 0.00010159
2017-10-10T13:04:38.121156: step 1966, loss 0.446792, acc 0.859375, learning_rate 0.000101584
2017-10-10T13:04:38.648046: step 1967, loss 0.24695, acc 0.953125, learning_rate 0.000101577
2017-10-10T13:04:39.133008: step 1968, loss 0.503847, acc 0.859375, learning_rate 0.000101571
2017-10-10T13:04:39.752849: step 1969, loss 0.676533, acc 0.828125, learning_rate 0.000101565
2017-10-10T13:04:40.228923: step 1970, loss 0.324057, acc 0.890625, learning_rate 0.000101558
2017-10-10T13:04:40.812898: step 1971, loss 0.300461, acc 0.890625, learning_rate 0.000101552
2017-10-10T13:04:41.336740: step 1972, loss 0.328656, acc 0.859375, learning_rate 0.000101546
2017-10-10T13:04:41.830682: step 1973, loss 0.513245, acc 0.8125, learning_rate 0.000101539
2017-10-10T13:04:42.292371: step 1974, loss 0.231989, acc 0.90625, learning_rate 0.000101533
2017-10-10T13:04:42.791986: step 1975, loss 0.543518, acc 0.84375, learning_rate 0.000101527
2017-10-10T13:04:43.282376: step 1976, loss 0.418387, acc 0.875, learning_rate 0.00010152
2017-10-10T13:04:43.864873: step 1977, loss 0.387293, acc 0.875, learning_rate 0.000101514
2017-10-10T13:04:44.416852: step 1978, loss 0.287359, acc 0.90625, learning_rate 0.000101508
2017-10-10T13:04:44.901013: step 1979, loss 0.421143, acc 0.84375, learning_rate 0.000101502
2017-10-10T13:04:45.440052: step 1980, loss 0.387973, acc 0.875, learning_rate 0.000101496
2017-10-10T13:04:46.009005: step 1981, loss 0.374292, acc 0.90625, learning_rate 0.00010149
2017-10-10T13:04:46.440891: step 1982, loss 0.390399, acc 0.84375, learning_rate 0.000101484
2017-10-10T13:04:47.033081: step 1983, loss 0.457122, acc 0.8125, learning_rate 0.000101478
2017-10-10T13:04:47.567832: step 1984, loss 0.492318, acc 0.8125, learning_rate 0.000101472
2017-10-10T13:04:48.129027: step 1985, loss 0.337253, acc 0.859375, learning_rate 0.000101466
2017-10-10T13:04:48.785169: step 1986, loss 0.3096, acc 0.90625, learning_rate 0.00010146
2017-10-10T13:04:49.212975: step 1987, loss 0.362247, acc 0.875, learning_rate 0.000101454
2017-10-10T13:04:49.593599: step 1988, loss 0.5103, acc 0.765625, learning_rate 0.000101448
2017-10-10T13:04:49.953888: step 1989, loss 0.385025, acc 0.890625, learning_rate 0.000101442
2017-10-10T13:04:50.464934: step 1990, loss 0.466208, acc 0.90625, learning_rate 0.000101436
2017-10-10T13:04:50.921165: step 1991, loss 0.436389, acc 0.84375, learning_rate 0.00010143
2017-10-10T13:04:51.576897: step 1992, loss 0.379513, acc 0.890625, learning_rate 0.000101424
2017-10-10T13:04:51.997078: step 1993, loss 0.534601, acc 0.8125, learning_rate 0.000101418
2017-10-10T13:04:52.509136: step 1994, loss 0.488585, acc 0.828125, learning_rate 0.000101413
2017-10-10T13:04:53.097150: step 1995, loss 0.155538, acc 0.96875, learning_rate 0.000101407
2017-10-10T13:04:53.638890: step 1996, loss 0.373617, acc 0.890625, learning_rate 0.000101401
2017-10-10T13:04:54.238754: step 1997, loss 0.631252, acc 0.875, learning_rate 0.000101395
2017-10-10T13:04:54.795813: step 1998, loss 0.248492, acc 0.9375, learning_rate 0.00010139
2017-10-10T13:04:55.226301: step 1999, loss 0.337422, acc 0.90625, learning_rate 0.000101384
2017-10-10T13:04:55.642068: step 2000, loss 0.328303, acc 0.84375, learning_rate 0.000101378

Evaluation:
2017-10-10T13:04:56.829011: step 2000, loss 0.285467, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2000

2017-10-10T13:04:58.422747: step 2001, loss 0.237804, acc 0.890625, learning_rate 0.000101373
2017-10-10T13:04:58.961825: step 2002, loss 0.448689, acc 0.859375, learning_rate 0.000101367
2017-10-10T13:04:59.500197: step 2003, loss 0.349026, acc 0.859375, learning_rate 0.000101362
2017-10-10T13:05:00.056726: step 2004, loss 0.383271, acc 0.828125, learning_rate 0.000101356
2017-10-10T13:05:00.587685: step 2005, loss 0.453579, acc 0.890625, learning_rate 0.00010135
2017-10-10T13:05:01.096874: step 2006, loss 0.303892, acc 0.90625, learning_rate 0.000101345
2017-10-10T13:05:01.624866: step 2007, loss 0.308866, acc 0.890625, learning_rate 0.000101339
2017-10-10T13:05:02.182746: step 2008, loss 0.528496, acc 0.828125, learning_rate 0.000101334
2017-10-10T13:05:02.713778: step 2009, loss 0.469884, acc 0.859375, learning_rate 0.000101328
2017-10-10T13:05:03.249166: step 2010, loss 0.629044, acc 0.796875, learning_rate 0.000101323
2017-10-10T13:05:03.776853: step 2011, loss 0.47202, acc 0.859375, learning_rate 0.000101318
2017-10-10T13:05:04.288885: step 2012, loss 0.325673, acc 0.890625, learning_rate 0.000101312
2017-10-10T13:05:04.809052: step 2013, loss 0.569486, acc 0.8125, learning_rate 0.000101307
2017-10-10T13:05:05.340845: step 2014, loss 0.280805, acc 0.921875, learning_rate 0.000101302
2017-10-10T13:05:05.844945: step 2015, loss 0.454531, acc 0.828125, learning_rate 0.000101296
2017-10-10T13:05:06.425337: step 2016, loss 0.298718, acc 0.90625, learning_rate 0.000101291
2017-10-10T13:05:06.948829: step 2017, loss 0.456796, acc 0.859375, learning_rate 0.000101286
2017-10-10T13:05:07.541185: step 2018, loss 0.276684, acc 0.90625, learning_rate 0.00010128
2017-10-10T13:05:08.065138: step 2019, loss 0.319065, acc 0.875, learning_rate 0.000101275
2017-10-10T13:05:08.608835: step 2020, loss 0.317766, acc 0.890625, learning_rate 0.00010127
2017-10-10T13:05:09.122358: step 2021, loss 0.364263, acc 0.875, learning_rate 0.000101265
2017-10-10T13:05:09.648254: step 2022, loss 0.341442, acc 0.890625, learning_rate 0.00010126
2017-10-10T13:05:10.181636: step 2023, loss 0.361863, acc 0.828125, learning_rate 0.000101255
2017-10-10T13:05:10.662827: step 2024, loss 0.313518, acc 0.90625, learning_rate 0.000101249
2017-10-10T13:05:11.261949: step 2025, loss 0.391742, acc 0.828125, learning_rate 0.000101244
2017-10-10T13:05:11.918312: step 2026, loss 0.390859, acc 0.90625, learning_rate 0.000101239
2017-10-10T13:05:12.413003: step 2027, loss 0.613062, acc 0.875, learning_rate 0.000101234
2017-10-10T13:05:12.731304: step 2028, loss 0.444812, acc 0.859375, learning_rate 0.000101229
2017-10-10T13:05:13.136878: step 2029, loss 0.544651, acc 0.8125, learning_rate 0.000101224
2017-10-10T13:05:13.687265: step 2030, loss 0.372137, acc 0.875, learning_rate 0.000101219
2017-10-10T13:05:14.185679: step 2031, loss 0.350061, acc 0.859375, learning_rate 0.000101214
2017-10-10T13:05:14.722501: step 2032, loss 0.335291, acc 0.890625, learning_rate 0.000101209
2017-10-10T13:05:15.256322: step 2033, loss 0.33309, acc 0.890625, learning_rate 0.000101204
2017-10-10T13:05:15.802179: step 2034, loss 0.372868, acc 0.859375, learning_rate 0.000101199
2017-10-10T13:05:16.432969: step 2035, loss 0.385338, acc 0.953125, learning_rate 0.000101194
2017-10-10T13:05:17.069687: step 2036, loss 0.340218, acc 0.890625, learning_rate 0.00010119
2017-10-10T13:05:17.514156: step 2037, loss 0.361063, acc 0.90625, learning_rate 0.000101185
2017-10-10T13:05:17.918176: step 2038, loss 0.487737, acc 0.875, learning_rate 0.00010118
2017-10-10T13:05:18.416325: step 2039, loss 0.370372, acc 0.859375, learning_rate 0.000101175
2017-10-10T13:05:18.964918: step 2040, loss 0.471331, acc 0.8125, learning_rate 0.00010117

Evaluation:
2017-10-10T13:05:20.118384: step 2040, loss 0.283936, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2040

2017-10-10T13:05:21.767333: step 2041, loss 0.489573, acc 0.8125, learning_rate 0.000101166
2017-10-10T13:05:22.334398: step 2042, loss 0.548097, acc 0.765625, learning_rate 0.000101161
2017-10-10T13:05:22.892814: step 2043, loss 0.453986, acc 0.84375, learning_rate 0.000101156
2017-10-10T13:05:23.454101: step 2044, loss 0.510284, acc 0.828125, learning_rate 0.000101151
2017-10-10T13:05:23.964979: step 2045, loss 0.362873, acc 0.875, learning_rate 0.000101147
2017-10-10T13:05:24.528994: step 2046, loss 0.382001, acc 0.90625, learning_rate 0.000101142
2017-10-10T13:05:25.004982: step 2047, loss 0.444056, acc 0.875, learning_rate 0.000101137
2017-10-10T13:05:25.462881: step 2048, loss 0.263631, acc 0.890625, learning_rate 0.000101133
2017-10-10T13:05:25.947861: step 2049, loss 0.312113, acc 0.90625, learning_rate 0.000101128
2017-10-10T13:05:26.461929: step 2050, loss 0.296012, acc 0.90625, learning_rate 0.000101123
2017-10-10T13:05:27.026447: step 2051, loss 0.34771, acc 0.859375, learning_rate 0.000101119
2017-10-10T13:05:27.553056: step 2052, loss 0.224872, acc 0.953125, learning_rate 0.000101114
2017-10-10T13:05:28.126416: step 2053, loss 0.424352, acc 0.90625, learning_rate 0.00010111
2017-10-10T13:05:28.701116: step 2054, loss 0.380568, acc 0.890625, learning_rate 0.000101105
2017-10-10T13:05:29.256891: step 2055, loss 0.485186, acc 0.84375, learning_rate 0.000101101
2017-10-10T13:05:29.933060: step 2056, loss 0.457924, acc 0.875, learning_rate 0.000101096
2017-10-10T13:05:30.492867: step 2057, loss 0.42688, acc 0.875, learning_rate 0.000101092
2017-10-10T13:05:30.970341: step 2058, loss 0.415667, acc 0.823529, learning_rate 0.000101087
2017-10-10T13:05:31.460984: step 2059, loss 0.379621, acc 0.828125, learning_rate 0.000101083
2017-10-10T13:05:32.053534: step 2060, loss 0.203021, acc 0.9375, learning_rate 0.000101078
2017-10-10T13:05:32.599595: step 2061, loss 0.558729, acc 0.84375, learning_rate 0.000101074
2017-10-10T13:05:33.039324: step 2062, loss 0.539091, acc 0.828125, learning_rate 0.00010107
2017-10-10T13:05:33.506254: step 2063, loss 0.351978, acc 0.890625, learning_rate 0.000101065
2017-10-10T13:05:34.076993: step 2064, loss 0.263601, acc 0.875, learning_rate 0.000101061
2017-10-10T13:05:34.660906: step 2065, loss 0.434027, acc 0.875, learning_rate 0.000101057
2017-10-10T13:05:35.277150: step 2066, loss 0.330646, acc 0.84375, learning_rate 0.000101052
2017-10-10T13:05:35.658981: step 2067, loss 0.580066, acc 0.828125, learning_rate 0.000101048
2017-10-10T13:05:36.110783: step 2068, loss 0.330885, acc 0.890625, learning_rate 0.000101044
2017-10-10T13:05:36.552063: step 2069, loss 0.234196, acc 0.9375, learning_rate 0.000101039
2017-10-10T13:05:37.100882: step 2070, loss 0.396421, acc 0.921875, learning_rate 0.000101035
2017-10-10T13:05:37.613481: step 2071, loss 0.403356, acc 0.890625, learning_rate 0.000101031
2017-10-10T13:05:38.193040: step 2072, loss 0.316301, acc 0.90625, learning_rate 0.000101027
2017-10-10T13:05:38.724832: step 2073, loss 0.312044, acc 0.875, learning_rate 0.000101023
2017-10-10T13:05:39.245014: step 2074, loss 0.485861, acc 0.875, learning_rate 0.000101018
2017-10-10T13:05:39.908292: step 2075, loss 0.389752, acc 0.84375, learning_rate 0.000101014
2017-10-10T13:05:40.357870: step 2076, loss 0.434738, acc 0.875, learning_rate 0.00010101
2017-10-10T13:05:40.816442: step 2077, loss 0.407799, acc 0.8125, learning_rate 0.000101006
2017-10-10T13:05:41.269352: step 2078, loss 0.557232, acc 0.734375, learning_rate 0.000101002
2017-10-10T13:05:41.772875: step 2079, loss 0.432047, acc 0.875, learning_rate 0.000100998
2017-10-10T13:05:42.281538: step 2080, loss 0.434408, acc 0.890625, learning_rate 0.000100994

Evaluation:
2017-10-10T13:05:43.395662: step 2080, loss 0.28388, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2080

2017-10-10T13:05:44.957889: step 2081, loss 0.237336, acc 0.9375, learning_rate 0.00010099
2017-10-10T13:05:45.458522: step 2082, loss 0.363257, acc 0.90625, learning_rate 0.000100986
2017-10-10T13:05:46.001803: step 2083, loss 0.484473, acc 0.828125, learning_rate 0.000100982
2017-10-10T13:05:46.532887: step 2084, loss 0.435388, acc 0.890625, learning_rate 0.000100978
2017-10-10T13:05:47.113929: step 2085, loss 0.363027, acc 0.890625, learning_rate 0.000100974
2017-10-10T13:05:47.632844: step 2086, loss 0.352938, acc 0.84375, learning_rate 0.00010097
2017-10-10T13:05:48.133499: step 2087, loss 0.518848, acc 0.875, learning_rate 0.000100966
2017-10-10T13:05:48.609729: step 2088, loss 0.386267, acc 0.90625, learning_rate 0.000100962
2017-10-10T13:05:49.151536: step 2089, loss 0.449573, acc 0.875, learning_rate 0.000100958
2017-10-10T13:05:49.712866: step 2090, loss 0.562372, acc 0.875, learning_rate 0.000100954
2017-10-10T13:05:50.264881: step 2091, loss 0.4123, acc 0.84375, learning_rate 0.00010095
2017-10-10T13:05:50.776763: step 2092, loss 0.458936, acc 0.828125, learning_rate 0.000100946
2017-10-10T13:05:51.286915: step 2093, loss 0.525104, acc 0.859375, learning_rate 0.000100942
2017-10-10T13:05:51.831921: step 2094, loss 0.401799, acc 0.828125, learning_rate 0.000100938
2017-10-10T13:05:52.358586: step 2095, loss 0.482588, acc 0.84375, learning_rate 0.000100935
2017-10-10T13:05:52.920972: step 2096, loss 0.545109, acc 0.859375, learning_rate 0.000100931
2017-10-10T13:05:53.488842: step 2097, loss 0.335985, acc 0.90625, learning_rate 0.000100927
2017-10-10T13:05:54.008863: step 2098, loss 0.290797, acc 0.890625, learning_rate 0.000100923
2017-10-10T13:05:54.565094: step 2099, loss 0.318935, acc 0.890625, learning_rate 0.000100919
2017-10-10T13:05:55.086404: step 2100, loss 0.394575, acc 0.90625, learning_rate 0.000100916
2017-10-10T13:05:55.591558: step 2101, loss 0.407434, acc 0.890625, learning_rate 0.000100912
2017-10-10T13:05:56.094988: step 2102, loss 0.274043, acc 0.9375, learning_rate 0.000100908
2017-10-10T13:05:56.630686: step 2103, loss 0.377173, acc 0.84375, learning_rate 0.000100904
2017-10-10T13:05:57.205276: step 2104, loss 0.384486, acc 0.90625, learning_rate 0.000100901
2017-10-10T13:05:57.660832: step 2105, loss 0.463092, acc 0.859375, learning_rate 0.000100897
2017-10-10T13:05:58.213016: step 2106, loss 0.296781, acc 0.9375, learning_rate 0.000100893
2017-10-10T13:05:58.672869: step 2107, loss 0.307811, acc 0.90625, learning_rate 0.00010089
2017-10-10T13:05:59.121035: step 2108, loss 0.442761, acc 0.859375, learning_rate 0.000100886
2017-10-10T13:05:59.609006: step 2109, loss 0.27035, acc 0.90625, learning_rate 0.000100883
2017-10-10T13:06:00.071636: step 2110, loss 0.404789, acc 0.828125, learning_rate 0.000100879
2017-10-10T13:06:00.581265: step 2111, loss 0.459498, acc 0.859375, learning_rate 0.000100875
2017-10-10T13:06:01.086470: step 2112, loss 0.534111, acc 0.796875, learning_rate 0.000100872
2017-10-10T13:06:01.588654: step 2113, loss 0.267124, acc 0.953125, learning_rate 0.000100868
2017-10-10T13:06:02.100997: step 2114, loss 0.617949, acc 0.765625, learning_rate 0.000100865
2017-10-10T13:06:02.765002: step 2115, loss 0.275965, acc 0.921875, learning_rate 0.000100861
2017-10-10T13:06:03.212686: step 2116, loss 0.301144, acc 0.90625, learning_rate 0.000100858
2017-10-10T13:06:03.674305: step 2117, loss 0.395238, acc 0.828125, learning_rate 0.000100854
2017-10-10T13:06:04.129416: step 2118, loss 0.458144, acc 0.84375, learning_rate 0.000100851
2017-10-10T13:06:04.618221: step 2119, loss 0.483014, acc 0.828125, learning_rate 0.000100847
2017-10-10T13:06:05.154050: step 2120, loss 0.481912, acc 0.828125, learning_rate 0.000100844

Evaluation:
2017-10-10T13:06:06.233888: step 2120, loss 0.285331, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2120

2017-10-10T13:06:07.759373: step 2121, loss 0.239405, acc 0.921875, learning_rate 0.00010084
2017-10-10T13:06:08.320779: step 2122, loss 0.600166, acc 0.828125, learning_rate 0.000100837
2017-10-10T13:06:08.772885: step 2123, loss 0.341738, acc 0.90625, learning_rate 0.000100833
2017-10-10T13:06:09.276956: step 2124, loss 0.282079, acc 0.890625, learning_rate 0.00010083
2017-10-10T13:06:09.836532: step 2125, loss 0.340877, acc 0.875, learning_rate 0.000100827
2017-10-10T13:06:10.312957: step 2126, loss 0.302463, acc 0.90625, learning_rate 0.000100823
2017-10-10T13:06:10.861050: step 2127, loss 0.500105, acc 0.828125, learning_rate 0.00010082
2017-10-10T13:06:11.407323: step 2128, loss 0.375821, acc 0.875, learning_rate 0.000100817
2017-10-10T13:06:11.917009: step 2129, loss 0.277923, acc 0.90625, learning_rate 0.000100813
2017-10-10T13:06:12.409384: step 2130, loss 0.409123, acc 0.859375, learning_rate 0.00010081
2017-10-10T13:06:12.924849: step 2131, loss 0.361234, acc 0.90625, learning_rate 0.000100807
2017-10-10T13:06:13.476914: step 2132, loss 0.405156, acc 0.796875, learning_rate 0.000100803
2017-10-10T13:06:14.066038: step 2133, loss 0.27245, acc 0.890625, learning_rate 0.0001008
2017-10-10T13:06:14.586526: step 2134, loss 0.377835, acc 0.875, learning_rate 0.000100797
2017-10-10T13:06:15.116829: step 2135, loss 0.298221, acc 0.921875, learning_rate 0.000100793
2017-10-10T13:06:15.640840: step 2136, loss 0.325449, acc 0.890625, learning_rate 0.00010079
2017-10-10T13:06:16.184924: step 2137, loss 0.473841, acc 0.859375, learning_rate 0.000100787
2017-10-10T13:06:16.689314: step 2138, loss 0.458838, acc 0.875, learning_rate 0.000100784
2017-10-10T13:06:17.223591: step 2139, loss 0.440044, acc 0.875, learning_rate 0.000100781
2017-10-10T13:06:17.788954: step 2140, loss 0.356046, acc 0.875, learning_rate 0.000100777
2017-10-10T13:06:18.324858: step 2141, loss 0.314723, acc 0.859375, learning_rate 0.000100774
2017-10-10T13:06:18.856818: step 2142, loss 0.300011, acc 0.875, learning_rate 0.000100771
2017-10-10T13:06:19.408502: step 2143, loss 0.313295, acc 0.875, learning_rate 0.000100768
2017-10-10T13:06:19.958165: step 2144, loss 0.327849, acc 0.875, learning_rate 0.000100765
2017-10-10T13:06:20.565279: step 2145, loss 0.434944, acc 0.875, learning_rate 0.000100762
2017-10-10T13:06:21.130038: step 2146, loss 0.297611, acc 0.875, learning_rate 0.000100759
2017-10-10T13:06:21.573318: step 2147, loss 0.415528, acc 0.890625, learning_rate 0.000100755
2017-10-10T13:06:21.895364: step 2148, loss 0.335269, acc 0.90625, learning_rate 0.000100752
2017-10-10T13:06:22.293768: step 2149, loss 0.456838, acc 0.875, learning_rate 0.000100749
2017-10-10T13:06:22.764899: step 2150, loss 0.434848, acc 0.828125, learning_rate 0.000100746
2017-10-10T13:06:23.332221: step 2151, loss 0.451074, acc 0.859375, learning_rate 0.000100743
2017-10-10T13:06:23.838584: step 2152, loss 0.35713, acc 0.875, learning_rate 0.00010074
2017-10-10T13:06:24.369553: step 2153, loss 0.376947, acc 0.859375, learning_rate 0.000100737
2017-10-10T13:06:24.804132: step 2154, loss 0.514127, acc 0.84375, learning_rate 0.000100734
2017-10-10T13:06:25.428861: step 2155, loss 0.474416, acc 0.84375, learning_rate 0.000100731
2017-10-10T13:06:25.885852: step 2156, loss 0.296964, acc 0.901961, learning_rate 0.000100728
2017-10-10T13:06:26.280851: step 2157, loss 0.22441, acc 0.9375, learning_rate 0.000100725
2017-10-10T13:06:26.724852: step 2158, loss 0.448223, acc 0.890625, learning_rate 0.000100722
2017-10-10T13:06:27.221032: step 2159, loss 0.447082, acc 0.859375, learning_rate 0.000100719
2017-10-10T13:06:27.805521: step 2160, loss 0.362949, acc 0.859375, learning_rate 0.000100716

Evaluation:
2017-10-10T13:06:28.928923: step 2160, loss 0.284191, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2160

2017-10-10T13:06:30.770225: step 2161, loss 0.447377, acc 0.875, learning_rate 0.000100713
2017-10-10T13:06:31.303277: step 2162, loss 0.33829, acc 0.921875, learning_rate 0.000100711
2017-10-10T13:06:31.833090: step 2163, loss 0.246726, acc 0.90625, learning_rate 0.000100708
2017-10-10T13:06:32.364833: step 2164, loss 0.641386, acc 0.828125, learning_rate 0.000100705
2017-10-10T13:06:32.872509: step 2165, loss 0.514338, acc 0.84375, learning_rate 0.000100702
2017-10-10T13:06:33.396895: step 2166, loss 0.609479, acc 0.828125, learning_rate 0.000100699
2017-10-10T13:06:33.958343: step 2167, loss 0.326125, acc 0.875, learning_rate 0.000100696
2017-10-10T13:06:34.458528: step 2168, loss 0.43642, acc 0.875, learning_rate 0.000100693
2017-10-10T13:06:34.989238: step 2169, loss 0.331867, acc 0.921875, learning_rate 0.00010069
2017-10-10T13:06:35.517130: step 2170, loss 0.417967, acc 0.84375, learning_rate 0.000100688
2017-10-10T13:06:36.087487: step 2171, loss 0.421749, acc 0.84375, learning_rate 0.000100685
2017-10-10T13:06:36.624155: step 2172, loss 0.453717, acc 0.828125, learning_rate 0.000100682
2017-10-10T13:06:37.192315: step 2173, loss 0.40738, acc 0.875, learning_rate 0.000100679
2017-10-10T13:06:37.729440: step 2174, loss 0.391049, acc 0.875, learning_rate 0.000100677
2017-10-10T13:06:38.301683: step 2175, loss 0.315084, acc 0.859375, learning_rate 0.000100674
2017-10-10T13:06:38.860179: step 2176, loss 0.312467, acc 0.875, learning_rate 0.000100671
2017-10-10T13:06:39.356854: step 2177, loss 0.191427, acc 0.953125, learning_rate 0.000100668
2017-10-10T13:06:39.878828: step 2178, loss 0.346046, acc 0.890625, learning_rate 0.000100666
2017-10-10T13:06:40.396231: step 2179, loss 0.531, acc 0.8125, learning_rate 0.000100663
2017-10-10T13:06:40.916846: step 2180, loss 0.395895, acc 0.859375, learning_rate 0.00010066
2017-10-10T13:06:41.420881: step 2181, loss 0.640824, acc 0.8125, learning_rate 0.000100657
2017-10-10T13:06:41.938019: step 2182, loss 0.372425, acc 0.90625, learning_rate 0.000100655
2017-10-10T13:06:42.528254: step 2183, loss 0.420352, acc 0.796875, learning_rate 0.000100652
2017-10-10T13:06:43.049120: step 2184, loss 0.357088, acc 0.890625, learning_rate 0.000100649
2017-10-10T13:06:43.472888: step 2185, loss 0.292721, acc 0.890625, learning_rate 0.000100647
2017-10-10T13:06:43.968925: step 2186, loss 0.346529, acc 0.875, learning_rate 0.000100644
2017-10-10T13:06:44.564838: step 2187, loss 0.527051, acc 0.796875, learning_rate 0.000100641
2017-10-10T13:06:44.940980: step 2188, loss 0.68241, acc 0.71875, learning_rate 0.000100639
2017-10-10T13:06:45.429320: step 2189, loss 0.285262, acc 0.890625, learning_rate 0.000100636
2017-10-10T13:06:45.886318: step 2190, loss 0.288082, acc 0.9375, learning_rate 0.000100634
2017-10-10T13:06:46.408992: step 2191, loss 0.376409, acc 0.84375, learning_rate 0.000100631
2017-10-10T13:06:46.960953: step 2192, loss 0.365556, acc 0.875, learning_rate 0.000100628
2017-10-10T13:06:47.436913: step 2193, loss 0.445219, acc 0.875, learning_rate 0.000100626
2017-10-10T13:06:47.961012: step 2194, loss 0.24313, acc 0.9375, learning_rate 0.000100623
2017-10-10T13:06:48.597218: step 2195, loss 0.302656, acc 0.84375, learning_rate 0.000100621
2017-10-10T13:06:49.022561: step 2196, loss 0.413251, acc 0.875, learning_rate 0.000100618
2017-10-10T13:06:49.396575: step 2197, loss 0.319349, acc 0.90625, learning_rate 0.000100616
2017-10-10T13:06:49.752925: step 2198, loss 0.237615, acc 0.9375, learning_rate 0.000100613
2017-10-10T13:06:50.285294: step 2199, loss 0.199521, acc 0.9375, learning_rate 0.000100611
2017-10-10T13:06:50.779159: step 2200, loss 0.444301, acc 0.875, learning_rate 0.000100608

Evaluation:
2017-10-10T13:06:51.877556: step 2200, loss 0.283868, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2200

2017-10-10T13:06:53.345141: step 2201, loss 0.250852, acc 0.921875, learning_rate 0.000100606
2017-10-10T13:06:53.913545: step 2202, loss 0.441565, acc 0.859375, learning_rate 0.000100603
2017-10-10T13:06:54.504705: step 2203, loss 0.36256, acc 0.921875, learning_rate 0.000100601
2017-10-10T13:06:54.976905: step 2204, loss 0.229621, acc 0.953125, learning_rate 0.000100598
2017-10-10T13:06:55.468443: step 2205, loss 0.294883, acc 0.90625, learning_rate 0.000100596
2017-10-10T13:06:56.045038: step 2206, loss 0.312344, acc 0.890625, learning_rate 0.000100594
2017-10-10T13:06:56.601151: step 2207, loss 0.350239, acc 0.890625, learning_rate 0.000100591
2017-10-10T13:06:57.277072: step 2208, loss 0.381492, acc 0.84375, learning_rate 0.000100589
2017-10-10T13:06:57.825219: step 2209, loss 0.522353, acc 0.84375, learning_rate 0.000100586
2017-10-10T13:06:58.348077: step 2210, loss 0.254816, acc 0.921875, learning_rate 0.000100584
2017-10-10T13:06:58.905170: step 2211, loss 0.525732, acc 0.859375, learning_rate 0.000100581
2017-10-10T13:06:59.547447: step 2212, loss 0.289055, acc 0.921875, learning_rate 0.000100579
2017-10-10T13:07:00.092197: step 2213, loss 0.386429, acc 0.9375, learning_rate 0.000100577
2017-10-10T13:07:00.655670: step 2214, loss 0.361141, acc 0.875, learning_rate 0.000100574
2017-10-10T13:07:01.205102: step 2215, loss 0.576973, acc 0.8125, learning_rate 0.000100572
2017-10-10T13:07:01.748985: step 2216, loss 0.375332, acc 0.921875, learning_rate 0.00010057
2017-10-10T13:07:02.388871: step 2217, loss 0.35557, acc 0.84375, learning_rate 0.000100567
2017-10-10T13:07:02.868912: step 2218, loss 0.319286, acc 0.90625, learning_rate 0.000100565
2017-10-10T13:07:03.363322: step 2219, loss 0.510655, acc 0.84375, learning_rate 0.000100563
2017-10-10T13:07:03.927261: step 2220, loss 0.273871, acc 0.90625, learning_rate 0.00010056
2017-10-10T13:07:04.480851: step 2221, loss 0.362202, acc 0.875, learning_rate 0.000100558
2017-10-10T13:07:05.077692: step 2222, loss 0.492567, acc 0.84375, learning_rate 0.000100556
2017-10-10T13:07:05.664921: step 2223, loss 0.475947, acc 0.859375, learning_rate 0.000100554
2017-10-10T13:07:06.196858: step 2224, loss 0.406186, acc 0.90625, learning_rate 0.000100551
2017-10-10T13:07:06.650062: step 2225, loss 0.279184, acc 0.90625, learning_rate 0.000100549
2017-10-10T13:07:07.116313: step 2226, loss 0.3384, acc 0.875, learning_rate 0.000100547
2017-10-10T13:07:07.701021: step 2227, loss 0.45263, acc 0.875, learning_rate 0.000100545
2017-10-10T13:07:08.156859: step 2228, loss 0.321086, acc 0.890625, learning_rate 0.000100542
2017-10-10T13:07:08.589947: step 2229, loss 0.371612, acc 0.859375, learning_rate 0.00010054
2017-10-10T13:07:09.105164: step 2230, loss 0.501181, acc 0.828125, learning_rate 0.000100538
2017-10-10T13:07:09.625366: step 2231, loss 0.304806, acc 0.921875, learning_rate 0.000100536
2017-10-10T13:07:10.104399: step 2232, loss 0.323575, acc 0.875, learning_rate 0.000100534
2017-10-10T13:07:10.661161: step 2233, loss 0.461045, acc 0.84375, learning_rate 0.000100531
2017-10-10T13:07:11.275876: step 2234, loss 0.526787, acc 0.8125, learning_rate 0.000100529
2017-10-10T13:07:11.680106: step 2235, loss 0.407587, acc 0.859375, learning_rate 0.000100527
2017-10-10T13:07:12.041439: step 2236, loss 0.466462, acc 0.796875, learning_rate 0.000100525
2017-10-10T13:07:12.553158: step 2237, loss 0.397931, acc 0.875, learning_rate 0.000100523
2017-10-10T13:07:13.036265: step 2238, loss 0.388631, acc 0.90625, learning_rate 0.000100521
2017-10-10T13:07:13.569642: step 2239, loss 0.321175, acc 0.875, learning_rate 0.000100519
2017-10-10T13:07:14.065094: step 2240, loss 0.316307, acc 0.890625, learning_rate 0.000100516

Evaluation:
2017-10-10T13:07:15.108881: step 2240, loss 0.283425, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2240

2017-10-10T13:07:16.816954: step 2241, loss 0.430182, acc 0.859375, learning_rate 0.000100514
2017-10-10T13:07:17.352920: step 2242, loss 0.235841, acc 0.953125, learning_rate 0.000100512
2017-10-10T13:07:17.858057: step 2243, loss 0.509798, acc 0.765625, learning_rate 0.00010051
2017-10-10T13:07:18.349011: step 2244, loss 0.290818, acc 0.859375, learning_rate 0.000100508
2017-10-10T13:07:18.955645: step 2245, loss 0.303995, acc 0.875, learning_rate 0.000100506
2017-10-10T13:07:19.541841: step 2246, loss 0.377511, acc 0.921875, learning_rate 0.000100504
2017-10-10T13:07:20.082545: step 2247, loss 0.41083, acc 0.890625, learning_rate 0.000100502
2017-10-10T13:07:20.970086: step 2248, loss 0.451172, acc 0.796875, learning_rate 0.0001005
2017-10-10T13:07:21.519629: step 2249, loss 0.356021, acc 0.890625, learning_rate 0.000100498
2017-10-10T13:07:22.086543: step 2250, loss 0.33173, acc 0.875, learning_rate 0.000100496
2017-10-10T13:07:22.633452: step 2251, loss 0.470459, acc 0.828125, learning_rate 0.000100494
2017-10-10T13:07:23.115199: step 2252, loss 0.343379, acc 0.890625, learning_rate 0.000100492
2017-10-10T13:07:23.634392: step 2253, loss 0.300431, acc 0.921875, learning_rate 0.00010049
2017-10-10T13:07:24.067531: step 2254, loss 0.271693, acc 0.941176, learning_rate 0.000100488
2017-10-10T13:07:24.541272: step 2255, loss 0.512358, acc 0.875, learning_rate 0.000100486
2017-10-10T13:07:25.045597: step 2256, loss 0.541693, acc 0.796875, learning_rate 0.000100484
2017-10-10T13:07:25.571219: step 2257, loss 0.387304, acc 0.828125, learning_rate 0.000100482
2017-10-10T13:07:26.088870: step 2258, loss 0.537747, acc 0.828125, learning_rate 0.00010048
2017-10-10T13:07:26.592946: step 2259, loss 0.35096, acc 0.90625, learning_rate 0.000100478
2017-10-10T13:07:27.164950: step 2260, loss 0.244285, acc 0.953125, learning_rate 0.000100476
2017-10-10T13:07:27.704830: step 2261, loss 0.297874, acc 0.90625, learning_rate 0.000100474
2017-10-10T13:07:28.261017: step 2262, loss 0.456468, acc 0.859375, learning_rate 0.000100472
2017-10-10T13:07:28.837107: step 2263, loss 0.315568, acc 0.90625, learning_rate 0.00010047
2017-10-10T13:07:29.340815: step 2264, loss 0.310659, acc 0.875, learning_rate 0.000100468
2017-10-10T13:07:29.767118: step 2265, loss 0.251769, acc 0.921875, learning_rate 0.000100466
2017-10-10T13:07:30.149070: step 2266, loss 0.360557, acc 0.890625, learning_rate 0.000100464
2017-10-10T13:07:30.780054: step 2267, loss 0.466375, acc 0.84375, learning_rate 0.000100462
2017-10-10T13:07:31.284876: step 2268, loss 0.384923, acc 0.8125, learning_rate 0.000100461
2017-10-10T13:07:31.736851: step 2269, loss 0.466833, acc 0.8125, learning_rate 0.000100459
2017-10-10T13:07:32.241369: step 2270, loss 0.401317, acc 0.875, learning_rate 0.000100457
2017-10-10T13:07:32.781073: step 2271, loss 0.327373, acc 0.90625, learning_rate 0.000100455
2017-10-10T13:07:33.264905: step 2272, loss 0.449449, acc 0.828125, learning_rate 0.000100453
2017-10-10T13:07:33.849200: step 2273, loss 0.502082, acc 0.84375, learning_rate 0.000100451
2017-10-10T13:07:34.408909: step 2274, loss 0.31568, acc 0.921875, learning_rate 0.000100449
2017-10-10T13:07:34.856560: step 2275, loss 0.258172, acc 0.890625, learning_rate 0.000100448
2017-10-10T13:07:35.300843: step 2276, loss 0.284922, acc 0.921875, learning_rate 0.000100446
2017-10-10T13:07:35.819126: step 2277, loss 0.423495, acc 0.84375, learning_rate 0.000100444
2017-10-10T13:07:36.409079: step 2278, loss 0.509919, acc 0.78125, learning_rate 0.000100442
2017-10-10T13:07:36.920858: step 2279, loss 0.338447, acc 0.90625, learning_rate 0.00010044
2017-10-10T13:07:37.432951: step 2280, loss 0.357902, acc 0.90625, learning_rate 0.000100439

Evaluation:
2017-10-10T13:07:38.632883: step 2280, loss 0.282304, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2280

2017-10-10T13:07:40.520875: step 2281, loss 0.563583, acc 0.8125, learning_rate 0.000100437
2017-10-10T13:07:41.097900: step 2282, loss 0.463693, acc 0.859375, learning_rate 0.000100435
2017-10-10T13:07:41.651706: step 2283, loss 0.328532, acc 0.875, learning_rate 0.000100433
2017-10-10T13:07:42.197079: step 2284, loss 0.301391, acc 0.90625, learning_rate 0.000100431
2017-10-10T13:07:42.750395: step 2285, loss 0.624749, acc 0.78125, learning_rate 0.00010043
2017-10-10T13:07:43.226177: step 2286, loss 0.492123, acc 0.875, learning_rate 0.000100428
2017-10-10T13:07:43.739983: step 2287, loss 0.376833, acc 0.859375, learning_rate 0.000100426
2017-10-10T13:07:44.273411: step 2288, loss 0.320989, acc 0.921875, learning_rate 0.000100424
2017-10-10T13:07:44.813992: step 2289, loss 0.449474, acc 0.859375, learning_rate 0.000100423
2017-10-10T13:07:45.379258: step 2290, loss 0.320685, acc 0.90625, learning_rate 0.000100421
2017-10-10T13:07:45.924894: step 2291, loss 0.375159, acc 0.890625, learning_rate 0.000100419
2017-10-10T13:07:46.452826: step 2292, loss 0.279422, acc 0.875, learning_rate 0.000100418
2017-10-10T13:07:46.983060: step 2293, loss 0.454466, acc 0.8125, learning_rate 0.000100416
2017-10-10T13:07:47.520893: step 2294, loss 0.534372, acc 0.859375, learning_rate 0.000100414
2017-10-10T13:07:48.048981: step 2295, loss 0.287757, acc 0.921875, learning_rate 0.000100412
2017-10-10T13:07:48.571459: step 2296, loss 0.301176, acc 0.875, learning_rate 0.000100411
2017-10-10T13:07:49.112853: step 2297, loss 0.238079, acc 0.953125, learning_rate 0.000100409
2017-10-10T13:07:49.624965: step 2298, loss 0.571318, acc 0.84375, learning_rate 0.000100407
2017-10-10T13:07:50.094926: step 2299, loss 0.540457, acc 0.84375, learning_rate 0.000100406
2017-10-10T13:07:50.628982: step 2300, loss 0.506045, acc 0.84375, learning_rate 0.000100404
2017-10-10T13:07:51.139246: step 2301, loss 0.319567, acc 0.890625, learning_rate 0.000100402
2017-10-10T13:07:51.732792: step 2302, loss 0.54196, acc 0.84375, learning_rate 0.000100401
2017-10-10T13:07:52.220855: step 2303, loss 0.413582, acc 0.890625, learning_rate 0.000100399
2017-10-10T13:07:52.652407: step 2304, loss 0.199508, acc 0.921875, learning_rate 0.000100398
2017-10-10T13:07:53.196499: step 2305, loss 0.473425, acc 0.84375, learning_rate 0.000100396
2017-10-10T13:07:53.738634: step 2306, loss 0.454211, acc 0.828125, learning_rate 0.000100394
2017-10-10T13:07:54.384831: step 2307, loss 0.460717, acc 0.796875, learning_rate 0.000100393
2017-10-10T13:07:54.844874: step 2308, loss 0.346449, acc 0.859375, learning_rate 0.000100391
2017-10-10T13:07:55.304357: step 2309, loss 0.613942, acc 0.6875, learning_rate 0.000100389
2017-10-10T13:07:55.765763: step 2310, loss 0.349209, acc 0.84375, learning_rate 0.000100388
2017-10-10T13:07:56.312921: step 2311, loss 0.24559, acc 0.90625, learning_rate 0.000100386
2017-10-10T13:07:56.902052: step 2312, loss 0.213561, acc 0.953125, learning_rate 0.000100385
2017-10-10T13:07:57.677684: step 2313, loss 0.334321, acc 0.890625, learning_rate 0.000100383
2017-10-10T13:07:58.153605: step 2314, loss 0.330174, acc 0.859375, learning_rate 0.000100382
2017-10-10T13:07:58.638166: step 2315, loss 0.366066, acc 0.859375, learning_rate 0.00010038
2017-10-10T13:07:59.237381: step 2316, loss 0.74736, acc 0.828125, learning_rate 0.000100378
2017-10-10T13:07:59.717249: step 2317, loss 0.297222, acc 0.90625, learning_rate 0.000100377
2017-10-10T13:08:00.178398: step 2318, loss 0.554979, acc 0.8125, learning_rate 0.000100375
2017-10-10T13:08:00.699831: step 2319, loss 0.281598, acc 0.9375, learning_rate 0.000100374
2017-10-10T13:08:01.170784: step 2320, loss 0.384108, acc 0.875, learning_rate 0.000100372

Evaluation:
2017-10-10T13:08:02.329977: step 2320, loss 0.281122, acc 0.890647

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2320

2017-10-10T13:08:03.777185: step 2321, loss 0.306947, acc 0.921875, learning_rate 0.000100371
2017-10-10T13:08:04.368687: step 2322, loss 0.318892, acc 0.890625, learning_rate 0.000100369
2017-10-10T13:08:04.916716: step 2323, loss 0.223895, acc 0.9375, learning_rate 0.000100368
2017-10-10T13:08:05.427058: step 2324, loss 0.310708, acc 0.890625, learning_rate 0.000100366
2017-10-10T13:08:05.984814: step 2325, loss 0.631801, acc 0.765625, learning_rate 0.000100365
2017-10-10T13:08:06.572157: step 2326, loss 0.210382, acc 0.953125, learning_rate 0.000100363
2017-10-10T13:08:07.080091: step 2327, loss 0.314499, acc 0.890625, learning_rate 0.000100362
2017-10-10T13:08:07.600593: step 2328, loss 0.339898, acc 0.890625, learning_rate 0.00010036
2017-10-10T13:08:08.155908: step 2329, loss 0.547557, acc 0.796875, learning_rate 0.000100359
2017-10-10T13:08:08.669364: step 2330, loss 0.421451, acc 0.8125, learning_rate 0.000100357
2017-10-10T13:08:09.239541: step 2331, loss 0.34153, acc 0.9375, learning_rate 0.000100356
2017-10-10T13:08:09.764854: step 2332, loss 0.213404, acc 0.921875, learning_rate 0.000100354
2017-10-10T13:08:10.346658: step 2333, loss 0.347316, acc 0.875, learning_rate 0.000100353
2017-10-10T13:08:10.887609: step 2334, loss 0.633065, acc 0.734375, learning_rate 0.000100352
2017-10-10T13:08:11.444716: step 2335, loss 0.400838, acc 0.875, learning_rate 0.00010035
2017-10-10T13:08:12.007533: step 2336, loss 0.453663, acc 0.875, learning_rate 0.000100349
2017-10-10T13:08:12.553682: step 2337, loss 0.420137, acc 0.84375, learning_rate 0.000100347
2017-10-10T13:08:13.141841: step 2338, loss 0.463211, acc 0.796875, learning_rate 0.000100346
2017-10-10T13:08:13.768589: step 2339, loss 0.47722, acc 0.84375, learning_rate 0.000100344
2017-10-10T13:08:14.373664: step 2340, loss 0.273238, acc 0.90625, learning_rate 0.000100343
2017-10-10T13:08:14.920068: step 2341, loss 0.256148, acc 0.90625, learning_rate 0.000100342
2017-10-10T13:08:15.294040: step 2342, loss 0.249115, acc 0.890625, learning_rate 0.00010034
2017-10-10T13:08:15.719316: step 2343, loss 0.320143, acc 0.921875, learning_rate 0.000100339
2017-10-10T13:08:16.178178: step 2344, loss 0.31887, acc 0.875, learning_rate 0.000100338
2017-10-10T13:08:16.772899: step 2345, loss 0.677838, acc 0.78125, learning_rate 0.000100336
2017-10-10T13:08:17.343270: step 2346, loss 0.487698, acc 0.828125, learning_rate 0.000100335
2017-10-10T13:08:17.781946: step 2347, loss 0.279798, acc 0.90625, learning_rate 0.000100333
2017-10-10T13:08:18.228912: step 2348, loss 0.415345, acc 0.875, learning_rate 0.000100332
2017-10-10T13:08:18.801692: step 2349, loss 0.480473, acc 0.828125, learning_rate 0.000100331
2017-10-10T13:08:19.360826: step 2350, loss 0.483283, acc 0.859375, learning_rate 0.000100329
2017-10-10T13:08:19.948849: step 2351, loss 0.422377, acc 0.890625, learning_rate 0.000100328
2017-10-10T13:08:20.490294: step 2352, loss 0.496342, acc 0.843137, learning_rate 0.000100327
2017-10-10T13:08:20.956090: step 2353, loss 0.256514, acc 0.921875, learning_rate 0.000100325
2017-10-10T13:08:21.405510: step 2354, loss 0.337294, acc 0.875, learning_rate 0.000100324
2017-10-10T13:08:21.982731: step 2355, loss 0.306597, acc 0.890625, learning_rate 0.000100323
2017-10-10T13:08:22.549810: step 2356, loss 0.387698, acc 0.796875, learning_rate 0.000100321
2017-10-10T13:08:23.128248: step 2357, loss 0.446725, acc 0.859375, learning_rate 0.00010032
2017-10-10T13:08:23.738054: step 2358, loss 0.356031, acc 0.921875, learning_rate 0.000100319
2017-10-10T13:08:24.290582: step 2359, loss 0.348161, acc 0.859375, learning_rate 0.000100317
2017-10-10T13:08:24.819773: step 2360, loss 0.35609, acc 0.890625, learning_rate 0.000100316

Evaluation:
2017-10-10T13:08:26.000420: step 2360, loss 0.279757, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2360

2017-10-10T13:08:27.581076: step 2361, loss 0.29714, acc 0.890625, learning_rate 0.000100315
2017-10-10T13:08:28.173650: step 2362, loss 0.35364, acc 0.890625, learning_rate 0.000100314
2017-10-10T13:08:28.579492: step 2363, loss 0.496125, acc 0.859375, learning_rate 0.000100312
2017-10-10T13:08:29.054832: step 2364, loss 0.67425, acc 0.84375, learning_rate 0.000100311
2017-10-10T13:08:29.509016: step 2365, loss 0.313833, acc 0.890625, learning_rate 0.00010031
2017-10-10T13:08:30.081104: step 2366, loss 0.383433, acc 0.921875, learning_rate 0.000100308
2017-10-10T13:08:30.561621: step 2367, loss 0.276533, acc 0.921875, learning_rate 0.000100307
2017-10-10T13:08:31.037170: step 2368, loss 0.301672, acc 0.890625, learning_rate 0.000100306
2017-10-10T13:08:31.490848: step 2369, loss 0.270886, acc 0.921875, learning_rate 0.000100305
2017-10-10T13:08:32.036938: step 2370, loss 0.30875, acc 0.90625, learning_rate 0.000100303
2017-10-10T13:08:32.577498: step 2371, loss 0.483388, acc 0.859375, learning_rate 0.000100302
2017-10-10T13:08:33.099424: step 2372, loss 0.231916, acc 0.921875, learning_rate 0.000100301
2017-10-10T13:08:33.994366: step 2373, loss 0.389991, acc 0.875, learning_rate 0.0001003
2017-10-10T13:08:34.577021: step 2374, loss 0.266003, acc 0.921875, learning_rate 0.000100299
2017-10-10T13:08:35.117396: step 2375, loss 0.56842, acc 0.84375, learning_rate 0.000100297
2017-10-10T13:08:35.665015: step 2376, loss 0.276143, acc 0.921875, learning_rate 0.000100296
2017-10-10T13:08:36.233099: step 2377, loss 0.455813, acc 0.84375, learning_rate 0.000100295
2017-10-10T13:08:36.791635: step 2378, loss 0.49154, acc 0.875, learning_rate 0.000100294
2017-10-10T13:08:37.344841: step 2379, loss 0.565583, acc 0.859375, learning_rate 0.000100292
2017-10-10T13:08:38.040208: step 2380, loss 0.398293, acc 0.859375, learning_rate 0.000100291
2017-10-10T13:08:38.496394: step 2381, loss 0.335141, acc 0.90625, learning_rate 0.00010029
2017-10-10T13:08:38.949588: step 2382, loss 0.331925, acc 0.875, learning_rate 0.000100289
2017-10-10T13:08:39.492026: step 2383, loss 0.403549, acc 0.796875, learning_rate 0.000100288
2017-10-10T13:08:40.030032: step 2384, loss 0.339588, acc 0.875, learning_rate 0.000100287
2017-10-10T13:08:40.640579: step 2385, loss 0.277601, acc 0.921875, learning_rate 0.000100285
2017-10-10T13:08:41.249661: step 2386, loss 0.312841, acc 0.921875, learning_rate 0.000100284
2017-10-10T13:08:41.637836: step 2387, loss 0.434803, acc 0.875, learning_rate 0.000100283
2017-10-10T13:08:42.061197: step 2388, loss 0.320083, acc 0.875, learning_rate 0.000100282
2017-10-10T13:08:42.596891: step 2389, loss 0.353108, acc 0.84375, learning_rate 0.000100281
2017-10-10T13:08:43.225982: step 2390, loss 0.436738, acc 0.8125, learning_rate 0.00010028
2017-10-10T13:08:43.828962: step 2391, loss 0.247377, acc 0.90625, learning_rate 0.000100278
2017-10-10T13:08:44.303748: step 2392, loss 0.351637, acc 0.90625, learning_rate 0.000100277
2017-10-10T13:08:44.746749: step 2393, loss 0.461282, acc 0.859375, learning_rate 0.000100276
2017-10-10T13:08:45.284961: step 2394, loss 0.284791, acc 0.90625, learning_rate 0.000100275
2017-10-10T13:08:45.808820: step 2395, loss 0.401908, acc 0.859375, learning_rate 0.000100274
2017-10-10T13:08:46.338188: step 2396, loss 0.290759, acc 0.9375, learning_rate 0.000100273
2017-10-10T13:08:46.888859: step 2397, loss 0.429745, acc 0.859375, learning_rate 0.000100272
2017-10-10T13:08:47.394736: step 2398, loss 0.304644, acc 0.859375, learning_rate 0.000100271
2017-10-10T13:08:47.937493: step 2399, loss 0.500988, acc 0.875, learning_rate 0.00010027
2017-10-10T13:08:48.469164: step 2400, loss 0.377597, acc 0.875, learning_rate 0.000100268

Evaluation:
2017-10-10T13:08:49.637984: step 2400, loss 0.279731, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2400

2017-10-10T13:08:51.271727: step 2401, loss 0.242859, acc 0.921875, learning_rate 0.000100267
2017-10-10T13:08:51.777083: step 2402, loss 0.342052, acc 0.84375, learning_rate 0.000100266
2017-10-10T13:08:52.242764: step 2403, loss 0.238492, acc 0.90625, learning_rate 0.000100265
2017-10-10T13:08:52.785310: step 2404, loss 0.448351, acc 0.90625, learning_rate 0.000100264
2017-10-10T13:08:53.328953: step 2405, loss 0.313904, acc 0.90625, learning_rate 0.000100263
2017-10-10T13:08:53.893170: step 2406, loss 0.501652, acc 0.8125, learning_rate 0.000100262
2017-10-10T13:08:54.444919: step 2407, loss 0.292334, acc 0.90625, learning_rate 0.000100261
2017-10-10T13:08:54.952922: step 2408, loss 0.45282, acc 0.8125, learning_rate 0.00010026
2017-10-10T13:08:55.477224: step 2409, loss 0.291071, acc 0.921875, learning_rate 0.000100259
2017-10-10T13:08:56.081080: step 2410, loss 0.490045, acc 0.84375, learning_rate 0.000100258
2017-10-10T13:08:56.617147: step 2411, loss 0.464924, acc 0.8125, learning_rate 0.000100257
2017-10-10T13:08:57.101388: step 2412, loss 0.498411, acc 0.78125, learning_rate 0.000100256
2017-10-10T13:08:57.629014: step 2413, loss 0.501799, acc 0.84375, learning_rate 0.000100255
2017-10-10T13:08:58.181155: step 2414, loss 0.27164, acc 0.875, learning_rate 0.000100253
2017-10-10T13:08:58.756940: step 2415, loss 0.223814, acc 0.921875, learning_rate 0.000100252
2017-10-10T13:08:59.306555: step 2416, loss 0.53092, acc 0.78125, learning_rate 0.000100251
2017-10-10T13:08:59.972874: step 2417, loss 0.344289, acc 0.875, learning_rate 0.00010025
2017-10-10T13:09:00.614165: step 2418, loss 0.179917, acc 0.9375, learning_rate 0.000100249
2017-10-10T13:09:01.053200: step 2419, loss 0.364511, acc 0.859375, learning_rate 0.000100248
2017-10-10T13:09:01.483962: step 2420, loss 0.376497, acc 0.859375, learning_rate 0.000100247
2017-10-10T13:09:01.981123: step 2421, loss 0.319603, acc 0.875, learning_rate 0.000100246
2017-10-10T13:09:02.628875: step 2422, loss 0.363283, acc 0.859375, learning_rate 0.000100245
2017-10-10T13:09:03.148718: step 2423, loss 0.381432, acc 0.90625, learning_rate 0.000100244
2017-10-10T13:09:03.684839: step 2424, loss 0.32264, acc 0.921875, learning_rate 0.000100243
2017-10-10T13:09:04.323850: step 2425, loss 0.485038, acc 0.8125, learning_rate 0.000100242
2017-10-10T13:09:04.764886: step 2426, loss 0.242537, acc 0.9375, learning_rate 0.000100241
2017-10-10T13:09:05.188837: step 2427, loss 0.300695, acc 0.90625, learning_rate 0.00010024
2017-10-10T13:09:05.797282: step 2428, loss 0.444331, acc 0.859375, learning_rate 0.000100239
2017-10-10T13:09:06.244891: step 2429, loss 0.509715, acc 0.84375, learning_rate 0.000100238
2017-10-10T13:09:06.688534: step 2430, loss 0.387525, acc 0.90625, learning_rate 0.000100237
2017-10-10T13:09:07.079616: step 2431, loss 0.333983, acc 0.90625, learning_rate 0.000100236
2017-10-10T13:09:07.643321: step 2432, loss 0.290868, acc 0.90625, learning_rate 0.000100235
2017-10-10T13:09:08.252074: step 2433, loss 0.281738, acc 0.90625, learning_rate 0.000100235
2017-10-10T13:09:08.757111: step 2434, loss 0.431392, acc 0.828125, learning_rate 0.000100234
2017-10-10T13:09:09.230702: step 2435, loss 0.306917, acc 0.921875, learning_rate 0.000100233
2017-10-10T13:09:09.733033: step 2436, loss 0.317666, acc 0.890625, learning_rate 0.000100232
2017-10-10T13:09:10.237132: step 2437, loss 0.292982, acc 0.90625, learning_rate 0.000100231
2017-10-10T13:09:10.824985: step 2438, loss 0.505281, acc 0.875, learning_rate 0.00010023
2017-10-10T13:09:11.381397: step 2439, loss 0.33542, acc 0.890625, learning_rate 0.000100229
2017-10-10T13:09:11.923823: step 2440, loss 0.483841, acc 0.84375, learning_rate 0.000100228

Evaluation:
2017-10-10T13:09:13.068969: step 2440, loss 0.279363, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2440

2017-10-10T13:09:14.838496: step 2441, loss 0.392729, acc 0.828125, learning_rate 0.000100227
2017-10-10T13:09:15.366621: step 2442, loss 0.649325, acc 0.84375, learning_rate 0.000100226
2017-10-10T13:09:15.881115: step 2443, loss 0.332139, acc 0.859375, learning_rate 0.000100225
2017-10-10T13:09:16.379030: step 2444, loss 0.561612, acc 0.84375, learning_rate 0.000100224
2017-10-10T13:09:16.905165: step 2445, loss 0.481277, acc 0.890625, learning_rate 0.000100223
2017-10-10T13:09:17.460004: step 2446, loss 0.329206, acc 0.875, learning_rate 0.000100222
2017-10-10T13:09:17.879448: step 2447, loss 0.274468, acc 0.921875, learning_rate 0.000100221
2017-10-10T13:09:18.480910: step 2448, loss 0.30389, acc 0.875, learning_rate 0.000100221
2017-10-10T13:09:18.980462: step 2449, loss 0.496077, acc 0.828125, learning_rate 0.00010022
2017-10-10T13:09:19.435569: step 2450, loss 0.721746, acc 0.705882, learning_rate 0.000100219
2017-10-10T13:09:19.954262: step 2451, loss 0.514459, acc 0.84375, learning_rate 0.000100218
2017-10-10T13:09:20.353000: step 2452, loss 0.332421, acc 0.921875, learning_rate 0.000100217
2017-10-10T13:09:20.856939: step 2453, loss 0.329024, acc 0.859375, learning_rate 0.000100216
2017-10-10T13:09:21.365035: step 2454, loss 0.341287, acc 0.921875, learning_rate 0.000100215
2017-10-10T13:09:21.818853: step 2455, loss 0.317158, acc 0.921875, learning_rate 0.000100214
2017-10-10T13:09:22.351702: step 2456, loss 0.285915, acc 0.859375, learning_rate 0.000100213
2017-10-10T13:09:22.956888: step 2457, loss 0.4733, acc 0.828125, learning_rate 0.000100213
2017-10-10T13:09:23.519153: step 2458, loss 0.384447, acc 0.84375, learning_rate 0.000100212
2017-10-10T13:09:23.864989: step 2459, loss 0.384516, acc 0.859375, learning_rate 0.000100211
2017-10-10T13:09:24.333856: step 2460, loss 0.301986, acc 0.890625, learning_rate 0.00010021
2017-10-10T13:09:24.888996: step 2461, loss 0.393725, acc 0.90625, learning_rate 0.000100209
2017-10-10T13:09:25.368986: step 2462, loss 0.45078, acc 0.8125, learning_rate 0.000100208
2017-10-10T13:09:25.873000: step 2463, loss 0.419859, acc 0.875, learning_rate 0.000100207
2017-10-10T13:09:26.366610: step 2464, loss 0.239725, acc 0.953125, learning_rate 0.000100207
2017-10-10T13:09:26.917209: step 2465, loss 0.434666, acc 0.90625, learning_rate 0.000100206
2017-10-10T13:09:27.448823: step 2466, loss 0.381746, acc 0.875, learning_rate 0.000100205
2017-10-10T13:09:27.860358: step 2467, loss 0.436155, acc 0.828125, learning_rate 0.000100204
2017-10-10T13:09:28.464871: step 2468, loss 0.424278, acc 0.890625, learning_rate 0.000100203
2017-10-10T13:09:29.052600: step 2469, loss 0.279708, acc 0.875, learning_rate 0.000100202
2017-10-10T13:09:29.528939: step 2470, loss 0.592649, acc 0.765625, learning_rate 0.000100202
2017-10-10T13:09:29.988565: step 2471, loss 0.395972, acc 0.84375, learning_rate 0.000100201
2017-10-10T13:09:30.554768: step 2472, loss 0.458057, acc 0.90625, learning_rate 0.0001002
2017-10-10T13:09:31.085431: step 2473, loss 0.476226, acc 0.890625, learning_rate 0.000100199
2017-10-10T13:09:31.613167: step 2474, loss 0.370929, acc 0.90625, learning_rate 0.000100198
2017-10-10T13:09:32.156917: step 2475, loss 0.53385, acc 0.859375, learning_rate 0.000100198
2017-10-10T13:09:32.680950: step 2476, loss 0.270595, acc 0.90625, learning_rate 0.000100197
2017-10-10T13:09:33.180733: step 2477, loss 0.379817, acc 0.875, learning_rate 0.000100196
2017-10-10T13:09:33.649130: step 2478, loss 0.454312, acc 0.84375, learning_rate 0.000100195
2017-10-10T13:09:34.229073: step 2479, loss 0.285463, acc 0.9375, learning_rate 0.000100194
2017-10-10T13:09:34.830733: step 2480, loss 0.321776, acc 0.859375, learning_rate 0.000100194

Evaluation:
2017-10-10T13:09:35.956983: step 2480, loss 0.277958, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2480

2017-10-10T13:09:37.588939: step 2481, loss 0.391059, acc 0.859375, learning_rate 0.000100193
2017-10-10T13:09:38.125002: step 2482, loss 0.388855, acc 0.84375, learning_rate 0.000100192
2017-10-10T13:09:38.750970: step 2483, loss 0.522987, acc 0.796875, learning_rate 0.000100191
2017-10-10T13:09:39.234127: step 2484, loss 0.30782, acc 0.859375, learning_rate 0.00010019
2017-10-10T13:09:39.730173: step 2485, loss 0.372328, acc 0.921875, learning_rate 0.00010019
2017-10-10T13:09:40.268100: step 2486, loss 0.393718, acc 0.8125, learning_rate 0.000100189
2017-10-10T13:09:40.816936: step 2487, loss 0.452271, acc 0.8125, learning_rate 0.000100188
2017-10-10T13:09:41.449011: step 2488, loss 0.321384, acc 0.90625, learning_rate 0.000100187
2017-10-10T13:09:41.982881: step 2489, loss 0.377896, acc 0.890625, learning_rate 0.000100187
2017-10-10T13:09:42.554435: step 2490, loss 0.447718, acc 0.875, learning_rate 0.000100186
2017-10-10T13:09:43.120729: step 2491, loss 0.214279, acc 0.90625, learning_rate 0.000100185
2017-10-10T13:09:43.620414: step 2492, loss 0.210159, acc 0.953125, learning_rate 0.000100184
2017-10-10T13:09:44.172912: step 2493, loss 0.254443, acc 0.9375, learning_rate 0.000100183
2017-10-10T13:09:44.709969: step 2494, loss 0.4844, acc 0.828125, learning_rate 0.000100183
2017-10-10T13:09:45.262708: step 2495, loss 0.432018, acc 0.828125, learning_rate 0.000100182
2017-10-10T13:09:45.819169: step 2496, loss 0.394138, acc 0.875, learning_rate 0.000100181
2017-10-10T13:09:46.308822: step 2497, loss 0.481964, acc 0.828125, learning_rate 0.000100181
2017-10-10T13:09:46.761577: step 2498, loss 0.480754, acc 0.84375, learning_rate 0.00010018
2017-10-10T13:09:47.204906: step 2499, loss 0.288492, acc 0.921875, learning_rate 0.000100179
2017-10-10T13:09:47.711086: step 2500, loss 0.365533, acc 0.875, learning_rate 0.000100178
2017-10-10T13:09:48.224998: step 2501, loss 0.397787, acc 0.828125, learning_rate 0.000100178
2017-10-10T13:09:48.737568: step 2502, loss 0.29907, acc 0.875, learning_rate 0.000100177
2017-10-10T13:09:49.248976: step 2503, loss 0.414104, acc 0.859375, learning_rate 0.000100176
2017-10-10T13:09:49.900375: step 2504, loss 0.386773, acc 0.828125, learning_rate 0.000100175
2017-10-10T13:09:50.324047: step 2505, loss 0.317879, acc 0.90625, learning_rate 0.000100175
2017-10-10T13:09:50.703759: step 2506, loss 0.257208, acc 0.921875, learning_rate 0.000100174
2017-10-10T13:09:51.081031: step 2507, loss 0.498771, acc 0.828125, learning_rate 0.000100173
2017-10-10T13:09:51.756119: step 2508, loss 0.334799, acc 0.828125, learning_rate 0.000100173
2017-10-10T13:09:52.320948: step 2509, loss 0.344799, acc 0.890625, learning_rate 0.000100172
2017-10-10T13:09:52.725043: step 2510, loss 0.35574, acc 0.84375, learning_rate 0.000100171
2017-10-10T13:09:53.115037: step 2511, loss 0.303037, acc 0.921875, learning_rate 0.00010017
2017-10-10T13:09:53.667208: step 2512, loss 0.340823, acc 0.828125, learning_rate 0.00010017
2017-10-10T13:09:54.206972: step 2513, loss 0.256653, acc 0.9375, learning_rate 0.000100169
2017-10-10T13:09:54.757658: step 2514, loss 0.533684, acc 0.796875, learning_rate 0.000100168
2017-10-10T13:09:55.329024: step 2515, loss 0.355885, acc 0.859375, learning_rate 0.000100168
2017-10-10T13:09:55.901982: step 2516, loss 0.515351, acc 0.84375, learning_rate 0.000100167
2017-10-10T13:09:56.431383: step 2517, loss 0.591877, acc 0.828125, learning_rate 0.000100166
2017-10-10T13:09:56.993202: step 2518, loss 0.304409, acc 0.875, learning_rate 0.000100166
2017-10-10T13:09:57.551512: step 2519, loss 0.288615, acc 0.875, learning_rate 0.000100165
2017-10-10T13:09:58.099787: step 2520, loss 0.247015, acc 0.921875, learning_rate 0.000100164

Evaluation:
2017-10-10T13:09:59.286148: step 2520, loss 0.279248, acc 0.892086

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2520

2017-10-10T13:10:00.865127: step 2521, loss 0.457017, acc 0.859375, learning_rate 0.000100164
2017-10-10T13:10:01.311825: step 2522, loss 0.547316, acc 0.796875, learning_rate 0.000100163
2017-10-10T13:10:01.853571: step 2523, loss 0.252631, acc 0.90625, learning_rate 0.000100162
2017-10-10T13:10:02.428787: step 2524, loss 0.26351, acc 0.9375, learning_rate 0.000100162
2017-10-10T13:10:02.999184: step 2525, loss 0.305915, acc 0.90625, learning_rate 0.000100161
2017-10-10T13:10:03.521014: step 2526, loss 0.288524, acc 0.90625, learning_rate 0.00010016
2017-10-10T13:10:04.038141: step 2527, loss 0.502923, acc 0.859375, learning_rate 0.00010016
2017-10-10T13:10:04.577318: step 2528, loss 0.32647, acc 0.875, learning_rate 0.000100159
2017-10-10T13:10:05.109124: step 2529, loss 0.3507, acc 0.875, learning_rate 0.000100158
2017-10-10T13:10:05.645856: step 2530, loss 0.460912, acc 0.78125, learning_rate 0.000100158
2017-10-10T13:10:06.316953: step 2531, loss 0.362674, acc 0.875, learning_rate 0.000100157
2017-10-10T13:10:06.885091: step 2532, loss 0.285264, acc 0.921875, learning_rate 0.000100156
2017-10-10T13:10:07.421100: step 2533, loss 0.353792, acc 0.875, learning_rate 0.000100156
2017-10-10T13:10:07.967113: step 2534, loss 0.307775, acc 0.890625, learning_rate 0.000100155
2017-10-10T13:10:08.508910: step 2535, loss 0.33131, acc 0.921875, learning_rate 0.000100155
2017-10-10T13:10:09.097438: step 2536, loss 0.263053, acc 0.90625, learning_rate 0.000100154
2017-10-10T13:10:09.640239: step 2537, loss 0.449705, acc 0.8125, learning_rate 0.000100153
2017-10-10T13:10:10.122603: step 2538, loss 0.354464, acc 0.859375, learning_rate 0.000100153
2017-10-10T13:10:10.570157: step 2539, loss 0.292522, acc 0.890625, learning_rate 0.000100152
2017-10-10T13:10:11.234015: step 2540, loss 0.395445, acc 0.90625, learning_rate 0.000100151
2017-10-10T13:10:11.698411: step 2541, loss 0.400714, acc 0.859375, learning_rate 0.000100151
2017-10-10T13:10:12.240597: step 2542, loss 0.252812, acc 0.921875, learning_rate 0.00010015
2017-10-10T13:10:12.771877: step 2543, loss 0.586056, acc 0.890625, learning_rate 0.00010015
2017-10-10T13:10:13.397024: step 2544, loss 0.330241, acc 0.890625, learning_rate 0.000100149
2017-10-10T13:10:13.876456: step 2545, loss 0.483214, acc 0.8125, learning_rate 0.000100148
2017-10-10T13:10:14.340637: step 2546, loss 0.451761, acc 0.8125, learning_rate 0.000100148
2017-10-10T13:10:14.820896: step 2547, loss 0.368812, acc 0.84375, learning_rate 0.000100147
2017-10-10T13:10:15.340976: step 2548, loss 0.224154, acc 0.901961, learning_rate 0.000100147
2017-10-10T13:10:15.873444: step 2549, loss 0.444475, acc 0.84375, learning_rate 0.000100146
2017-10-10T13:10:16.294446: step 2550, loss 0.425141, acc 0.890625, learning_rate 0.000100145
2017-10-10T13:10:16.773462: step 2551, loss 0.349741, acc 0.921875, learning_rate 0.000100145
2017-10-10T13:10:17.346292: step 2552, loss 0.391924, acc 0.875, learning_rate 0.000100144
2017-10-10T13:10:17.895686: step 2553, loss 0.309081, acc 0.90625, learning_rate 0.000100144
2017-10-10T13:10:18.442900: step 2554, loss 0.465761, acc 0.828125, learning_rate 0.000100143
2017-10-10T13:10:18.968223: step 2555, loss 0.319277, acc 0.875, learning_rate 0.000100142
2017-10-10T13:10:19.503025: step 2556, loss 0.286427, acc 0.875, learning_rate 0.000100142
2017-10-10T13:10:20.013936: step 2557, loss 0.444054, acc 0.828125, learning_rate 0.000100141
2017-10-10T13:10:20.592167: step 2558, loss 0.341371, acc 0.890625, learning_rate 0.000100141
2017-10-10T13:10:21.145675: step 2559, loss 0.406333, acc 0.921875, learning_rate 0.00010014
2017-10-10T13:10:21.728879: step 2560, loss 0.41837, acc 0.84375, learning_rate 0.00010014

Evaluation:
2017-10-10T13:10:22.936023: step 2560, loss 0.279223, acc 0.893525

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2560

2017-10-10T13:10:24.725073: step 2561, loss 0.617231, acc 0.78125, learning_rate 0.000100139
2017-10-10T13:10:25.160875: step 2562, loss 0.491627, acc 0.859375, learning_rate 0.000100138
2017-10-10T13:10:25.632855: step 2563, loss 0.372302, acc 0.875, learning_rate 0.000100138
2017-10-10T13:10:26.150103: step 2564, loss 0.340207, acc 0.890625, learning_rate 0.000100137
2017-10-10T13:10:26.702029: step 2565, loss 0.310365, acc 0.890625, learning_rate 0.000100137
2017-10-10T13:10:27.255359: step 2566, loss 0.612474, acc 0.8125, learning_rate 0.000100136
2017-10-10T13:10:27.769462: step 2567, loss 0.293258, acc 0.875, learning_rate 0.000100136
2017-10-10T13:10:28.310367: step 2568, loss 0.457994, acc 0.84375, learning_rate 0.000100135
2017-10-10T13:10:28.873181: step 2569, loss 0.578361, acc 0.8125, learning_rate 0.000100134
2017-10-10T13:10:29.406413: step 2570, loss 0.288443, acc 0.90625, learning_rate 0.000100134
2017-10-10T13:10:29.964655: step 2571, loss 0.247655, acc 0.90625, learning_rate 0.000100133
2017-10-10T13:10:30.484951: step 2572, loss 0.513986, acc 0.78125, learning_rate 0.000100133
2017-10-10T13:10:31.028867: step 2573, loss 0.503614, acc 0.796875, learning_rate 0.000100132
2017-10-10T13:10:31.564897: step 2574, loss 0.515292, acc 0.78125, learning_rate 0.000100132
2017-10-10T13:10:32.029059: step 2575, loss 0.251788, acc 0.953125, learning_rate 0.000100131
2017-10-10T13:10:32.639512: step 2576, loss 0.350684, acc 0.859375, learning_rate 0.000100131
2017-10-10T13:10:33.172866: step 2577, loss 0.512772, acc 0.859375, learning_rate 0.00010013
2017-10-10T13:10:33.618360: step 2578, loss 0.467228, acc 0.859375, learning_rate 0.00010013
2017-10-10T13:10:34.020937: step 2579, loss 0.400453, acc 0.90625, learning_rate 0.000100129
2017-10-10T13:10:34.592860: step 2580, loss 0.425965, acc 0.84375, learning_rate 0.000100129
2017-10-10T13:10:35.153076: step 2581, loss 0.414903, acc 0.859375, learning_rate 0.000100128
2017-10-10T13:10:35.746800: step 2582, loss 0.276667, acc 0.90625, learning_rate 0.000100128
2017-10-10T13:10:36.360854: step 2583, loss 0.581854, acc 0.84375, learning_rate 0.000100127
2017-10-10T13:10:36.966132: step 2584, loss 0.406385, acc 0.859375, learning_rate 0.000100126
2017-10-10T13:10:37.422963: step 2585, loss 0.339, acc 0.890625, learning_rate 0.000100126
2017-10-10T13:10:38.060949: step 2586, loss 0.413891, acc 0.8125, learning_rate 0.000100125
2017-10-10T13:10:38.665149: step 2587, loss 0.365298, acc 0.9375, learning_rate 0.000100125
2017-10-10T13:10:39.096893: step 2588, loss 0.478315, acc 0.796875, learning_rate 0.000100124
2017-10-10T13:10:39.530262: step 2589, loss 0.376234, acc 0.875, learning_rate 0.000100124
2017-10-10T13:10:40.024960: step 2590, loss 0.543822, acc 0.84375, learning_rate 0.000100123
2017-10-10T13:10:40.571696: step 2591, loss 0.400961, acc 0.890625, learning_rate 0.000100123
2017-10-10T13:10:41.049213: step 2592, loss 0.388848, acc 0.84375, learning_rate 0.000100122
2017-10-10T13:10:41.576984: step 2593, loss 0.563528, acc 0.796875, learning_rate 0.000100122
2017-10-10T13:10:42.110486: step 2594, loss 0.233739, acc 0.921875, learning_rate 0.000100121
2017-10-10T13:10:42.660923: step 2595, loss 0.329528, acc 0.859375, learning_rate 0.000100121
2017-10-10T13:10:43.219590: step 2596, loss 0.38286, acc 0.890625, learning_rate 0.00010012
2017-10-10T13:10:43.778773: step 2597, loss 0.249746, acc 0.953125, learning_rate 0.00010012
2017-10-10T13:10:44.352398: step 2598, loss 0.319745, acc 0.921875, learning_rate 0.000100119
2017-10-10T13:10:44.848867: step 2599, loss 0.330936, acc 0.890625, learning_rate 0.000100119
2017-10-10T13:10:45.408931: step 2600, loss 0.260121, acc 0.90625, learning_rate 0.000100118

Evaluation:
2017-10-10T13:10:46.634200: step 2600, loss 0.280226, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2600

2017-10-10T13:10:48.133876: step 2601, loss 0.400856, acc 0.875, learning_rate 0.000100118
2017-10-10T13:10:48.688204: step 2602, loss 0.389542, acc 0.84375, learning_rate 0.000100117
2017-10-10T13:10:49.252937: step 2603, loss 0.285415, acc 0.890625, learning_rate 0.000100117
2017-10-10T13:10:49.840559: step 2604, loss 0.378339, acc 0.828125, learning_rate 0.000100117
2017-10-10T13:10:50.377238: step 2605, loss 0.275762, acc 0.90625, learning_rate 0.000100116
2017-10-10T13:10:50.945203: step 2606, loss 0.292759, acc 0.890625, learning_rate 0.000100116
2017-10-10T13:10:51.529195: step 2607, loss 0.318067, acc 0.875, learning_rate 0.000100115
2017-10-10T13:10:52.024090: step 2608, loss 0.376855, acc 0.859375, learning_rate 0.000100115
2017-10-10T13:10:52.510774: step 2609, loss 0.406498, acc 0.859375, learning_rate 0.000100114
2017-10-10T13:10:53.041026: step 2610, loss 0.281191, acc 0.90625, learning_rate 0.000100114
2017-10-10T13:10:53.578562: step 2611, loss 0.477154, acc 0.796875, learning_rate 0.000100113
2017-10-10T13:10:54.087891: step 2612, loss 0.251879, acc 0.90625, learning_rate 0.000100113
2017-10-10T13:10:54.574840: step 2613, loss 0.428773, acc 0.859375, learning_rate 0.000100112
2017-10-10T13:10:55.195581: step 2614, loss 0.383492, acc 0.84375, learning_rate 0.000100112
2017-10-10T13:10:55.759313: step 2615, loss 0.443925, acc 0.828125, learning_rate 0.000100111
2017-10-10T13:10:56.240948: step 2616, loss 0.370594, acc 0.859375, learning_rate 0.000100111
2017-10-10T13:10:56.651505: step 2617, loss 0.324699, acc 0.859375, learning_rate 0.000100111
2017-10-10T13:10:57.240858: step 2618, loss 0.448575, acc 0.859375, learning_rate 0.00010011
2017-10-10T13:10:57.769033: step 2619, loss 0.357368, acc 0.859375, learning_rate 0.00010011
2017-10-10T13:10:58.207120: step 2620, loss 0.392852, acc 0.890625, learning_rate 0.000100109
2017-10-10T13:10:58.828888: step 2621, loss 0.348865, acc 0.859375, learning_rate 0.000100109
2017-10-10T13:10:59.305099: step 2622, loss 0.241575, acc 0.9375, learning_rate 0.000100108
2017-10-10T13:10:59.820920: step 2623, loss 0.428955, acc 0.859375, learning_rate 0.000100108
2017-10-10T13:11:00.287936: step 2624, loss 0.464276, acc 0.875, learning_rate 0.000100107
2017-10-10T13:11:00.828688: step 2625, loss 0.274011, acc 0.921875, learning_rate 0.000100107
2017-10-10T13:11:01.382048: step 2626, loss 0.340785, acc 0.859375, learning_rate 0.000100107
2017-10-10T13:11:01.824022: step 2627, loss 0.348426, acc 0.921875, learning_rate 0.000100106
2017-10-10T13:11:02.250718: step 2628, loss 0.317486, acc 0.90625, learning_rate 0.000100106
2017-10-10T13:11:02.812989: step 2629, loss 0.317793, acc 0.84375, learning_rate 0.000100105
2017-10-10T13:11:03.383301: step 2630, loss 0.53902, acc 0.828125, learning_rate 0.000100105
2017-10-10T13:11:03.915758: step 2631, loss 0.339585, acc 0.953125, learning_rate 0.000100104
2017-10-10T13:11:04.329051: step 2632, loss 0.355099, acc 0.84375, learning_rate 0.000100104
2017-10-10T13:11:04.778322: step 2633, loss 0.367711, acc 0.875, learning_rate 0.000100104
2017-10-10T13:11:05.241073: step 2634, loss 0.290943, acc 0.90625, learning_rate 0.000100103
2017-10-10T13:11:05.739796: step 2635, loss 0.539125, acc 0.8125, learning_rate 0.000100103
2017-10-10T13:11:06.281204: step 2636, loss 0.472798, acc 0.828125, learning_rate 0.000100102
2017-10-10T13:11:06.853009: step 2637, loss 0.371066, acc 0.90625, learning_rate 0.000100102
2017-10-10T13:11:07.427897: step 2638, loss 0.286598, acc 0.90625, learning_rate 0.000100101
2017-10-10T13:11:07.957207: step 2639, loss 0.311495, acc 0.890625, learning_rate 0.000100101
2017-10-10T13:11:08.517392: step 2640, loss 0.342297, acc 0.90625, learning_rate 0.000100101

Evaluation:
2017-10-10T13:11:09.750729: step 2640, loss 0.27764, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2640

2017-10-10T13:11:11.361082: step 2641, loss 0.325373, acc 0.890625, learning_rate 0.0001001
2017-10-10T13:11:11.901273: step 2642, loss 0.230505, acc 0.9375, learning_rate 0.0001001
2017-10-10T13:11:12.507068: step 2643, loss 0.328148, acc 0.9375, learning_rate 0.000100099
2017-10-10T13:11:13.028922: step 2644, loss 0.31453, acc 0.859375, learning_rate 0.000100099
2017-10-10T13:11:13.548821: step 2645, loss 0.329544, acc 0.875, learning_rate 0.000100099
2017-10-10T13:11:13.976811: step 2646, loss 0.354761, acc 0.882353, learning_rate 0.000100098
2017-10-10T13:11:14.558438: step 2647, loss 0.306086, acc 0.890625, learning_rate 0.000100098
2017-10-10T13:11:15.097739: step 2648, loss 0.218612, acc 0.9375, learning_rate 0.000100097
2017-10-10T13:11:15.623121: step 2649, loss 0.438007, acc 0.859375, learning_rate 0.000100097
2017-10-10T13:11:16.137011: step 2650, loss 0.353834, acc 0.90625, learning_rate 0.000100097
2017-10-10T13:11:16.672467: step 2651, loss 0.380484, acc 0.90625, learning_rate 0.000100096
2017-10-10T13:11:17.196896: step 2652, loss 0.372043, acc 0.859375, learning_rate 0.000100096
2017-10-10T13:11:17.729573: step 2653, loss 0.263389, acc 0.90625, learning_rate 0.000100095
2017-10-10T13:11:18.324975: step 2654, loss 0.271898, acc 0.921875, learning_rate 0.000100095
2017-10-10T13:11:18.882964: step 2655, loss 0.423734, acc 0.921875, learning_rate 0.000100095
2017-10-10T13:11:19.306686: step 2656, loss 0.284291, acc 0.921875, learning_rate 0.000100094
2017-10-10T13:11:19.765010: step 2657, loss 0.284361, acc 0.921875, learning_rate 0.000100094
2017-10-10T13:11:20.224856: step 2658, loss 0.400609, acc 0.84375, learning_rate 0.000100093
2017-10-10T13:11:20.744824: step 2659, loss 0.177838, acc 0.9375, learning_rate 0.000100093
2017-10-10T13:11:21.178461: step 2660, loss 0.338267, acc 0.875, learning_rate 0.000100093
2017-10-10T13:11:21.848638: step 2661, loss 0.489103, acc 0.8125, learning_rate 0.000100092
2017-10-10T13:11:22.298700: step 2662, loss 0.414072, acc 0.875, learning_rate 0.000100092
2017-10-10T13:11:22.753027: step 2663, loss 0.234818, acc 0.9375, learning_rate 0.000100092
2017-10-10T13:11:23.164560: step 2664, loss 0.587773, acc 0.78125, learning_rate 0.000100091
2017-10-10T13:11:23.779947: step 2665, loss 0.464787, acc 0.890625, learning_rate 0.000100091
2017-10-10T13:11:24.254401: step 2666, loss 0.372372, acc 0.90625, learning_rate 0.00010009
2017-10-10T13:11:24.682000: step 2667, loss 0.375757, acc 0.890625, learning_rate 0.00010009
2017-10-10T13:11:25.163834: step 2668, loss 0.608629, acc 0.75, learning_rate 0.00010009
2017-10-10T13:11:25.699861: step 2669, loss 0.3379, acc 0.859375, learning_rate 0.000100089
2017-10-10T13:11:26.244889: step 2670, loss 0.37727, acc 0.859375, learning_rate 0.000100089
2017-10-10T13:11:26.836920: step 2671, loss 0.653187, acc 0.796875, learning_rate 0.000100089
2017-10-10T13:11:27.307963: step 2672, loss 0.303293, acc 0.9375, learning_rate 0.000100088
2017-10-10T13:11:27.853049: step 2673, loss 0.33153, acc 0.890625, learning_rate 0.000100088
2017-10-10T13:11:28.359436: step 2674, loss 0.336558, acc 0.921875, learning_rate 0.000100088
2017-10-10T13:11:28.897373: step 2675, loss 0.342209, acc 0.859375, learning_rate 0.000100087
2017-10-10T13:11:29.451372: step 2676, loss 0.721236, acc 0.734375, learning_rate 0.000100087
2017-10-10T13:11:29.993074: step 2677, loss 0.412121, acc 0.875, learning_rate 0.000100086
2017-10-10T13:11:30.531892: step 2678, loss 0.339219, acc 0.90625, learning_rate 0.000100086
2017-10-10T13:11:31.037010: step 2679, loss 0.241129, acc 0.921875, learning_rate 0.000100086
2017-10-10T13:11:31.604707: step 2680, loss 0.473463, acc 0.875, learning_rate 0.000100085

Evaluation:
2017-10-10T13:11:32.920948: step 2680, loss 0.275806, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2680

2017-10-10T13:11:34.652892: step 2681, loss 0.304409, acc 0.875, learning_rate 0.000100085
2017-10-10T13:11:35.213221: step 2682, loss 0.344089, acc 0.890625, learning_rate 0.000100085
2017-10-10T13:11:35.710808: step 2683, loss 0.312482, acc 0.890625, learning_rate 0.000100084
2017-10-10T13:11:36.258827: step 2684, loss 0.413475, acc 0.875, learning_rate 0.000100084
2017-10-10T13:11:36.764474: step 2685, loss 0.534054, acc 0.8125, learning_rate 0.000100084
2017-10-10T13:11:37.251965: step 2686, loss 0.465245, acc 0.796875, learning_rate 0.000100083
2017-10-10T13:11:37.834700: step 2687, loss 0.402721, acc 0.828125, learning_rate 0.000100083
2017-10-10T13:11:38.410321: step 2688, loss 0.285296, acc 0.890625, learning_rate 0.000100083
2017-10-10T13:11:38.927507: step 2689, loss 0.412742, acc 0.859375, learning_rate 0.000100082
2017-10-10T13:11:39.517049: step 2690, loss 0.470555, acc 0.890625, learning_rate 0.000100082
2017-10-10T13:11:40.020991: step 2691, loss 0.593494, acc 0.875, learning_rate 0.000100082
2017-10-10T13:11:40.509544: step 2692, loss 0.302872, acc 0.890625, learning_rate 0.000100081
2017-10-10T13:11:41.167871: step 2693, loss 0.386968, acc 0.875, learning_rate 0.000100081
2017-10-10T13:11:41.763367: step 2694, loss 0.39669, acc 0.890625, learning_rate 0.000100081
2017-10-10T13:11:42.205303: step 2695, loss 0.387365, acc 0.890625, learning_rate 0.00010008
2017-10-10T13:11:42.595989: step 2696, loss 0.278508, acc 0.890625, learning_rate 0.00010008
2017-10-10T13:11:43.087741: step 2697, loss 0.323272, acc 0.890625, learning_rate 0.00010008
2017-10-10T13:11:43.588950: step 2698, loss 0.545535, acc 0.75, learning_rate 0.000100079
2017-10-10T13:11:44.123042: step 2699, loss 0.412032, acc 0.84375, learning_rate 0.000100079
2017-10-10T13:11:44.702864: step 2700, loss 0.369476, acc 0.890625, learning_rate 0.000100079
2017-10-10T13:11:45.264708: step 2701, loss 0.328969, acc 0.890625, learning_rate 0.000100078
2017-10-10T13:11:45.708188: step 2702, loss 0.305488, acc 0.890625, learning_rate 0.000100078
2017-10-10T13:11:46.092842: step 2703, loss 0.520781, acc 0.78125, learning_rate 0.000100078
2017-10-10T13:11:46.649096: step 2704, loss 0.388184, acc 0.859375, learning_rate 0.000100077
2017-10-10T13:11:47.097029: step 2705, loss 0.399484, acc 0.90625, learning_rate 0.000100077
2017-10-10T13:11:47.536878: step 2706, loss 0.359698, acc 0.9375, learning_rate 0.000100077
2017-10-10T13:11:47.894466: step 2707, loss 0.446966, acc 0.828125, learning_rate 0.000100076
2017-10-10T13:11:48.301202: step 2708, loss 0.290477, acc 0.890625, learning_rate 0.000100076
2017-10-10T13:11:48.780861: step 2709, loss 0.363149, acc 0.875, learning_rate 0.000100076
2017-10-10T13:11:49.323757: step 2710, loss 0.327413, acc 0.90625, learning_rate 0.000100076
2017-10-10T13:11:49.842432: step 2711, loss 0.295233, acc 0.921875, learning_rate 0.000100075
2017-10-10T13:11:50.356831: step 2712, loss 0.381218, acc 0.9375, learning_rate 0.000100075
2017-10-10T13:11:50.936983: step 2713, loss 0.418233, acc 0.890625, learning_rate 0.000100075
2017-10-10T13:11:51.514624: step 2714, loss 0.310491, acc 0.9375, learning_rate 0.000100074
2017-10-10T13:11:52.072879: step 2715, loss 0.306019, acc 0.875, learning_rate 0.000100074
2017-10-10T13:11:52.603037: step 2716, loss 0.400402, acc 0.828125, learning_rate 0.000100074
2017-10-10T13:11:53.120835: step 2717, loss 0.364336, acc 0.90625, learning_rate 0.000100073
2017-10-10T13:11:53.647569: step 2718, loss 0.29932, acc 0.890625, learning_rate 0.000100073
2017-10-10T13:11:54.178830: step 2719, loss 0.506706, acc 0.84375, learning_rate 0.000100073
2017-10-10T13:11:54.746631: step 2720, loss 0.416908, acc 0.859375, learning_rate 0.000100073

Evaluation:
2017-10-10T13:11:55.899171: step 2720, loss 0.276848, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2720

2017-10-10T13:11:57.398794: step 2721, loss 0.507711, acc 0.828125, learning_rate 0.000100072
2017-10-10T13:11:57.962960: step 2722, loss 0.359823, acc 0.859375, learning_rate 0.000100072
2017-10-10T13:11:58.464830: step 2723, loss 0.244094, acc 0.90625, learning_rate 0.000100072
2017-10-10T13:11:59.006982: step 2724, loss 0.349567, acc 0.859375, learning_rate 0.000100071
2017-10-10T13:11:59.555157: step 2725, loss 0.273177, acc 0.9375, learning_rate 0.000100071
2017-10-10T13:12:00.061789: step 2726, loss 0.408161, acc 0.8125, learning_rate 0.000100071
2017-10-10T13:12:00.604837: step 2727, loss 0.245845, acc 0.921875, learning_rate 0.00010007
2017-10-10T13:12:01.133321: step 2728, loss 0.292281, acc 0.890625, learning_rate 0.00010007
2017-10-10T13:12:01.686892: step 2729, loss 0.522653, acc 0.84375, learning_rate 0.00010007
2017-10-10T13:12:02.307742: step 2730, loss 0.481151, acc 0.828125, learning_rate 0.00010007
2017-10-10T13:12:02.865059: step 2731, loss 0.452481, acc 0.859375, learning_rate 0.000100069
2017-10-10T13:12:03.400915: step 2732, loss 0.287144, acc 0.875, learning_rate 0.000100069
2017-10-10T13:12:03.961107: step 2733, loss 0.307376, acc 0.90625, learning_rate 0.000100069
2017-10-10T13:12:04.574332: step 2734, loss 0.473495, acc 0.875, learning_rate 0.000100068
2017-10-10T13:12:05.151337: step 2735, loss 0.448949, acc 0.859375, learning_rate 0.000100068
2017-10-10T13:12:05.544809: step 2736, loss 0.338192, acc 0.921875, learning_rate 0.000100068
2017-10-10T13:12:05.960831: step 2737, loss 0.322961, acc 0.859375, learning_rate 0.000100068
2017-10-10T13:12:06.534405: step 2738, loss 0.427587, acc 0.84375, learning_rate 0.000100067
2017-10-10T13:12:07.125997: step 2739, loss 0.59483, acc 0.796875, learning_rate 0.000100067
2017-10-10T13:12:07.652947: step 2740, loss 0.392734, acc 0.84375, learning_rate 0.000100067
2017-10-10T13:12:08.205720: step 2741, loss 0.508349, acc 0.875, learning_rate 0.000100067
2017-10-10T13:12:08.631832: step 2742, loss 0.48644, acc 0.859375, learning_rate 0.000100066
2017-10-10T13:12:09.096870: step 2743, loss 0.473568, acc 0.84375, learning_rate 0.000100066
2017-10-10T13:12:09.624852: step 2744, loss 0.417826, acc 0.882353, learning_rate 0.000100066
2017-10-10T13:12:10.222351: step 2745, loss 0.32003, acc 0.90625, learning_rate 0.000100065
2017-10-10T13:12:10.646188: step 2746, loss 0.435673, acc 0.84375, learning_rate 0.000100065
2017-10-10T13:12:11.084671: step 2747, loss 0.508593, acc 0.796875, learning_rate 0.000100065
2017-10-10T13:12:11.648108: step 2748, loss 0.423788, acc 0.875, learning_rate 0.000100065
2017-10-10T13:12:12.237083: step 2749, loss 0.387619, acc 0.84375, learning_rate 0.000100064
2017-10-10T13:12:12.793202: step 2750, loss 0.302982, acc 0.921875, learning_rate 0.000100064
2017-10-10T13:12:13.373053: step 2751, loss 0.518814, acc 0.859375, learning_rate 0.000100064
2017-10-10T13:12:13.927529: step 2752, loss 0.362005, acc 0.875, learning_rate 0.000100064
2017-10-10T13:12:14.370007: step 2753, loss 0.249829, acc 0.890625, learning_rate 0.000100063
2017-10-10T13:12:14.845701: step 2754, loss 0.353278, acc 0.84375, learning_rate 0.000100063
2017-10-10T13:12:15.400629: step 2755, loss 0.328745, acc 0.875, learning_rate 0.000100063
2017-10-10T13:12:15.940223: step 2756, loss 0.453416, acc 0.84375, learning_rate 0.000100063
2017-10-10T13:12:16.451195: step 2757, loss 0.380168, acc 0.890625, learning_rate 0.000100062
2017-10-10T13:12:17.008943: step 2758, loss 0.322817, acc 0.90625, learning_rate 0.000100062
2017-10-10T13:12:17.506629: step 2759, loss 0.476681, acc 0.796875, learning_rate 0.000100062
2017-10-10T13:12:18.076280: step 2760, loss 0.2749, acc 0.890625, learning_rate 0.000100062

Evaluation:
2017-10-10T13:12:19.254435: step 2760, loss 0.276656, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2760

2017-10-10T13:12:20.809067: step 2761, loss 0.293315, acc 0.90625, learning_rate 0.000100061
2017-10-10T13:12:21.320879: step 2762, loss 0.353157, acc 0.890625, learning_rate 0.000100061
2017-10-10T13:12:21.893154: step 2763, loss 0.422725, acc 0.828125, learning_rate 0.000100061
2017-10-10T13:12:22.421069: step 2764, loss 0.381228, acc 0.890625, learning_rate 0.000100061
2017-10-10T13:12:22.921301: step 2765, loss 0.277953, acc 0.90625, learning_rate 0.00010006
2017-10-10T13:12:23.481179: step 2766, loss 0.356277, acc 0.90625, learning_rate 0.00010006
2017-10-10T13:12:24.017003: step 2767, loss 0.362942, acc 0.890625, learning_rate 0.00010006
2017-10-10T13:12:24.546687: step 2768, loss 0.506553, acc 0.859375, learning_rate 0.00010006
2017-10-10T13:12:25.048897: step 2769, loss 0.346692, acc 0.90625, learning_rate 0.000100059
2017-10-10T13:12:25.611798: step 2770, loss 0.546256, acc 0.796875, learning_rate 0.000100059
2017-10-10T13:12:26.237068: step 2771, loss 0.401028, acc 0.859375, learning_rate 0.000100059
2017-10-10T13:12:26.732239: step 2772, loss 0.249764, acc 0.9375, learning_rate 0.000100059
2017-10-10T13:12:27.292910: step 2773, loss 0.27235, acc 0.890625, learning_rate 0.000100058
2017-10-10T13:12:27.879256: step 2774, loss 0.38782, acc 0.875, learning_rate 0.000100058
2017-10-10T13:12:28.328506: step 2775, loss 0.346687, acc 0.875, learning_rate 0.000100058
2017-10-10T13:12:28.796371: step 2776, loss 0.538827, acc 0.828125, learning_rate 0.000100058
2017-10-10T13:12:29.252901: step 2777, loss 0.321442, acc 0.890625, learning_rate 0.000100057
2017-10-10T13:12:29.828892: step 2778, loss 0.355002, acc 0.875, learning_rate 0.000100057
2017-10-10T13:12:30.376803: step 2779, loss 0.325583, acc 0.875, learning_rate 0.000100057
2017-10-10T13:12:31.038180: step 2780, loss 0.308141, acc 0.859375, learning_rate 0.000100057
2017-10-10T13:12:31.598779: step 2781, loss 0.443304, acc 0.875, learning_rate 0.000100056
2017-10-10T13:12:32.039583: step 2782, loss 0.580281, acc 0.859375, learning_rate 0.000100056
2017-10-10T13:12:32.448309: step 2783, loss 0.375986, acc 0.90625, learning_rate 0.000100056
2017-10-10T13:12:32.995843: step 2784, loss 0.414919, acc 0.875, learning_rate 0.000100056
2017-10-10T13:12:33.452843: step 2785, loss 0.30924, acc 0.890625, learning_rate 0.000100056
2017-10-10T13:12:33.928670: step 2786, loss 0.384559, acc 0.859375, learning_rate 0.000100055
2017-10-10T13:12:34.489172: step 2787, loss 0.303747, acc 0.921875, learning_rate 0.000100055
2017-10-10T13:12:35.057146: step 2788, loss 0.371731, acc 0.875, learning_rate 0.000100055
2017-10-10T13:12:35.588492: step 2789, loss 0.42593, acc 0.875, learning_rate 0.000100055
2017-10-10T13:12:36.141551: step 2790, loss 0.462117, acc 0.890625, learning_rate 0.000100054
2017-10-10T13:12:36.725178: step 2791, loss 0.454094, acc 0.8125, learning_rate 0.000100054
2017-10-10T13:12:37.305102: step 2792, loss 0.221821, acc 0.90625, learning_rate 0.000100054
2017-10-10T13:12:37.833674: step 2793, loss 0.338119, acc 0.875, learning_rate 0.000100054
2017-10-10T13:12:38.345074: step 2794, loss 0.335895, acc 0.875, learning_rate 0.000100054
2017-10-10T13:12:38.964587: step 2795, loss 0.510724, acc 0.875, learning_rate 0.000100053
2017-10-10T13:12:39.476902: step 2796, loss 0.369722, acc 0.875, learning_rate 0.000100053
2017-10-10T13:12:40.045040: step 2797, loss 0.179249, acc 0.953125, learning_rate 0.000100053
2017-10-10T13:12:40.585376: step 2798, loss 0.337334, acc 0.90625, learning_rate 0.000100053
2017-10-10T13:12:41.075043: step 2799, loss 0.280473, acc 0.921875, learning_rate 0.000100052
2017-10-10T13:12:41.662993: step 2800, loss 0.343095, acc 0.890625, learning_rate 0.000100052

Evaluation:
2017-10-10T13:12:42.902663: step 2800, loss 0.27553, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2800

2017-10-10T13:12:44.616256: step 2801, loss 0.232281, acc 0.953125, learning_rate 0.000100052
2017-10-10T13:12:45.158743: step 2802, loss 0.268994, acc 0.921875, learning_rate 0.000100052
2017-10-10T13:12:45.709459: step 2803, loss 0.332254, acc 0.890625, learning_rate 0.000100052
2017-10-10T13:12:46.262766: step 2804, loss 0.227768, acc 0.921875, learning_rate 0.000100051
2017-10-10T13:12:46.798025: step 2805, loss 0.386482, acc 0.890625, learning_rate 0.000100051
2017-10-10T13:12:47.341459: step 2806, loss 0.334214, acc 0.875, learning_rate 0.000100051
2017-10-10T13:12:47.893140: step 2807, loss 0.382997, acc 0.828125, learning_rate 0.000100051
2017-10-10T13:12:48.468030: step 2808, loss 0.280395, acc 0.921875, learning_rate 0.000100051
2017-10-10T13:12:49.035399: step 2809, loss 0.3467, acc 0.859375, learning_rate 0.00010005
2017-10-10T13:12:49.600095: step 2810, loss 0.364455, acc 0.890625, learning_rate 0.00010005
2017-10-10T13:12:50.124851: step 2811, loss 0.340567, acc 0.875, learning_rate 0.00010005
2017-10-10T13:12:50.668861: step 2812, loss 0.378584, acc 0.859375, learning_rate 0.00010005
2017-10-10T13:12:51.308896: step 2813, loss 0.390079, acc 0.90625, learning_rate 0.00010005
2017-10-10T13:12:51.759389: step 2814, loss 0.389581, acc 0.859375, learning_rate 0.000100049
2017-10-10T13:12:52.191634: step 2815, loss 0.379732, acc 0.90625, learning_rate 0.000100049
2017-10-10T13:12:52.627031: step 2816, loss 0.212275, acc 0.921875, learning_rate 0.000100049
2017-10-10T13:12:53.174282: step 2817, loss 0.277485, acc 0.890625, learning_rate 0.000100049
2017-10-10T13:12:53.732848: step 2818, loss 0.333933, acc 0.890625, learning_rate 0.000100049
2017-10-10T13:12:54.340892: step 2819, loss 0.323876, acc 0.875, learning_rate 0.000100048
2017-10-10T13:12:54.892946: step 2820, loss 0.195184, acc 0.953125, learning_rate 0.000100048
2017-10-10T13:12:55.397137: step 2821, loss 0.485955, acc 0.875, learning_rate 0.000100048
2017-10-10T13:12:55.965845: step 2822, loss 0.58375, acc 0.859375, learning_rate 0.000100048
2017-10-10T13:12:56.352989: step 2823, loss 0.324641, acc 0.921875, learning_rate 0.000100048
2017-10-10T13:12:56.803796: step 2824, loss 0.578575, acc 0.84375, learning_rate 0.000100047
2017-10-10T13:12:57.260824: step 2825, loss 0.584689, acc 0.8125, learning_rate 0.000100047
2017-10-10T13:12:57.779203: step 2826, loss 0.552659, acc 0.84375, learning_rate 0.000100047
2017-10-10T13:12:58.348869: step 2827, loss 0.448149, acc 0.875, learning_rate 0.000100047
2017-10-10T13:12:58.875793: step 2828, loss 0.252712, acc 0.921875, learning_rate 0.000100047
2017-10-10T13:12:59.373805: step 2829, loss 0.375255, acc 0.9375, learning_rate 0.000100046
2017-10-10T13:12:59.745029: step 2830, loss 0.4825, acc 0.8125, learning_rate 0.000100046
2017-10-10T13:13:00.360419: step 2831, loss 0.38976, acc 0.859375, learning_rate 0.000100046
2017-10-10T13:13:00.893166: step 2832, loss 0.316021, acc 0.921875, learning_rate 0.000100046
2017-10-10T13:13:01.484949: step 2833, loss 0.377668, acc 0.875, learning_rate 0.000100046
2017-10-10T13:13:02.060842: step 2834, loss 0.344419, acc 0.921875, learning_rate 0.000100045
2017-10-10T13:13:02.607740: step 2835, loss 0.451552, acc 0.828125, learning_rate 0.000100045
2017-10-10T13:13:03.173296: step 2836, loss 0.417335, acc 0.875, learning_rate 0.000100045
2017-10-10T13:13:03.728040: step 2837, loss 0.420859, acc 0.875, learning_rate 0.000100045
2017-10-10T13:13:04.280675: step 2838, loss 0.480856, acc 0.828125, learning_rate 0.000100045
2017-10-10T13:13:04.896860: step 2839, loss 0.296857, acc 0.921875, learning_rate 0.000100045
2017-10-10T13:13:05.411211: step 2840, loss 0.358445, acc 0.859375, learning_rate 0.000100044

Evaluation:
2017-10-10T13:13:06.684216: step 2840, loss 0.274702, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2840

2017-10-10T13:13:08.120195: step 2841, loss 0.302225, acc 0.90625, learning_rate 0.000100044
2017-10-10T13:13:08.512893: step 2842, loss 0.605224, acc 0.784314, learning_rate 0.000100044
2017-10-10T13:13:09.045147: step 2843, loss 0.303097, acc 0.921875, learning_rate 0.000100044
2017-10-10T13:13:09.604874: step 2844, loss 0.45076, acc 0.84375, learning_rate 0.000100044
2017-10-10T13:13:10.112877: step 2845, loss 0.388124, acc 0.9375, learning_rate 0.000100043
2017-10-10T13:13:10.685609: step 2846, loss 0.224347, acc 0.90625, learning_rate 0.000100043
2017-10-10T13:13:11.188956: step 2847, loss 0.502251, acc 0.8125, learning_rate 0.000100043
2017-10-10T13:13:11.748859: step 2848, loss 0.344142, acc 0.828125, learning_rate 0.000100043
2017-10-10T13:13:12.264874: step 2849, loss 0.450601, acc 0.828125, learning_rate 0.000100043
2017-10-10T13:13:12.744430: step 2850, loss 0.464636, acc 0.828125, learning_rate 0.000100043
2017-10-10T13:13:13.247574: step 2851, loss 0.311153, acc 0.890625, learning_rate 0.000100042
2017-10-10T13:13:13.792958: step 2852, loss 0.319402, acc 0.921875, learning_rate 0.000100042
2017-10-10T13:13:14.352984: step 2853, loss 0.42459, acc 0.859375, learning_rate 0.000100042
2017-10-10T13:13:14.873005: step 2854, loss 0.312451, acc 0.890625, learning_rate 0.000100042
2017-10-10T13:13:15.348822: step 2855, loss 0.394044, acc 0.875, learning_rate 0.000100042
2017-10-10T13:13:15.829081: step 2856, loss 0.380588, acc 0.90625, learning_rate 0.000100042
2017-10-10T13:13:16.297812: step 2857, loss 0.361038, acc 0.84375, learning_rate 0.000100041
2017-10-10T13:13:16.864857: step 2858, loss 0.204786, acc 0.96875, learning_rate 0.000100041
2017-10-10T13:13:17.365048: step 2859, loss 0.289886, acc 0.875, learning_rate 0.000100041
2017-10-10T13:13:17.888857: step 2860, loss 0.31806, acc 0.921875, learning_rate 0.000100041
2017-10-10T13:13:18.293030: step 2861, loss 0.376931, acc 0.859375, learning_rate 0.000100041
2017-10-10T13:13:18.900449: step 2862, loss 0.272345, acc 0.921875, learning_rate 0.000100041
2017-10-10T13:13:19.269740: step 2863, loss 0.416436, acc 0.859375, learning_rate 0.00010004
2017-10-10T13:13:19.684783: step 2864, loss 0.419335, acc 0.890625, learning_rate 0.00010004
2017-10-10T13:13:20.089295: step 2865, loss 0.313099, acc 0.890625, learning_rate 0.00010004
2017-10-10T13:13:20.477193: step 2866, loss 0.333386, acc 0.890625, learning_rate 0.00010004
2017-10-10T13:13:21.053181: step 2867, loss 0.269947, acc 0.90625, learning_rate 0.00010004
2017-10-10T13:13:21.610286: step 2868, loss 0.405979, acc 0.90625, learning_rate 0.00010004
2017-10-10T13:13:22.096925: step 2869, loss 0.441545, acc 0.875, learning_rate 0.000100039
2017-10-10T13:13:22.613182: step 2870, loss 0.285916, acc 0.890625, learning_rate 0.000100039
2017-10-10T13:13:23.161013: step 2871, loss 0.25803, acc 0.921875, learning_rate 0.000100039
2017-10-10T13:13:23.659617: step 2872, loss 0.273383, acc 0.921875, learning_rate 0.000100039
2017-10-10T13:13:24.199122: step 2873, loss 0.431526, acc 0.859375, learning_rate 0.000100039
2017-10-10T13:13:24.751379: step 2874, loss 0.256102, acc 0.9375, learning_rate 0.000100039
2017-10-10T13:13:25.236684: step 2875, loss 0.365326, acc 0.875, learning_rate 0.000100038
2017-10-10T13:13:25.721090: step 2876, loss 0.12873, acc 0.96875, learning_rate 0.000100038
2017-10-10T13:13:26.241234: step 2877, loss 0.358166, acc 0.90625, learning_rate 0.000100038
2017-10-10T13:13:26.797754: step 2878, loss 0.196716, acc 0.9375, learning_rate 0.000100038
2017-10-10T13:13:27.394354: step 2879, loss 0.392777, acc 0.859375, learning_rate 0.000100038
2017-10-10T13:13:27.901570: step 2880, loss 0.352967, acc 0.90625, learning_rate 0.000100038

Evaluation:
2017-10-10T13:13:29.118166: step 2880, loss 0.275364, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2880

2017-10-10T13:13:30.718042: step 2881, loss 0.486248, acc 0.875, learning_rate 0.000100038
2017-10-10T13:13:31.293050: step 2882, loss 0.412508, acc 0.90625, learning_rate 0.000100037
2017-10-10T13:13:31.818786: step 2883, loss 0.429738, acc 0.875, learning_rate 0.000100037
2017-10-10T13:13:32.380908: step 2884, loss 0.401355, acc 0.875, learning_rate 0.000100037
2017-10-10T13:13:32.949058: step 2885, loss 0.24642, acc 0.953125, learning_rate 0.000100037
2017-10-10T13:13:33.500406: step 2886, loss 0.311179, acc 0.890625, learning_rate 0.000100037
2017-10-10T13:13:34.010127: step 2887, loss 0.376158, acc 0.90625, learning_rate 0.000100037
2017-10-10T13:13:34.508111: step 2888, loss 0.485395, acc 0.828125, learning_rate 0.000100036
2017-10-10T13:13:35.021186: step 2889, loss 0.293242, acc 0.890625, learning_rate 0.000100036
2017-10-10T13:13:35.599555: step 2890, loss 0.39469, acc 0.859375, learning_rate 0.000100036
2017-10-10T13:13:36.156059: step 2891, loss 0.589942, acc 0.8125, learning_rate 0.000100036
2017-10-10T13:13:36.728916: step 2892, loss 0.281045, acc 0.90625, learning_rate 0.000100036
2017-10-10T13:13:37.245127: step 2893, loss 0.432268, acc 0.828125, learning_rate 0.000100036
2017-10-10T13:13:37.691406: step 2894, loss 0.412893, acc 0.859375, learning_rate 0.000100036
2017-10-10T13:13:38.092836: step 2895, loss 0.179493, acc 0.9375, learning_rate 0.000100035
2017-10-10T13:13:38.645231: step 2896, loss 0.386153, acc 0.890625, learning_rate 0.000100035
2017-10-10T13:13:39.077058: step 2897, loss 0.397063, acc 0.875, learning_rate 0.000100035
2017-10-10T13:13:39.648952: step 2898, loss 0.302262, acc 0.890625, learning_rate 0.000100035
2017-10-10T13:13:40.109422: step 2899, loss 0.335819, acc 0.921875, learning_rate 0.000100035
2017-10-10T13:13:40.681153: step 2900, loss 0.330876, acc 0.890625, learning_rate 0.000100035
2017-10-10T13:13:41.304896: step 2901, loss 0.255172, acc 0.875, learning_rate 0.000100035
2017-10-10T13:13:41.837070: step 2902, loss 0.260927, acc 0.921875, learning_rate 0.000100034
2017-10-10T13:13:42.291145: step 2903, loss 0.342683, acc 0.90625, learning_rate 0.000100034
2017-10-10T13:13:42.700823: step 2904, loss 0.332601, acc 0.875, learning_rate 0.000100034
2017-10-10T13:13:43.153546: step 2905, loss 0.40154, acc 0.859375, learning_rate 0.000100034
2017-10-10T13:13:43.597135: step 2906, loss 0.356195, acc 0.90625, learning_rate 0.000100034
2017-10-10T13:13:44.140993: step 2907, loss 0.406338, acc 0.875, learning_rate 0.000100034
2017-10-10T13:13:44.627414: step 2908, loss 0.394645, acc 0.84375, learning_rate 0.000100034
2017-10-10T13:13:45.137059: step 2909, loss 0.432128, acc 0.8125, learning_rate 0.000100033
2017-10-10T13:13:45.666407: step 2910, loss 0.279923, acc 0.921875, learning_rate 0.000100033
2017-10-10T13:13:46.198640: step 2911, loss 0.251217, acc 0.890625, learning_rate 0.000100033
2017-10-10T13:13:46.757099: step 2912, loss 0.316871, acc 0.890625, learning_rate 0.000100033
2017-10-10T13:13:47.256803: step 2913, loss 0.33658, acc 0.90625, learning_rate 0.000100033
2017-10-10T13:13:47.821028: step 2914, loss 0.324636, acc 0.84375, learning_rate 0.000100033
2017-10-10T13:13:48.380961: step 2915, loss 0.387495, acc 0.84375, learning_rate 0.000100033
2017-10-10T13:13:48.952849: step 2916, loss 0.310489, acc 0.890625, learning_rate 0.000100033
2017-10-10T13:13:49.432996: step 2917, loss 0.378142, acc 0.859375, learning_rate 0.000100032
2017-10-10T13:13:49.948410: step 2918, loss 0.263677, acc 0.90625, learning_rate 0.000100032
2017-10-10T13:13:50.493933: step 2919, loss 0.458276, acc 0.84375, learning_rate 0.000100032
2017-10-10T13:13:50.997285: step 2920, loss 0.514128, acc 0.828125, learning_rate 0.000100032

Evaluation:
2017-10-10T13:13:52.260147: step 2920, loss 0.274895, acc 0.894964

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2920

2017-10-10T13:13:54.034502: step 2921, loss 0.341605, acc 0.90625, learning_rate 0.000100032
2017-10-10T13:13:54.521363: step 2922, loss 0.372403, acc 0.875, learning_rate 0.000100032
2017-10-10T13:13:55.010415: step 2923, loss 0.32747, acc 0.859375, learning_rate 0.000100032
2017-10-10T13:13:55.487156: step 2924, loss 0.144898, acc 0.953125, learning_rate 0.000100031
2017-10-10T13:13:56.001344: step 2925, loss 0.199465, acc 0.9375, learning_rate 0.000100031
2017-10-10T13:13:56.525007: step 2926, loss 0.410479, acc 0.84375, learning_rate 0.000100031
2017-10-10T13:13:57.044648: step 2927, loss 0.507671, acc 0.78125, learning_rate 0.000100031
2017-10-10T13:13:57.608873: step 2928, loss 0.485022, acc 0.765625, learning_rate 0.000100031
2017-10-10T13:13:58.130984: step 2929, loss 0.486393, acc 0.796875, learning_rate 0.000100031
2017-10-10T13:13:58.636871: step 2930, loss 0.496246, acc 0.859375, learning_rate 0.000100031
2017-10-10T13:13:59.164929: step 2931, loss 0.374566, acc 0.890625, learning_rate 0.000100031
2017-10-10T13:13:59.775346: step 2932, loss 0.489861, acc 0.8125, learning_rate 0.00010003
2017-10-10T13:14:00.249986: step 2933, loss 0.434245, acc 0.875, learning_rate 0.00010003
2017-10-10T13:14:00.673843: step 2934, loss 0.269056, acc 0.890625, learning_rate 0.00010003
2017-10-10T13:14:01.112826: step 2935, loss 0.44786, acc 0.84375, learning_rate 0.00010003
2017-10-10T13:14:01.641005: step 2936, loss 0.359892, acc 0.890625, learning_rate 0.00010003
2017-10-10T13:14:02.199635: step 2937, loss 0.337967, acc 0.90625, learning_rate 0.00010003
2017-10-10T13:14:02.653012: step 2938, loss 0.298778, acc 0.90625, learning_rate 0.00010003
2017-10-10T13:14:03.276900: step 2939, loss 0.488566, acc 0.8125, learning_rate 0.00010003
2017-10-10T13:14:03.737134: step 2940, loss 0.293882, acc 0.921569, learning_rate 0.000100029
2017-10-10T13:14:04.328863: step 2941, loss 0.38002, acc 0.859375, learning_rate 0.000100029
2017-10-10T13:14:04.837518: step 2942, loss 0.483979, acc 0.796875, learning_rate 0.000100029
2017-10-10T13:14:05.242308: step 2943, loss 0.387739, acc 0.859375, learning_rate 0.000100029
2017-10-10T13:14:05.669436: step 2944, loss 0.376498, acc 0.875, learning_rate 0.000100029
2017-10-10T13:14:06.189670: step 2945, loss 0.380952, acc 0.890625, learning_rate 0.000100029
2017-10-10T13:14:06.721109: step 2946, loss 0.321029, acc 0.90625, learning_rate 0.000100029
2017-10-10T13:14:07.232859: step 2947, loss 0.324799, acc 0.875, learning_rate 0.000100029
2017-10-10T13:14:07.775952: step 2948, loss 0.255088, acc 0.90625, learning_rate 0.000100029
2017-10-10T13:14:08.309045: step 2949, loss 0.423889, acc 0.859375, learning_rate 0.000100028
2017-10-10T13:14:08.864888: step 2950, loss 0.489428, acc 0.828125, learning_rate 0.000100028
2017-10-10T13:14:09.408476: step 2951, loss 0.491473, acc 0.859375, learning_rate 0.000100028
2017-10-10T13:14:09.970956: step 2952, loss 0.435914, acc 0.890625, learning_rate 0.000100028
2017-10-10T13:14:10.517009: step 2953, loss 0.370174, acc 0.828125, learning_rate 0.000100028
2017-10-10T13:14:11.041749: step 2954, loss 0.293266, acc 0.890625, learning_rate 0.000100028
2017-10-10T13:14:11.482574: step 2955, loss 0.254394, acc 0.921875, learning_rate 0.000100028
2017-10-10T13:14:11.981617: step 2956, loss 0.351107, acc 0.921875, learning_rate 0.000100028
2017-10-10T13:14:12.429037: step 2957, loss 0.322979, acc 0.90625, learning_rate 0.000100028
2017-10-10T13:14:12.933017: step 2958, loss 0.369339, acc 0.84375, learning_rate 0.000100027
2017-10-10T13:14:13.505017: step 2959, loss 0.353388, acc 0.875, learning_rate 0.000100027
2017-10-10T13:14:13.985748: step 2960, loss 0.165656, acc 0.984375, learning_rate 0.000100027

Evaluation:
2017-10-10T13:14:15.249037: step 2960, loss 0.273771, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-2960

2017-10-10T13:14:16.768766: step 2961, loss 0.394357, acc 0.875, learning_rate 0.000100027
2017-10-10T13:14:17.272837: step 2962, loss 0.36064, acc 0.9375, learning_rate 0.000100027
2017-10-10T13:14:17.864898: step 2963, loss 0.365324, acc 0.8125, learning_rate 0.000100027
2017-10-10T13:14:18.432976: step 2964, loss 0.32042, acc 0.921875, learning_rate 0.000100027
2017-10-10T13:14:18.960869: step 2965, loss 0.394009, acc 0.90625, learning_rate 0.000100027
2017-10-10T13:14:19.424974: step 2966, loss 0.414612, acc 0.84375, learning_rate 0.000100027
2017-10-10T13:14:19.977699: step 2967, loss 0.590372, acc 0.84375, learning_rate 0.000100026
2017-10-10T13:14:20.540838: step 2968, loss 0.271891, acc 0.890625, learning_rate 0.000100026
2017-10-10T13:14:21.052931: step 2969, loss 0.199484, acc 0.9375, learning_rate 0.000100026
2017-10-10T13:14:21.548943: step 2970, loss 0.47522, acc 0.828125, learning_rate 0.000100026
2017-10-10T13:14:21.972995: step 2971, loss 0.283927, acc 0.921875, learning_rate 0.000100026
2017-10-10T13:14:22.564849: step 2972, loss 0.377615, acc 0.84375, learning_rate 0.000100026
2017-10-10T13:14:23.132912: step 2973, loss 0.365207, acc 0.90625, learning_rate 0.000100026
2017-10-10T13:14:23.581750: step 2974, loss 0.243755, acc 0.9375, learning_rate 0.000100026
2017-10-10T13:14:23.980890: step 2975, loss 0.59284, acc 0.765625, learning_rate 0.000100026
2017-10-10T13:14:24.561122: step 2976, loss 0.188957, acc 0.921875, learning_rate 0.000100025
2017-10-10T13:14:25.096917: step 2977, loss 0.421286, acc 0.890625, learning_rate 0.000100025
2017-10-10T13:14:25.624886: step 2978, loss 0.403658, acc 0.8125, learning_rate 0.000100025
2017-10-10T13:14:26.185030: step 2979, loss 0.436478, acc 0.859375, learning_rate 0.000100025
2017-10-10T13:14:26.773176: step 2980, loss 0.332559, acc 0.921875, learning_rate 0.000100025
2017-10-10T13:14:27.333537: step 2981, loss 0.35129, acc 0.859375, learning_rate 0.000100025
2017-10-10T13:14:27.704855: step 2982, loss 0.302945, acc 0.921875, learning_rate 0.000100025
2017-10-10T13:14:28.177441: step 2983, loss 0.404536, acc 0.859375, learning_rate 0.000100025
2017-10-10T13:14:28.611586: step 2984, loss 0.230196, acc 0.90625, learning_rate 0.000100025
2017-10-10T13:14:29.168860: step 2985, loss 0.434364, acc 0.84375, learning_rate 0.000100025
2017-10-10T13:14:29.656911: step 2986, loss 0.326806, acc 0.875, learning_rate 0.000100024
2017-10-10T13:14:30.133015: step 2987, loss 0.200041, acc 0.9375, learning_rate 0.000100024
2017-10-10T13:14:30.653400: step 2988, loss 0.456564, acc 0.828125, learning_rate 0.000100024
2017-10-10T13:14:31.232935: step 2989, loss 0.430234, acc 0.8125, learning_rate 0.000100024
2017-10-10T13:14:31.756116: step 2990, loss 0.44979, acc 0.859375, learning_rate 0.000100024
2017-10-10T13:14:32.240903: step 2991, loss 0.155264, acc 0.953125, learning_rate 0.000100024
2017-10-10T13:14:32.568900: step 2992, loss 0.363535, acc 0.90625, learning_rate 0.000100024
2017-10-10T13:14:33.136949: step 2993, loss 0.219182, acc 0.9375, learning_rate 0.000100024
2017-10-10T13:14:33.716490: step 2994, loss 0.351857, acc 0.921875, learning_rate 0.000100024
2017-10-10T13:14:34.249288: step 2995, loss 0.314782, acc 0.875, learning_rate 0.000100024
2017-10-10T13:14:34.755432: step 2996, loss 0.269813, acc 0.921875, learning_rate 0.000100023
2017-10-10T13:14:35.323780: step 2997, loss 0.231868, acc 0.953125, learning_rate 0.000100023
2017-10-10T13:14:35.867114: step 2998, loss 0.354374, acc 0.890625, learning_rate 0.000100023
2017-10-10T13:14:36.337843: step 2999, loss 0.500326, acc 0.875, learning_rate 0.000100023
2017-10-10T13:14:36.830506: step 3000, loss 0.395468, acc 0.9375, learning_rate 0.000100023

Evaluation:
2017-10-10T13:14:38.031654: step 3000, loss 0.272975, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3000

2017-10-10T13:14:39.672983: step 3001, loss 0.242301, acc 0.9375, learning_rate 0.000100023
2017-10-10T13:14:40.231502: step 3002, loss 0.556492, acc 0.796875, learning_rate 0.000100023
2017-10-10T13:14:40.736813: step 3003, loss 0.421561, acc 0.84375, learning_rate 0.000100023
2017-10-10T13:14:41.324403: step 3004, loss 0.299594, acc 0.859375, learning_rate 0.000100023
2017-10-10T13:14:41.888831: step 3005, loss 0.395695, acc 0.859375, learning_rate 0.000100023
2017-10-10T13:14:42.474341: step 3006, loss 0.409735, acc 0.90625, learning_rate 0.000100023
2017-10-10T13:14:43.000936: step 3007, loss 0.382514, acc 0.875, learning_rate 0.000100022
2017-10-10T13:14:43.516880: step 3008, loss 0.470657, acc 0.78125, learning_rate 0.000100022
2017-10-10T13:14:44.061627: step 3009, loss 0.267678, acc 0.875, learning_rate 0.000100022
2017-10-10T13:14:44.556878: step 3010, loss 0.443492, acc 0.828125, learning_rate 0.000100022
2017-10-10T13:14:45.016706: step 3011, loss 0.264154, acc 0.921875, learning_rate 0.000100022
2017-10-10T13:14:45.661077: step 3012, loss 0.394066, acc 0.859375, learning_rate 0.000100022
2017-10-10T13:14:46.156749: step 3013, loss 0.268046, acc 0.890625, learning_rate 0.000100022
2017-10-10T13:14:46.604615: step 3014, loss 0.285338, acc 0.890625, learning_rate 0.000100022
2017-10-10T13:14:47.078680: step 3015, loss 0.435536, acc 0.859375, learning_rate 0.000100022
2017-10-10T13:14:47.625080: step 3016, loss 0.529875, acc 0.875, learning_rate 0.000100022
2017-10-10T13:14:48.191415: step 3017, loss 0.403651, acc 0.890625, learning_rate 0.000100022
2017-10-10T13:14:48.704961: step 3018, loss 0.314867, acc 0.890625, learning_rate 0.000100021
2017-10-10T13:14:49.280980: step 3019, loss 0.406391, acc 0.84375, learning_rate 0.000100021
2017-10-10T13:14:49.788319: step 3020, loss 0.294456, acc 0.90625, learning_rate 0.000100021
2017-10-10T13:14:50.292826: step 3021, loss 0.397906, acc 0.875, learning_rate 0.000100021
2017-10-10T13:14:50.813325: step 3022, loss 0.408489, acc 0.875, learning_rate 0.000100021
2017-10-10T13:14:51.304342: step 3023, loss 0.605569, acc 0.828125, learning_rate 0.000100021
2017-10-10T13:14:51.751504: step 3024, loss 0.306975, acc 0.875, learning_rate 0.000100021
2017-10-10T13:14:52.307987: step 3025, loss 0.532367, acc 0.828125, learning_rate 0.000100021
2017-10-10T13:14:52.849472: step 3026, loss 0.552909, acc 0.78125, learning_rate 0.000100021
2017-10-10T13:14:53.411919: step 3027, loss 0.337178, acc 0.828125, learning_rate 0.000100021
2017-10-10T13:14:53.986998: step 3028, loss 0.467699, acc 0.875, learning_rate 0.000100021
2017-10-10T13:14:54.553974: step 3029, loss 0.229887, acc 0.953125, learning_rate 0.00010002
2017-10-10T13:14:55.096836: step 3030, loss 0.282851, acc 0.90625, learning_rate 0.00010002
2017-10-10T13:14:55.616951: step 3031, loss 0.286256, acc 0.90625, learning_rate 0.00010002
2017-10-10T13:14:56.228848: step 3032, loss 0.206715, acc 0.9375, learning_rate 0.00010002
2017-10-10T13:14:56.812985: step 3033, loss 0.543644, acc 0.890625, learning_rate 0.00010002
2017-10-10T13:14:57.300892: step 3034, loss 0.593222, acc 0.828125, learning_rate 0.00010002
2017-10-10T13:14:57.825123: step 3035, loss 0.176165, acc 0.953125, learning_rate 0.00010002
2017-10-10T13:14:58.332867: step 3036, loss 0.375178, acc 0.84375, learning_rate 0.00010002
2017-10-10T13:14:58.892990: step 3037, loss 0.204451, acc 0.921875, learning_rate 0.00010002
2017-10-10T13:14:59.320862: step 3038, loss 0.629165, acc 0.843137, learning_rate 0.00010002
2017-10-10T13:14:59.850268: step 3039, loss 0.288594, acc 0.921875, learning_rate 0.00010002
2017-10-10T13:15:00.393384: step 3040, loss 0.308461, acc 0.9375, learning_rate 0.00010002

Evaluation:
2017-10-10T13:15:01.683262: step 3040, loss 0.272109, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3040

2017-10-10T13:15:03.273117: step 3041, loss 0.270254, acc 0.90625, learning_rate 0.00010002
2017-10-10T13:15:03.800628: step 3042, loss 0.701905, acc 0.796875, learning_rate 0.000100019
2017-10-10T13:15:04.336966: step 3043, loss 0.368521, acc 0.875, learning_rate 0.000100019
2017-10-10T13:15:04.916923: step 3044, loss 0.363693, acc 0.875, learning_rate 0.000100019
2017-10-10T13:15:05.420876: step 3045, loss 0.29747, acc 0.90625, learning_rate 0.000100019
2017-10-10T13:15:05.973718: step 3046, loss 0.16748, acc 0.96875, learning_rate 0.000100019
2017-10-10T13:15:06.553160: step 3047, loss 0.333545, acc 0.890625, learning_rate 0.000100019
2017-10-10T13:15:07.128716: step 3048, loss 0.308925, acc 0.84375, learning_rate 0.000100019
2017-10-10T13:15:07.678991: step 3049, loss 0.478457, acc 0.84375, learning_rate 0.000100019
2017-10-10T13:15:08.252924: step 3050, loss 0.471503, acc 0.90625, learning_rate 0.000100019
2017-10-10T13:15:08.783245: step 3051, loss 0.279131, acc 0.953125, learning_rate 0.000100019
2017-10-10T13:15:09.369243: step 3052, loss 0.33305, acc 0.921875, learning_rate 0.000100019
2017-10-10T13:15:09.768302: step 3053, loss 0.444518, acc 0.875, learning_rate 0.000100019
2017-10-10T13:15:10.190943: step 3054, loss 0.375439, acc 0.875, learning_rate 0.000100018
2017-10-10T13:15:10.602230: step 3055, loss 0.201712, acc 0.9375, learning_rate 0.000100018
2017-10-10T13:15:11.185413: step 3056, loss 0.30637, acc 0.953125, learning_rate 0.000100018
2017-10-10T13:15:11.745306: step 3057, loss 0.211418, acc 0.96875, learning_rate 0.000100018
2017-10-10T13:15:12.246970: step 3058, loss 0.29875, acc 0.90625, learning_rate 0.000100018
2017-10-10T13:15:12.744867: step 3059, loss 0.283152, acc 0.921875, learning_rate 0.000100018
2017-10-10T13:15:13.293079: step 3060, loss 0.308527, acc 0.859375, learning_rate 0.000100018
2017-10-10T13:15:13.912926: step 3061, loss 0.351155, acc 0.859375, learning_rate 0.000100018
2017-10-10T13:15:14.384845: step 3062, loss 0.59329, acc 0.84375, learning_rate 0.000100018
2017-10-10T13:15:14.820849: step 3063, loss 0.263974, acc 0.90625, learning_rate 0.000100018
2017-10-10T13:15:15.295741: step 3064, loss 0.347306, acc 0.875, learning_rate 0.000100018
2017-10-10T13:15:15.832990: step 3065, loss 0.29692, acc 0.90625, learning_rate 0.000100018
2017-10-10T13:15:16.376575: step 3066, loss 0.29641, acc 0.90625, learning_rate 0.000100018
2017-10-10T13:15:16.905168: step 3067, loss 0.410632, acc 0.859375, learning_rate 0.000100018
2017-10-10T13:15:17.401232: step 3068, loss 0.230739, acc 0.921875, learning_rate 0.000100017
2017-10-10T13:15:17.960030: step 3069, loss 0.457562, acc 0.796875, learning_rate 0.000100017
2017-10-10T13:15:18.404621: step 3070, loss 0.240132, acc 0.921875, learning_rate 0.000100017
2017-10-10T13:15:19.005114: step 3071, loss 0.303479, acc 0.859375, learning_rate 0.000100017
2017-10-10T13:15:19.612849: step 3072, loss 0.344266, acc 0.875, learning_rate 0.000100017
2017-10-10T13:15:20.118695: step 3073, loss 0.411384, acc 0.875, learning_rate 0.000100017
2017-10-10T13:15:20.648543: step 3074, loss 0.364791, acc 0.875, learning_rate 0.000100017
2017-10-10T13:15:21.236597: step 3075, loss 0.319431, acc 0.875, learning_rate 0.000100017
2017-10-10T13:15:21.753072: step 3076, loss 0.401858, acc 0.90625, learning_rate 0.000100017
2017-10-10T13:15:22.300895: step 3077, loss 0.349582, acc 0.890625, learning_rate 0.000100017
2017-10-10T13:15:22.832932: step 3078, loss 0.431666, acc 0.90625, learning_rate 0.000100017
2017-10-10T13:15:23.364891: step 3079, loss 0.475617, acc 0.859375, learning_rate 0.000100017
2017-10-10T13:15:23.966571: step 3080, loss 0.261216, acc 0.890625, learning_rate 0.000100017

Evaluation:
2017-10-10T13:15:25.277031: step 3080, loss 0.270809, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3080

2017-10-10T13:15:27.117045: step 3081, loss 0.448779, acc 0.859375, learning_rate 0.000100017
2017-10-10T13:15:27.675703: step 3082, loss 0.35259, acc 0.875, learning_rate 0.000100016
2017-10-10T13:15:28.176950: step 3083, loss 0.222097, acc 0.96875, learning_rate 0.000100016
2017-10-10T13:15:28.656949: step 3084, loss 0.442744, acc 0.90625, learning_rate 0.000100016
2017-10-10T13:15:29.122835: step 3085, loss 0.44915, acc 0.8125, learning_rate 0.000100016
2017-10-10T13:15:29.728945: step 3086, loss 0.265679, acc 0.921875, learning_rate 0.000100016
2017-10-10T13:15:30.256965: step 3087, loss 0.442272, acc 0.890625, learning_rate 0.000100016
2017-10-10T13:15:30.705176: step 3088, loss 0.46937, acc 0.859375, learning_rate 0.000100016
2017-10-10T13:15:31.244947: step 3089, loss 0.379411, acc 0.8125, learning_rate 0.000100016
2017-10-10T13:15:31.783720: step 3090, loss 0.336965, acc 0.875, learning_rate 0.000100016
2017-10-10T13:15:32.345207: step 3091, loss 0.624886, acc 0.84375, learning_rate 0.000100016
2017-10-10T13:15:32.785415: step 3092, loss 0.430802, acc 0.875, learning_rate 0.000100016
2017-10-10T13:15:33.227476: step 3093, loss 0.317079, acc 0.890625, learning_rate 0.000100016
2017-10-10T13:15:33.701267: step 3094, loss 0.341654, acc 0.90625, learning_rate 0.000100016
2017-10-10T13:15:34.274164: step 3095, loss 0.610891, acc 0.78125, learning_rate 0.000100016
2017-10-10T13:15:34.806579: step 3096, loss 0.509361, acc 0.8125, learning_rate 0.000100016
2017-10-10T13:15:35.232897: step 3097, loss 0.372443, acc 0.921875, learning_rate 0.000100016
2017-10-10T13:15:35.699969: step 3098, loss 0.416249, acc 0.828125, learning_rate 0.000100015
2017-10-10T13:15:36.204487: step 3099, loss 0.522361, acc 0.828125, learning_rate 0.000100015
2017-10-10T13:15:36.745261: step 3100, loss 0.386241, acc 0.8125, learning_rate 0.000100015
2017-10-10T13:15:37.429025: step 3101, loss 0.331625, acc 0.859375, learning_rate 0.000100015
2017-10-10T13:15:37.874763: step 3102, loss 0.203576, acc 0.90625, learning_rate 0.000100015
2017-10-10T13:15:38.312357: step 3103, loss 0.477158, acc 0.875, learning_rate 0.000100015
2017-10-10T13:15:38.852592: step 3104, loss 0.223732, acc 0.921875, learning_rate 0.000100015
2017-10-10T13:15:39.408864: step 3105, loss 0.411038, acc 0.875, learning_rate 0.000100015
2017-10-10T13:15:39.928167: step 3106, loss 0.360023, acc 0.875, learning_rate 0.000100015
2017-10-10T13:15:40.457539: step 3107, loss 0.458983, acc 0.84375, learning_rate 0.000100015
2017-10-10T13:15:40.976915: step 3108, loss 0.239442, acc 0.9375, learning_rate 0.000100015
2017-10-10T13:15:41.493223: step 3109, loss 0.363891, acc 0.84375, learning_rate 0.000100015
2017-10-10T13:15:42.019529: step 3110, loss 0.268175, acc 0.921875, learning_rate 0.000100015
2017-10-10T13:15:42.513999: step 3111, loss 0.49616, acc 0.875, learning_rate 0.000100015
2017-10-10T13:15:43.052870: step 3112, loss 0.351547, acc 0.84375, learning_rate 0.000100015
2017-10-10T13:15:43.582355: step 3113, loss 0.342379, acc 0.90625, learning_rate 0.000100015
2017-10-10T13:15:44.112675: step 3114, loss 0.286187, acc 0.90625, learning_rate 0.000100014
2017-10-10T13:15:44.684844: step 3115, loss 0.534849, acc 0.828125, learning_rate 0.000100014
2017-10-10T13:15:45.268826: step 3116, loss 0.359294, acc 0.875, learning_rate 0.000100014
2017-10-10T13:15:45.801912: step 3117, loss 0.494196, acc 0.8125, learning_rate 0.000100014
2017-10-10T13:15:46.356898: step 3118, loss 0.3308, acc 0.84375, learning_rate 0.000100014
2017-10-10T13:15:46.910751: step 3119, loss 0.271223, acc 0.890625, learning_rate 0.000100014
2017-10-10T13:15:47.417331: step 3120, loss 0.612025, acc 0.796875, learning_rate 0.000100014

Evaluation:
2017-10-10T13:15:48.540935: step 3120, loss 0.27159, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3120

2017-10-10T13:15:49.912981: step 3121, loss 0.278829, acc 0.921875, learning_rate 0.000100014
2017-10-10T13:15:50.333245: step 3122, loss 0.238042, acc 0.921875, learning_rate 0.000100014
2017-10-10T13:15:50.825352: step 3123, loss 0.330803, acc 0.890625, learning_rate 0.000100014
2017-10-10T13:15:51.296123: step 3124, loss 0.396463, acc 0.84375, learning_rate 0.000100014
2017-10-10T13:15:51.817128: step 3125, loss 0.262349, acc 0.921875, learning_rate 0.000100014
2017-10-10T13:15:52.390632: step 3126, loss 0.315822, acc 0.921875, learning_rate 0.000100014
2017-10-10T13:15:52.905076: step 3127, loss 0.335667, acc 0.859375, learning_rate 0.000100014
2017-10-10T13:15:53.368906: step 3128, loss 0.356418, acc 0.875, learning_rate 0.000100014
2017-10-10T13:15:53.951192: step 3129, loss 0.368924, acc 0.90625, learning_rate 0.000100014
2017-10-10T13:15:54.553009: step 3130, loss 0.238478, acc 0.9375, learning_rate 0.000100014
2017-10-10T13:15:55.187334: step 3131, loss 0.232088, acc 0.953125, learning_rate 0.000100014
2017-10-10T13:15:55.628580: step 3132, loss 0.33731, acc 0.921875, learning_rate 0.000100013
2017-10-10T13:15:56.057171: step 3133, loss 0.327146, acc 0.890625, learning_rate 0.000100013
2017-10-10T13:15:56.589123: step 3134, loss 0.358961, acc 0.90625, learning_rate 0.000100013
2017-10-10T13:15:57.177155: step 3135, loss 0.241366, acc 0.90625, learning_rate 0.000100013
2017-10-10T13:15:57.564907: step 3136, loss 0.469556, acc 0.882353, learning_rate 0.000100013
2017-10-10T13:15:58.184986: step 3137, loss 0.372705, acc 0.875, learning_rate 0.000100013
2017-10-10T13:15:58.735485: step 3138, loss 0.366731, acc 0.84375, learning_rate 0.000100013
2017-10-10T13:15:59.218709: step 3139, loss 0.437076, acc 0.875, learning_rate 0.000100013
2017-10-10T13:15:59.715837: step 3140, loss 0.336136, acc 0.859375, learning_rate 0.000100013
2017-10-10T13:16:00.265916: step 3141, loss 0.37043, acc 0.859375, learning_rate 0.000100013
2017-10-10T13:16:00.798747: step 3142, loss 0.345665, acc 0.890625, learning_rate 0.000100013
2017-10-10T13:16:01.285440: step 3143, loss 0.226791, acc 0.90625, learning_rate 0.000100013
2017-10-10T13:16:01.744154: step 3144, loss 0.628129, acc 0.8125, learning_rate 0.000100013
2017-10-10T13:16:02.259302: step 3145, loss 0.212225, acc 0.9375, learning_rate 0.000100013
2017-10-10T13:16:02.744885: step 3146, loss 0.163473, acc 0.984375, learning_rate 0.000100013
2017-10-10T13:16:03.304840: step 3147, loss 0.447816, acc 0.859375, learning_rate 0.000100013
2017-10-10T13:16:03.820553: step 3148, loss 0.243387, acc 0.90625, learning_rate 0.000100013
2017-10-10T13:16:04.368853: step 3149, loss 0.498064, acc 0.859375, learning_rate 0.000100013
2017-10-10T13:16:04.925997: step 3150, loss 0.428184, acc 0.90625, learning_rate 0.000100012
2017-10-10T13:16:05.497894: step 3151, loss 0.287842, acc 0.921875, learning_rate 0.000100012
2017-10-10T13:16:06.042743: step 3152, loss 0.314824, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:16:06.608031: step 3153, loss 0.327948, acc 0.9375, learning_rate 0.000100012
2017-10-10T13:16:07.197022: step 3154, loss 0.393146, acc 0.875, learning_rate 0.000100012
2017-10-10T13:16:07.718767: step 3155, loss 0.330795, acc 0.875, learning_rate 0.000100012
2017-10-10T13:16:08.221630: step 3156, loss 0.325684, acc 0.875, learning_rate 0.000100012
2017-10-10T13:16:08.767569: step 3157, loss 0.365078, acc 0.828125, learning_rate 0.000100012
2017-10-10T13:16:09.324850: step 3158, loss 0.461643, acc 0.84375, learning_rate 0.000100012
2017-10-10T13:16:09.868452: step 3159, loss 0.485198, acc 0.828125, learning_rate 0.000100012
2017-10-10T13:16:10.428266: step 3160, loss 0.396621, acc 0.859375, learning_rate 0.000100012

Evaluation:
2017-10-10T13:16:11.672887: step 3160, loss 0.271059, acc 0.896403

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3160

2017-10-10T13:16:13.236527: step 3161, loss 0.558984, acc 0.8125, learning_rate 0.000100012
2017-10-10T13:16:13.768186: step 3162, loss 0.359412, acc 0.875, learning_rate 0.000100012
2017-10-10T13:16:14.288368: step 3163, loss 0.336538, acc 0.90625, learning_rate 0.000100012
2017-10-10T13:16:14.772513: step 3164, loss 0.280702, acc 0.90625, learning_rate 0.000100012
2017-10-10T13:16:15.291486: step 3165, loss 0.354115, acc 0.859375, learning_rate 0.000100012
2017-10-10T13:16:15.795914: step 3166, loss 0.366047, acc 0.859375, learning_rate 0.000100012
2017-10-10T13:16:16.294236: step 3167, loss 0.47091, acc 0.875, learning_rate 0.000100012
2017-10-10T13:16:16.840833: step 3168, loss 0.469152, acc 0.84375, learning_rate 0.000100012
2017-10-10T13:16:17.387283: step 3169, loss 0.60567, acc 0.78125, learning_rate 0.000100012
2017-10-10T13:16:17.992995: step 3170, loss 0.195188, acc 0.953125, learning_rate 0.000100012
2017-10-10T13:16:18.488034: step 3171, loss 0.29207, acc 0.890625, learning_rate 0.000100011
2017-10-10T13:16:18.919251: step 3172, loss 0.317824, acc 0.90625, learning_rate 0.000100011
2017-10-10T13:16:19.363247: step 3173, loss 0.421205, acc 0.84375, learning_rate 0.000100011
2017-10-10T13:16:19.930786: step 3174, loss 0.451281, acc 0.90625, learning_rate 0.000100011
2017-10-10T13:16:20.463311: step 3175, loss 0.272936, acc 0.953125, learning_rate 0.000100011
2017-10-10T13:16:21.012047: step 3176, loss 0.361141, acc 0.890625, learning_rate 0.000100011
2017-10-10T13:16:21.704950: step 3177, loss 0.149349, acc 0.984375, learning_rate 0.000100011
2017-10-10T13:16:22.242805: step 3178, loss 0.433666, acc 0.796875, learning_rate 0.000100011
2017-10-10T13:16:22.701125: step 3179, loss 0.464526, acc 0.921875, learning_rate 0.000100011
2017-10-10T13:16:23.184015: step 3180, loss 0.367646, acc 0.890625, learning_rate 0.000100011
2017-10-10T13:16:23.783842: step 3181, loss 0.289092, acc 0.890625, learning_rate 0.000100011
2017-10-10T13:16:24.243649: step 3182, loss 0.268759, acc 0.90625, learning_rate 0.000100011
2017-10-10T13:16:24.684020: step 3183, loss 0.256628, acc 0.90625, learning_rate 0.000100011
2017-10-10T13:16:25.248857: step 3184, loss 0.462316, acc 0.84375, learning_rate 0.000100011
2017-10-10T13:16:25.805086: step 3185, loss 0.306286, acc 0.90625, learning_rate 0.000100011
2017-10-10T13:16:26.368936: step 3186, loss 0.271359, acc 0.921875, learning_rate 0.000100011
2017-10-10T13:16:26.844891: step 3187, loss 0.364662, acc 0.9375, learning_rate 0.000100011
2017-10-10T13:16:27.396852: step 3188, loss 0.403313, acc 0.859375, learning_rate 0.000100011
2017-10-10T13:16:27.905117: step 3189, loss 0.383257, acc 0.890625, learning_rate 0.000100011
2017-10-10T13:16:28.436412: step 3190, loss 0.338155, acc 0.90625, learning_rate 0.000100011
2017-10-10T13:16:28.972871: step 3191, loss 0.436756, acc 0.859375, learning_rate 0.000100011
2017-10-10T13:16:29.532897: step 3192, loss 0.399525, acc 0.890625, learning_rate 0.000100011
2017-10-10T13:16:30.045046: step 3193, loss 0.434598, acc 0.828125, learning_rate 0.00010001
2017-10-10T13:16:30.593285: step 3194, loss 0.371029, acc 0.859375, learning_rate 0.00010001
2017-10-10T13:16:31.129751: step 3195, loss 0.613863, acc 0.875, learning_rate 0.00010001
2017-10-10T13:16:31.637074: step 3196, loss 0.402564, acc 0.84375, learning_rate 0.00010001
2017-10-10T13:16:32.167265: step 3197, loss 0.455342, acc 0.859375, learning_rate 0.00010001
2017-10-10T13:16:32.612884: step 3198, loss 0.258143, acc 0.875, learning_rate 0.00010001
2017-10-10T13:16:33.138746: step 3199, loss 0.233386, acc 0.90625, learning_rate 0.00010001
2017-10-10T13:16:33.591061: step 3200, loss 0.282232, acc 0.9375, learning_rate 0.00010001

Evaluation:
2017-10-10T13:16:34.973974: step 3200, loss 0.27047, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3200

2017-10-10T13:16:36.688134: step 3201, loss 0.453603, acc 0.875, learning_rate 0.00010001
2017-10-10T13:16:37.216871: step 3202, loss 0.270137, acc 0.921875, learning_rate 0.00010001
2017-10-10T13:16:37.731001: step 3203, loss 0.250803, acc 0.890625, learning_rate 0.00010001
2017-10-10T13:16:38.276958: step 3204, loss 0.462916, acc 0.890625, learning_rate 0.00010001
2017-10-10T13:16:38.849096: step 3205, loss 0.281428, acc 0.890625, learning_rate 0.00010001
2017-10-10T13:16:39.417133: step 3206, loss 0.342769, acc 0.84375, learning_rate 0.00010001
2017-10-10T13:16:39.965063: step 3207, loss 0.42372, acc 0.890625, learning_rate 0.00010001
2017-10-10T13:16:40.481740: step 3208, loss 0.462349, acc 0.859375, learning_rate 0.00010001
2017-10-10T13:16:41.060856: step 3209, loss 0.433188, acc 0.8125, learning_rate 0.00010001
2017-10-10T13:16:41.640864: step 3210, loss 0.283501, acc 0.890625, learning_rate 0.00010001
2017-10-10T13:16:42.241768: step 3211, loss 0.339649, acc 0.890625, learning_rate 0.00010001
2017-10-10T13:16:42.642781: step 3212, loss 0.304129, acc 0.90625, learning_rate 0.00010001
2017-10-10T13:16:43.093859: step 3213, loss 0.311924, acc 0.9375, learning_rate 0.00010001
2017-10-10T13:16:43.612852: step 3214, loss 0.54036, acc 0.796875, learning_rate 0.00010001
2017-10-10T13:16:44.152960: step 3215, loss 0.304789, acc 0.875, learning_rate 0.00010001
2017-10-10T13:16:44.666445: step 3216, loss 0.285624, acc 0.921875, learning_rate 0.00010001
2017-10-10T13:16:45.268874: step 3217, loss 0.410574, acc 0.84375, learning_rate 0.000100009
2017-10-10T13:16:45.889053: step 3218, loss 0.248484, acc 0.953125, learning_rate 0.000100009
2017-10-10T13:16:46.436946: step 3219, loss 0.326411, acc 0.875, learning_rate 0.000100009
2017-10-10T13:16:46.795118: step 3220, loss 0.263769, acc 0.890625, learning_rate 0.000100009
2017-10-10T13:16:47.152843: step 3221, loss 0.452004, acc 0.875, learning_rate 0.000100009
2017-10-10T13:16:47.582729: step 3222, loss 0.258914, acc 0.921875, learning_rate 0.000100009
2017-10-10T13:16:48.136863: step 3223, loss 0.339394, acc 0.90625, learning_rate 0.000100009
2017-10-10T13:16:48.673291: step 3224, loss 0.27384, acc 0.921875, learning_rate 0.000100009
2017-10-10T13:16:49.136897: step 3225, loss 0.485851, acc 0.890625, learning_rate 0.000100009
2017-10-10T13:16:49.680928: step 3226, loss 0.294671, acc 0.921875, learning_rate 0.000100009
2017-10-10T13:16:50.273008: step 3227, loss 0.397603, acc 0.84375, learning_rate 0.000100009
2017-10-10T13:16:50.857187: step 3228, loss 0.348212, acc 0.890625, learning_rate 0.000100009
2017-10-10T13:16:51.351945: step 3229, loss 0.24603, acc 0.890625, learning_rate 0.000100009
2017-10-10T13:16:51.927137: step 3230, loss 0.712459, acc 0.8125, learning_rate 0.000100009
2017-10-10T13:16:52.436893: step 3231, loss 0.346289, acc 0.90625, learning_rate 0.000100009
2017-10-10T13:16:52.957427: step 3232, loss 0.404431, acc 0.875, learning_rate 0.000100009
2017-10-10T13:16:53.485018: step 3233, loss 0.37352, acc 0.84375, learning_rate 0.000100009
2017-10-10T13:16:53.956889: step 3234, loss 0.329271, acc 0.901961, learning_rate 0.000100009
2017-10-10T13:16:54.488960: step 3235, loss 0.281371, acc 0.90625, learning_rate 0.000100009
2017-10-10T13:16:54.991052: step 3236, loss 0.475832, acc 0.84375, learning_rate 0.000100009
2017-10-10T13:16:55.528311: step 3237, loss 0.247785, acc 0.890625, learning_rate 0.000100009
2017-10-10T13:16:56.048937: step 3238, loss 0.234978, acc 0.90625, learning_rate 0.000100009
2017-10-10T13:16:56.529011: step 3239, loss 0.262908, acc 0.90625, learning_rate 0.000100009
2017-10-10T13:16:57.110920: step 3240, loss 0.391054, acc 0.875, learning_rate 0.000100009

Evaluation:
2017-10-10T13:16:58.356871: step 3240, loss 0.270413, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3240

2017-10-10T13:16:59.850969: step 3241, loss 0.421372, acc 0.875, learning_rate 0.000100009
2017-10-10T13:17:00.338424: step 3242, loss 0.346091, acc 0.90625, learning_rate 0.000100009
2017-10-10T13:17:00.872916: step 3243, loss 0.335273, acc 0.890625, learning_rate 0.000100009
2017-10-10T13:17:01.424891: step 3244, loss 0.299782, acc 0.921875, learning_rate 0.000100009
2017-10-10T13:17:01.949969: step 3245, loss 0.352412, acc 0.859375, learning_rate 0.000100008
2017-10-10T13:17:02.455880: step 3246, loss 0.41224, acc 0.84375, learning_rate 0.000100008
2017-10-10T13:17:03.002874: step 3247, loss 0.304795, acc 0.90625, learning_rate 0.000100008
2017-10-10T13:17:03.452888: step 3248, loss 0.410022, acc 0.890625, learning_rate 0.000100008
2017-10-10T13:17:04.207385: step 3249, loss 0.333021, acc 0.90625, learning_rate 0.000100008
2017-10-10T13:17:04.833128: step 3250, loss 0.171769, acc 0.953125, learning_rate 0.000100008
2017-10-10T13:17:05.305126: step 3251, loss 0.411439, acc 0.859375, learning_rate 0.000100008
2017-10-10T13:17:05.677833: step 3252, loss 0.421647, acc 0.84375, learning_rate 0.000100008
2017-10-10T13:17:06.112815: step 3253, loss 0.339626, acc 0.859375, learning_rate 0.000100008
2017-10-10T13:17:06.676859: step 3254, loss 0.602688, acc 0.765625, learning_rate 0.000100008
2017-10-10T13:17:07.266960: step 3255, loss 0.379772, acc 0.875, learning_rate 0.000100008
2017-10-10T13:17:07.820956: step 3256, loss 0.335126, acc 0.859375, learning_rate 0.000100008
2017-10-10T13:17:08.404931: step 3257, loss 0.444366, acc 0.890625, learning_rate 0.000100008
2017-10-10T13:17:08.975592: step 3258, loss 0.290403, acc 0.9375, learning_rate 0.000100008
2017-10-10T13:17:09.529240: step 3259, loss 0.354475, acc 0.859375, learning_rate 0.000100008
2017-10-10T13:17:10.009241: step 3260, loss 0.346841, acc 0.859375, learning_rate 0.000100008
2017-10-10T13:17:10.465484: step 3261, loss 0.319598, acc 0.90625, learning_rate 0.000100008
2017-10-10T13:17:10.943377: step 3262, loss 0.426726, acc 0.875, learning_rate 0.000100008
2017-10-10T13:17:11.481113: step 3263, loss 0.403644, acc 0.84375, learning_rate 0.000100008
2017-10-10T13:17:12.017077: step 3264, loss 0.380401, acc 0.84375, learning_rate 0.000100008
2017-10-10T13:17:12.524600: step 3265, loss 0.522472, acc 0.859375, learning_rate 0.000100008
2017-10-10T13:17:13.000856: step 3266, loss 0.209117, acc 0.9375, learning_rate 0.000100008
2017-10-10T13:17:13.608833: step 3267, loss 0.417196, acc 0.765625, learning_rate 0.000100008
2017-10-10T13:17:14.104936: step 3268, loss 0.323276, acc 0.875, learning_rate 0.000100008
2017-10-10T13:17:14.665184: step 3269, loss 0.374844, acc 0.890625, learning_rate 0.000100008
2017-10-10T13:17:15.163090: step 3270, loss 0.542358, acc 0.84375, learning_rate 0.000100008
2017-10-10T13:17:15.719358: step 3271, loss 0.331172, acc 0.875, learning_rate 0.000100008
2017-10-10T13:17:16.194567: step 3272, loss 0.303123, acc 0.90625, learning_rate 0.000100008
2017-10-10T13:17:16.732968: step 3273, loss 0.240696, acc 0.9375, learning_rate 0.000100008
2017-10-10T13:17:17.273955: step 3274, loss 0.324759, acc 0.875, learning_rate 0.000100008
2017-10-10T13:17:17.795728: step 3275, loss 0.2822, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:17:18.325108: step 3276, loss 0.27756, acc 0.890625, learning_rate 0.000100007
2017-10-10T13:17:18.940947: step 3277, loss 0.411974, acc 0.890625, learning_rate 0.000100007
2017-10-10T13:17:19.469700: step 3278, loss 0.318815, acc 0.859375, learning_rate 0.000100007
2017-10-10T13:17:19.981716: step 3279, loss 0.483974, acc 0.84375, learning_rate 0.000100007
2017-10-10T13:17:20.525201: step 3280, loss 0.336808, acc 0.890625, learning_rate 0.000100007

Evaluation:
2017-10-10T13:17:21.834708: step 3280, loss 0.270929, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3280

2017-10-10T13:17:23.450065: step 3281, loss 0.315992, acc 0.90625, learning_rate 0.000100007
2017-10-10T13:17:23.951381: step 3282, loss 0.263708, acc 0.890625, learning_rate 0.000100007
2017-10-10T13:17:24.492948: step 3283, loss 0.300037, acc 0.90625, learning_rate 0.000100007
2017-10-10T13:17:25.002907: step 3284, loss 0.457594, acc 0.859375, learning_rate 0.000100007
2017-10-10T13:17:25.549349: step 3285, loss 0.442576, acc 0.890625, learning_rate 0.000100007
2017-10-10T13:17:26.065608: step 3286, loss 0.533574, acc 0.890625, learning_rate 0.000100007
2017-10-10T13:17:26.593202: step 3287, loss 0.32699, acc 0.90625, learning_rate 0.000100007
2017-10-10T13:17:27.178760: step 3288, loss 0.36331, acc 0.875, learning_rate 0.000100007
2017-10-10T13:17:27.708857: step 3289, loss 0.296789, acc 0.875, learning_rate 0.000100007
2017-10-10T13:17:28.340785: step 3290, loss 0.309319, acc 0.890625, learning_rate 0.000100007
2017-10-10T13:17:28.779165: step 3291, loss 0.356216, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:17:29.238676: step 3292, loss 0.501886, acc 0.859375, learning_rate 0.000100007
2017-10-10T13:17:29.785008: step 3293, loss 0.387437, acc 0.890625, learning_rate 0.000100007
2017-10-10T13:17:30.329160: step 3294, loss 0.4061, acc 0.859375, learning_rate 0.000100007
2017-10-10T13:17:30.857080: step 3295, loss 0.361049, acc 0.890625, learning_rate 0.000100007
2017-10-10T13:17:31.368162: step 3296, loss 0.267869, acc 0.875, learning_rate 0.000100007
2017-10-10T13:17:32.038153: step 3297, loss 0.418365, acc 0.875, learning_rate 0.000100007
2017-10-10T13:17:32.569234: step 3298, loss 0.417284, acc 0.8125, learning_rate 0.000100007
2017-10-10T13:17:32.963018: step 3299, loss 0.439435, acc 0.890625, learning_rate 0.000100007
2017-10-10T13:17:33.279365: step 3300, loss 0.314373, acc 0.90625, learning_rate 0.000100007
2017-10-10T13:17:33.700854: step 3301, loss 0.37106, acc 0.859375, learning_rate 0.000100007
2017-10-10T13:17:34.267944: step 3302, loss 0.457244, acc 0.8125, learning_rate 0.000100007
2017-10-10T13:17:34.831881: step 3303, loss 0.341824, acc 0.890625, learning_rate 0.000100007
2017-10-10T13:17:35.390142: step 3304, loss 0.346828, acc 0.84375, learning_rate 0.000100007
2017-10-10T13:17:35.954424: step 3305, loss 0.313946, acc 0.9375, learning_rate 0.000100007
2017-10-10T13:17:36.482808: step 3306, loss 0.41366, acc 0.875, learning_rate 0.000100007
2017-10-10T13:17:36.967414: step 3307, loss 0.222111, acc 0.921875, learning_rate 0.000100007
2017-10-10T13:17:37.411740: step 3308, loss 0.277424, acc 0.90625, learning_rate 0.000100007
2017-10-10T13:17:37.856983: step 3309, loss 0.414332, acc 0.828125, learning_rate 0.000100007
2017-10-10T13:17:38.436230: step 3310, loss 0.446923, acc 0.828125, learning_rate 0.000100006
2017-10-10T13:17:38.916421: step 3311, loss 0.248991, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:17:39.393138: step 3312, loss 0.24814, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:17:39.935091: step 3313, loss 0.454466, acc 0.828125, learning_rate 0.000100006
2017-10-10T13:17:40.467022: step 3314, loss 0.450577, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:17:40.936910: step 3315, loss 0.372327, acc 0.859375, learning_rate 0.000100006
2017-10-10T13:17:41.472632: step 3316, loss 0.369525, acc 0.875, learning_rate 0.000100006
2017-10-10T13:17:42.024829: step 3317, loss 0.272951, acc 0.90625, learning_rate 0.000100006
2017-10-10T13:17:42.568862: step 3318, loss 0.366308, acc 0.859375, learning_rate 0.000100006
2017-10-10T13:17:43.106667: step 3319, loss 0.14821, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:17:43.642295: step 3320, loss 0.42549, acc 0.859375, learning_rate 0.000100006

Evaluation:
2017-10-10T13:17:45.004855: step 3320, loss 0.268454, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3320

2017-10-10T13:17:46.809046: step 3321, loss 0.306109, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:17:47.344681: step 3322, loss 0.341874, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:17:47.829117: step 3323, loss 0.345138, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:17:48.288949: step 3324, loss 0.292352, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:17:48.852381: step 3325, loss 0.311055, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:17:49.404997: step 3326, loss 0.55317, acc 0.828125, learning_rate 0.000100006
2017-10-10T13:17:49.948972: step 3327, loss 0.346649, acc 0.859375, learning_rate 0.000100006
2017-10-10T13:17:50.380878: step 3328, loss 0.218972, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:17:50.961808: step 3329, loss 0.386032, acc 0.859375, learning_rate 0.000100006
2017-10-10T13:17:51.536420: step 3330, loss 0.612443, acc 0.765625, learning_rate 0.000100006
2017-10-10T13:17:51.947931: step 3331, loss 0.493078, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:17:52.284340: step 3332, loss 0.525452, acc 0.784314, learning_rate 0.000100006
2017-10-10T13:17:52.752344: step 3333, loss 0.271109, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:17:53.304831: step 3334, loss 0.346789, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:17:53.866266: step 3335, loss 0.279998, acc 0.9375, learning_rate 0.000100006
2017-10-10T13:17:54.344863: step 3336, loss 0.376849, acc 0.859375, learning_rate 0.000100006
2017-10-10T13:17:54.909810: step 3337, loss 0.38669, acc 0.8125, learning_rate 0.000100006
2017-10-10T13:17:55.468935: step 3338, loss 0.429853, acc 0.859375, learning_rate 0.000100006
2017-10-10T13:17:55.879735: step 3339, loss 0.306584, acc 0.875, learning_rate 0.000100006
2017-10-10T13:17:56.236620: step 3340, loss 0.339206, acc 0.828125, learning_rate 0.000100006
2017-10-10T13:17:56.551787: step 3341, loss 0.16096, acc 0.96875, learning_rate 0.000100006
2017-10-10T13:17:57.057594: step 3342, loss 0.413407, acc 0.859375, learning_rate 0.000100006
2017-10-10T13:17:57.553365: step 3343, loss 0.342737, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:17:58.092871: step 3344, loss 0.200841, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:17:58.655769: step 3345, loss 0.212165, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:17:59.195733: step 3346, loss 0.427704, acc 0.859375, learning_rate 0.000100006
2017-10-10T13:17:59.712904: step 3347, loss 0.518437, acc 0.859375, learning_rate 0.000100006
2017-10-10T13:18:00.172888: step 3348, loss 0.32942, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:18:00.732836: step 3349, loss 0.350055, acc 0.890625, learning_rate 0.000100006
2017-10-10T13:18:01.261135: step 3350, loss 0.28072, acc 0.921875, learning_rate 0.000100006
2017-10-10T13:18:01.838349: step 3351, loss 0.401798, acc 0.875, learning_rate 0.000100005
2017-10-10T13:18:02.269081: step 3352, loss 0.598729, acc 0.8125, learning_rate 0.000100005
2017-10-10T13:18:02.848034: step 3353, loss 0.234839, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:18:03.332401: step 3354, loss 0.384652, acc 0.8125, learning_rate 0.000100005
2017-10-10T13:18:03.783625: step 3355, loss 0.207426, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:18:04.348639: step 3356, loss 0.358146, acc 0.875, learning_rate 0.000100005
2017-10-10T13:18:04.929302: step 3357, loss 0.396906, acc 0.890625, learning_rate 0.000100005
2017-10-10T13:18:05.415176: step 3358, loss 0.370093, acc 0.875, learning_rate 0.000100005
2017-10-10T13:18:05.965613: step 3359, loss 0.277791, acc 0.890625, learning_rate 0.000100005
2017-10-10T13:18:06.495000: step 3360, loss 0.308582, acc 0.9375, learning_rate 0.000100005

Evaluation:
2017-10-10T13:18:07.755840: step 3360, loss 0.267574, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3360

2017-10-10T13:18:09.184845: step 3361, loss 0.382275, acc 0.875, learning_rate 0.000100005
2017-10-10T13:18:09.692864: step 3362, loss 0.336111, acc 0.890625, learning_rate 0.000100005
2017-10-10T13:18:10.253728: step 3363, loss 0.249776, acc 0.890625, learning_rate 0.000100005
2017-10-10T13:18:10.769812: step 3364, loss 0.276948, acc 0.9375, learning_rate 0.000100005
2017-10-10T13:18:11.343913: step 3365, loss 0.301937, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:18:11.854863: step 3366, loss 0.236531, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:18:12.417668: step 3367, loss 0.415109, acc 0.84375, learning_rate 0.000100005
2017-10-10T13:18:12.988817: step 3368, loss 0.329813, acc 0.875, learning_rate 0.000100005
2017-10-10T13:18:13.536054: step 3369, loss 0.205749, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:18:14.131383: step 3370, loss 0.306955, acc 0.890625, learning_rate 0.000100005
2017-10-10T13:18:14.740058: step 3371, loss 0.479627, acc 0.859375, learning_rate 0.000100005
2017-10-10T13:18:15.200895: step 3372, loss 0.271914, acc 0.890625, learning_rate 0.000100005
2017-10-10T13:18:15.656649: step 3373, loss 0.539379, acc 0.84375, learning_rate 0.000100005
2017-10-10T13:18:16.240923: step 3374, loss 0.341389, acc 0.875, learning_rate 0.000100005
2017-10-10T13:18:16.684870: step 3375, loss 0.306105, acc 0.890625, learning_rate 0.000100005
2017-10-10T13:18:17.176868: step 3376, loss 0.357023, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:18:17.688864: step 3377, loss 0.320807, acc 0.890625, learning_rate 0.000100005
2017-10-10T13:18:18.289207: step 3378, loss 0.327708, acc 0.859375, learning_rate 0.000100005
2017-10-10T13:18:18.807328: step 3379, loss 0.334823, acc 0.828125, learning_rate 0.000100005
2017-10-10T13:18:19.292323: step 3380, loss 0.428187, acc 0.890625, learning_rate 0.000100005
2017-10-10T13:18:19.732843: step 3381, loss 0.409486, acc 0.875, learning_rate 0.000100005
2017-10-10T13:18:20.196824: step 3382, loss 0.349988, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:18:20.721784: step 3383, loss 0.414091, acc 0.890625, learning_rate 0.000100005
2017-10-10T13:18:21.240877: step 3384, loss 0.281064, acc 0.875, learning_rate 0.000100005
2017-10-10T13:18:21.817048: step 3385, loss 0.436675, acc 0.84375, learning_rate 0.000100005
2017-10-10T13:18:22.384400: step 3386, loss 0.433605, acc 0.875, learning_rate 0.000100005
2017-10-10T13:18:22.953973: step 3387, loss 0.27845, acc 0.921875, learning_rate 0.000100005
2017-10-10T13:18:23.524826: step 3388, loss 0.323596, acc 0.859375, learning_rate 0.000100005
2017-10-10T13:18:24.088367: step 3389, loss 0.499419, acc 0.875, learning_rate 0.000100005
2017-10-10T13:18:24.622475: step 3390, loss 0.404605, acc 0.875, learning_rate 0.000100005
2017-10-10T13:18:25.160715: step 3391, loss 0.28863, acc 0.90625, learning_rate 0.000100005
2017-10-10T13:18:25.725000: step 3392, loss 0.314329, acc 0.875, learning_rate 0.000100005
2017-10-10T13:18:26.212950: step 3393, loss 0.422746, acc 0.828125, learning_rate 0.000100005
2017-10-10T13:18:26.755994: step 3394, loss 0.38551, acc 0.84375, learning_rate 0.000100005
2017-10-10T13:18:27.253166: step 3395, loss 0.22665, acc 0.90625, learning_rate 0.000100005
2017-10-10T13:18:27.717947: step 3396, loss 0.397663, acc 0.8125, learning_rate 0.000100005
2017-10-10T13:18:28.278752: step 3397, loss 0.418018, acc 0.875, learning_rate 0.000100005
2017-10-10T13:18:28.774361: step 3398, loss 0.42343, acc 0.875, learning_rate 0.000100005
2017-10-10T13:18:29.313185: step 3399, loss 0.281921, acc 0.859375, learning_rate 0.000100005
2017-10-10T13:18:29.870474: step 3400, loss 0.524027, acc 0.828125, learning_rate 0.000100004

Evaluation:
2017-10-10T13:18:31.193971: step 3400, loss 0.268146, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3400

2017-10-10T13:18:32.840121: step 3401, loss 0.390171, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:18:33.396197: step 3402, loss 0.534626, acc 0.796875, learning_rate 0.000100004
2017-10-10T13:18:33.952262: step 3403, loss 0.284287, acc 0.859375, learning_rate 0.000100004
2017-10-10T13:18:34.592605: step 3404, loss 0.331924, acc 0.875, learning_rate 0.000100004
2017-10-10T13:18:35.161248: step 3405, loss 0.285106, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:18:35.742031: step 3406, loss 0.338632, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:18:36.224703: step 3407, loss 0.443193, acc 0.859375, learning_rate 0.000100004
2017-10-10T13:18:36.719754: step 3408, loss 0.229416, acc 0.953125, learning_rate 0.000100004
2017-10-10T13:18:37.282797: step 3409, loss 0.256431, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:18:37.915153: step 3410, loss 0.408469, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:18:38.400976: step 3411, loss 0.373048, acc 0.859375, learning_rate 0.000100004
2017-10-10T13:18:38.885965: step 3412, loss 0.313188, acc 0.84375, learning_rate 0.000100004
2017-10-10T13:18:39.438718: step 3413, loss 0.234991, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:18:39.897486: step 3414, loss 0.467836, acc 0.859375, learning_rate 0.000100004
2017-10-10T13:18:40.456950: step 3415, loss 0.299833, acc 0.875, learning_rate 0.000100004
2017-10-10T13:18:41.101045: step 3416, loss 0.38182, acc 0.859375, learning_rate 0.000100004
2017-10-10T13:18:41.566470: step 3417, loss 0.295346, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:18:42.068928: step 3418, loss 0.415857, acc 0.84375, learning_rate 0.000100004
2017-10-10T13:18:42.615792: step 3419, loss 0.338337, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:18:43.036850: step 3420, loss 0.326457, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:18:43.486556: step 3421, loss 0.347879, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:18:44.024696: step 3422, loss 0.215668, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:18:44.597793: step 3423, loss 0.26523, acc 0.875, learning_rate 0.000100004
2017-10-10T13:18:45.151039: step 3424, loss 0.325194, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:18:45.708928: step 3425, loss 0.616794, acc 0.75, learning_rate 0.000100004
2017-10-10T13:18:46.292999: step 3426, loss 0.418084, acc 0.84375, learning_rate 0.000100004
2017-10-10T13:18:46.880206: step 3427, loss 0.253337, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:18:47.426808: step 3428, loss 0.26106, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:18:47.968873: step 3429, loss 0.392968, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:18:48.380290: step 3430, loss 0.488917, acc 0.901961, learning_rate 0.000100004
2017-10-10T13:18:48.866670: step 3431, loss 0.200622, acc 0.96875, learning_rate 0.000100004
2017-10-10T13:18:49.325164: step 3432, loss 0.288598, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:18:49.901240: step 3433, loss 0.42476, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:18:50.408062: step 3434, loss 0.281545, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:18:50.932335: step 3435, loss 0.443897, acc 0.875, learning_rate 0.000100004
2017-10-10T13:18:51.396846: step 3436, loss 0.28514, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:18:51.968923: step 3437, loss 0.345596, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:18:52.642242: step 3438, loss 0.545594, acc 0.828125, learning_rate 0.000100004
2017-10-10T13:18:53.174375: step 3439, loss 0.246615, acc 0.9375, learning_rate 0.000100004
2017-10-10T13:18:53.674878: step 3440, loss 0.342858, acc 0.921875, learning_rate 0.000100004

Evaluation:
2017-10-10T13:18:54.957233: step 3440, loss 0.26846, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3440

2017-10-10T13:18:56.652862: step 3441, loss 0.449134, acc 0.84375, learning_rate 0.000100004
2017-10-10T13:18:57.233847: step 3442, loss 0.410445, acc 0.828125, learning_rate 0.000100004
2017-10-10T13:18:57.712864: step 3443, loss 0.311746, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:18:58.194540: step 3444, loss 0.358133, acc 0.875, learning_rate 0.000100004
2017-10-10T13:18:58.780102: step 3445, loss 0.323724, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:18:59.267949: step 3446, loss 0.452306, acc 0.875, learning_rate 0.000100004
2017-10-10T13:18:59.727984: step 3447, loss 0.387871, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:19:00.401260: step 3448, loss 0.336256, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:19:00.883972: step 3449, loss 0.273823, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:19:01.320879: step 3450, loss 0.454318, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:19:01.767272: step 3451, loss 0.130355, acc 0.984375, learning_rate 0.000100004
2017-10-10T13:19:02.336849: step 3452, loss 0.293067, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:19:02.916866: step 3453, loss 0.297916, acc 0.875, learning_rate 0.000100004
2017-10-10T13:19:03.474552: step 3454, loss 0.317126, acc 0.875, learning_rate 0.000100004
2017-10-10T13:19:03.955773: step 3455, loss 0.342243, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:19:04.369138: step 3456, loss 0.433697, acc 0.875, learning_rate 0.000100004
2017-10-10T13:19:04.930232: step 3457, loss 0.30193, acc 0.90625, learning_rate 0.000100004
2017-10-10T13:19:05.559819: step 3458, loss 0.295281, acc 0.890625, learning_rate 0.000100004
2017-10-10T13:19:06.039449: step 3459, loss 0.16903, acc 0.921875, learning_rate 0.000100004
2017-10-10T13:19:06.508960: step 3460, loss 0.359262, acc 0.84375, learning_rate 0.000100004
2017-10-10T13:19:06.969112: step 3461, loss 0.399941, acc 0.828125, learning_rate 0.000100004
2017-10-10T13:19:07.507765: step 3462, loss 0.377279, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:19:08.063964: step 3463, loss 0.364609, acc 0.84375, learning_rate 0.000100003
2017-10-10T13:19:08.605086: step 3464, loss 0.346393, acc 0.875, learning_rate 0.000100003
2017-10-10T13:19:09.176142: step 3465, loss 0.372812, acc 0.84375, learning_rate 0.000100003
2017-10-10T13:19:09.728338: step 3466, loss 0.600361, acc 0.859375, learning_rate 0.000100003
2017-10-10T13:19:10.257125: step 3467, loss 0.432999, acc 0.8125, learning_rate 0.000100003
2017-10-10T13:19:10.816956: step 3468, loss 0.298988, acc 0.875, learning_rate 0.000100003
2017-10-10T13:19:11.444843: step 3469, loss 0.171913, acc 0.96875, learning_rate 0.000100003
2017-10-10T13:19:11.909295: step 3470, loss 0.218832, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:19:12.430930: step 3471, loss 0.398117, acc 0.875, learning_rate 0.000100003
2017-10-10T13:19:12.997394: step 3472, loss 0.311971, acc 0.859375, learning_rate 0.000100003
2017-10-10T13:19:13.525340: step 3473, loss 0.369971, acc 0.84375, learning_rate 0.000100003
2017-10-10T13:19:14.132929: step 3474, loss 0.4464, acc 0.859375, learning_rate 0.000100003
2017-10-10T13:19:14.669567: step 3475, loss 0.366627, acc 0.875, learning_rate 0.000100003
2017-10-10T13:19:15.204891: step 3476, loss 0.239992, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:19:15.709036: step 3477, loss 0.347159, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:19:16.253536: step 3478, loss 0.257952, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:19:16.705055: step 3479, loss 0.197019, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:19:17.293150: step 3480, loss 0.388545, acc 0.890625, learning_rate 0.000100003

Evaluation:
2017-10-10T13:19:18.609649: step 3480, loss 0.267724, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3480

2017-10-10T13:19:20.324964: step 3481, loss 0.316799, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:20.881235: step 3482, loss 0.379784, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:19:21.409174: step 3483, loss 0.272425, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:19:21.965118: step 3484, loss 0.236822, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:22.541204: step 3485, loss 0.286466, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:23.100971: step 3486, loss 0.252589, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:23.653005: step 3487, loss 0.254602, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:24.242382: step 3488, loss 0.330327, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:19:24.654329: step 3489, loss 0.253494, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:19:25.065059: step 3490, loss 0.28636, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:25.758907: step 3491, loss 0.390643, acc 0.84375, learning_rate 0.000100003
2017-10-10T13:19:26.236414: step 3492, loss 0.271883, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:19:26.661220: step 3493, loss 0.272237, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:19:27.116306: step 3494, loss 0.362961, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:19:27.678902: step 3495, loss 0.434377, acc 0.84375, learning_rate 0.000100003
2017-10-10T13:19:28.224878: step 3496, loss 0.3622, acc 0.875, learning_rate 0.000100003
2017-10-10T13:19:28.792937: step 3497, loss 0.221741, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:19:29.297249: step 3498, loss 0.813877, acc 0.765625, learning_rate 0.000100003
2017-10-10T13:19:29.755827: step 3499, loss 0.371272, acc 0.875, learning_rate 0.000100003
2017-10-10T13:19:30.201990: step 3500, loss 0.534473, acc 0.8125, learning_rate 0.000100003
2017-10-10T13:19:30.770358: step 3501, loss 0.442763, acc 0.8125, learning_rate 0.000100003
2017-10-10T13:19:31.296953: step 3502, loss 0.436792, acc 0.828125, learning_rate 0.000100003
2017-10-10T13:19:31.802726: step 3503, loss 0.309583, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:19:32.316209: step 3504, loss 0.222353, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:19:32.813061: step 3505, loss 0.368484, acc 0.875, learning_rate 0.000100003
2017-10-10T13:19:33.357128: step 3506, loss 0.353044, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:19:33.883720: step 3507, loss 0.356535, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:34.371146: step 3508, loss 0.319715, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:34.940853: step 3509, loss 0.339426, acc 0.859375, learning_rate 0.000100003
2017-10-10T13:19:35.473482: step 3510, loss 0.429538, acc 0.875, learning_rate 0.000100003
2017-10-10T13:19:35.977583: step 3511, loss 0.481799, acc 0.84375, learning_rate 0.000100003
2017-10-10T13:19:36.525113: step 3512, loss 0.339896, acc 0.875, learning_rate 0.000100003
2017-10-10T13:19:37.022373: step 3513, loss 0.313338, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:19:37.527743: step 3514, loss 0.50114, acc 0.84375, learning_rate 0.000100003
2017-10-10T13:19:38.102554: step 3515, loss 0.45113, acc 0.8125, learning_rate 0.000100003
2017-10-10T13:19:38.622260: step 3516, loss 0.238752, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:19:39.140209: step 3517, loss 0.242021, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:39.675356: step 3518, loss 0.226821, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:40.208439: step 3519, loss 0.251865, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:40.764440: step 3520, loss 0.382597, acc 0.828125, learning_rate 0.000100003

Evaluation:
2017-10-10T13:19:42.072814: step 3520, loss 0.267323, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3520

2017-10-10T13:19:43.588525: step 3521, loss 0.371223, acc 0.84375, learning_rate 0.000100003
2017-10-10T13:19:44.060973: step 3522, loss 0.412656, acc 0.859375, learning_rate 0.000100003
2017-10-10T13:19:44.572989: step 3523, loss 0.444258, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:19:45.125333: step 3524, loss 0.31028, acc 0.875, learning_rate 0.000100003
2017-10-10T13:19:45.628967: step 3525, loss 0.475483, acc 0.828125, learning_rate 0.000100003
2017-10-10T13:19:46.238790: step 3526, loss 0.225173, acc 0.9375, learning_rate 0.000100003
2017-10-10T13:19:46.753371: step 3527, loss 0.274514, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:47.230464: step 3528, loss 0.137225, acc 0.960784, learning_rate 0.000100003
2017-10-10T13:19:47.676455: step 3529, loss 0.359827, acc 0.859375, learning_rate 0.000100003
2017-10-10T13:19:48.117783: step 3530, loss 0.198678, acc 0.921875, learning_rate 0.000100003
2017-10-10T13:19:48.671595: step 3531, loss 0.364917, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:49.220296: step 3532, loss 0.234777, acc 0.953125, learning_rate 0.000100003
2017-10-10T13:19:49.704890: step 3533, loss 0.539968, acc 0.828125, learning_rate 0.000100003
2017-10-10T13:19:50.208922: step 3534, loss 0.385357, acc 0.859375, learning_rate 0.000100003
2017-10-10T13:19:50.733111: step 3535, loss 0.304159, acc 0.875, learning_rate 0.000100003
2017-10-10T13:19:51.336910: step 3536, loss 0.266767, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:19:51.921120: step 3537, loss 0.364472, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:52.429123: step 3538, loss 0.458811, acc 0.828125, learning_rate 0.000100003
2017-10-10T13:19:52.815815: step 3539, loss 0.448083, acc 0.84375, learning_rate 0.000100003
2017-10-10T13:19:53.331046: step 3540, loss 0.308129, acc 0.90625, learning_rate 0.000100003
2017-10-10T13:19:53.833039: step 3541, loss 0.316462, acc 0.890625, learning_rate 0.000100003
2017-10-10T13:19:54.345015: step 3542, loss 0.401094, acc 0.828125, learning_rate 0.000100003
2017-10-10T13:19:54.900938: step 3543, loss 0.374204, acc 0.875, learning_rate 0.000100003
2017-10-10T13:19:55.511533: step 3544, loss 0.36253, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:19:56.080995: step 3545, loss 0.240956, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:19:56.695098: step 3546, loss 0.523726, acc 0.84375, learning_rate 0.000100002
2017-10-10T13:19:57.262262: step 3547, loss 0.347299, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:19:57.765517: step 3548, loss 0.306385, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:19:58.288945: step 3549, loss 0.335049, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:19:58.832933: step 3550, loss 0.252617, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:19:59.280636: step 3551, loss 0.331543, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:19:59.685050: step 3552, loss 0.268891, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:00.220937: step 3553, loss 0.441676, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:20:00.745335: step 3554, loss 0.264577, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:01.288849: step 3555, loss 0.323979, acc 0.875, learning_rate 0.000100002
2017-10-10T13:20:01.881407: step 3556, loss 0.239176, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:20:02.401200: step 3557, loss 0.623734, acc 0.75, learning_rate 0.000100002
2017-10-10T13:20:02.932860: step 3558, loss 0.26811, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:03.484990: step 3559, loss 0.351149, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:20:04.046440: step 3560, loss 0.323071, acc 0.9375, learning_rate 0.000100002

Evaluation:
2017-10-10T13:20:05.516827: step 3560, loss 0.267856, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3560

2017-10-10T13:20:07.076177: step 3561, loss 0.174405, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:20:07.604885: step 3562, loss 0.401741, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:20:08.141166: step 3563, loss 0.291249, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:08.658392: step 3564, loss 0.248202, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:09.233007: step 3565, loss 0.309924, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:09.821135: step 3566, loss 0.222131, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:10.257925: step 3567, loss 0.343955, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:20:10.704007: step 3568, loss 0.305968, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:11.111574: step 3569, loss 0.319844, acc 0.84375, learning_rate 0.000100002
2017-10-10T13:20:11.646404: step 3570, loss 0.322565, acc 0.875, learning_rate 0.000100002
2017-10-10T13:20:12.204634: step 3571, loss 0.593911, acc 0.875, learning_rate 0.000100002
2017-10-10T13:20:12.648563: step 3572, loss 0.31764, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:20:13.091524: step 3573, loss 0.464246, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:20:13.556870: step 3574, loss 0.419137, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:20:14.028959: step 3575, loss 0.280064, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:14.549133: step 3576, loss 0.312686, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:15.172656: step 3577, loss 0.333461, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:20:15.677838: step 3578, loss 0.301786, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:16.055584: step 3579, loss 0.338724, acc 0.875, learning_rate 0.000100002
2017-10-10T13:20:16.443794: step 3580, loss 0.338719, acc 0.875, learning_rate 0.000100002
2017-10-10T13:20:16.921679: step 3581, loss 0.462027, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:17.469040: step 3582, loss 0.342395, acc 0.875, learning_rate 0.000100002
2017-10-10T13:20:18.000260: step 3583, loss 0.379404, acc 0.8125, learning_rate 0.000100002
2017-10-10T13:20:18.479724: step 3584, loss 0.32344, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:18.977251: step 3585, loss 0.244765, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:19.400419: step 3586, loss 0.638321, acc 0.796875, learning_rate 0.000100002
2017-10-10T13:20:20.032930: step 3587, loss 0.242544, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:20.588858: step 3588, loss 0.252164, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:20:21.084871: step 3589, loss 0.22569, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:20:21.548995: step 3590, loss 0.36359, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:20:22.084949: step 3591, loss 0.240773, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:20:22.645003: step 3592, loss 0.396928, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:20:23.236903: step 3593, loss 0.462448, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:20:23.732923: step 3594, loss 0.199591, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:20:24.310884: step 3595, loss 0.535851, acc 0.8125, learning_rate 0.000100002
2017-10-10T13:20:24.874588: step 3596, loss 0.27895, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:25.399798: step 3597, loss 0.492966, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:20:25.933855: step 3598, loss 0.277968, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:20:26.440828: step 3599, loss 0.257828, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:26.956885: step 3600, loss 0.277369, acc 0.90625, learning_rate 0.000100002

Evaluation:
2017-10-10T13:20:28.216794: step 3600, loss 0.264515, acc 0.903597

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3600

2017-10-10T13:20:29.945127: step 3601, loss 0.426212, acc 0.84375, learning_rate 0.000100002
2017-10-10T13:20:30.412127: step 3602, loss 0.355928, acc 0.875, learning_rate 0.000100002
2017-10-10T13:20:30.961130: step 3603, loss 0.222597, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:20:31.487728: step 3604, loss 0.306928, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:32.064949: step 3605, loss 0.358253, acc 0.875, learning_rate 0.000100002
2017-10-10T13:20:32.664184: step 3606, loss 0.244788, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:33.196898: step 3607, loss 0.560497, acc 0.75, learning_rate 0.000100002
2017-10-10T13:20:33.670272: step 3608, loss 0.311046, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:34.207150: step 3609, loss 0.481803, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:20:34.788858: step 3610, loss 0.193632, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:35.366566: step 3611, loss 0.418796, acc 0.828125, learning_rate 0.000100002
2017-10-10T13:20:35.861080: step 3612, loss 0.279565, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:20:36.294367: step 3613, loss 0.333344, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:36.747671: step 3614, loss 0.373844, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:20:37.295512: step 3615, loss 0.385195, acc 0.8125, learning_rate 0.000100002
2017-10-10T13:20:37.826908: step 3616, loss 0.369132, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:38.425074: step 3617, loss 0.261922, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:39.116848: step 3618, loss 0.376458, acc 0.875, learning_rate 0.000100002
2017-10-10T13:20:39.544824: step 3619, loss 0.431469, acc 0.8125, learning_rate 0.000100002
2017-10-10T13:20:39.937924: step 3620, loss 0.42919, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:20:40.443908: step 3621, loss 0.34892, acc 0.84375, learning_rate 0.000100002
2017-10-10T13:20:40.957049: step 3622, loss 0.42534, acc 0.8125, learning_rate 0.000100002
2017-10-10T13:20:41.496293: step 3623, loss 0.337687, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:41.973077: step 3624, loss 0.65508, acc 0.828125, learning_rate 0.000100002
2017-10-10T13:20:42.461170: step 3625, loss 0.294594, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:20:42.906390: step 3626, loss 0.413642, acc 0.803922, learning_rate 0.000100002
2017-10-10T13:20:43.296936: step 3627, loss 0.604635, acc 0.828125, learning_rate 0.000100002
2017-10-10T13:20:43.839201: step 3628, loss 0.310516, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:44.398409: step 3629, loss 0.240487, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:44.983658: step 3630, loss 0.478025, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:20:45.614401: step 3631, loss 0.311372, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:46.112830: step 3632, loss 0.341981, acc 0.875, learning_rate 0.000100002
2017-10-10T13:20:46.648861: step 3633, loss 0.377504, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:20:47.192915: step 3634, loss 0.54269, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:20:47.765101: step 3635, loss 0.31298, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:48.287863: step 3636, loss 0.39794, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:20:48.904874: step 3637, loss 0.216412, acc 0.953125, learning_rate 0.000100002
2017-10-10T13:20:49.497992: step 3638, loss 0.357654, acc 0.875, learning_rate 0.000100002
2017-10-10T13:20:50.035087: step 3639, loss 0.406569, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:50.544918: step 3640, loss 0.292516, acc 0.890625, learning_rate 0.000100002

Evaluation:
2017-10-10T13:20:51.928291: step 3640, loss 0.263926, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3640

2017-10-10T13:20:53.321244: step 3641, loss 0.400617, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:53.800864: step 3642, loss 0.481096, acc 0.828125, learning_rate 0.000100002
2017-10-10T13:20:54.307445: step 3643, loss 0.339567, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:54.848980: step 3644, loss 0.384483, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:20:55.258904: step 3645, loss 0.377756, acc 0.84375, learning_rate 0.000100002
2017-10-10T13:20:56.062719: step 3646, loss 0.273506, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:20:56.522642: step 3647, loss 0.406864, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:20:57.080931: step 3648, loss 0.290904, acc 0.9375, learning_rate 0.000100002
2017-10-10T13:20:57.582523: step 3649, loss 0.645269, acc 0.828125, learning_rate 0.000100002
2017-10-10T13:20:58.110386: step 3650, loss 0.241441, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:20:58.640862: step 3651, loss 0.287314, acc 0.875, learning_rate 0.000100002
2017-10-10T13:20:59.228866: step 3652, loss 0.423869, acc 0.84375, learning_rate 0.000100002
2017-10-10T13:20:59.768950: step 3653, loss 0.342585, acc 0.875, learning_rate 0.000100002
2017-10-10T13:21:00.232810: step 3654, loss 0.380879, acc 0.875, learning_rate 0.000100002
2017-10-10T13:21:00.702883: step 3655, loss 0.346124, acc 0.875, learning_rate 0.000100002
2017-10-10T13:21:01.284435: step 3656, loss 0.336001, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:21:01.808317: step 3657, loss 0.264578, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:21:02.304845: step 3658, loss 0.291379, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:21:02.887780: step 3659, loss 0.259347, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:21:03.357084: step 3660, loss 0.390182, acc 0.875, learning_rate 0.000100002
2017-10-10T13:21:03.766331: step 3661, loss 0.301268, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:21:04.200917: step 3662, loss 0.351569, acc 0.859375, learning_rate 0.000100002
2017-10-10T13:21:04.703861: step 3663, loss 0.253514, acc 0.921875, learning_rate 0.000100002
2017-10-10T13:21:05.188889: step 3664, loss 0.411521, acc 0.84375, learning_rate 0.000100002
2017-10-10T13:21:05.716865: step 3665, loss 0.373546, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:21:06.274667: step 3666, loss 0.429641, acc 0.796875, learning_rate 0.000100002
2017-10-10T13:21:06.897002: step 3667, loss 0.291606, acc 0.890625, learning_rate 0.000100002
2017-10-10T13:21:07.411470: step 3668, loss 0.286047, acc 0.90625, learning_rate 0.000100002
2017-10-10T13:21:08.015178: step 3669, loss 0.173386, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:21:08.577219: step 3670, loss 0.439618, acc 0.8125, learning_rate 0.000100001
2017-10-10T13:21:09.100536: step 3671, loss 0.359179, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:21:09.656851: step 3672, loss 0.348758, acc 0.875, learning_rate 0.000100001
2017-10-10T13:21:10.186099: step 3673, loss 0.348114, acc 0.875, learning_rate 0.000100001
2017-10-10T13:21:10.716385: step 3674, loss 0.338777, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:21:11.281173: step 3675, loss 0.229231, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:21:11.752499: step 3676, loss 0.397322, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:12.263497: step 3677, loss 0.298075, acc 0.875, learning_rate 0.000100001
2017-10-10T13:21:12.809170: step 3678, loss 0.329645, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:21:13.345015: step 3679, loss 0.264976, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:21:13.858980: step 3680, loss 0.380861, acc 0.859375, learning_rate 0.000100001

Evaluation:
2017-10-10T13:21:15.285008: step 3680, loss 0.263375, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3680

2017-10-10T13:21:16.812894: step 3681, loss 0.25667, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:17.400888: step 3682, loss 0.315068, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:17.968864: step 3683, loss 0.291148, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:21:18.541305: step 3684, loss 0.223229, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:21:19.036886: step 3685, loss 0.358297, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:21:19.556929: step 3686, loss 0.286883, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:20.051145: step 3687, loss 0.46008, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:21:20.572927: step 3688, loss 0.446793, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:21:21.122609: step 3689, loss 0.357031, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:21:21.744850: step 3690, loss 0.261359, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:22.213622: step 3691, loss 0.466036, acc 0.875, learning_rate 0.000100001
2017-10-10T13:21:22.682687: step 3692, loss 0.232707, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:21:23.076837: step 3693, loss 0.360112, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:21:23.630194: step 3694, loss 0.382684, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:21:24.172886: step 3695, loss 0.362248, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:21:24.688750: step 3696, loss 0.328079, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:21:25.192911: step 3697, loss 0.357292, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:25.664899: step 3698, loss 0.47981, acc 0.875, learning_rate 0.000100001
2017-10-10T13:21:26.222228: step 3699, loss 0.468559, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:21:26.788860: step 3700, loss 0.377928, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:27.253111: step 3701, loss 0.433238, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:21:27.636931: step 3702, loss 0.516186, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:21:28.038654: step 3703, loss 0.443346, acc 0.875, learning_rate 0.000100001
2017-10-10T13:21:28.571354: step 3704, loss 0.326824, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:21:29.141073: step 3705, loss 0.509516, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:21:29.553185: step 3706, loss 0.256927, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:21:30.064431: step 3707, loss 0.333878, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:21:30.564297: step 3708, loss 0.326392, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:21:31.035984: step 3709, loss 0.188894, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:21:31.499275: step 3710, loss 0.314013, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:21:32.046161: step 3711, loss 0.532359, acc 0.796875, learning_rate 0.000100001
2017-10-10T13:21:32.524943: step 3712, loss 0.301533, acc 0.875, learning_rate 0.000100001
2017-10-10T13:21:33.009431: step 3713, loss 0.348358, acc 0.875, learning_rate 0.000100001
2017-10-10T13:21:33.508998: step 3714, loss 0.343694, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:33.940234: step 3715, loss 0.322364, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:34.464824: step 3716, loss 0.235511, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:21:35.016855: step 3717, loss 0.304556, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:21:35.580574: step 3718, loss 0.501844, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:21:36.136856: step 3719, loss 0.409668, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:21:36.731213: step 3720, loss 0.330503, acc 0.90625, learning_rate 0.000100001

Evaluation:
2017-10-10T13:21:38.191052: step 3720, loss 0.263345, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3720

2017-10-10T13:21:39.923396: step 3721, loss 0.295546, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:21:40.454388: step 3722, loss 0.390254, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:21:41.007519: step 3723, loss 0.2916, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:41.491645: step 3724, loss 0.309662, acc 0.901961, learning_rate 0.000100001
2017-10-10T13:21:41.933699: step 3725, loss 0.259983, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:21:42.326379: step 3726, loss 0.252199, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:21:42.723799: step 3727, loss 0.49611, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:21:43.222510: step 3728, loss 0.310904, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:21:43.756960: step 3729, loss 0.252061, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:44.304139: step 3730, loss 0.539726, acc 0.8125, learning_rate 0.000100001
2017-10-10T13:21:44.900984: step 3731, loss 0.388262, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:21:45.495861: step 3732, loss 0.450287, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:21:46.025342: step 3733, loss 0.345013, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:21:46.433377: step 3734, loss 0.251545, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:46.868835: step 3735, loss 0.275818, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:21:47.324904: step 3736, loss 0.186373, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:21:47.947177: step 3737, loss 0.357012, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:21:48.443757: step 3738, loss 0.294382, acc 0.875, learning_rate 0.000100001
2017-10-10T13:21:49.019748: step 3739, loss 0.392053, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:21:49.493038: step 3740, loss 0.35376, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:21:50.004851: step 3741, loss 0.343393, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:21:50.509708: step 3742, loss 0.382123, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:21:50.897908: step 3743, loss 0.360206, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:21:51.350189: step 3744, loss 0.395434, acc 0.875, learning_rate 0.000100001
2017-10-10T13:21:51.756815: step 3745, loss 0.278929, acc 0.875, learning_rate 0.000100001
2017-10-10T13:21:52.229779: step 3746, loss 0.351414, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:52.803082: step 3747, loss 0.31272, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:53.332987: step 3748, loss 0.316032, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:53.840616: step 3749, loss 0.276582, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:21:54.373290: step 3750, loss 0.375518, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:54.940412: step 3751, loss 0.273503, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:55.556920: step 3752, loss 0.289422, acc 0.875, learning_rate 0.000100001
2017-10-10T13:21:56.081081: step 3753, loss 0.323445, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:21:56.630803: step 3754, loss 0.357585, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:21:57.148985: step 3755, loss 0.187869, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:21:57.689089: step 3756, loss 0.422252, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:21:58.250899: step 3757, loss 0.373495, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:21:58.818133: step 3758, loss 0.290969, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:21:59.313004: step 3759, loss 0.376087, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:21:59.833592: step 3760, loss 0.450875, acc 0.828125, learning_rate 0.000100001

Evaluation:
2017-10-10T13:22:01.243221: step 3760, loss 0.264425, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3760

2017-10-10T13:22:02.720843: step 3761, loss 0.375565, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:22:03.246972: step 3762, loss 0.312511, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:22:03.801570: step 3763, loss 0.18971, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:22:04.370956: step 3764, loss 0.329057, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:22:04.941526: step 3765, loss 0.330312, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:22:05.392011: step 3766, loss 0.319868, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:22:05.872997: step 3767, loss 0.275982, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:22:06.424905: step 3768, loss 0.272946, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:22:06.981087: step 3769, loss 0.34262, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:22:07.549076: step 3770, loss 0.351741, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:22:08.137102: step 3771, loss 0.29874, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:22:08.720852: step 3772, loss 0.391182, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:22:09.196855: step 3773, loss 0.333875, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:09.606533: step 3774, loss 0.316636, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:22:10.112939: step 3775, loss 0.290939, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:22:10.671127: step 3776, loss 0.390701, acc 0.875, learning_rate 0.000100001
2017-10-10T13:22:11.173057: step 3777, loss 0.3138, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:22:11.712879: step 3778, loss 0.232531, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:22:12.216976: step 3779, loss 0.19056, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:22:12.816868: step 3780, loss 0.393194, acc 0.8125, learning_rate 0.000100001
2017-10-10T13:22:13.492864: step 3781, loss 0.22088, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:22:13.992507: step 3782, loss 0.309619, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:14.471763: step 3783, loss 0.424267, acc 0.875, learning_rate 0.000100001
2017-10-10T13:22:15.056909: step 3784, loss 0.255967, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:22:15.543561: step 3785, loss 0.227895, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:22:16.121269: step 3786, loss 0.392686, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:16.690458: step 3787, loss 0.513359, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:22:17.216925: step 3788, loss 0.36056, acc 0.875, learning_rate 0.000100001
2017-10-10T13:22:17.667722: step 3789, loss 0.514576, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:22:18.196966: step 3790, loss 0.486393, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:22:18.792940: step 3791, loss 0.310934, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:22:19.300884: step 3792, loss 0.322173, acc 0.875, learning_rate 0.000100001
2017-10-10T13:22:19.753048: step 3793, loss 0.47845, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:22:20.320997: step 3794, loss 0.318297, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:20.912861: step 3795, loss 0.320539, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:22:21.424847: step 3796, loss 0.432366, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:22:21.960885: step 3797, loss 0.397246, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:22:22.517741: step 3798, loss 0.278288, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:22:23.064797: step 3799, loss 0.216209, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:22:23.600965: step 3800, loss 0.264857, acc 0.921875, learning_rate 0.000100001

Evaluation:
2017-10-10T13:22:24.921897: step 3800, loss 0.263467, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3800

2017-10-10T13:22:26.531828: step 3801, loss 0.25801, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:27.026199: step 3802, loss 0.330384, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:22:27.619006: step 3803, loss 0.319833, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:22:28.126592: step 3804, loss 0.45457, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:22:29.230885: step 3805, loss 0.434518, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:22:29.701019: step 3806, loss 0.387052, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:30.257414: step 3807, loss 0.34066, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:30.796551: step 3808, loss 0.266094, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:22:31.342686: step 3809, loss 0.306111, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:22:31.873011: step 3810, loss 0.330763, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:22:32.388517: step 3811, loss 0.281061, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:22:32.804881: step 3812, loss 0.327067, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:22:33.240857: step 3813, loss 0.565538, acc 0.796875, learning_rate 0.000100001
2017-10-10T13:22:33.823178: step 3814, loss 0.269178, acc 0.875, learning_rate 0.000100001
2017-10-10T13:22:34.366961: step 3815, loss 0.22795, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:22:34.881988: step 3816, loss 0.368633, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:22:35.403502: step 3817, loss 0.266812, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:36.002583: step 3818, loss 0.285703, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:22:36.413588: step 3819, loss 0.334244, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:36.925219: step 3820, loss 0.326772, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:37.600858: step 3821, loss 0.357725, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:22:38.100697: step 3822, loss 0.390314, acc 0.882353, learning_rate 0.000100001
2017-10-10T13:22:38.553148: step 3823, loss 0.388191, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:22:38.999021: step 3824, loss 0.363437, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:22:39.532846: step 3825, loss 0.396903, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:22:40.076521: step 3826, loss 0.215512, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:22:40.659257: step 3827, loss 0.300958, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:22:41.222223: step 3828, loss 0.256018, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:22:41.806464: step 3829, loss 0.289541, acc 0.875, learning_rate 0.000100001
2017-10-10T13:22:42.385077: step 3830, loss 0.215966, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:22:42.946478: step 3831, loss 0.181646, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:22:43.499619: step 3832, loss 0.271073, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:22:43.997585: step 3833, loss 0.218937, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:22:44.552994: step 3834, loss 0.258604, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:22:45.074635: step 3835, loss 0.262038, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:22:45.612002: step 3836, loss 0.416188, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:22:46.148948: step 3837, loss 0.485749, acc 0.8125, learning_rate 0.000100001
2017-10-10T13:22:46.676907: step 3838, loss 0.240775, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:22:47.197534: step 3839, loss 0.314812, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:22:47.732940: step 3840, loss 0.218098, acc 0.921875, learning_rate 0.000100001

Evaluation:
2017-10-10T13:22:48.991535: step 3840, loss 0.260739, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3840

2017-10-10T13:22:50.673019: step 3841, loss 0.255414, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:51.285041: step 3842, loss 0.491962, acc 0.8125, learning_rate 0.000100001
2017-10-10T13:22:51.808918: step 3843, loss 0.296226, acc 0.875, learning_rate 0.000100001
2017-10-10T13:22:52.216512: step 3844, loss 0.23415, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:22:52.628900: step 3845, loss 0.540866, acc 0.796875, learning_rate 0.000100001
2017-10-10T13:22:53.048863: step 3846, loss 0.255718, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:22:53.546325: step 3847, loss 0.347072, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:22:54.077287: step 3848, loss 0.37249, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:54.599158: step 3849, loss 0.326752, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:22:55.172878: step 3850, loss 0.435641, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:22:55.600100: step 3851, loss 0.254518, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:56.070722: step 3852, loss 0.317079, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:56.616995: step 3853, loss 0.350195, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:22:57.180468: step 3854, loss 0.247264, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:22:57.729147: step 3855, loss 0.324414, acc 0.875, learning_rate 0.000100001
2017-10-10T13:22:58.268851: step 3856, loss 0.410148, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:22:58.822964: step 3857, loss 0.34505, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:22:59.378855: step 3858, loss 0.389067, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:22:59.940994: step 3859, loss 0.347582, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:23:00.499056: step 3860, loss 0.217378, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:23:01.089168: step 3861, loss 0.302247, acc 0.875, learning_rate 0.000100001
2017-10-10T13:23:01.569287: step 3862, loss 0.394935, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:23:02.028858: step 3863, loss 0.376199, acc 0.875, learning_rate 0.000100001
2017-10-10T13:23:02.470619: step 3864, loss 0.505712, acc 0.8125, learning_rate 0.000100001
2017-10-10T13:23:02.954679: step 3865, loss 0.297459, acc 0.875, learning_rate 0.000100001
2017-10-10T13:23:03.369126: step 3866, loss 0.312411, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:23:03.789126: step 3867, loss 0.529534, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:23:04.331933: step 3868, loss 0.330074, acc 0.875, learning_rate 0.000100001
2017-10-10T13:23:04.864835: step 3869, loss 0.241122, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:23:05.413083: step 3870, loss 0.215214, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:23:05.983850: step 3871, loss 0.330768, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:23:06.497384: step 3872, loss 0.474631, acc 0.875, learning_rate 0.000100001
2017-10-10T13:23:07.053214: step 3873, loss 0.243474, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:23:07.635411: step 3874, loss 0.455583, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:23:08.166443: step 3875, loss 0.391997, acc 0.875, learning_rate 0.000100001
2017-10-10T13:23:08.718478: step 3876, loss 0.29373, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:23:09.261270: step 3877, loss 0.291261, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:23:09.820913: step 3878, loss 0.306906, acc 0.875, learning_rate 0.000100001
2017-10-10T13:23:10.405041: step 3879, loss 0.586717, acc 0.796875, learning_rate 0.000100001
2017-10-10T13:23:10.947891: step 3880, loss 0.216243, acc 0.890625, learning_rate 0.000100001

Evaluation:
2017-10-10T13:23:12.285553: step 3880, loss 0.262147, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3880

2017-10-10T13:23:13.648624: step 3881, loss 0.298336, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:23:14.193026: step 3882, loss 0.443995, acc 0.8125, learning_rate 0.000100001
2017-10-10T13:23:14.749009: step 3883, loss 0.345843, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:23:15.364861: step 3884, loss 0.252611, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:23:15.875993: step 3885, loss 0.281854, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:23:16.339729: step 3886, loss 0.36608, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:23:16.896989: step 3887, loss 0.468779, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:23:17.462227: step 3888, loss 0.263443, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:23:17.945683: step 3889, loss 0.346467, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:23:18.378421: step 3890, loss 0.357307, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:23:18.816975: step 3891, loss 0.314399, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:23:19.281148: step 3892, loss 0.302856, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:23:19.794103: step 3893, loss 0.21333, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:23:20.323033: step 3894, loss 0.608565, acc 0.8125, learning_rate 0.000100001
2017-10-10T13:23:20.822929: step 3895, loss 0.442712, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:23:21.303882: step 3896, loss 0.399209, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:23:21.759691: step 3897, loss 0.389354, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:23:22.283569: step 3898, loss 0.450951, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:23:22.792470: step 3899, loss 0.37427, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:23:23.256916: step 3900, loss 0.347856, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:23:23.788894: step 3901, loss 0.292843, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:23:24.279607: step 3902, loss 0.31596, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:23:24.860852: step 3903, loss 0.444788, acc 0.875, learning_rate 0.000100001
2017-10-10T13:23:25.417089: step 3904, loss 0.323183, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:23:25.927152: step 3905, loss 0.265434, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:23:26.378288: step 3906, loss 0.403252, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:23:26.941487: step 3907, loss 0.365736, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:23:27.466129: step 3908, loss 0.321057, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:23:28.019169: step 3909, loss 0.155689, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:23:28.515047: step 3910, loss 0.304442, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:23:28.961132: step 3911, loss 0.554144, acc 0.78125, learning_rate 0.000100001
2017-10-10T13:23:29.491506: step 3912, loss 0.213673, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:23:30.027811: step 3913, loss 0.299154, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:23:30.569249: step 3914, loss 0.477017, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:23:31.077085: step 3915, loss 0.430556, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:23:31.613215: step 3916, loss 0.315662, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:23:32.133119: step 3917, loss 0.39237, acc 0.875, learning_rate 0.000100001
2017-10-10T13:23:32.633348: step 3918, loss 0.369149, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:23:33.228909: step 3919, loss 0.293302, acc 0.875, learning_rate 0.000100001
2017-10-10T13:23:33.681998: step 3920, loss 0.257725, acc 0.921569, learning_rate 0.000100001

Evaluation:
2017-10-10T13:23:35.159954: step 3920, loss 0.261609, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3920

2017-10-10T13:23:36.714916: step 3921, loss 0.399071, acc 0.84375, learning_rate 0.000100001
2017-10-10T13:23:37.224958: step 3922, loss 0.365874, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:23:37.812881: step 3923, loss 0.262539, acc 0.9375, learning_rate 0.000100001
2017-10-10T13:23:38.332909: step 3924, loss 0.48492, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:23:38.748082: step 3925, loss 0.218854, acc 0.953125, learning_rate 0.000100001
2017-10-10T13:23:39.209060: step 3926, loss 0.401132, acc 0.859375, learning_rate 0.000100001
2017-10-10T13:23:39.693196: step 3927, loss 0.438215, acc 0.828125, learning_rate 0.000100001
2017-10-10T13:23:40.303778: step 3928, loss 0.365476, acc 0.875, learning_rate 0.000100001
2017-10-10T13:23:40.805014: step 3929, loss 0.478409, acc 0.8125, learning_rate 0.000100001
2017-10-10T13:23:41.313046: step 3930, loss 0.238935, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:23:41.796519: step 3931, loss 0.248343, acc 0.96875, learning_rate 0.000100001
2017-10-10T13:23:42.261143: step 3932, loss 0.249318, acc 0.90625, learning_rate 0.000100001
2017-10-10T13:23:42.725653: step 3933, loss 0.334595, acc 0.890625, learning_rate 0.000100001
2017-10-10T13:23:43.290291: step 3934, loss 0.324608, acc 0.875, learning_rate 0.000100001
2017-10-10T13:23:43.777181: step 3935, loss 0.235313, acc 0.921875, learning_rate 0.000100001
2017-10-10T13:23:44.362333: step 3936, loss 0.423048, acc 0.875, learning_rate 0.000100001
2017-10-10T13:23:44.900862: step 3937, loss 0.376331, acc 0.859375, learning_rate 0.0001
2017-10-10T13:23:45.467093: step 3938, loss 0.552521, acc 0.84375, learning_rate 0.0001
2017-10-10T13:23:46.078751: step 3939, loss 0.401004, acc 0.84375, learning_rate 0.0001
2017-10-10T13:23:46.656845: step 3940, loss 0.312015, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:47.235998: step 3941, loss 0.346346, acc 0.890625, learning_rate 0.0001
2017-10-10T13:23:47.813043: step 3942, loss 0.386187, acc 0.84375, learning_rate 0.0001
2017-10-10T13:23:48.389064: step 3943, loss 0.360436, acc 0.84375, learning_rate 0.0001
2017-10-10T13:23:48.875542: step 3944, loss 0.203921, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:49.352917: step 3945, loss 0.2696, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:49.894825: step 3946, loss 0.25586, acc 0.921875, learning_rate 0.0001
2017-10-10T13:23:50.384932: step 3947, loss 0.467876, acc 0.828125, learning_rate 0.0001
2017-10-10T13:23:50.912855: step 3948, loss 0.310511, acc 0.875, learning_rate 0.0001
2017-10-10T13:23:51.405856: step 3949, loss 0.361734, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:51.962203: step 3950, loss 0.499481, acc 0.8125, learning_rate 0.0001
2017-10-10T13:23:52.501331: step 3951, loss 0.345717, acc 0.84375, learning_rate 0.0001
2017-10-10T13:23:53.066662: step 3952, loss 0.201933, acc 0.96875, learning_rate 0.0001
2017-10-10T13:23:53.656248: step 3953, loss 0.393224, acc 0.859375, learning_rate 0.0001
2017-10-10T13:23:54.181178: step 3954, loss 0.245021, acc 0.9375, learning_rate 0.0001
2017-10-10T13:23:54.756881: step 3955, loss 0.305768, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:55.349930: step 3956, loss 0.220136, acc 0.953125, learning_rate 0.0001
2017-10-10T13:23:55.860496: step 3957, loss 0.274316, acc 0.90625, learning_rate 0.0001
2017-10-10T13:23:56.439870: step 3958, loss 0.369398, acc 0.890625, learning_rate 0.0001
2017-10-10T13:23:57.107209: step 3959, loss 0.286584, acc 0.875, learning_rate 0.0001
2017-10-10T13:23:57.553053: step 3960, loss 0.38545, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:23:58.965082: step 3960, loss 0.261946, acc 0.897842

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-3960

2017-10-10T13:24:00.803484: step 3961, loss 0.615876, acc 0.765625, learning_rate 0.0001
2017-10-10T13:24:01.373526: step 3962, loss 0.232614, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:01.823202: step 3963, loss 0.169634, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:02.285063: step 3964, loss 0.502854, acc 0.765625, learning_rate 0.0001
2017-10-10T13:24:02.794551: step 3965, loss 0.315453, acc 0.84375, learning_rate 0.0001
2017-10-10T13:24:03.291140: step 3966, loss 0.284635, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:03.870165: step 3967, loss 0.364671, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:04.349773: step 3968, loss 0.268498, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:04.841957: step 3969, loss 0.436039, acc 0.828125, learning_rate 0.0001
2017-10-10T13:24:05.376070: step 3970, loss 0.532348, acc 0.796875, learning_rate 0.0001
2017-10-10T13:24:05.949181: step 3971, loss 0.437819, acc 0.828125, learning_rate 0.0001
2017-10-10T13:24:06.458188: step 3972, loss 0.338878, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:06.984943: step 3973, loss 0.41275, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:07.540856: step 3974, loss 0.332392, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:08.028934: step 3975, loss 0.21829, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:08.599588: step 3976, loss 0.296602, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:09.104882: step 3977, loss 0.169919, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:09.685106: step 3978, loss 0.323016, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:10.212854: step 3979, loss 0.227629, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:10.724901: step 3980, loss 0.408943, acc 0.84375, learning_rate 0.0001
2017-10-10T13:24:11.235999: step 3981, loss 0.225578, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:11.644388: step 3982, loss 0.121418, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:12.092801: step 3983, loss 0.375557, acc 0.84375, learning_rate 0.0001
2017-10-10T13:24:12.543389: step 3984, loss 0.374913, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:13.120874: step 3985, loss 0.248202, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:13.636876: step 3986, loss 0.420061, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:14.151650: step 3987, loss 0.339783, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:14.648067: step 3988, loss 0.55117, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:15.187444: step 3989, loss 0.379324, acc 0.84375, learning_rate 0.0001
2017-10-10T13:24:15.704868: step 3990, loss 0.43943, acc 0.828125, learning_rate 0.0001
2017-10-10T13:24:16.157027: step 3991, loss 0.305708, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:16.703855: step 3992, loss 0.229772, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:17.196274: step 3993, loss 0.318401, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:17.732854: step 3994, loss 0.384838, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:18.320403: step 3995, loss 0.406578, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:18.800895: step 3996, loss 0.515283, acc 0.84375, learning_rate 0.0001
2017-10-10T13:24:19.315377: step 3997, loss 0.30532, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:19.753178: step 3998, loss 0.411326, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:20.243059: step 3999, loss 0.2923, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:20.803653: step 4000, loss 0.206528, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:24:22.156885: step 4000, loss 0.261823, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4000

2017-10-10T13:24:23.564675: step 4001, loss 0.356117, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:24.163027: step 4002, loss 0.464207, acc 0.828125, learning_rate 0.0001
2017-10-10T13:24:24.721584: step 4003, loss 0.353967, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:25.183865: step 4004, loss 0.276414, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:25.665996: step 4005, loss 0.29454, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:26.181016: step 4006, loss 0.252697, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:26.689375: step 4007, loss 0.347151, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:27.248934: step 4008, loss 0.271359, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:27.643778: step 4009, loss 0.31362, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:28.100932: step 4010, loss 0.433412, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:28.608941: step 4011, loss 0.361199, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:29.137109: step 4012, loss 0.372793, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:29.664908: step 4013, loss 0.475586, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:30.141047: step 4014, loss 0.434527, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:30.708894: step 4015, loss 0.363523, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:31.259609: step 4016, loss 0.389133, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:31.797056: step 4017, loss 0.299676, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:32.233658: step 4018, loss 0.29241, acc 0.921569, learning_rate 0.0001
2017-10-10T13:24:32.708926: step 4019, loss 0.264518, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:33.303427: step 4020, loss 0.414351, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:33.863677: step 4021, loss 0.368373, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:34.424293: step 4022, loss 0.373706, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:34.834542: step 4023, loss 0.297262, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:35.249031: step 4024, loss 0.380601, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:35.750423: step 4025, loss 0.278819, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:36.220928: step 4026, loss 0.306735, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:36.676985: step 4027, loss 0.31426, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:37.233556: step 4028, loss 0.449103, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:37.800918: step 4029, loss 0.29332, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:38.345144: step 4030, loss 0.286752, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:38.928840: step 4031, loss 0.411307, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:39.472526: step 4032, loss 0.325034, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:40.015743: step 4033, loss 0.412357, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:40.521926: step 4034, loss 0.426126, acc 0.828125, learning_rate 0.0001
2017-10-10T13:24:41.052852: step 4035, loss 0.371801, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:41.545232: step 4036, loss 0.303532, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:42.068677: step 4037, loss 0.247835, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:42.581016: step 4038, loss 0.269698, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:43.127823: step 4039, loss 0.144676, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:43.688884: step 4040, loss 0.473523, acc 0.8125, learning_rate 0.0001

Evaluation:
2017-10-10T13:24:45.081046: step 4040, loss 0.262213, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4040

2017-10-10T13:24:46.695136: step 4041, loss 0.203992, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:47.262402: step 4042, loss 0.219667, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:47.744840: step 4043, loss 0.353987, acc 0.828125, learning_rate 0.0001
2017-10-10T13:24:48.218600: step 4044, loss 0.329447, acc 0.875, learning_rate 0.0001
2017-10-10T13:24:48.736893: step 4045, loss 0.350211, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:49.201071: step 4046, loss 0.471124, acc 0.796875, learning_rate 0.0001
2017-10-10T13:24:49.721007: step 4047, loss 0.381364, acc 0.9375, learning_rate 0.0001
2017-10-10T13:24:50.273128: step 4048, loss 0.359608, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:50.768911: step 4049, loss 0.219192, acc 0.96875, learning_rate 0.0001
2017-10-10T13:24:51.207161: step 4050, loss 0.344648, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:51.616977: step 4051, loss 0.192677, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:52.127822: step 4052, loss 0.426442, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:52.703484: step 4053, loss 0.174062, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:53.232563: step 4054, loss 0.227531, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:53.725105: step 4055, loss 0.444775, acc 0.84375, learning_rate 0.0001
2017-10-10T13:24:54.233053: step 4056, loss 0.20609, acc 0.921875, learning_rate 0.0001
2017-10-10T13:24:54.811027: step 4057, loss 0.292211, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:55.369638: step 4058, loss 0.350161, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:55.893990: step 4059, loss 0.339109, acc 0.890625, learning_rate 0.0001
2017-10-10T13:24:56.372025: step 4060, loss 0.311672, acc 0.90625, learning_rate 0.0001
2017-10-10T13:24:57.031559: step 4061, loss 0.422444, acc 0.84375, learning_rate 0.0001
2017-10-10T13:24:57.509055: step 4062, loss 0.494037, acc 0.796875, learning_rate 0.0001
2017-10-10T13:24:57.931151: step 4063, loss 0.2285, acc 0.953125, learning_rate 0.0001
2017-10-10T13:24:58.328892: step 4064, loss 0.314831, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:58.849145: step 4065, loss 0.48148, acc 0.8125, learning_rate 0.0001
2017-10-10T13:24:59.401196: step 4066, loss 0.306049, acc 0.859375, learning_rate 0.0001
2017-10-10T13:24:59.978640: step 4067, loss 0.39728, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:00.538112: step 4068, loss 0.27038, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:01.059133: step 4069, loss 0.328311, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:01.595248: step 4070, loss 0.464046, acc 0.859375, learning_rate 0.0001
2017-10-10T13:25:02.160954: step 4071, loss 0.65331, acc 0.8125, learning_rate 0.0001
2017-10-10T13:25:02.694972: step 4072, loss 0.395913, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:03.236981: step 4073, loss 0.322588, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:03.795376: step 4074, loss 0.338867, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:04.340150: step 4075, loss 0.462603, acc 0.84375, learning_rate 0.0001
2017-10-10T13:25:04.865321: step 4076, loss 0.30019, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:05.418894: step 4077, loss 0.212173, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:05.978082: step 4078, loss 0.338254, acc 0.859375, learning_rate 0.0001
2017-10-10T13:25:06.488493: step 4079, loss 0.471059, acc 0.78125, learning_rate 0.0001
2017-10-10T13:25:07.002569: step 4080, loss 0.367355, acc 0.84375, learning_rate 0.0001

Evaluation:
2017-10-10T13:25:08.424839: step 4080, loss 0.26164, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4080

2017-10-10T13:25:10.088996: step 4081, loss 0.388179, acc 0.8125, learning_rate 0.0001
2017-10-10T13:25:10.612702: step 4082, loss 0.298733, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:11.111245: step 4083, loss 0.210522, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:11.517093: step 4084, loss 0.290258, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:12.044964: step 4085, loss 0.302125, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:12.612818: step 4086, loss 0.25775, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:13.144256: step 4087, loss 0.232482, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:13.583284: step 4088, loss 0.301153, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:13.957013: step 4089, loss 0.487724, acc 0.8125, learning_rate 0.0001
2017-10-10T13:25:14.384074: step 4090, loss 0.298553, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:14.969075: step 4091, loss 0.315244, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:15.515209: step 4092, loss 0.253517, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:15.916887: step 4093, loss 0.388847, acc 0.875, learning_rate 0.0001
2017-10-10T13:25:16.512974: step 4094, loss 0.219909, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:17.032625: step 4095, loss 0.316894, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:17.553086: step 4096, loss 0.255932, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:18.163755: step 4097, loss 0.26235, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:18.692955: step 4098, loss 0.536035, acc 0.84375, learning_rate 0.0001
2017-10-10T13:25:19.314802: step 4099, loss 0.276215, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:19.864984: step 4100, loss 0.251577, acc 0.859375, learning_rate 0.0001
2017-10-10T13:25:20.436867: step 4101, loss 0.434262, acc 0.859375, learning_rate 0.0001
2017-10-10T13:25:20.883147: step 4102, loss 0.40737, acc 0.84375, learning_rate 0.0001
2017-10-10T13:25:21.335253: step 4103, loss 0.362997, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:21.882173: step 4104, loss 0.241556, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:22.429012: step 4105, loss 0.369329, acc 0.84375, learning_rate 0.0001
2017-10-10T13:25:22.973883: step 4106, loss 0.370301, acc 0.859375, learning_rate 0.0001
2017-10-10T13:25:23.525182: step 4107, loss 0.3205, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:24.174189: step 4108, loss 0.349878, acc 0.859375, learning_rate 0.0001
2017-10-10T13:25:24.695156: step 4109, loss 0.472341, acc 0.859375, learning_rate 0.0001
2017-10-10T13:25:25.312063: step 4110, loss 0.255256, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:25.855839: step 4111, loss 0.307084, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:26.376172: step 4112, loss 0.280329, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:26.927007: step 4113, loss 0.306388, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:27.473342: step 4114, loss 0.338915, acc 0.859375, learning_rate 0.0001
2017-10-10T13:25:28.004845: step 4115, loss 0.320445, acc 0.84375, learning_rate 0.0001
2017-10-10T13:25:28.360743: step 4116, loss 0.374376, acc 0.843137, learning_rate 0.0001
2017-10-10T13:25:28.924830: step 4117, loss 0.253215, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:29.369304: step 4118, loss 0.295674, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:29.803281: step 4119, loss 0.3117, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:30.372693: step 4120, loss 0.314815, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:25:31.806294: step 4120, loss 0.261754, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4120

2017-10-10T13:25:33.509026: step 4121, loss 0.351172, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:34.010585: step 4122, loss 0.264291, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:34.512318: step 4123, loss 0.548058, acc 0.859375, learning_rate 0.0001
2017-10-10T13:25:35.036986: step 4124, loss 0.338586, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:35.602887: step 4125, loss 0.198209, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:36.168928: step 4126, loss 0.35778, acc 0.875, learning_rate 0.0001
2017-10-10T13:25:36.700831: step 4127, loss 0.401651, acc 0.875, learning_rate 0.0001
2017-10-10T13:25:37.186827: step 4128, loss 0.435151, acc 0.8125, learning_rate 0.0001
2017-10-10T13:25:37.662037: step 4129, loss 0.288281, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:38.228821: step 4130, loss 0.229225, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:38.782476: step 4131, loss 0.258153, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:39.322627: step 4132, loss 0.553021, acc 0.8125, learning_rate 0.0001
2017-10-10T13:25:39.947259: step 4133, loss 0.387646, acc 0.84375, learning_rate 0.0001
2017-10-10T13:25:40.475924: step 4134, loss 0.213017, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:41.009421: step 4135, loss 0.314196, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:41.524867: step 4136, loss 0.225233, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:42.046894: step 4137, loss 0.282857, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:42.585105: step 4138, loss 0.330334, acc 0.875, learning_rate 0.0001
2017-10-10T13:25:43.109161: step 4139, loss 0.225144, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:43.673032: step 4140, loss 0.309066, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:44.226322: step 4141, loss 0.249727, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:44.670713: step 4142, loss 0.455477, acc 0.859375, learning_rate 0.0001
2017-10-10T13:25:45.062780: step 4143, loss 0.194455, acc 0.96875, learning_rate 0.0001
2017-10-10T13:25:45.509054: step 4144, loss 0.318217, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:46.068160: step 4145, loss 0.409759, acc 0.875, learning_rate 0.0001
2017-10-10T13:25:46.572268: step 4146, loss 0.390332, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:47.091623: step 4147, loss 0.228771, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:47.540461: step 4148, loss 0.418307, acc 0.84375, learning_rate 0.0001
2017-10-10T13:25:48.115545: step 4149, loss 0.237824, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:48.556891: step 4150, loss 0.265406, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:49.069933: step 4151, loss 0.262572, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:49.626065: step 4152, loss 0.347658, acc 0.84375, learning_rate 0.0001
2017-10-10T13:25:50.172957: step 4153, loss 0.377078, acc 0.859375, learning_rate 0.0001
2017-10-10T13:25:50.755164: step 4154, loss 0.367335, acc 0.84375, learning_rate 0.0001
2017-10-10T13:25:51.272843: step 4155, loss 0.225885, acc 0.9375, learning_rate 0.0001
2017-10-10T13:25:51.825684: step 4156, loss 0.188848, acc 0.921875, learning_rate 0.0001
2017-10-10T13:25:52.385798: step 4157, loss 0.258311, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:52.961634: step 4158, loss 0.454998, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:53.498463: step 4159, loss 0.339915, acc 0.875, learning_rate 0.0001
2017-10-10T13:25:54.055940: step 4160, loss 0.309272, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:25:55.461118: step 4160, loss 0.259653, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4160

2017-10-10T13:25:56.841055: step 4161, loss 0.345189, acc 0.890625, learning_rate 0.0001
2017-10-10T13:25:57.271107: step 4162, loss 0.360079, acc 0.875, learning_rate 0.0001
2017-10-10T13:25:57.693858: step 4163, loss 0.109245, acc 0.984375, learning_rate 0.0001
2017-10-10T13:25:58.091462: step 4164, loss 0.393211, acc 0.84375, learning_rate 0.0001
2017-10-10T13:25:58.685497: step 4165, loss 0.263189, acc 0.953125, learning_rate 0.0001
2017-10-10T13:25:59.236904: step 4166, loss 0.271061, acc 0.90625, learning_rate 0.0001
2017-10-10T13:25:59.746732: step 4167, loss 0.177004, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:00.174973: step 4168, loss 0.286576, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:00.620431: step 4169, loss 0.592372, acc 0.84375, learning_rate 0.0001
2017-10-10T13:26:01.148859: step 4170, loss 0.30666, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:01.736879: step 4171, loss 0.370121, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:02.233784: step 4172, loss 0.401122, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:02.723065: step 4173, loss 0.369264, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:03.195923: step 4174, loss 0.584555, acc 0.78125, learning_rate 0.0001
2017-10-10T13:26:03.682195: step 4175, loss 0.352315, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:04.194881: step 4176, loss 0.504662, acc 0.828125, learning_rate 0.0001
2017-10-10T13:26:04.728995: step 4177, loss 0.279068, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:05.238702: step 4178, loss 0.250576, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:05.720214: step 4179, loss 0.359636, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:06.251804: step 4180, loss 0.410648, acc 0.84375, learning_rate 0.0001
2017-10-10T13:26:06.831678: step 4181, loss 0.429498, acc 0.84375, learning_rate 0.0001
2017-10-10T13:26:07.306391: step 4182, loss 0.352631, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:07.744122: step 4183, loss 0.31003, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:08.183395: step 4184, loss 0.279793, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:08.738957: step 4185, loss 0.231301, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:09.294623: step 4186, loss 0.234124, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:09.857027: step 4187, loss 0.373584, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:10.384857: step 4188, loss 0.272868, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:10.859771: step 4189, loss 0.258574, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:11.354893: step 4190, loss 0.329026, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:11.888927: step 4191, loss 0.254489, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:12.360351: step 4192, loss 0.518665, acc 0.796875, learning_rate 0.0001
2017-10-10T13:26:12.913080: step 4193, loss 0.285817, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:13.414151: step 4194, loss 0.403676, acc 0.84375, learning_rate 0.0001
2017-10-10T13:26:13.989012: step 4195, loss 0.392453, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:14.504907: step 4196, loss 0.21129, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:15.073067: step 4197, loss 0.307765, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:15.633737: step 4198, loss 0.318498, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:16.126480: step 4199, loss 0.509464, acc 0.8125, learning_rate 0.0001
2017-10-10T13:26:16.625110: step 4200, loss 0.270489, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:26:18.057056: step 4200, loss 0.258548, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4200

2017-10-10T13:26:19.708714: step 4201, loss 0.277402, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:20.277513: step 4202, loss 0.481081, acc 0.796875, learning_rate 0.0001
2017-10-10T13:26:20.736789: step 4203, loss 0.297031, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:21.147701: step 4204, loss 0.34069, acc 0.84375, learning_rate 0.0001
2017-10-10T13:26:21.666757: step 4205, loss 0.525148, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:22.238066: step 4206, loss 0.334132, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:22.752905: step 4207, loss 0.405723, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:23.176816: step 4208, loss 0.384882, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:23.580841: step 4209, loss 0.226999, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:24.122136: step 4210, loss 0.394015, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:24.666727: step 4211, loss 0.278124, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:25.209075: step 4212, loss 0.276141, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:25.793254: step 4213, loss 0.258415, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:26.260925: step 4214, loss 0.378528, acc 0.882353, learning_rate 0.0001
2017-10-10T13:26:26.768321: step 4215, loss 0.291349, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:27.252972: step 4216, loss 0.260332, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:27.813138: step 4217, loss 0.415968, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:28.316888: step 4218, loss 0.4603, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:28.860887: step 4219, loss 0.398927, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:29.388745: step 4220, loss 0.234116, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:29.931849: step 4221, loss 0.26307, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:30.453028: step 4222, loss 0.331624, acc 0.84375, learning_rate 0.0001
2017-10-10T13:26:30.936453: step 4223, loss 0.289906, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:31.359537: step 4224, loss 0.296248, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:31.769089: step 4225, loss 0.375633, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:32.309823: step 4226, loss 0.50886, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:32.808882: step 4227, loss 0.388878, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:33.368913: step 4228, loss 0.35082, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:33.884870: step 4229, loss 0.265216, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:34.439576: step 4230, loss 0.395416, acc 0.828125, learning_rate 0.0001
2017-10-10T13:26:34.961408: step 4231, loss 0.272216, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:35.474257: step 4232, loss 0.308333, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:36.001104: step 4233, loss 0.351831, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:36.668854: step 4234, loss 0.151593, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:37.220484: step 4235, loss 0.223271, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:37.733857: step 4236, loss 0.371052, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:38.264879: step 4237, loss 0.346581, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:38.772911: step 4238, loss 0.452424, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:39.277944: step 4239, loss 0.332923, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:39.724883: step 4240, loss 0.457339, acc 0.796875, learning_rate 0.0001

Evaluation:
2017-10-10T13:26:41.115233: step 4240, loss 0.25877, acc 0.899281

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4240

2017-10-10T13:26:42.965029: step 4241, loss 0.260278, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:43.468958: step 4242, loss 0.309057, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:43.902535: step 4243, loss 0.238939, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:44.452427: step 4244, loss 0.413751, acc 0.828125, learning_rate 0.0001
2017-10-10T13:26:45.048602: step 4245, loss 0.4594, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:45.468297: step 4246, loss 0.352756, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:45.849690: step 4247, loss 0.37016, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:46.251892: step 4248, loss 0.215494, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:46.744836: step 4249, loss 0.307696, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:47.316838: step 4250, loss 0.337248, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:47.865164: step 4251, loss 0.383351, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:48.410877: step 4252, loss 0.225219, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:48.981223: step 4253, loss 0.288282, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:49.504883: step 4254, loss 0.480783, acc 0.8125, learning_rate 0.0001
2017-10-10T13:26:50.037021: step 4255, loss 0.281996, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:50.496991: step 4256, loss 0.230376, acc 0.9375, learning_rate 0.0001
2017-10-10T13:26:51.048866: step 4257, loss 0.38718, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:51.596918: step 4258, loss 0.352268, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:52.160866: step 4259, loss 0.287304, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:52.652977: step 4260, loss 0.375809, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:53.171827: step 4261, loss 0.159221, acc 0.953125, learning_rate 0.0001
2017-10-10T13:26:53.728992: step 4262, loss 0.428448, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:54.189263: step 4263, loss 0.390677, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:54.557079: step 4264, loss 0.519694, acc 0.84375, learning_rate 0.0001
2017-10-10T13:26:54.955703: step 4265, loss 0.308553, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:55.389309: step 4266, loss 0.394347, acc 0.890625, learning_rate 0.0001
2017-10-10T13:26:55.912982: step 4267, loss 0.495881, acc 0.84375, learning_rate 0.0001
2017-10-10T13:26:56.485093: step 4268, loss 0.372166, acc 0.921875, learning_rate 0.0001
2017-10-10T13:26:57.113054: step 4269, loss 0.338826, acc 0.90625, learning_rate 0.0001
2017-10-10T13:26:57.625068: step 4270, loss 0.346498, acc 0.875, learning_rate 0.0001
2017-10-10T13:26:58.216585: step 4271, loss 0.329236, acc 0.859375, learning_rate 0.0001
2017-10-10T13:26:58.700312: step 4272, loss 0.410188, acc 0.8125, learning_rate 0.0001
2017-10-10T13:26:59.209260: step 4273, loss 0.45981, acc 0.78125, learning_rate 0.0001
2017-10-10T13:26:59.730212: step 4274, loss 0.375087, acc 0.859375, learning_rate 0.0001
2017-10-10T13:27:00.212798: step 4275, loss 0.44822, acc 0.84375, learning_rate 0.0001
2017-10-10T13:27:00.680911: step 4276, loss 0.377451, acc 0.859375, learning_rate 0.0001
2017-10-10T13:27:01.297016: step 4277, loss 0.382433, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:01.853510: step 4278, loss 0.415592, acc 0.828125, learning_rate 0.0001
2017-10-10T13:27:02.372867: step 4279, loss 0.331114, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:02.930106: step 4280, loss 0.361435, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:27:04.332835: step 4280, loss 0.257181, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4280

2017-10-10T13:27:05.757092: step 4281, loss 0.315907, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:06.267733: step 4282, loss 0.278494, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:06.711445: step 4283, loss 0.222808, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:07.196864: step 4284, loss 0.423877, acc 0.78125, learning_rate 0.0001
2017-10-10T13:27:07.668905: step 4285, loss 0.304155, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:08.219529: step 4286, loss 0.315546, acc 0.859375, learning_rate 0.0001
2017-10-10T13:27:08.753104: step 4287, loss 0.275277, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:09.208083: step 4288, loss 0.332099, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:09.647928: step 4289, loss 0.208027, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:10.195651: step 4290, loss 0.255589, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:10.747376: step 4291, loss 0.316909, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:11.263768: step 4292, loss 0.251323, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:11.748582: step 4293, loss 0.303366, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:12.323666: step 4294, loss 0.341277, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:12.833082: step 4295, loss 0.3278, acc 0.84375, learning_rate 0.0001
2017-10-10T13:27:13.335741: step 4296, loss 0.387572, acc 0.84375, learning_rate 0.0001
2017-10-10T13:27:13.765353: step 4297, loss 0.280795, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:14.313830: step 4298, loss 0.26236, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:14.792989: step 4299, loss 0.358423, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:15.290560: step 4300, loss 0.493373, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:15.706115: step 4301, loss 0.331549, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:16.264834: step 4302, loss 0.450962, acc 0.859375, learning_rate 0.0001
2017-10-10T13:27:16.819992: step 4303, loss 0.183176, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:17.334561: step 4304, loss 0.226074, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:17.804841: step 4305, loss 0.320269, acc 0.84375, learning_rate 0.0001
2017-10-10T13:27:18.324913: step 4306, loss 0.368507, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:18.856851: step 4307, loss 0.445823, acc 0.84375, learning_rate 0.0001
2017-10-10T13:27:19.368978: step 4308, loss 0.37248, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:19.904930: step 4309, loss 0.440151, acc 0.859375, learning_rate 0.0001
2017-10-10T13:27:20.473187: step 4310, loss 0.264987, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:20.947173: step 4311, loss 0.257498, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:21.417363: step 4312, loss 0.337715, acc 0.862745, learning_rate 0.0001
2017-10-10T13:27:21.925188: step 4313, loss 0.24856, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:22.453088: step 4314, loss 0.314709, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:22.970519: step 4315, loss 0.305069, acc 0.859375, learning_rate 0.0001
2017-10-10T13:27:23.421178: step 4316, loss 0.208108, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:23.969865: step 4317, loss 0.189862, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:24.549122: step 4318, loss 0.277517, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:25.072982: step 4319, loss 0.262524, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:25.603835: step 4320, loss 0.382092, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:27:26.995826: step 4320, loss 0.257579, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4320

2017-10-10T13:27:28.589059: step 4321, loss 0.276548, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:29.084843: step 4322, loss 0.334448, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:29.547962: step 4323, loss 0.161841, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:29.910569: step 4324, loss 0.336194, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:30.368986: step 4325, loss 0.414847, acc 0.828125, learning_rate 0.0001
2017-10-10T13:27:30.941046: step 4326, loss 0.198575, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:31.497099: step 4327, loss 0.314296, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:31.906207: step 4328, loss 0.306345, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:32.345439: step 4329, loss 0.267326, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:32.760915: step 4330, loss 0.305864, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:33.305122: step 4331, loss 0.358443, acc 0.84375, learning_rate 0.0001
2017-10-10T13:27:33.775727: step 4332, loss 0.289011, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:34.276299: step 4333, loss 0.437602, acc 0.828125, learning_rate 0.0001
2017-10-10T13:27:34.768995: step 4334, loss 0.476344, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:35.288965: step 4335, loss 0.371617, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:35.861581: step 4336, loss 0.37899, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:36.428988: step 4337, loss 0.309866, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:36.943385: step 4338, loss 0.431533, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:37.524862: step 4339, loss 0.185746, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:38.101035: step 4340, loss 0.222828, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:38.664887: step 4341, loss 0.531292, acc 0.8125, learning_rate 0.0001
2017-10-10T13:27:39.212836: step 4342, loss 0.33794, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:39.774677: step 4343, loss 0.232386, acc 0.96875, learning_rate 0.0001
2017-10-10T13:27:40.324850: step 4344, loss 0.487356, acc 0.84375, learning_rate 0.0001
2017-10-10T13:27:40.792834: step 4345, loss 0.159839, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:41.207265: step 4346, loss 0.520396, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:41.577062: step 4347, loss 0.300875, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:42.147917: step 4348, loss 0.406254, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:42.682670: step 4349, loss 0.353167, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:43.272604: step 4350, loss 0.305472, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:43.896629: step 4351, loss 0.392008, acc 0.828125, learning_rate 0.0001
2017-10-10T13:27:44.428856: step 4352, loss 0.454127, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:44.908190: step 4353, loss 0.295896, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:45.444976: step 4354, loss 0.247787, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:45.937410: step 4355, loss 0.301389, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:46.456880: step 4356, loss 0.434904, acc 0.859375, learning_rate 0.0001
2017-10-10T13:27:46.963469: step 4357, loss 0.477922, acc 0.8125, learning_rate 0.0001
2017-10-10T13:27:47.481439: step 4358, loss 0.307926, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:48.052997: step 4359, loss 0.335648, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:48.636981: step 4360, loss 0.29754, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:27:50.007542: step 4360, loss 0.256558, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4360

2017-10-10T13:27:51.774226: step 4361, loss 0.330422, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:52.224964: step 4362, loss 0.194348, acc 0.9375, learning_rate 0.0001
2017-10-10T13:27:52.640415: step 4363, loss 0.413972, acc 0.84375, learning_rate 0.0001
2017-10-10T13:27:53.091320: step 4364, loss 0.171721, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:53.716863: step 4365, loss 0.247373, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:54.288864: step 4366, loss 0.395287, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:54.680955: step 4367, loss 0.302401, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:55.113970: step 4368, loss 0.409141, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:55.529006: step 4369, loss 0.171606, acc 0.953125, learning_rate 0.0001
2017-10-10T13:27:56.089752: step 4370, loss 0.295456, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:56.625100: step 4371, loss 0.281606, acc 0.921875, learning_rate 0.0001
2017-10-10T13:27:57.196978: step 4372, loss 0.366621, acc 0.90625, learning_rate 0.0001
2017-10-10T13:27:57.700453: step 4373, loss 0.36592, acc 0.828125, learning_rate 0.0001
2017-10-10T13:27:58.209058: step 4374, loss 0.287135, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:58.734394: step 4375, loss 0.501609, acc 0.875, learning_rate 0.0001
2017-10-10T13:27:59.279774: step 4376, loss 0.255678, acc 0.890625, learning_rate 0.0001
2017-10-10T13:27:59.768920: step 4377, loss 0.198103, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:00.319034: step 4378, loss 0.262477, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:00.857213: step 4379, loss 0.398947, acc 0.8125, learning_rate 0.0001
2017-10-10T13:28:01.437025: step 4380, loss 0.281766, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:01.920890: step 4381, loss 0.399689, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:02.460857: step 4382, loss 0.536993, acc 0.84375, learning_rate 0.0001
2017-10-10T13:28:02.920930: step 4383, loss 0.241729, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:03.496417: step 4384, loss 0.249195, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:04.027512: step 4385, loss 0.394684, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:04.576274: step 4386, loss 0.338971, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:05.073691: step 4387, loss 0.312669, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:05.485718: step 4388, loss 0.279772, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:06.051539: step 4389, loss 0.303723, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:06.593448: step 4390, loss 0.353128, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:07.131448: step 4391, loss 0.381297, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:07.649319: step 4392, loss 0.409121, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:08.097062: step 4393, loss 0.527325, acc 0.796875, learning_rate 0.0001
2017-10-10T13:28:08.563633: step 4394, loss 0.412682, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:09.077225: step 4395, loss 0.391218, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:09.636900: step 4396, loss 0.538321, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:10.233679: step 4397, loss 0.414061, acc 0.828125, learning_rate 0.0001
2017-10-10T13:28:10.795888: step 4398, loss 0.382148, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:11.286141: step 4399, loss 0.394986, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:11.749030: step 4400, loss 0.480516, acc 0.8125, learning_rate 0.0001

Evaluation:
2017-10-10T13:28:13.140823: step 4400, loss 0.256411, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4400

2017-10-10T13:28:14.486250: step 4401, loss 0.136406, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:15.053762: step 4402, loss 0.242169, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:15.493003: step 4403, loss 0.189304, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:15.888994: step 4404, loss 0.231565, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:16.376845: step 4405, loss 0.292805, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:16.969295: step 4406, loss 0.3872, acc 0.84375, learning_rate 0.0001
2017-10-10T13:28:17.430515: step 4407, loss 0.318032, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:17.863929: step 4408, loss 0.349687, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:18.370707: step 4409, loss 0.361502, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:18.804930: step 4410, loss 0.355664, acc 0.803922, learning_rate 0.0001
2017-10-10T13:28:19.348135: step 4411, loss 0.221312, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:19.908271: step 4412, loss 0.27096, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:20.400973: step 4413, loss 0.323867, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:20.979795: step 4414, loss 0.422923, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:21.496903: step 4415, loss 0.298413, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:22.007334: step 4416, loss 0.320747, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:22.479248: step 4417, loss 0.277096, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:22.948853: step 4418, loss 0.423384, acc 0.8125, learning_rate 0.0001
2017-10-10T13:28:23.438912: step 4419, loss 0.345564, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:23.960556: step 4420, loss 0.328545, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:24.449255: step 4421, loss 0.357731, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:24.964062: step 4422, loss 0.312923, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:25.514080: step 4423, loss 0.452004, acc 0.828125, learning_rate 0.0001
2017-10-10T13:28:26.117035: step 4424, loss 0.307755, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:26.673081: step 4425, loss 0.192204, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:27.301141: step 4426, loss 0.278199, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:27.912872: step 4427, loss 0.326943, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:28.329031: step 4428, loss 0.48798, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:28.733024: step 4429, loss 0.408254, acc 0.828125, learning_rate 0.0001
2017-10-10T13:28:29.283676: step 4430, loss 0.510353, acc 0.828125, learning_rate 0.0001
2017-10-10T13:28:29.743159: step 4431, loss 0.309372, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:30.233240: step 4432, loss 0.216145, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:30.763112: step 4433, loss 0.227662, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:31.316578: step 4434, loss 0.328665, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:31.844941: step 4435, loss 0.248807, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:32.383089: step 4436, loss 0.383417, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:32.951748: step 4437, loss 0.2179, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:33.472045: step 4438, loss 0.218253, acc 0.953125, learning_rate 0.0001
2017-10-10T13:28:34.047784: step 4439, loss 0.275039, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:34.581137: step 4440, loss 0.315676, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:28:36.070734: step 4440, loss 0.256518, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4440

2017-10-10T13:28:37.755461: step 4441, loss 0.247209, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:38.248238: step 4442, loss 0.362134, acc 0.84375, learning_rate 0.0001
2017-10-10T13:28:38.705294: step 4443, loss 0.301324, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:39.161436: step 4444, loss 0.446528, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:39.716697: step 4445, loss 0.310632, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:40.273503: step 4446, loss 0.253322, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:40.724834: step 4447, loss 0.482029, acc 0.78125, learning_rate 0.0001
2017-10-10T13:28:41.156715: step 4448, loss 0.333955, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:41.596169: step 4449, loss 0.318037, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:42.136731: step 4450, loss 0.139946, acc 0.96875, learning_rate 0.0001
2017-10-10T13:28:42.688602: step 4451, loss 0.352005, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:43.216750: step 4452, loss 0.317226, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:43.788531: step 4453, loss 0.350412, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:44.360596: step 4454, loss 0.275249, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:44.932831: step 4455, loss 0.453537, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:45.457007: step 4456, loss 0.279526, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:45.985155: step 4457, loss 0.423478, acc 0.84375, learning_rate 0.0001
2017-10-10T13:28:46.510759: step 4458, loss 0.342663, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:47.053867: step 4459, loss 0.353017, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:47.590017: step 4460, loss 0.216179, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:48.154122: step 4461, loss 0.431293, acc 0.828125, learning_rate 0.0001
2017-10-10T13:28:48.695894: step 4462, loss 0.286384, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:49.260200: step 4463, loss 0.206446, acc 0.9375, learning_rate 0.0001
2017-10-10T13:28:49.828157: step 4464, loss 0.239089, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:50.464554: step 4465, loss 0.333863, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:51.053242: step 4466, loss 0.325248, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:51.523696: step 4467, loss 0.21752, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:51.960876: step 4468, loss 0.330188, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:52.496957: step 4469, loss 0.26896, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:53.053393: step 4470, loss 0.332125, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:53.613825: step 4471, loss 0.278589, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:54.157810: step 4472, loss 0.306229, acc 0.859375, learning_rate 0.0001
2017-10-10T13:28:54.767534: step 4473, loss 0.286111, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:55.308445: step 4474, loss 0.237569, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:55.829064: step 4475, loss 0.277731, acc 0.90625, learning_rate 0.0001
2017-10-10T13:28:56.377249: step 4476, loss 0.334651, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:56.917012: step 4477, loss 0.283748, acc 0.890625, learning_rate 0.0001
2017-10-10T13:28:57.435394: step 4478, loss 0.336608, acc 0.875, learning_rate 0.0001
2017-10-10T13:28:58.000967: step 4479, loss 0.218198, acc 0.921875, learning_rate 0.0001
2017-10-10T13:28:58.564367: step 4480, loss 0.278954, acc 0.859375, learning_rate 0.0001

Evaluation:
2017-10-10T13:29:00.057891: step 4480, loss 0.255596, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4480

2017-10-10T13:29:01.688897: step 4481, loss 0.479735, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:02.155034: step 4482, loss 0.338062, acc 0.859375, learning_rate 0.0001
2017-10-10T13:29:02.582432: step 4483, loss 0.227463, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:03.080888: step 4484, loss 0.206344, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:03.700887: step 4485, loss 0.306752, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:04.284882: step 4486, loss 0.209996, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:04.748919: step 4487, loss 0.544093, acc 0.828125, learning_rate 0.0001
2017-10-10T13:29:05.238615: step 4488, loss 0.223389, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:05.798257: step 4489, loss 0.304955, acc 0.859375, learning_rate 0.0001
2017-10-10T13:29:06.312868: step 4490, loss 0.342819, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:06.816799: step 4491, loss 0.504579, acc 0.765625, learning_rate 0.0001
2017-10-10T13:29:07.384840: step 4492, loss 0.301312, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:07.893011: step 4493, loss 0.400865, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:08.412830: step 4494, loss 0.391552, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:08.953045: step 4495, loss 0.260161, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:09.448846: step 4496, loss 0.188933, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:10.009848: step 4497, loss 0.259256, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:10.554331: step 4498, loss 0.288583, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:11.096959: step 4499, loss 0.26981, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:11.577150: step 4500, loss 0.447491, acc 0.8125, learning_rate 0.0001
2017-10-10T13:29:12.139067: step 4501, loss 0.48471, acc 0.78125, learning_rate 0.0001
2017-10-10T13:29:12.661647: step 4502, loss 0.341106, acc 0.828125, learning_rate 0.0001
2017-10-10T13:29:13.254284: step 4503, loss 0.589475, acc 0.84375, learning_rate 0.0001
2017-10-10T13:29:13.788884: step 4504, loss 0.340965, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:14.379031: step 4505, loss 0.155119, acc 0.96875, learning_rate 0.0001
2017-10-10T13:29:14.900848: step 4506, loss 0.380105, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:15.335951: step 4507, loss 0.306473, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:15.734250: step 4508, loss 0.159262, acc 0.980392, learning_rate 0.0001
2017-10-10T13:29:16.148853: step 4509, loss 0.251792, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:16.693397: step 4510, loss 0.340546, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:17.169496: step 4511, loss 0.245184, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:17.680832: step 4512, loss 0.31822, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:18.157016: step 4513, loss 0.396538, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:18.668902: step 4514, loss 0.384232, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:19.253011: step 4515, loss 0.148636, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:19.773161: step 4516, loss 0.301552, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:20.248146: step 4517, loss 0.555538, acc 0.796875, learning_rate 0.0001
2017-10-10T13:29:20.724092: step 4518, loss 0.339207, acc 0.828125, learning_rate 0.0001
2017-10-10T13:29:21.301778: step 4519, loss 0.25674, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:21.865930: step 4520, loss 0.194176, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:29:23.320975: step 4520, loss 0.25559, acc 0.902158

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4520

2017-10-10T13:29:24.844457: step 4521, loss 0.260893, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:25.269209: step 4522, loss 0.381394, acc 0.84375, learning_rate 0.0001
2017-10-10T13:29:25.725144: step 4523, loss 0.23628, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:26.320901: step 4524, loss 0.170642, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:26.881023: step 4525, loss 0.235278, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:27.340286: step 4526, loss 0.321661, acc 0.84375, learning_rate 0.0001
2017-10-10T13:29:27.801282: step 4527, loss 0.288965, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:28.240639: step 4528, loss 0.197279, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:28.791584: step 4529, loss 0.234766, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:29.346304: step 4530, loss 0.332133, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:29.901173: step 4531, loss 0.395908, acc 0.84375, learning_rate 0.0001
2017-10-10T13:29:30.457210: step 4532, loss 0.29555, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:31.037833: step 4533, loss 0.314149, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:31.555301: step 4534, loss 0.271808, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:32.129618: step 4535, loss 0.302125, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:32.661422: step 4536, loss 0.305098, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:33.269846: step 4537, loss 0.325466, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:33.753052: step 4538, loss 0.465477, acc 0.859375, learning_rate 0.0001
2017-10-10T13:29:34.368426: step 4539, loss 0.204675, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:34.925002: step 4540, loss 0.317592, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:35.420061: step 4541, loss 0.288413, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:35.953010: step 4542, loss 0.44392, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:36.492998: step 4543, loss 0.394336, acc 0.859375, learning_rate 0.0001
2017-10-10T13:29:36.990788: step 4544, loss 0.375519, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:37.550337: step 4545, loss 0.387982, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:38.192960: step 4546, loss 0.323225, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:38.660940: step 4547, loss 0.386698, acc 0.859375, learning_rate 0.0001
2017-10-10T13:29:39.048830: step 4548, loss 0.175783, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:39.547722: step 4549, loss 0.285964, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:40.089062: step 4550, loss 0.350071, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:40.660882: step 4551, loss 0.320243, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:41.187436: step 4552, loss 0.548004, acc 0.84375, learning_rate 0.0001
2017-10-10T13:29:41.700918: step 4553, loss 0.265649, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:42.205712: step 4554, loss 0.32104, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:42.638812: step 4555, loss 0.463661, acc 0.84375, learning_rate 0.0001
2017-10-10T13:29:43.115672: step 4556, loss 0.389488, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:43.576901: step 4557, loss 0.258792, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:44.064545: step 4558, loss 0.245063, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:44.604988: step 4559, loss 0.341484, acc 0.84375, learning_rate 0.0001
2017-10-10T13:29:45.221218: step 4560, loss 0.492263, acc 0.8125, learning_rate 0.0001

Evaluation:
2017-10-10T13:29:46.623631: step 4560, loss 0.254998, acc 0.900719

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4560

2017-10-10T13:29:47.998499: step 4561, loss 0.275516, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:48.404770: step 4562, loss 0.313909, acc 0.859375, learning_rate 0.0001
2017-10-10T13:29:48.872946: step 4563, loss 0.251932, acc 0.953125, learning_rate 0.0001
2017-10-10T13:29:49.308960: step 4564, loss 0.465629, acc 0.84375, learning_rate 0.0001
2017-10-10T13:29:49.874306: step 4565, loss 0.42385, acc 0.8125, learning_rate 0.0001
2017-10-10T13:29:50.421075: step 4566, loss 0.33435, acc 0.84375, learning_rate 0.0001
2017-10-10T13:29:50.830551: step 4567, loss 0.235966, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:51.250782: step 4568, loss 0.299053, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:51.697404: step 4569, loss 0.399847, acc 0.84375, learning_rate 0.0001
2017-10-10T13:29:52.256724: step 4570, loss 0.168128, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:52.721126: step 4571, loss 0.36897, acc 0.859375, learning_rate 0.0001
2017-10-10T13:29:53.191999: step 4572, loss 0.239212, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:53.662962: step 4573, loss 0.246925, acc 0.90625, learning_rate 0.0001
2017-10-10T13:29:54.202404: step 4574, loss 0.411832, acc 0.859375, learning_rate 0.0001
2017-10-10T13:29:54.764860: step 4575, loss 0.303121, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:55.316830: step 4576, loss 0.538036, acc 0.828125, learning_rate 0.0001
2017-10-10T13:29:55.921190: step 4577, loss 0.363242, acc 0.890625, learning_rate 0.0001
2017-10-10T13:29:56.504971: step 4578, loss 0.36223, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:56.972326: step 4579, loss 0.350747, acc 0.859375, learning_rate 0.0001
2017-10-10T13:29:57.488931: step 4580, loss 0.213548, acc 0.9375, learning_rate 0.0001
2017-10-10T13:29:58.044083: step 4581, loss 0.318044, acc 0.875, learning_rate 0.0001
2017-10-10T13:29:58.588848: step 4582, loss 0.233668, acc 0.921875, learning_rate 0.0001
2017-10-10T13:29:59.111774: step 4583, loss 0.446587, acc 0.859375, learning_rate 0.0001
2017-10-10T13:29:59.685203: step 4584, loss 0.330492, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:00.224881: step 4585, loss 0.47603, acc 0.796875, learning_rate 0.0001
2017-10-10T13:30:00.856041: step 4586, loss 0.262432, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:01.404831: step 4587, loss 0.435299, acc 0.84375, learning_rate 0.0001
2017-10-10T13:30:01.843799: step 4588, loss 0.328463, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:02.310642: step 4589, loss 0.331951, acc 0.84375, learning_rate 0.0001
2017-10-10T13:30:02.876856: step 4590, loss 0.227462, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:03.424828: step 4591, loss 0.270045, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:03.955598: step 4592, loss 0.275537, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:04.480881: step 4593, loss 0.29492, acc 0.859375, learning_rate 0.0001
2017-10-10T13:30:04.945209: step 4594, loss 0.412302, acc 0.828125, learning_rate 0.0001
2017-10-10T13:30:05.464890: step 4595, loss 0.460311, acc 0.84375, learning_rate 0.0001
2017-10-10T13:30:05.962950: step 4596, loss 0.630029, acc 0.84375, learning_rate 0.0001
2017-10-10T13:30:06.500912: step 4597, loss 0.344201, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:07.032770: step 4598, loss 0.181673, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:07.512951: step 4599, loss 0.406368, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:08.033642: step 4600, loss 0.323585, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:30:09.428298: step 4600, loss 0.255458, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4600

2017-10-10T13:30:11.001907: step 4601, loss 0.39408, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:11.392730: step 4602, loss 0.305136, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:11.840544: step 4603, loss 0.339328, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:12.306575: step 4604, loss 0.274864, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:12.861707: step 4605, loss 0.294779, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:13.377040: step 4606, loss 0.16764, acc 0.960784, learning_rate 0.0001
2017-10-10T13:30:13.872259: step 4607, loss 0.257122, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:14.321840: step 4608, loss 0.163281, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:14.788881: step 4609, loss 0.473386, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:15.315904: step 4610, loss 0.349762, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:15.807843: step 4611, loss 0.507408, acc 0.859375, learning_rate 0.0001
2017-10-10T13:30:16.336859: step 4612, loss 0.27973, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:16.904998: step 4613, loss 0.210723, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:17.504847: step 4614, loss 0.263073, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:18.055920: step 4615, loss 0.397305, acc 0.859375, learning_rate 0.0001
2017-10-10T13:30:18.586692: step 4616, loss 0.308806, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:19.121060: step 4617, loss 0.323488, acc 0.859375, learning_rate 0.0001
2017-10-10T13:30:19.663908: step 4618, loss 0.314249, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:20.138798: step 4619, loss 0.359165, acc 0.84375, learning_rate 0.0001
2017-10-10T13:30:20.694700: step 4620, loss 0.351255, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:21.286674: step 4621, loss 0.386544, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:21.782051: step 4622, loss 0.281267, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:22.288131: step 4623, loss 0.325155, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:22.849518: step 4624, loss 0.495751, acc 0.859375, learning_rate 0.0001
2017-10-10T13:30:23.410816: step 4625, loss 0.324121, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:23.999723: step 4626, loss 0.412015, acc 0.859375, learning_rate 0.0001
2017-10-10T13:30:24.524863: step 4627, loss 0.303988, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:24.980855: step 4628, loss 0.194862, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:25.439745: step 4629, loss 0.290143, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:25.844851: step 4630, loss 0.395156, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:26.335280: step 4631, loss 0.42392, acc 0.828125, learning_rate 0.0001
2017-10-10T13:30:26.832989: step 4632, loss 0.212424, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:27.335992: step 4633, loss 0.403139, acc 0.859375, learning_rate 0.0001
2017-10-10T13:30:27.770261: step 4634, loss 0.347056, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:28.247532: step 4635, loss 0.195656, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:28.736860: step 4636, loss 0.307936, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:29.307140: step 4637, loss 0.351252, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:29.812708: step 4638, loss 0.495608, acc 0.78125, learning_rate 0.0001
2017-10-10T13:30:30.395261: step 4639, loss 0.337255, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:30.984896: step 4640, loss 0.251544, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:30:32.651122: step 4640, loss 0.253166, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4640

2017-10-10T13:30:34.369231: step 4641, loss 0.521444, acc 0.78125, learning_rate 0.0001
2017-10-10T13:30:34.710275: step 4642, loss 0.528483, acc 0.84375, learning_rate 0.0001
2017-10-10T13:30:35.162844: step 4643, loss 0.364709, acc 0.84375, learning_rate 0.0001
2017-10-10T13:30:35.872849: step 4644, loss 0.197651, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:36.428898: step 4645, loss 0.28221, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:36.899587: step 4646, loss 0.289891, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:37.373941: step 4647, loss 0.199607, acc 0.984375, learning_rate 0.0001
2017-10-10T13:30:37.786226: step 4648, loss 0.232002, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:38.325056: step 4649, loss 0.290723, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:38.948836: step 4650, loss 0.352052, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:39.522853: step 4651, loss 0.233834, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:40.051703: step 4652, loss 0.335736, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:40.602777: step 4653, loss 0.232316, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:41.161215: step 4654, loss 0.196247, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:41.651550: step 4655, loss 0.273531, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:42.245168: step 4656, loss 0.252523, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:42.773239: step 4657, loss 0.441332, acc 0.84375, learning_rate 0.0001
2017-10-10T13:30:43.299060: step 4658, loss 0.359121, acc 0.828125, learning_rate 0.0001
2017-10-10T13:30:43.832981: step 4659, loss 0.322849, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:44.408848: step 4660, loss 0.202717, acc 0.9375, learning_rate 0.0001
2017-10-10T13:30:44.966420: step 4661, loss 0.241744, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:45.541143: step 4662, loss 0.460923, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:46.121885: step 4663, loss 0.417521, acc 0.796875, learning_rate 0.0001
2017-10-10T13:30:46.661726: step 4664, loss 0.457779, acc 0.859375, learning_rate 0.0001
2017-10-10T13:30:47.304851: step 4665, loss 0.32015, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:47.882916: step 4666, loss 0.367178, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:48.328309: step 4667, loss 0.520322, acc 0.78125, learning_rate 0.0001
2017-10-10T13:30:48.788811: step 4668, loss 0.434861, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:49.369340: step 4669, loss 0.25787, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:49.940900: step 4670, loss 0.216274, acc 0.921875, learning_rate 0.0001
2017-10-10T13:30:50.528898: step 4671, loss 0.28255, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:51.042943: step 4672, loss 0.325863, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:51.559990: step 4673, loss 0.482955, acc 0.828125, learning_rate 0.0001
2017-10-10T13:30:52.103935: step 4674, loss 0.30974, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:52.688916: step 4675, loss 0.331512, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:53.245263: step 4676, loss 0.24061, acc 0.96875, learning_rate 0.0001
2017-10-10T13:30:53.773137: step 4677, loss 0.283086, acc 0.953125, learning_rate 0.0001
2017-10-10T13:30:54.301522: step 4678, loss 0.488126, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:54.855971: step 4679, loss 0.363704, acc 0.875, learning_rate 0.0001
2017-10-10T13:30:55.387827: step 4680, loss 0.219622, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:30:56.780849: step 4680, loss 0.253336, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4680

2017-10-10T13:30:58.157635: step 4681, loss 0.27991, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:58.736829: step 4682, loss 0.377451, acc 0.890625, learning_rate 0.0001
2017-10-10T13:30:59.280997: step 4683, loss 0.316691, acc 0.90625, learning_rate 0.0001
2017-10-10T13:30:59.949143: step 4684, loss 0.169919, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:00.493026: step 4685, loss 0.357636, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:00.855358: step 4686, loss 0.221047, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:01.293058: step 4687, loss 0.432154, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:01.888946: step 4688, loss 0.280287, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:02.338692: step 4689, loss 0.346065, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:02.737935: step 4690, loss 0.271888, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:03.459297: step 4691, loss 0.222779, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:04.020824: step 4692, loss 0.21818, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:04.572197: step 4693, loss 0.286079, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:05.093359: step 4694, loss 0.317863, acc 0.875, learning_rate 0.0001
2017-10-10T13:31:05.628825: step 4695, loss 0.355633, acc 0.875, learning_rate 0.0001
2017-10-10T13:31:06.153568: step 4696, loss 0.409752, acc 0.828125, learning_rate 0.0001
2017-10-10T13:31:06.648829: step 4697, loss 0.428924, acc 0.84375, learning_rate 0.0001
2017-10-10T13:31:07.229295: step 4698, loss 0.460268, acc 0.828125, learning_rate 0.0001
2017-10-10T13:31:07.733957: step 4699, loss 0.241152, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:08.276186: step 4700, loss 0.251042, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:08.881070: step 4701, loss 0.133655, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:09.363470: step 4702, loss 0.23648, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:09.907583: step 4703, loss 0.268471, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:10.400838: step 4704, loss 0.476464, acc 0.882353, learning_rate 0.0001
2017-10-10T13:31:11.024385: step 4705, loss 0.410166, acc 0.875, learning_rate 0.0001
2017-10-10T13:31:11.462817: step 4706, loss 0.35735, acc 0.875, learning_rate 0.0001
2017-10-10T13:31:11.920869: step 4707, loss 0.250362, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:12.469779: step 4708, loss 0.211887, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:12.972294: step 4709, loss 0.364079, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:13.500913: step 4710, loss 0.219305, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:14.072163: step 4711, loss 0.257154, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:14.609163: step 4712, loss 0.308627, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:15.202068: step 4713, loss 0.364966, acc 0.859375, learning_rate 0.0001
2017-10-10T13:31:15.771100: step 4714, loss 0.240346, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:16.284122: step 4715, loss 0.324684, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:16.808979: step 4716, loss 0.354393, acc 0.84375, learning_rate 0.0001
2017-10-10T13:31:17.388851: step 4717, loss 0.320067, acc 0.875, learning_rate 0.0001
2017-10-10T13:31:17.961036: step 4718, loss 0.219202, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:18.540912: step 4719, loss 0.355586, acc 0.84375, learning_rate 0.0001
2017-10-10T13:31:19.112961: step 4720, loss 0.298194, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:31:20.543770: step 4720, loss 0.253523, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4720

2017-10-10T13:31:22.088383: step 4721, loss 0.331943, acc 0.859375, learning_rate 0.0001
2017-10-10T13:31:22.652987: step 4722, loss 0.436389, acc 0.828125, learning_rate 0.0001
2017-10-10T13:31:23.301278: step 4723, loss 0.305993, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:23.769187: step 4724, loss 0.390016, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:24.200849: step 4725, loss 0.181986, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:24.718798: step 4726, loss 0.165274, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:25.257382: step 4727, loss 0.38498, acc 0.8125, learning_rate 0.0001
2017-10-10T13:31:25.744917: step 4728, loss 0.35763, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:26.205363: step 4729, loss 0.420555, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:26.709091: step 4730, loss 0.28728, acc 0.875, learning_rate 0.0001
2017-10-10T13:31:27.266565: step 4731, loss 0.31672, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:27.732965: step 4732, loss 0.376522, acc 0.859375, learning_rate 0.0001
2017-10-10T13:31:28.276918: step 4733, loss 0.350053, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:28.765235: step 4734, loss 0.264914, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:29.353082: step 4735, loss 0.254461, acc 0.875, learning_rate 0.0001
2017-10-10T13:31:29.948967: step 4736, loss 0.39032, acc 0.859375, learning_rate 0.0001
2017-10-10T13:31:30.462820: step 4737, loss 0.352612, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:30.953064: step 4738, loss 0.365702, acc 0.859375, learning_rate 0.0001
2017-10-10T13:31:31.447133: step 4739, loss 0.568039, acc 0.8125, learning_rate 0.0001
2017-10-10T13:31:31.989169: step 4740, loss 0.198239, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:32.580366: step 4741, loss 0.250772, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:33.200958: step 4742, loss 0.272615, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:33.745935: step 4743, loss 0.365727, acc 0.84375, learning_rate 0.0001
2017-10-10T13:31:34.152586: step 4744, loss 0.452268, acc 0.84375, learning_rate 0.0001
2017-10-10T13:31:34.594159: step 4745, loss 0.212583, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:35.192906: step 4746, loss 0.594117, acc 0.78125, learning_rate 0.0001
2017-10-10T13:31:35.709443: step 4747, loss 0.265459, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:36.220005: step 4748, loss 0.204846, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:36.758359: step 4749, loss 0.354707, acc 0.875, learning_rate 0.0001
2017-10-10T13:31:37.284222: step 4750, loss 0.215664, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:37.865140: step 4751, loss 0.256082, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:38.476951: step 4752, loss 0.383834, acc 0.875, learning_rate 0.0001
2017-10-10T13:31:38.998658: step 4753, loss 0.241633, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:39.580398: step 4754, loss 0.364511, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:40.077021: step 4755, loss 0.340868, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:40.532801: step 4756, loss 0.322348, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:41.105325: step 4757, loss 0.234953, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:41.616120: step 4758, loss 0.231268, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:42.123440: step 4759, loss 0.425643, acc 0.796875, learning_rate 0.0001
2017-10-10T13:31:42.669792: step 4760, loss 0.454765, acc 0.828125, learning_rate 0.0001

Evaluation:
2017-10-10T13:31:43.900308: step 4760, loss 0.254887, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4760

2017-10-10T13:31:45.623677: step 4761, loss 0.370502, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:46.082735: step 4762, loss 0.355127, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:46.553199: step 4763, loss 0.473659, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:47.008343: step 4764, loss 0.353113, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:47.593906: step 4765, loss 0.469321, acc 0.859375, learning_rate 0.0001
2017-10-10T13:31:48.111948: step 4766, loss 0.361818, acc 0.84375, learning_rate 0.0001
2017-10-10T13:31:48.588473: step 4767, loss 0.319121, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:49.113032: step 4768, loss 0.334661, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:49.700897: step 4769, loss 0.517243, acc 0.765625, learning_rate 0.0001
2017-10-10T13:31:50.248838: step 4770, loss 0.389076, acc 0.859375, learning_rate 0.0001
2017-10-10T13:31:50.767126: step 4771, loss 0.331524, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:51.280177: step 4772, loss 0.298342, acc 0.875, learning_rate 0.0001
2017-10-10T13:31:51.797220: step 4773, loss 0.321312, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:52.261059: step 4774, loss 0.420062, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:52.833541: step 4775, loss 0.253562, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:53.394386: step 4776, loss 0.193936, acc 0.953125, learning_rate 0.0001
2017-10-10T13:31:53.912879: step 4777, loss 0.221846, acc 0.90625, learning_rate 0.0001
2017-10-10T13:31:54.468833: step 4778, loss 0.34561, acc 0.859375, learning_rate 0.0001
2017-10-10T13:31:55.009750: step 4779, loss 0.319486, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:55.508257: step 4780, loss 0.309578, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:56.033016: step 4781, loss 0.212474, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:56.641229: step 4782, loss 0.326921, acc 0.890625, learning_rate 0.0001
2017-10-10T13:31:57.013025: step 4783, loss 0.252672, acc 0.921875, learning_rate 0.0001
2017-10-10T13:31:57.409166: step 4784, loss 0.38402, acc 0.875, learning_rate 0.0001
2017-10-10T13:31:57.854348: step 4785, loss 0.430474, acc 0.84375, learning_rate 0.0001
2017-10-10T13:31:58.259893: step 4786, loss 0.277771, acc 0.9375, learning_rate 0.0001
2017-10-10T13:31:58.740868: step 4787, loss 0.160623, acc 0.984375, learning_rate 0.0001
2017-10-10T13:31:59.268279: step 4788, loss 0.184297, acc 0.96875, learning_rate 0.0001
2017-10-10T13:31:59.844359: step 4789, loss 0.258163, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:00.386471: step 4790, loss 0.277584, acc 0.875, learning_rate 0.0001
2017-10-10T13:32:00.928462: step 4791, loss 0.214181, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:01.466440: step 4792, loss 0.312842, acc 0.875, learning_rate 0.0001
2017-10-10T13:32:02.041296: step 4793, loss 0.390699, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:02.608055: step 4794, loss 0.366926, acc 0.859375, learning_rate 0.0001
2017-10-10T13:32:03.127398: step 4795, loss 0.354849, acc 0.84375, learning_rate 0.0001
2017-10-10T13:32:03.701203: step 4796, loss 0.332793, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:04.335141: step 4797, loss 0.311298, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:04.876888: step 4798, loss 0.199067, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:05.395728: step 4799, loss 0.231855, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:05.933027: step 4800, loss 0.314601, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:32:07.281074: step 4800, loss 0.252437, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4800

2017-10-10T13:32:08.818575: step 4801, loss 0.298107, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:09.277687: step 4802, loss 0.342992, acc 0.882353, learning_rate 0.0001
2017-10-10T13:32:09.708773: step 4803, loss 0.261255, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:10.169045: step 4804, loss 0.361906, acc 0.859375, learning_rate 0.0001
2017-10-10T13:32:10.653248: step 4805, loss 0.491781, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:11.202788: step 4806, loss 0.186513, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:11.655427: step 4807, loss 0.24881, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:12.176998: step 4808, loss 0.198301, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:12.648961: step 4809, loss 0.38435, acc 0.859375, learning_rate 0.0001
2017-10-10T13:32:13.186263: step 4810, loss 0.416226, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:13.710202: step 4811, loss 0.175859, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:14.248883: step 4812, loss 0.419339, acc 0.84375, learning_rate 0.0001
2017-10-10T13:32:14.784965: step 4813, loss 0.436988, acc 0.8125, learning_rate 0.0001
2017-10-10T13:32:15.359866: step 4814, loss 0.275223, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:15.939306: step 4815, loss 0.332311, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:16.451234: step 4816, loss 0.457161, acc 0.84375, learning_rate 0.0001
2017-10-10T13:32:16.993591: step 4817, loss 0.317855, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:17.397177: step 4818, loss 0.378084, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:17.854283: step 4819, loss 0.287942, acc 0.859375, learning_rate 0.0001
2017-10-10T13:32:18.362700: step 4820, loss 0.356952, acc 0.875, learning_rate 0.0001
2017-10-10T13:32:18.903849: step 4821, loss 0.225977, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:19.501108: step 4822, loss 0.192098, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:20.123050: step 4823, loss 0.433648, acc 0.84375, learning_rate 0.0001
2017-10-10T13:32:20.560327: step 4824, loss 0.241947, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:21.024935: step 4825, loss 0.280131, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:21.616931: step 4826, loss 0.301651, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:22.097005: step 4827, loss 0.307441, acc 0.875, learning_rate 0.0001
2017-10-10T13:32:22.581330: step 4828, loss 0.176234, acc 0.9375, learning_rate 0.0001
2017-10-10T13:32:23.060878: step 4829, loss 0.253743, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:23.632864: step 4830, loss 0.23019, acc 0.859375, learning_rate 0.0001
2017-10-10T13:32:24.185785: step 4831, loss 0.303166, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:24.731695: step 4832, loss 0.311873, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:25.240318: step 4833, loss 0.305152, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:25.798948: step 4834, loss 0.351334, acc 0.875, learning_rate 0.0001
2017-10-10T13:32:26.382701: step 4835, loss 0.278724, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:26.939199: step 4836, loss 0.285912, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:27.464909: step 4837, loss 0.248511, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:28.048979: step 4838, loss 0.236264, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:28.455410: step 4839, loss 0.389446, acc 0.875, learning_rate 0.0001
2017-10-10T13:32:28.962933: step 4840, loss 0.194379, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:32:32.604327: step 4840, loss 0.253111, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4840

2017-10-10T13:32:35.484926: step 4841, loss 0.343667, acc 0.859375, learning_rate 0.0001
2017-10-10T13:32:35.988999: step 4842, loss 0.330678, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:36.545257: step 4843, loss 0.283066, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:37.090733: step 4844, loss 0.257552, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:37.665360: step 4845, loss 0.386854, acc 0.859375, learning_rate 0.0001
2017-10-10T13:32:38.213821: step 4846, loss 0.392501, acc 0.875, learning_rate 0.0001
2017-10-10T13:32:38.810918: step 4847, loss 0.448344, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:39.316516: step 4848, loss 0.327403, acc 0.84375, learning_rate 0.0001
2017-10-10T13:32:39.828902: step 4849, loss 0.236561, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:40.458043: step 4850, loss 0.34307, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:40.986111: step 4851, loss 0.416648, acc 0.8125, learning_rate 0.0001
2017-10-10T13:32:41.457085: step 4852, loss 0.283737, acc 0.859375, learning_rate 0.0001
2017-10-10T13:32:41.952957: step 4853, loss 0.270869, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:42.501099: step 4854, loss 0.304964, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:43.033157: step 4855, loss 0.254224, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:43.536866: step 4856, loss 0.186828, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:44.060891: step 4857, loss 0.424349, acc 0.8125, learning_rate 0.0001
2017-10-10T13:32:44.600864: step 4858, loss 0.288299, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:45.144971: step 4859, loss 0.26575, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:45.642456: step 4860, loss 0.375623, acc 0.875, learning_rate 0.0001
2017-10-10T13:32:46.192890: step 4861, loss 0.410082, acc 0.828125, learning_rate 0.0001
2017-10-10T13:32:46.771225: step 4862, loss 0.428948, acc 0.859375, learning_rate 0.0001
2017-10-10T13:32:47.309034: step 4863, loss 0.276438, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:47.964911: step 4864, loss 0.249927, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:48.456260: step 4865, loss 0.316928, acc 0.859375, learning_rate 0.0001
2017-10-10T13:32:48.989675: step 4866, loss 0.281113, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:49.527371: step 4867, loss 0.4417, acc 0.84375, learning_rate 0.0001
2017-10-10T13:32:50.126775: step 4868, loss 0.283226, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:50.652380: step 4869, loss 0.230146, acc 0.96875, learning_rate 0.0001
2017-10-10T13:32:51.196986: step 4870, loss 0.258325, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:51.757133: step 4871, loss 0.242301, acc 0.90625, learning_rate 0.0001
2017-10-10T13:32:52.269587: step 4872, loss 0.312571, acc 0.875, learning_rate 0.0001
2017-10-10T13:32:52.784966: step 4873, loss 0.244346, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:53.420815: step 4874, loss 0.520746, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:53.953073: step 4875, loss 0.471638, acc 0.875, learning_rate 0.0001
2017-10-10T13:32:54.357902: step 4876, loss 0.409891, acc 0.859375, learning_rate 0.0001
2017-10-10T13:32:54.816878: step 4877, loss 0.24931, acc 0.921875, learning_rate 0.0001
2017-10-10T13:32:55.449029: step 4878, loss 0.273788, acc 0.890625, learning_rate 0.0001
2017-10-10T13:32:55.983548: step 4879, loss 0.370479, acc 0.875, learning_rate 0.0001
2017-10-10T13:32:56.420850: step 4880, loss 0.298373, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:32:57.579257: step 4880, loss 0.252255, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4880

2017-10-10T13:32:59.338569: step 4881, loss 0.198184, acc 0.953125, learning_rate 0.0001
2017-10-10T13:32:59.805244: step 4882, loss 0.408997, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:00.334269: step 4883, loss 0.36348, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:00.828895: step 4884, loss 0.36488, acc 0.859375, learning_rate 0.0001
2017-10-10T13:33:01.282907: step 4885, loss 0.342493, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:01.812910: step 4886, loss 0.300456, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:02.308578: step 4887, loss 0.355553, acc 0.84375, learning_rate 0.0001
2017-10-10T13:33:02.915669: step 4888, loss 0.292578, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:03.379107: step 4889, loss 0.407415, acc 0.828125, learning_rate 0.0001
2017-10-10T13:33:03.814721: step 4890, loss 0.26339, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:04.262498: step 4891, loss 0.295525, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:04.772852: step 4892, loss 0.410634, acc 0.859375, learning_rate 0.0001
2017-10-10T13:33:05.345517: step 4893, loss 0.247828, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:05.909093: step 4894, loss 0.26464, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:06.452856: step 4895, loss 0.267697, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:07.018544: step 4896, loss 0.189493, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:07.530933: step 4897, loss 0.317796, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:08.036156: step 4898, loss 0.169029, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:08.537914: step 4899, loss 0.328346, acc 0.859375, learning_rate 0.0001
2017-10-10T13:33:09.008916: step 4900, loss 0.243439, acc 0.921569, learning_rate 0.0001
2017-10-10T13:33:09.532873: step 4901, loss 0.239728, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:10.044079: step 4902, loss 0.212116, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:10.589911: step 4903, loss 0.362624, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:11.132857: step 4904, loss 0.262137, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:11.684149: step 4905, loss 0.32289, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:12.236875: step 4906, loss 0.292205, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:12.794854: step 4907, loss 0.464412, acc 0.859375, learning_rate 0.0001
2017-10-10T13:33:13.382145: step 4908, loss 0.271869, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:13.887709: step 4909, loss 0.497413, acc 0.828125, learning_rate 0.0001
2017-10-10T13:33:14.480531: step 4910, loss 0.215191, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:14.988875: step 4911, loss 0.229625, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:15.500008: step 4912, loss 0.171691, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:16.097051: step 4913, loss 0.311779, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:16.717040: step 4914, loss 0.453212, acc 0.796875, learning_rate 0.0001
2017-10-10T13:33:17.167450: step 4915, loss 0.382094, acc 0.84375, learning_rate 0.0001
2017-10-10T13:33:17.668651: step 4916, loss 0.352737, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:18.289025: step 4917, loss 0.361909, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:18.760853: step 4918, loss 0.230615, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:19.212824: step 4919, loss 0.319056, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:19.634775: step 4920, loss 0.276788, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:33:20.735770: step 4920, loss 0.250522, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4920

2017-10-10T13:33:22.131462: step 4921, loss 0.249313, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:22.666023: step 4922, loss 0.358185, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:23.233016: step 4923, loss 0.381739, acc 0.859375, learning_rate 0.0001
2017-10-10T13:33:23.689135: step 4924, loss 0.35129, acc 0.859375, learning_rate 0.0001
2017-10-10T13:33:24.223953: step 4925, loss 0.239419, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:24.701144: step 4926, loss 0.312594, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:25.257085: step 4927, loss 0.470761, acc 0.859375, learning_rate 0.0001
2017-10-10T13:33:25.846312: step 4928, loss 0.265028, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:26.544861: step 4929, loss 0.424308, acc 0.78125, learning_rate 0.0001
2017-10-10T13:33:27.025209: step 4930, loss 0.320958, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:27.471376: step 4931, loss 0.361997, acc 0.859375, learning_rate 0.0001
2017-10-10T13:33:27.941186: step 4932, loss 0.260725, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:28.488978: step 4933, loss 0.116587, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:29.015444: step 4934, loss 0.210983, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:29.611671: step 4935, loss 0.375496, acc 0.84375, learning_rate 0.0001
2017-10-10T13:33:30.148874: step 4936, loss 0.204107, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:30.716826: step 4937, loss 0.154135, acc 0.984375, learning_rate 0.0001
2017-10-10T13:33:31.232399: step 4938, loss 0.334726, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:31.769044: step 4939, loss 0.261474, acc 0.96875, learning_rate 0.0001
2017-10-10T13:33:32.322886: step 4940, loss 0.32387, acc 0.84375, learning_rate 0.0001
2017-10-10T13:33:32.896332: step 4941, loss 0.283265, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:33.478894: step 4942, loss 0.294028, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:34.020366: step 4943, loss 0.392589, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:34.590962: step 4944, loss 0.341437, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:35.155062: step 4945, loss 0.346845, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:35.724696: step 4946, loss 0.435689, acc 0.84375, learning_rate 0.0001
2017-10-10T13:33:36.238742: step 4947, loss 0.271677, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:36.740949: step 4948, loss 0.306355, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:37.241029: step 4949, loss 0.247495, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:37.701065: step 4950, loss 0.219867, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:38.157093: step 4951, loss 0.427578, acc 0.828125, learning_rate 0.0001
2017-10-10T13:33:38.688878: step 4952, loss 0.544727, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:39.366120: step 4953, loss 0.236962, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:39.735519: step 4954, loss 0.365463, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:40.187142: step 4955, loss 0.27356, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:40.658258: step 4956, loss 0.271291, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:41.281015: step 4957, loss 0.186272, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:41.932890: step 4958, loss 0.302924, acc 0.859375, learning_rate 0.0001
2017-10-10T13:33:42.322601: step 4959, loss 0.37163, acc 0.859375, learning_rate 0.0001
2017-10-10T13:33:42.843850: step 4960, loss 0.178953, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:33:44.034636: step 4960, loss 0.250273, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-4960

2017-10-10T13:33:45.586110: step 4961, loss 0.236204, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:46.072301: step 4962, loss 0.288901, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:46.603690: step 4963, loss 0.293013, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:47.128824: step 4964, loss 0.253297, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:47.676457: step 4965, loss 0.250245, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:48.187457: step 4966, loss 0.308659, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:48.752873: step 4967, loss 0.241923, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:49.417219: step 4968, loss 0.285079, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:49.914039: step 4969, loss 0.275555, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:50.376840: step 4970, loss 0.505865, acc 0.8125, learning_rate 0.0001
2017-10-10T13:33:50.848981: step 4971, loss 0.330415, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:51.340938: step 4972, loss 0.259138, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:51.856943: step 4973, loss 0.293047, acc 0.84375, learning_rate 0.0001
2017-10-10T13:33:52.401302: step 4974, loss 0.40938, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:52.954878: step 4975, loss 0.331906, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:53.388895: step 4976, loss 0.182974, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:53.837257: step 4977, loss 0.339689, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:54.325084: step 4978, loss 0.221415, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:55.017015: step 4979, loss 0.332428, acc 0.90625, learning_rate 0.0001
2017-10-10T13:33:55.446593: step 4980, loss 0.431107, acc 0.875, learning_rate 0.0001
2017-10-10T13:33:56.010887: step 4981, loss 0.245605, acc 0.953125, learning_rate 0.0001
2017-10-10T13:33:56.461394: step 4982, loss 0.503643, acc 0.859375, learning_rate 0.0001
2017-10-10T13:33:57.104976: step 4983, loss 0.263825, acc 0.9375, learning_rate 0.0001
2017-10-10T13:33:57.582695: step 4984, loss 0.33019, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:58.046013: step 4985, loss 0.34293, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:58.565809: step 4986, loss 0.305851, acc 0.890625, learning_rate 0.0001
2017-10-10T13:33:59.040901: step 4987, loss 0.324567, acc 0.921875, learning_rate 0.0001
2017-10-10T13:33:59.592662: step 4988, loss 0.195724, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:00.078034: step 4989, loss 0.229041, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:00.590509: step 4990, loss 0.463859, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:01.116817: step 4991, loss 0.4148, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:01.620901: step 4992, loss 0.348526, acc 0.859375, learning_rate 0.0001
2017-10-10T13:34:02.284864: step 4993, loss 0.366898, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:02.780504: step 4994, loss 0.40634, acc 0.84375, learning_rate 0.0001
2017-10-10T13:34:03.229450: step 4995, loss 0.4945, acc 0.828125, learning_rate 0.0001
2017-10-10T13:34:03.784363: step 4996, loss 0.359099, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:04.408671: step 4997, loss 0.229729, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:04.789376: step 4998, loss 0.401215, acc 0.843137, learning_rate 0.0001
2017-10-10T13:34:05.279335: step 4999, loss 0.288993, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:05.832108: step 5000, loss 0.219597, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:34:07.040366: step 5000, loss 0.250329, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5000

2017-10-10T13:34:08.702796: step 5001, loss 0.326669, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:09.235849: step 5002, loss 0.310593, acc 0.859375, learning_rate 0.0001
2017-10-10T13:34:09.776690: step 5003, loss 0.272326, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:10.293076: step 5004, loss 0.454075, acc 0.828125, learning_rate 0.0001
2017-10-10T13:34:10.795298: step 5005, loss 0.234792, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:11.321134: step 5006, loss 0.404819, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:11.817032: step 5007, loss 0.333306, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:12.381088: step 5008, loss 0.225511, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:12.993264: step 5009, loss 0.25198, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:13.451478: step 5010, loss 0.227593, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:13.904825: step 5011, loss 0.214024, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:14.389448: step 5012, loss 0.35277, acc 0.84375, learning_rate 0.0001
2017-10-10T13:34:14.915226: step 5013, loss 0.463088, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:15.404910: step 5014, loss 0.277354, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:15.889247: step 5015, loss 0.270555, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:16.409103: step 5016, loss 0.258252, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:16.964892: step 5017, loss 0.396463, acc 0.84375, learning_rate 0.0001
2017-10-10T13:34:17.464094: step 5018, loss 0.203703, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:18.024419: step 5019, loss 0.366621, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:18.509056: step 5020, loss 0.412066, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:19.028573: step 5021, loss 0.401203, acc 0.859375, learning_rate 0.0001
2017-10-10T13:34:19.524907: step 5022, loss 0.33816, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:20.033017: step 5023, loss 0.250625, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:20.611706: step 5024, loss 0.333364, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:21.100451: step 5025, loss 0.303466, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:21.666340: step 5026, loss 0.379612, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:22.242543: step 5027, loss 0.350982, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:22.742347: step 5028, loss 0.35596, acc 0.859375, learning_rate 0.0001
2017-10-10T13:34:23.264526: step 5029, loss 0.412321, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:23.795205: step 5030, loss 0.241192, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:24.443538: step 5031, loss 0.451976, acc 0.84375, learning_rate 0.0001
2017-10-10T13:34:25.008438: step 5032, loss 0.272616, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:25.420822: step 5033, loss 0.244533, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:25.916893: step 5034, loss 0.373303, acc 0.859375, learning_rate 0.0001
2017-10-10T13:34:26.482568: step 5035, loss 0.308522, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:27.045325: step 5036, loss 0.242422, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:27.700901: step 5037, loss 0.285767, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:28.187305: step 5038, loss 0.327988, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:28.617850: step 5039, loss 0.226946, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:29.159781: step 5040, loss 0.307555, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:34:30.221045: step 5040, loss 0.24902, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5040

2017-10-10T13:34:31.617336: step 5041, loss 0.222958, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:32.158742: step 5042, loss 0.300395, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:32.660537: step 5043, loss 0.338271, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:33.201051: step 5044, loss 0.3354, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:33.816953: step 5045, loss 0.436722, acc 0.828125, learning_rate 0.0001
2017-10-10T13:34:34.331370: step 5046, loss 0.242157, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:34.888860: step 5047, loss 0.488938, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:35.458870: step 5048, loss 0.375202, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:36.071075: step 5049, loss 0.265171, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:36.569039: step 5050, loss 0.280008, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:36.890137: step 5051, loss 0.34815, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:37.294934: step 5052, loss 0.292126, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:37.818136: step 5053, loss 0.2412, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:38.452855: step 5054, loss 0.330658, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:38.936748: step 5055, loss 0.265811, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:39.447143: step 5056, loss 0.212381, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:39.960653: step 5057, loss 0.310214, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:40.494224: step 5058, loss 0.26603, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:41.014582: step 5059, loss 0.452657, acc 0.859375, learning_rate 0.0001
2017-10-10T13:34:41.506526: step 5060, loss 0.334669, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:42.075753: step 5061, loss 0.259463, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:42.563468: step 5062, loss 0.277762, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:43.072889: step 5063, loss 0.261433, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:43.616900: step 5064, loss 0.402815, acc 0.84375, learning_rate 0.0001
2017-10-10T13:34:44.085046: step 5065, loss 0.348181, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:44.620947: step 5066, loss 0.206716, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:45.132883: step 5067, loss 0.375486, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:45.609014: step 5068, loss 0.338858, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:46.149814: step 5069, loss 0.200965, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:46.744958: step 5070, loss 0.293411, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:47.365212: step 5071, loss 0.464526, acc 0.796875, learning_rate 0.0001
2017-10-10T13:34:47.860079: step 5072, loss 0.228998, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:48.298211: step 5073, loss 0.219444, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:48.736928: step 5074, loss 0.215433, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:49.319433: step 5075, loss 0.252414, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:49.791625: step 5076, loss 0.320757, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:50.373124: step 5077, loss 0.263699, acc 0.90625, learning_rate 0.0001
2017-10-10T13:34:50.939064: step 5078, loss 0.177575, acc 0.921875, learning_rate 0.0001
2017-10-10T13:34:51.332882: step 5079, loss 0.197658, acc 0.953125, learning_rate 0.0001
2017-10-10T13:34:51.765088: step 5080, loss 0.277509, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:34:52.979847: step 5080, loss 0.249287, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5080

2017-10-10T13:34:54.569177: step 5081, loss 0.261413, acc 0.859375, learning_rate 0.0001
2017-10-10T13:34:55.077176: step 5082, loss 0.160805, acc 0.96875, learning_rate 0.0001
2017-10-10T13:34:55.549195: step 5083, loss 0.467472, acc 0.828125, learning_rate 0.0001
2017-10-10T13:34:56.021094: step 5084, loss 0.307645, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:56.534549: step 5085, loss 0.376775, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:56.988628: step 5086, loss 0.422585, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:57.540900: step 5087, loss 0.377122, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:58.096886: step 5088, loss 0.213648, acc 0.9375, learning_rate 0.0001
2017-10-10T13:34:58.705034: step 5089, loss 0.390937, acc 0.875, learning_rate 0.0001
2017-10-10T13:34:59.279757: step 5090, loss 0.337991, acc 0.890625, learning_rate 0.0001
2017-10-10T13:34:59.763757: step 5091, loss 0.207352, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:00.239219: step 5092, loss 0.369212, acc 0.828125, learning_rate 0.0001
2017-10-10T13:35:00.824894: step 5093, loss 0.20712, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:01.392588: step 5094, loss 0.271804, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:01.971731: step 5095, loss 0.275763, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:02.396842: step 5096, loss 0.330191, acc 0.921569, learning_rate 0.0001
2017-10-10T13:35:02.941644: step 5097, loss 0.21173, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:03.473041: step 5098, loss 0.31702, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:03.972576: step 5099, loss 0.265405, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:04.488909: step 5100, loss 0.352003, acc 0.859375, learning_rate 0.0001
2017-10-10T13:35:04.972859: step 5101, loss 0.253166, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:05.492891: step 5102, loss 0.195459, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:06.044998: step 5103, loss 0.545328, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:06.629091: step 5104, loss 0.214149, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:07.149056: step 5105, loss 0.414933, acc 0.84375, learning_rate 0.0001
2017-10-10T13:35:07.649033: step 5106, loss 0.221815, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:08.148444: step 5107, loss 0.277502, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:08.669014: step 5108, loss 0.40766, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:09.272292: step 5109, loss 0.211439, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:09.843211: step 5110, loss 0.296749, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:10.493732: step 5111, loss 0.417798, acc 0.859375, learning_rate 0.0001
2017-10-10T13:35:10.968905: step 5112, loss 0.157741, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:11.427222: step 5113, loss 0.329798, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:11.965296: step 5114, loss 0.301162, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:12.509555: step 5115, loss 0.348505, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:13.100926: step 5116, loss 0.43139, acc 0.859375, learning_rate 0.0001
2017-10-10T13:35:13.676415: step 5117, loss 0.249522, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:14.078681: step 5118, loss 0.298682, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:14.533839: step 5119, loss 0.505211, acc 0.859375, learning_rate 0.0001
2017-10-10T13:35:15.016862: step 5120, loss 0.343487, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:35:16.162916: step 5120, loss 0.249272, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5120

2017-10-10T13:35:17.856257: step 5121, loss 0.362471, acc 0.84375, learning_rate 0.0001
2017-10-10T13:35:18.357167: step 5122, loss 0.513437, acc 0.828125, learning_rate 0.0001
2017-10-10T13:35:18.887985: step 5123, loss 0.285113, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:19.401161: step 5124, loss 0.332146, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:19.981019: step 5125, loss 0.377524, acc 0.828125, learning_rate 0.0001
2017-10-10T13:35:20.480972: step 5126, loss 0.510759, acc 0.859375, learning_rate 0.0001
2017-10-10T13:35:21.133405: step 5127, loss 0.23887, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:21.756825: step 5128, loss 0.387115, acc 0.859375, learning_rate 0.0001
2017-10-10T13:35:22.204409: step 5129, loss 0.268894, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:22.634632: step 5130, loss 0.322536, acc 0.859375, learning_rate 0.0001
2017-10-10T13:35:23.135387: step 5131, loss 0.301005, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:23.636077: step 5132, loss 0.561846, acc 0.828125, learning_rate 0.0001
2017-10-10T13:35:24.173267: step 5133, loss 0.305192, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:24.670363: step 5134, loss 0.244035, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:25.225074: step 5135, loss 0.284811, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:25.758155: step 5136, loss 0.345982, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:26.236987: step 5137, loss 0.368637, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:26.747869: step 5138, loss 0.346896, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:27.233003: step 5139, loss 0.333678, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:27.696998: step 5140, loss 0.152216, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:28.164841: step 5141, loss 0.244781, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:28.668925: step 5142, loss 0.276998, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:29.177189: step 5143, loss 0.44348, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:29.732375: step 5144, loss 0.314023, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:30.287874: step 5145, loss 0.353365, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:30.896655: step 5146, loss 0.315981, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:31.384862: step 5147, loss 0.304394, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:31.923053: step 5148, loss 0.235406, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:32.476942: step 5149, loss 0.167434, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:33.111515: step 5150, loss 0.192869, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:33.545349: step 5151, loss 0.276237, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:34.014316: step 5152, loss 0.328291, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:34.574351: step 5153, loss 0.25683, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:35.111480: step 5154, loss 0.254527, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:35.646201: step 5155, loss 0.369369, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:36.190213: step 5156, loss 0.384022, acc 0.8125, learning_rate 0.0001
2017-10-10T13:35:36.771183: step 5157, loss 0.213948, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:37.343207: step 5158, loss 0.240458, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:37.814780: step 5159, loss 0.326832, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:38.281011: step 5160, loss 0.333701, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:35:39.352861: step 5160, loss 0.249699, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5160

2017-10-10T13:35:41.249113: step 5161, loss 0.283151, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:41.772994: step 5162, loss 0.268694, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:42.250091: step 5163, loss 0.202538, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:42.798976: step 5164, loss 0.219542, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:43.337088: step 5165, loss 0.217674, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:43.872989: step 5166, loss 0.192088, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:44.512851: step 5167, loss 0.307902, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:45.108987: step 5168, loss 0.301943, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:45.491829: step 5169, loss 0.3299, acc 0.859375, learning_rate 0.0001
2017-10-10T13:35:45.858936: step 5170, loss 0.370122, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:46.268277: step 5171, loss 0.410649, acc 0.828125, learning_rate 0.0001
2017-10-10T13:35:46.721184: step 5172, loss 0.163711, acc 0.96875, learning_rate 0.0001
2017-10-10T13:35:47.301022: step 5173, loss 0.464333, acc 0.84375, learning_rate 0.0001
2017-10-10T13:35:47.780996: step 5174, loss 0.270386, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:48.360843: step 5175, loss 0.449168, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:48.980966: step 5176, loss 0.308911, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:49.456900: step 5177, loss 0.303757, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:49.924973: step 5178, loss 0.343691, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:50.529027: step 5179, loss 0.336834, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:51.022024: step 5180, loss 0.271932, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:51.488986: step 5181, loss 0.349413, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:52.012852: step 5182, loss 0.202115, acc 0.953125, learning_rate 0.0001
2017-10-10T13:35:52.672932: step 5183, loss 0.338328, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:53.140680: step 5184, loss 0.334361, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:53.697682: step 5185, loss 0.20573, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:54.252993: step 5186, loss 0.404578, acc 0.890625, learning_rate 0.0001
2017-10-10T13:35:54.713214: step 5187, loss 0.244748, acc 0.9375, learning_rate 0.0001
2017-10-10T13:35:55.237208: step 5188, loss 0.218991, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:55.740490: step 5189, loss 0.303434, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:56.060087: step 5190, loss 0.404096, acc 0.859375, learning_rate 0.0001
2017-10-10T13:35:56.455004: step 5191, loss 0.191613, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:56.924516: step 5192, loss 0.267995, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:57.369549: step 5193, loss 0.265324, acc 0.875, learning_rate 0.0001
2017-10-10T13:35:57.820957: step 5194, loss 0.202145, acc 0.941176, learning_rate 0.0001
2017-10-10T13:35:58.324694: step 5195, loss 0.229996, acc 0.921875, learning_rate 0.0001
2017-10-10T13:35:58.812552: step 5196, loss 0.300191, acc 0.90625, learning_rate 0.0001
2017-10-10T13:35:59.280870: step 5197, loss 0.205925, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:00.005126: step 5198, loss 0.275263, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:00.419808: step 5199, loss 0.34461, acc 0.84375, learning_rate 0.0001
2017-10-10T13:36:00.924832: step 5200, loss 0.256077, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:36:02.044942: step 5200, loss 0.248769, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5200

2017-10-10T13:36:03.430510: step 5201, loss 0.28118, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:03.903815: step 5202, loss 0.384905, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:04.532947: step 5203, loss 0.25527, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:05.049002: step 5204, loss 0.438035, acc 0.859375, learning_rate 0.0001
2017-10-10T13:36:05.533962: step 5205, loss 0.278371, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:06.058916: step 5206, loss 0.329553, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:06.508680: step 5207, loss 0.182896, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:07.020569: step 5208, loss 0.290651, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:07.552870: step 5209, loss 0.364836, acc 0.859375, learning_rate 0.0001
2017-10-10T13:36:08.168472: step 5210, loss 0.278902, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:08.598429: step 5211, loss 0.158886, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:09.045034: step 5212, loss 0.237148, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:09.542316: step 5213, loss 0.359908, acc 0.859375, learning_rate 0.0001
2017-10-10T13:36:10.026810: step 5214, loss 0.374224, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:10.494552: step 5215, loss 0.399382, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:11.024918: step 5216, loss 0.332844, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:11.582351: step 5217, loss 0.321146, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:12.045788: step 5218, loss 0.284015, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:12.586173: step 5219, loss 0.33991, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:13.140081: step 5220, loss 0.191218, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:13.680390: step 5221, loss 0.323385, acc 0.859375, learning_rate 0.0001
2017-10-10T13:36:14.209367: step 5222, loss 0.241244, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:14.759781: step 5223, loss 0.347842, acc 0.859375, learning_rate 0.0001
2017-10-10T13:36:15.301881: step 5224, loss 0.271314, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:15.828436: step 5225, loss 0.276456, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:16.357207: step 5226, loss 0.432481, acc 0.796875, learning_rate 0.0001
2017-10-10T13:36:16.904690: step 5227, loss 0.28016, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:17.441162: step 5228, loss 0.206293, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:17.965854: step 5229, loss 0.459566, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:18.540524: step 5230, loss 0.207462, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:19.111964: step 5231, loss 0.274533, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:19.742622: step 5232, loss 0.239018, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:20.164821: step 5233, loss 0.23754, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:20.580862: step 5234, loss 0.437548, acc 0.859375, learning_rate 0.0001
2017-10-10T13:36:20.980934: step 5235, loss 0.378419, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:21.532850: step 5236, loss 0.237488, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:22.048907: step 5237, loss 0.139529, acc 0.984375, learning_rate 0.0001
2017-10-10T13:36:22.608864: step 5238, loss 0.253444, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:23.184916: step 5239, loss 0.239332, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:23.823282: step 5240, loss 0.247892, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:36:24.768183: step 5240, loss 0.248084, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5240

2017-10-10T13:36:26.353165: step 5241, loss 0.270176, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:26.998192: step 5242, loss 0.388774, acc 0.8125, learning_rate 0.0001
2017-10-10T13:36:27.512861: step 5243, loss 0.188639, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:28.058334: step 5244, loss 0.292396, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:28.625016: step 5245, loss 0.259548, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:29.137783: step 5246, loss 0.343991, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:29.665413: step 5247, loss 0.405394, acc 0.828125, learning_rate 0.0001
2017-10-10T13:36:30.219224: step 5248, loss 0.305292, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:30.872419: step 5249, loss 0.199106, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:31.347948: step 5250, loss 0.257659, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:31.780123: step 5251, loss 0.346529, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:32.281034: step 5252, loss 0.281039, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:32.793960: step 5253, loss 0.256973, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:33.300916: step 5254, loss 0.264653, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:33.831452: step 5255, loss 0.340922, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:34.371053: step 5256, loss 0.380287, acc 0.84375, learning_rate 0.0001
2017-10-10T13:36:34.915247: step 5257, loss 0.288316, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:35.432896: step 5258, loss 0.195761, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:35.981597: step 5259, loss 0.279309, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:36.546731: step 5260, loss 0.219847, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:37.136458: step 5261, loss 0.290739, acc 0.859375, learning_rate 0.0001
2017-10-10T13:36:37.703545: step 5262, loss 0.460177, acc 0.828125, learning_rate 0.0001
2017-10-10T13:36:38.267991: step 5263, loss 0.243293, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:38.808290: step 5264, loss 0.343625, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:39.353664: step 5265, loss 0.347455, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:39.916270: step 5266, loss 0.224118, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:40.449026: step 5267, loss 0.20686, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:41.013217: step 5268, loss 0.216435, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:41.551005: step 5269, loss 0.379949, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:42.065749: step 5270, loss 0.358965, acc 0.953125, learning_rate 0.0001
2017-10-10T13:36:42.602693: step 5271, loss 0.424719, acc 0.859375, learning_rate 0.0001
2017-10-10T13:36:43.156086: step 5272, loss 0.499272, acc 0.828125, learning_rate 0.0001
2017-10-10T13:36:43.656827: step 5273, loss 0.348685, acc 0.859375, learning_rate 0.0001
2017-10-10T13:36:44.115256: step 5274, loss 0.403039, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:44.600956: step 5275, loss 0.358851, acc 0.84375, learning_rate 0.0001
2017-10-10T13:36:45.173338: step 5276, loss 0.279971, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:45.740879: step 5277, loss 0.494989, acc 0.875, learning_rate 0.0001
2017-10-10T13:36:46.346023: step 5278, loss 0.278602, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:46.769106: step 5279, loss 0.478284, acc 0.84375, learning_rate 0.0001
2017-10-10T13:36:47.239073: step 5280, loss 0.270773, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:36:48.354924: step 5280, loss 0.248937, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5280

2017-10-10T13:36:49.999575: step 5281, loss 0.194506, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:50.465785: step 5282, loss 0.632364, acc 0.828125, learning_rate 0.0001
2017-10-10T13:36:50.964885: step 5283, loss 0.338807, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:51.532939: step 5284, loss 0.325237, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:52.038527: step 5285, loss 0.19251, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:52.616943: step 5286, loss 0.272202, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:53.136967: step 5287, loss 0.502642, acc 0.796875, learning_rate 0.0001
2017-10-10T13:36:53.615037: step 5288, loss 0.26479, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:54.271632: step 5289, loss 0.308883, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:54.892003: step 5290, loss 0.17575, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:55.232982: step 5291, loss 0.193956, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:55.616944: step 5292, loss 0.246264, acc 0.921569, learning_rate 0.0001
2017-10-10T13:36:56.035856: step 5293, loss 0.591157, acc 0.796875, learning_rate 0.0001
2017-10-10T13:36:56.560883: step 5294, loss 0.276605, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:57.107068: step 5295, loss 0.238579, acc 0.9375, learning_rate 0.0001
2017-10-10T13:36:57.645559: step 5296, loss 0.203583, acc 0.921875, learning_rate 0.0001
2017-10-10T13:36:58.152820: step 5297, loss 0.20916, acc 0.96875, learning_rate 0.0001
2017-10-10T13:36:58.720887: step 5298, loss 0.362055, acc 0.890625, learning_rate 0.0001
2017-10-10T13:36:59.204919: step 5299, loss 0.340178, acc 0.90625, learning_rate 0.0001
2017-10-10T13:36:59.696897: step 5300, loss 0.213797, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:00.213593: step 5301, loss 0.181457, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:00.702223: step 5302, loss 0.296215, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:01.217676: step 5303, loss 0.41757, acc 0.84375, learning_rate 0.0001
2017-10-10T13:37:01.723091: step 5304, loss 0.424926, acc 0.8125, learning_rate 0.0001
2017-10-10T13:37:02.245586: step 5305, loss 0.179432, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:02.820797: step 5306, loss 0.329792, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:03.405210: step 5307, loss 0.380475, acc 0.859375, learning_rate 0.0001
2017-10-10T13:37:03.913115: step 5308, loss 0.31958, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:04.394182: step 5309, loss 0.425823, acc 0.84375, learning_rate 0.0001
2017-10-10T13:37:04.884952: step 5310, loss 0.297244, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:05.416828: step 5311, loss 0.201965, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:05.964933: step 5312, loss 0.23856, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:06.560860: step 5313, loss 0.28886, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:06.973497: step 5314, loss 0.339441, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:07.371054: step 5315, loss 0.164524, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:07.888956: step 5316, loss 0.28509, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:08.317231: step 5317, loss 0.283646, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:08.848960: step 5318, loss 0.325634, acc 0.859375, learning_rate 0.0001
2017-10-10T13:37:09.471804: step 5319, loss 0.286681, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:09.897021: step 5320, loss 0.265891, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:37:10.943493: step 5320, loss 0.248855, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5320

2017-10-10T13:37:12.444845: step 5321, loss 0.490861, acc 0.84375, learning_rate 0.0001
2017-10-10T13:37:12.972246: step 5322, loss 0.295086, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:13.522820: step 5323, loss 0.410615, acc 0.8125, learning_rate 0.0001
2017-10-10T13:37:14.137733: step 5324, loss 0.341672, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:14.698800: step 5325, loss 0.544205, acc 0.84375, learning_rate 0.0001
2017-10-10T13:37:15.241729: step 5326, loss 0.161643, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:15.748838: step 5327, loss 0.306065, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:16.282998: step 5328, loss 0.237116, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:16.772919: step 5329, loss 0.373613, acc 0.859375, learning_rate 0.0001
2017-10-10T13:37:17.312944: step 5330, loss 0.297672, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:17.877026: step 5331, loss 0.210702, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:18.462541: step 5332, loss 0.41831, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:18.896805: step 5333, loss 0.233973, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:19.368931: step 5334, loss 0.360596, acc 0.8125, learning_rate 0.0001
2017-10-10T13:37:19.868544: step 5335, loss 0.47103, acc 0.859375, learning_rate 0.0001
2017-10-10T13:37:20.351629: step 5336, loss 0.395867, acc 0.84375, learning_rate 0.0001
2017-10-10T13:37:20.823630: step 5337, loss 0.46809, acc 0.875, learning_rate 0.0001
2017-10-10T13:37:21.364966: step 5338, loss 0.308007, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:21.911559: step 5339, loss 0.358636, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:22.356807: step 5340, loss 0.279393, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:22.925027: step 5341, loss 0.291397, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:23.497015: step 5342, loss 0.331229, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:23.989004: step 5343, loss 0.430829, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:24.460518: step 5344, loss 0.259955, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:24.997131: step 5345, loss 0.433621, acc 0.78125, learning_rate 0.0001
2017-10-10T13:37:25.499805: step 5346, loss 0.39021, acc 0.875, learning_rate 0.0001
2017-10-10T13:37:26.003176: step 5347, loss 0.287538, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:26.535591: step 5348, loss 0.221572, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:27.164964: step 5349, loss 0.213701, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:27.650086: step 5350, loss 0.254845, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:28.101146: step 5351, loss 0.456469, acc 0.828125, learning_rate 0.0001
2017-10-10T13:37:28.693125: step 5352, loss 0.418329, acc 0.875, learning_rate 0.0001
2017-10-10T13:37:29.218382: step 5353, loss 0.117294, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:29.658305: step 5354, loss 0.333234, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:30.077269: step 5355, loss 0.297493, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:30.586758: step 5356, loss 0.280419, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:31.089010: step 5357, loss 0.214058, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:31.728976: step 5358, loss 0.433861, acc 0.859375, learning_rate 0.0001
2017-10-10T13:37:32.305147: step 5359, loss 0.34495, acc 0.875, learning_rate 0.0001
2017-10-10T13:37:32.710306: step 5360, loss 0.438103, acc 0.84375, learning_rate 0.0001

Evaluation:
2017-10-10T13:37:33.833754: step 5360, loss 0.248143, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5360

2017-10-10T13:37:35.532515: step 5361, loss 0.266245, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:36.088003: step 5362, loss 0.396964, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:36.614177: step 5363, loss 0.299428, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:37.165564: step 5364, loss 0.296747, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:37.631190: step 5365, loss 0.362431, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:38.175317: step 5366, loss 0.386563, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:38.765091: step 5367, loss 0.375506, acc 0.875, learning_rate 0.0001
2017-10-10T13:37:39.208931: step 5368, loss 0.316367, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:39.703364: step 5369, loss 0.345147, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:40.243367: step 5370, loss 0.295098, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:40.804771: step 5371, loss 0.140276, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:41.363126: step 5372, loss 0.503411, acc 0.84375, learning_rate 0.0001
2017-10-10T13:37:41.818039: step 5373, loss 0.381631, acc 0.84375, learning_rate 0.0001
2017-10-10T13:37:42.267538: step 5374, loss 0.280961, acc 0.875, learning_rate 0.0001
2017-10-10T13:37:42.808368: step 5375, loss 0.187798, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:43.359083: step 5376, loss 0.293158, acc 0.84375, learning_rate 0.0001
2017-10-10T13:37:43.910968: step 5377, loss 0.238934, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:44.504896: step 5378, loss 0.296849, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:45.053027: step 5379, loss 0.319054, acc 0.875, learning_rate 0.0001
2017-10-10T13:37:45.556848: step 5380, loss 0.14973, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:46.082056: step 5381, loss 0.438816, acc 0.859375, learning_rate 0.0001
2017-10-10T13:37:46.644631: step 5382, loss 0.341727, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:47.139868: step 5383, loss 0.178567, acc 0.96875, learning_rate 0.0001
2017-10-10T13:37:47.684721: step 5384, loss 0.278994, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:48.243838: step 5385, loss 0.258293, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:48.805035: step 5386, loss 0.273428, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:49.366652: step 5387, loss 0.186063, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:49.935415: step 5388, loss 0.258447, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:50.425106: step 5389, loss 0.225853, acc 0.953125, learning_rate 0.0001
2017-10-10T13:37:50.968853: step 5390, loss 0.279378, acc 0.901961, learning_rate 0.0001
2017-10-10T13:37:51.521525: step 5391, loss 0.310556, acc 0.890625, learning_rate 0.0001
2017-10-10T13:37:52.104852: step 5392, loss 0.353662, acc 0.84375, learning_rate 0.0001
2017-10-10T13:37:52.695472: step 5393, loss 0.188768, acc 0.9375, learning_rate 0.0001
2017-10-10T13:37:53.167664: step 5394, loss 0.441256, acc 0.84375, learning_rate 0.0001
2017-10-10T13:37:53.613812: step 5395, loss 0.332843, acc 0.875, learning_rate 0.0001
2017-10-10T13:37:54.064423: step 5396, loss 0.19386, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:54.605750: step 5397, loss 0.393262, acc 0.84375, learning_rate 0.0001
2017-10-10T13:37:55.259809: step 5398, loss 0.392753, acc 0.8125, learning_rate 0.0001
2017-10-10T13:37:55.709632: step 5399, loss 0.450465, acc 0.90625, learning_rate 0.0001
2017-10-10T13:37:56.172131: step 5400, loss 0.281045, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:37:57.357009: step 5400, loss 0.248869, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5400

2017-10-10T13:37:59.071180: step 5401, loss 0.168072, acc 0.921875, learning_rate 0.0001
2017-10-10T13:37:59.612972: step 5402, loss 0.291114, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:00.172506: step 5403, loss 0.293629, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:00.721165: step 5404, loss 0.325049, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:01.284213: step 5405, loss 0.379974, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:01.770590: step 5406, loss 0.238109, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:02.312658: step 5407, loss 0.218338, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:02.850156: step 5408, loss 0.297724, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:03.360912: step 5409, loss 0.236447, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:03.918697: step 5410, loss 0.258391, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:04.412848: step 5411, loss 0.526668, acc 0.828125, learning_rate 0.0001
2017-10-10T13:38:04.842608: step 5412, loss 0.254731, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:05.216868: step 5413, loss 0.260443, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:05.693060: step 5414, loss 0.304407, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:06.332850: step 5415, loss 0.27023, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:06.849862: step 5416, loss 0.363483, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:07.265090: step 5417, loss 0.17169, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:07.681201: step 5418, loss 0.209082, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:08.176589: step 5419, loss 0.158187, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:08.694805: step 5420, loss 0.147513, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:09.205431: step 5421, loss 0.133372, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:09.712152: step 5422, loss 0.435557, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:10.260416: step 5423, loss 0.315643, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:10.801072: step 5424, loss 0.292563, acc 0.859375, learning_rate 0.0001
2017-10-10T13:38:11.339628: step 5425, loss 0.512849, acc 0.828125, learning_rate 0.0001
2017-10-10T13:38:11.902557: step 5426, loss 0.247842, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:12.476514: step 5427, loss 0.268341, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:13.014416: step 5428, loss 0.455501, acc 0.84375, learning_rate 0.0001
2017-10-10T13:38:13.629144: step 5429, loss 0.316793, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:14.180821: step 5430, loss 0.258756, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:14.646799: step 5431, loss 0.263434, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:15.218240: step 5432, loss 0.328145, acc 0.859375, learning_rate 0.0001
2017-10-10T13:38:15.781138: step 5433, loss 0.268383, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:16.394281: step 5434, loss 0.28142, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:16.859958: step 5435, loss 0.236599, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:17.313113: step 5436, loss 0.291755, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:17.916876: step 5437, loss 0.191372, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:18.556046: step 5438, loss 0.137903, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:19.055348: step 5439, loss 0.208316, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:19.452859: step 5440, loss 0.319429, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:38:20.664672: step 5440, loss 0.248086, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5440

2017-10-10T13:38:22.140946: step 5441, loss 0.276976, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:22.705094: step 5442, loss 0.234511, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:23.292835: step 5443, loss 0.34334, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:23.912893: step 5444, loss 0.224531, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:24.480900: step 5445, loss 0.357531, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:25.006632: step 5446, loss 0.310767, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:25.553114: step 5447, loss 0.256779, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:26.083199: step 5448, loss 0.396774, acc 0.828125, learning_rate 0.0001
2017-10-10T13:38:26.625029: step 5449, loss 0.164196, acc 0.984375, learning_rate 0.0001
2017-10-10T13:38:27.229087: step 5450, loss 0.332395, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:27.741035: step 5451, loss 0.254278, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:28.115789: step 5452, loss 0.365512, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:28.606848: step 5453, loss 0.190321, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:29.057027: step 5454, loss 0.303658, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:29.613920: step 5455, loss 0.381055, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:30.213255: step 5456, loss 0.278987, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:30.741088: step 5457, loss 0.603771, acc 0.84375, learning_rate 0.0001
2017-10-10T13:38:31.169003: step 5458, loss 0.351174, acc 0.828125, learning_rate 0.0001
2017-10-10T13:38:31.693184: step 5459, loss 0.233675, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:32.247573: step 5460, loss 0.404714, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:32.745817: step 5461, loss 0.376985, acc 0.84375, learning_rate 0.0001
2017-10-10T13:38:33.264834: step 5462, loss 0.369652, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:33.799679: step 5463, loss 0.345901, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:34.336837: step 5464, loss 0.193689, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:34.882868: step 5465, loss 0.3499, acc 0.84375, learning_rate 0.0001
2017-10-10T13:38:35.392047: step 5466, loss 0.303478, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:35.929432: step 5467, loss 0.252731, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:36.548904: step 5468, loss 0.402937, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:37.124217: step 5469, loss 0.328614, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:37.649080: step 5470, loss 0.293575, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:38.276279: step 5471, loss 0.203561, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:38.764950: step 5472, loss 0.223193, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:39.216866: step 5473, loss 0.417224, acc 0.859375, learning_rate 0.0001
2017-10-10T13:38:39.737130: step 5474, loss 0.324321, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:40.292823: step 5475, loss 0.22568, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:40.832865: step 5476, loss 0.294604, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:41.452852: step 5477, loss 0.213832, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:41.969783: step 5478, loss 0.292549, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:42.427538: step 5479, loss 0.256799, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:42.887360: step 5480, loss 0.403156, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:38:44.253896: step 5480, loss 0.247478, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5480

2017-10-10T13:38:45.874201: step 5481, loss 0.182043, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:46.405770: step 5482, loss 0.336749, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:46.891826: step 5483, loss 0.23037, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:47.396892: step 5484, loss 0.282027, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:47.892466: step 5485, loss 0.227843, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:48.449982: step 5486, loss 0.438782, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:48.976764: step 5487, loss 0.34961, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:49.384961: step 5488, loss 0.3844, acc 0.843137, learning_rate 0.0001
2017-10-10T13:38:49.923069: step 5489, loss 0.297838, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:50.522114: step 5490, loss 0.289856, acc 0.84375, learning_rate 0.0001
2017-10-10T13:38:50.997232: step 5491, loss 0.242963, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:51.393023: step 5492, loss 0.191307, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:51.785082: step 5493, loss 0.242603, acc 0.9375, learning_rate 0.0001
2017-10-10T13:38:52.364015: step 5494, loss 0.351735, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:52.878839: step 5495, loss 0.263578, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:53.364166: step 5496, loss 0.409008, acc 0.828125, learning_rate 0.0001
2017-10-10T13:38:53.886529: step 5497, loss 0.219235, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:54.453170: step 5498, loss 0.420498, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:54.985822: step 5499, loss 0.313302, acc 0.90625, learning_rate 0.0001
2017-10-10T13:38:55.549700: step 5500, loss 0.158388, acc 0.96875, learning_rate 0.0001
2017-10-10T13:38:56.161274: step 5501, loss 0.602335, acc 0.8125, learning_rate 0.0001
2017-10-10T13:38:56.668822: step 5502, loss 0.223369, acc 0.953125, learning_rate 0.0001
2017-10-10T13:38:57.168836: step 5503, loss 0.383309, acc 0.828125, learning_rate 0.0001
2017-10-10T13:38:57.673243: step 5504, loss 0.423908, acc 0.875, learning_rate 0.0001
2017-10-10T13:38:58.175434: step 5505, loss 0.298708, acc 0.921875, learning_rate 0.0001
2017-10-10T13:38:58.654240: step 5506, loss 0.419293, acc 0.859375, learning_rate 0.0001
2017-10-10T13:38:59.185093: step 5507, loss 0.259664, acc 0.890625, learning_rate 0.0001
2017-10-10T13:38:59.824837: step 5508, loss 0.188398, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:00.358961: step 5509, loss 0.240846, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:00.940926: step 5510, loss 0.386475, acc 0.859375, learning_rate 0.0001
2017-10-10T13:39:01.464014: step 5511, loss 0.425257, acc 0.84375, learning_rate 0.0001
2017-10-10T13:39:01.915434: step 5512, loss 0.227907, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:02.368905: step 5513, loss 0.190452, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:02.924159: step 5514, loss 0.231954, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:03.457612: step 5515, loss 0.292757, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:03.999545: step 5516, loss 0.301875, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:04.616833: step 5517, loss 0.284901, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:05.105617: step 5518, loss 0.169787, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:05.569326: step 5519, loss 0.353314, acc 0.859375, learning_rate 0.0001
2017-10-10T13:39:06.128889: step 5520, loss 0.234045, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:39:07.288824: step 5520, loss 0.24704, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5520

2017-10-10T13:39:08.833140: step 5521, loss 0.242672, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:09.357820: step 5522, loss 0.232585, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:09.900299: step 5523, loss 0.360339, acc 0.859375, learning_rate 0.0001
2017-10-10T13:39:10.353133: step 5524, loss 0.306388, acc 0.875, learning_rate 0.0001
2017-10-10T13:39:10.868587: step 5525, loss 0.203623, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:11.332749: step 5526, loss 0.300155, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:11.824882: step 5527, loss 0.180181, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:12.348840: step 5528, loss 0.438467, acc 0.84375, learning_rate 0.0001
2017-10-10T13:39:12.916502: step 5529, loss 0.347487, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:13.437656: step 5530, loss 0.218894, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:14.069871: step 5531, loss 0.366394, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:14.534749: step 5532, loss 0.232739, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:15.003094: step 5533, loss 0.265859, acc 0.875, learning_rate 0.0001
2017-10-10T13:39:15.464116: step 5534, loss 0.194449, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:16.029069: step 5535, loss 0.286682, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:16.592580: step 5536, loss 0.100319, acc 1, learning_rate 0.0001
2017-10-10T13:39:17.205953: step 5537, loss 0.197169, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:17.701031: step 5538, loss 0.42705, acc 0.859375, learning_rate 0.0001
2017-10-10T13:39:18.189038: step 5539, loss 0.246556, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:18.710500: step 5540, loss 0.250099, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:19.292983: step 5541, loss 0.168546, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:19.787056: step 5542, loss 0.16872, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:20.268946: step 5543, loss 0.27377, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:20.778470: step 5544, loss 0.310066, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:21.331955: step 5545, loss 0.250336, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:21.889159: step 5546, loss 0.278415, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:22.548847: step 5547, loss 0.27498, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:23.080844: step 5548, loss 0.246164, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:23.612933: step 5549, loss 0.454751, acc 0.828125, learning_rate 0.0001
2017-10-10T13:39:24.198072: step 5550, loss 0.234544, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:24.653698: step 5551, loss 0.465051, acc 0.875, learning_rate 0.0001
2017-10-10T13:39:24.981074: step 5552, loss 0.32308, acc 0.875, learning_rate 0.0001
2017-10-10T13:39:25.505276: step 5553, loss 0.349022, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:25.997088: step 5554, loss 0.314445, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:26.568856: step 5555, loss 0.10971, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:27.117027: step 5556, loss 0.363072, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:27.626955: step 5557, loss 0.263753, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:27.962612: step 5558, loss 0.39391, acc 0.859375, learning_rate 0.0001
2017-10-10T13:39:28.408146: step 5559, loss 0.329559, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:28.821178: step 5560, loss 0.227723, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:39:29.979831: step 5560, loss 0.246085, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5560

2017-10-10T13:39:31.858133: step 5561, loss 0.170664, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:32.420969: step 5562, loss 0.434198, acc 0.8125, learning_rate 0.0001
2017-10-10T13:39:33.048853: step 5563, loss 0.43018, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:33.565057: step 5564, loss 0.310646, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:34.101030: step 5565, loss 0.360957, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:34.636909: step 5566, loss 0.288826, acc 0.875, learning_rate 0.0001
2017-10-10T13:39:35.173477: step 5567, loss 0.359577, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:35.668004: step 5568, loss 0.201571, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:36.208867: step 5569, loss 0.356961, acc 0.828125, learning_rate 0.0001
2017-10-10T13:39:36.877111: step 5570, loss 0.263325, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:37.395896: step 5571, loss 0.402611, acc 0.84375, learning_rate 0.0001
2017-10-10T13:39:37.824982: step 5572, loss 0.197491, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:38.232903: step 5573, loss 0.220835, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:38.771141: step 5574, loss 0.30423, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:39.314483: step 5575, loss 0.250384, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:39.807543: step 5576, loss 0.144237, acc 0.984375, learning_rate 0.0001
2017-10-10T13:39:40.371402: step 5577, loss 0.23883, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:40.907279: step 5578, loss 0.428318, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:41.452001: step 5579, loss 0.392621, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:42.004929: step 5580, loss 0.16376, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:42.569902: step 5581, loss 0.253206, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:43.096896: step 5582, loss 0.272495, acc 0.921875, learning_rate 0.0001
2017-10-10T13:39:43.628876: step 5583, loss 0.162071, acc 0.9375, learning_rate 0.0001
2017-10-10T13:39:44.105806: step 5584, loss 0.228178, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:44.689057: step 5585, loss 0.331512, acc 0.875, learning_rate 0.0001
2017-10-10T13:39:45.198011: step 5586, loss 0.306849, acc 0.901961, learning_rate 0.0001
2017-10-10T13:39:45.788826: step 5587, loss 0.337088, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:46.380924: step 5588, loss 0.399154, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:47.024973: step 5589, loss 0.135195, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:47.420883: step 5590, loss 0.250308, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:47.865097: step 5591, loss 0.329811, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:48.298689: step 5592, loss 0.28822, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:48.866248: step 5593, loss 0.300295, acc 0.859375, learning_rate 0.0001
2017-10-10T13:39:49.424309: step 5594, loss 0.441965, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:50.039337: step 5595, loss 0.272561, acc 0.875, learning_rate 0.0001
2017-10-10T13:39:50.613083: step 5596, loss 0.320931, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:51.061674: step 5597, loss 0.157226, acc 0.96875, learning_rate 0.0001
2017-10-10T13:39:51.529032: step 5598, loss 0.390411, acc 0.8125, learning_rate 0.0001
2017-10-10T13:39:52.084924: step 5599, loss 0.409687, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:52.564903: step 5600, loss 0.354113, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:39:53.641132: step 5600, loss 0.24518, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5600

2017-10-10T13:39:55.021984: step 5601, loss 0.319361, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:55.519942: step 5602, loss 0.299787, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:55.995841: step 5603, loss 0.318796, acc 0.875, learning_rate 0.0001
2017-10-10T13:39:56.539569: step 5604, loss 0.185011, acc 0.953125, learning_rate 0.0001
2017-10-10T13:39:57.099553: step 5605, loss 0.32591, acc 0.875, learning_rate 0.0001
2017-10-10T13:39:57.596870: step 5606, loss 0.528992, acc 0.796875, learning_rate 0.0001
2017-10-10T13:39:58.182583: step 5607, loss 0.299043, acc 0.890625, learning_rate 0.0001
2017-10-10T13:39:58.753011: step 5608, loss 0.414109, acc 0.84375, learning_rate 0.0001
2017-10-10T13:39:59.340939: step 5609, loss 0.258883, acc 0.90625, learning_rate 0.0001
2017-10-10T13:39:59.949053: step 5610, loss 0.289661, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:00.549044: step 5611, loss 0.24979, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:01.072810: step 5612, loss 0.33038, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:01.536846: step 5613, loss 0.364898, acc 0.859375, learning_rate 0.0001
2017-10-10T13:40:02.095493: step 5614, loss 0.438014, acc 0.828125, learning_rate 0.0001
2017-10-10T13:40:02.638902: step 5615, loss 0.192107, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:03.188873: step 5616, loss 0.291763, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:03.741241: step 5617, loss 0.317564, acc 0.875, learning_rate 0.0001
2017-10-10T13:40:04.388865: step 5618, loss 0.371713, acc 0.859375, learning_rate 0.0001
2017-10-10T13:40:04.916837: step 5619, loss 0.312819, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:05.526055: step 5620, loss 0.343897, acc 0.875, learning_rate 0.0001
2017-10-10T13:40:06.068854: step 5621, loss 0.231782, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:06.635735: step 5622, loss 0.12304, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:07.183195: step 5623, loss 0.196412, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:07.674463: step 5624, loss 0.35657, acc 0.875, learning_rate 0.0001
2017-10-10T13:40:08.217013: step 5625, loss 0.225877, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:08.798596: step 5626, loss 0.343999, acc 0.84375, learning_rate 0.0001
2017-10-10T13:40:09.400924: step 5627, loss 0.196996, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:09.961245: step 5628, loss 0.266508, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:10.600947: step 5629, loss 0.467404, acc 0.875, learning_rate 0.0001
2017-10-10T13:40:11.030647: step 5630, loss 0.186928, acc 0.96875, learning_rate 0.0001
2017-10-10T13:40:11.425940: step 5631, loss 0.305499, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:11.836936: step 5632, loss 0.32889, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:12.501264: step 5633, loss 0.172993, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:13.073063: step 5634, loss 0.268794, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:13.532317: step 5635, loss 0.402725, acc 0.859375, learning_rate 0.0001
2017-10-10T13:40:13.984934: step 5636, loss 0.210733, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:14.576897: step 5637, loss 0.510576, acc 0.859375, learning_rate 0.0001
2017-10-10T13:40:15.108829: step 5638, loss 0.341868, acc 0.84375, learning_rate 0.0001
2017-10-10T13:40:15.705233: step 5639, loss 0.161824, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:16.181045: step 5640, loss 0.455252, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:40:17.413080: step 5640, loss 0.244765, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5640

2017-10-10T13:40:19.102554: step 5641, loss 0.423951, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:19.646005: step 5642, loss 0.2219, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:20.209650: step 5643, loss 0.456814, acc 0.875, learning_rate 0.0001
2017-10-10T13:40:20.724969: step 5644, loss 0.313427, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:21.271996: step 5645, loss 0.309703, acc 0.875, learning_rate 0.0001
2017-10-10T13:40:21.729745: step 5646, loss 0.341942, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:22.229013: step 5647, loss 0.577244, acc 0.75, learning_rate 0.0001
2017-10-10T13:40:22.793065: step 5648, loss 0.281635, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:23.332999: step 5649, loss 0.173345, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:23.784959: step 5650, loss 0.451305, acc 0.859375, learning_rate 0.0001
2017-10-10T13:40:24.279761: step 5651, loss 0.144499, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:24.795409: step 5652, loss 0.348234, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:25.320997: step 5653, loss 0.126361, acc 0.984375, learning_rate 0.0001
2017-10-10T13:40:25.788475: step 5654, loss 0.330962, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:26.338844: step 5655, loss 0.298511, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:26.848304: step 5656, loss 0.287878, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:27.363662: step 5657, loss 0.280128, acc 0.859375, learning_rate 0.0001
2017-10-10T13:40:27.852918: step 5658, loss 0.391104, acc 0.875, learning_rate 0.0001
2017-10-10T13:40:28.396919: step 5659, loss 0.328086, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:28.900986: step 5660, loss 0.333827, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:29.365046: step 5661, loss 0.170535, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:29.821045: step 5662, loss 0.246475, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:30.211676: step 5663, loss 0.292724, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:30.776410: step 5664, loss 0.304331, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:31.339155: step 5665, loss 0.305075, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:31.956521: step 5666, loss 0.365159, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:32.468928: step 5667, loss 0.314315, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:33.031911: step 5668, loss 0.253649, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:33.588108: step 5669, loss 0.353402, acc 0.84375, learning_rate 0.0001
2017-10-10T13:40:34.147263: step 5670, loss 0.380235, acc 0.84375, learning_rate 0.0001
2017-10-10T13:40:34.612691: step 5671, loss 0.353397, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:35.075593: step 5672, loss 0.248575, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:35.560654: step 5673, loss 0.289601, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:36.140446: step 5674, loss 0.214404, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:36.629902: step 5675, loss 0.256045, acc 0.875, learning_rate 0.0001
2017-10-10T13:40:37.079265: step 5676, loss 0.224752, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:37.556867: step 5677, loss 0.472574, acc 0.84375, learning_rate 0.0001
2017-10-10T13:40:38.060839: step 5678, loss 0.274352, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:38.622306: step 5679, loss 0.215605, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:39.137110: step 5680, loss 0.390175, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:40:40.405262: step 5680, loss 0.244581, acc 0.905036

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5680

2017-10-10T13:40:42.144453: step 5681, loss 0.213427, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:42.651732: step 5682, loss 0.265624, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:43.158810: step 5683, loss 0.171311, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:43.606894: step 5684, loss 0.311683, acc 0.901961, learning_rate 0.0001
2017-10-10T13:40:44.132949: step 5685, loss 0.331528, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:44.644289: step 5686, loss 0.454807, acc 0.828125, learning_rate 0.0001
2017-10-10T13:40:45.190214: step 5687, loss 0.283878, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:45.723796: step 5688, loss 0.310251, acc 0.859375, learning_rate 0.0001
2017-10-10T13:40:46.349276: step 5689, loss 0.243322, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:46.817922: step 5690, loss 0.319766, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:47.262325: step 5691, loss 0.287038, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:47.800886: step 5692, loss 0.330827, acc 0.875, learning_rate 0.0001
2017-10-10T13:40:48.358898: step 5693, loss 0.282158, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:48.942223: step 5694, loss 0.207523, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:49.518613: step 5695, loss 0.254902, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:50.066970: step 5696, loss 0.226109, acc 0.9375, learning_rate 0.0001
2017-10-10T13:40:50.621172: step 5697, loss 0.279034, acc 0.875, learning_rate 0.0001
2017-10-10T13:40:51.132325: step 5698, loss 0.336475, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:51.681257: step 5699, loss 0.374566, acc 0.90625, learning_rate 0.0001
2017-10-10T13:40:52.265027: step 5700, loss 0.333619, acc 0.859375, learning_rate 0.0001
2017-10-10T13:40:52.822544: step 5701, loss 0.314657, acc 0.859375, learning_rate 0.0001
2017-10-10T13:40:53.300825: step 5702, loss 0.307866, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:53.796886: step 5703, loss 0.386498, acc 0.84375, learning_rate 0.0001
2017-10-10T13:40:54.255721: step 5704, loss 0.234063, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:54.820867: step 5705, loss 0.205257, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:55.342592: step 5706, loss 0.294953, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:55.844880: step 5707, loss 0.417411, acc 0.84375, learning_rate 0.0001
2017-10-10T13:40:56.440981: step 5708, loss 0.171238, acc 0.953125, learning_rate 0.0001
2017-10-10T13:40:56.931587: step 5709, loss 0.495934, acc 0.8125, learning_rate 0.0001
2017-10-10T13:40:57.389006: step 5710, loss 0.31926, acc 0.875, learning_rate 0.0001
2017-10-10T13:40:57.832355: step 5711, loss 0.420859, acc 0.859375, learning_rate 0.0001
2017-10-10T13:40:58.391764: step 5712, loss 0.306681, acc 0.921875, learning_rate 0.0001
2017-10-10T13:40:58.952932: step 5713, loss 0.301302, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:59.384996: step 5714, loss 0.258962, acc 0.890625, learning_rate 0.0001
2017-10-10T13:40:59.916905: step 5715, loss 0.193865, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:00.376926: step 5716, loss 0.274796, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:00.805318: step 5717, loss 0.352022, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:01.381014: step 5718, loss 0.230663, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:01.947226: step 5719, loss 0.366509, acc 0.875, learning_rate 0.0001
2017-10-10T13:41:02.437099: step 5720, loss 0.329753, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:41:03.742311: step 5720, loss 0.244721, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5720

2017-10-10T13:41:05.058372: step 5721, loss 0.288481, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:05.563350: step 5722, loss 0.364929, acc 0.875, learning_rate 0.0001
2017-10-10T13:41:06.052068: step 5723, loss 0.390129, acc 0.859375, learning_rate 0.0001
2017-10-10T13:41:06.620923: step 5724, loss 0.357083, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:07.137203: step 5725, loss 0.291976, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:07.662494: step 5726, loss 0.345508, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:08.289074: step 5727, loss 0.19405, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:08.933345: step 5728, loss 0.166382, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:09.376849: step 5729, loss 0.270576, acc 0.859375, learning_rate 0.0001
2017-10-10T13:41:09.764862: step 5730, loss 0.422072, acc 0.828125, learning_rate 0.0001
2017-10-10T13:41:10.229610: step 5731, loss 0.576568, acc 0.84375, learning_rate 0.0001
2017-10-10T13:41:10.719398: step 5732, loss 0.151582, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:11.253153: step 5733, loss 0.280616, acc 0.875, learning_rate 0.0001
2017-10-10T13:41:11.765278: step 5734, loss 0.158262, acc 0.984375, learning_rate 0.0001
2017-10-10T13:41:12.300900: step 5735, loss 0.318058, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:12.788933: step 5736, loss 0.396103, acc 0.84375, learning_rate 0.0001
2017-10-10T13:41:13.392977: step 5737, loss 0.180195, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:13.894132: step 5738, loss 0.229277, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:14.459743: step 5739, loss 0.394983, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:15.006730: step 5740, loss 0.34022, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:15.529143: step 5741, loss 0.2367, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:16.125102: step 5742, loss 0.372137, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:16.737654: step 5743, loss 0.360434, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:17.276974: step 5744, loss 0.458284, acc 0.875, learning_rate 0.0001
2017-10-10T13:41:17.795587: step 5745, loss 0.261836, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:18.258874: step 5746, loss 0.20612, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:18.716861: step 5747, loss 0.408163, acc 0.875, learning_rate 0.0001
2017-10-10T13:41:19.341125: step 5748, loss 0.560988, acc 0.8125, learning_rate 0.0001
2017-10-10T13:41:19.821077: step 5749, loss 0.281163, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:20.275687: step 5750, loss 0.279291, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:20.765356: step 5751, loss 0.491033, acc 0.796875, learning_rate 0.0001
2017-10-10T13:41:21.308867: step 5752, loss 0.242531, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:21.852905: step 5753, loss 0.267479, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:22.494823: step 5754, loss 0.193272, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:22.939209: step 5755, loss 0.178308, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:23.384820: step 5756, loss 0.443958, acc 0.875, learning_rate 0.0001
2017-10-10T13:41:23.911747: step 5757, loss 0.250431, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:24.481604: step 5758, loss 0.245679, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:25.052058: step 5759, loss 0.277099, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:25.596320: step 5760, loss 0.160724, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:41:26.732496: step 5760, loss 0.244075, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5760

2017-10-10T13:41:28.456990: step 5761, loss 0.346959, acc 0.875, learning_rate 0.0001
2017-10-10T13:41:29.048923: step 5762, loss 0.26547, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:29.579239: step 5763, loss 0.291374, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:30.161743: step 5764, loss 0.439518, acc 0.84375, learning_rate 0.0001
2017-10-10T13:41:30.692864: step 5765, loss 0.383184, acc 0.8125, learning_rate 0.0001
2017-10-10T13:41:31.241471: step 5766, loss 0.299428, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:31.800852: step 5767, loss 0.285405, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:32.446572: step 5768, loss 0.31959, acc 0.875, learning_rate 0.0001
2017-10-10T13:41:32.868828: step 5769, loss 0.224189, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:33.312803: step 5770, loss 0.275995, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:33.812858: step 5771, loss 0.286052, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:34.336907: step 5772, loss 0.32786, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:34.954122: step 5773, loss 0.310972, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:35.500789: step 5774, loss 0.333364, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:36.066806: step 5775, loss 0.275914, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:36.641402: step 5776, loss 0.274801, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:37.172463: step 5777, loss 0.276927, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:37.731312: step 5778, loss 0.389187, acc 0.859375, learning_rate 0.0001
2017-10-10T13:41:38.329241: step 5779, loss 0.288749, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:38.861176: step 5780, loss 0.189355, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:39.460922: step 5781, loss 0.378704, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:39.901153: step 5782, loss 0.176881, acc 0.941176, learning_rate 0.0001
2017-10-10T13:41:40.540302: step 5783, loss 0.288365, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:40.996991: step 5784, loss 0.154163, acc 0.96875, learning_rate 0.0001
2017-10-10T13:41:41.600953: step 5785, loss 0.259084, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:42.181231: step 5786, loss 0.485524, acc 0.84375, learning_rate 0.0001
2017-10-10T13:41:42.724310: step 5787, loss 0.281847, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:43.133085: step 5788, loss 0.223524, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:43.564200: step 5789, loss 0.360962, acc 0.84375, learning_rate 0.0001
2017-10-10T13:41:44.100915: step 5790, loss 0.225263, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:44.662818: step 5791, loss 0.244746, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:45.209050: step 5792, loss 0.38749, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:45.584836: step 5793, loss 0.301253, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:46.043462: step 5794, loss 0.429661, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:46.491426: step 5795, loss 0.374511, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:47.045000: step 5796, loss 0.297329, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:47.685005: step 5797, loss 0.280463, acc 0.859375, learning_rate 0.0001
2017-10-10T13:41:48.207851: step 5798, loss 0.313689, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:48.745029: step 5799, loss 0.169991, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:49.342918: step 5800, loss 0.210345, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:41:50.465024: step 5800, loss 0.24453, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5800

2017-10-10T13:41:52.228779: step 5801, loss 0.428217, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:52.685067: step 5802, loss 0.322002, acc 0.875, learning_rate 0.0001
2017-10-10T13:41:53.293326: step 5803, loss 0.27022, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:53.740954: step 5804, loss 0.195044, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:54.261121: step 5805, loss 0.16166, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:54.893310: step 5806, loss 0.365709, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:55.432841: step 5807, loss 0.394279, acc 0.828125, learning_rate 0.0001
2017-10-10T13:41:55.872849: step 5808, loss 0.319107, acc 0.90625, learning_rate 0.0001
2017-10-10T13:41:56.296535: step 5809, loss 0.288303, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:56.782930: step 5810, loss 0.284135, acc 0.921875, learning_rate 0.0001
2017-10-10T13:41:57.343899: step 5811, loss 0.276809, acc 0.890625, learning_rate 0.0001
2017-10-10T13:41:57.865008: step 5812, loss 0.161747, acc 0.953125, learning_rate 0.0001
2017-10-10T13:41:58.309002: step 5813, loss 0.230656, acc 0.9375, learning_rate 0.0001
2017-10-10T13:41:58.862240: step 5814, loss 0.225837, acc 0.859375, learning_rate 0.0001
2017-10-10T13:41:59.420885: step 5815, loss 0.420558, acc 0.84375, learning_rate 0.0001
2017-10-10T13:41:59.970323: step 5816, loss 0.338446, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:00.524977: step 5817, loss 0.230704, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:01.160916: step 5818, loss 0.352686, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:01.745040: step 5819, loss 0.259982, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:02.340914: step 5820, loss 0.408707, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:02.892975: step 5821, loss 0.340594, acc 0.859375, learning_rate 0.0001
2017-10-10T13:42:03.385019: step 5822, loss 0.552108, acc 0.875, learning_rate 0.0001
2017-10-10T13:42:03.853048: step 5823, loss 0.395564, acc 0.84375, learning_rate 0.0001
2017-10-10T13:42:04.384520: step 5824, loss 0.301859, acc 0.875, learning_rate 0.0001
2017-10-10T13:42:04.902033: step 5825, loss 0.482741, acc 0.875, learning_rate 0.0001
2017-10-10T13:42:05.437119: step 5826, loss 0.231821, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:06.042671: step 5827, loss 0.200727, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:06.500626: step 5828, loss 0.447639, acc 0.84375, learning_rate 0.0001
2017-10-10T13:42:07.074004: step 5829, loss 0.292777, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:07.601584: step 5830, loss 0.397421, acc 0.8125, learning_rate 0.0001
2017-10-10T13:42:08.210286: step 5831, loss 0.268602, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:08.652837: step 5832, loss 0.263408, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:09.111755: step 5833, loss 0.321304, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:09.624838: step 5834, loss 0.428972, acc 0.828125, learning_rate 0.0001
2017-10-10T13:42:10.106359: step 5835, loss 0.22703, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:10.560670: step 5836, loss 0.302627, acc 0.875, learning_rate 0.0001
2017-10-10T13:42:11.118861: step 5837, loss 0.235425, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:11.620096: step 5838, loss 0.178551, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:12.076973: step 5839, loss 0.149868, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:12.642485: step 5840, loss 0.264079, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:42:13.809068: step 5840, loss 0.242766, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5840

2017-10-10T13:42:15.287311: step 5841, loss 0.260232, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:15.732931: step 5842, loss 0.157366, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:16.347798: step 5843, loss 0.420544, acc 0.875, learning_rate 0.0001
2017-10-10T13:42:16.856892: step 5844, loss 0.230083, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:17.338182: step 5845, loss 0.235256, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:17.764965: step 5846, loss 0.204477, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:18.333059: step 5847, loss 0.298826, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:18.922331: step 5848, loss 0.304844, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:19.352541: step 5849, loss 0.103083, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:19.836973: step 5850, loss 0.349238, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:20.357371: step 5851, loss 0.37239, acc 0.859375, learning_rate 0.0001
2017-10-10T13:42:20.888574: step 5852, loss 0.474117, acc 0.84375, learning_rate 0.0001
2017-10-10T13:42:21.421389: step 5853, loss 0.254817, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:21.929152: step 5854, loss 0.158207, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:22.424998: step 5855, loss 0.334115, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:22.969895: step 5856, loss 0.25887, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:23.450659: step 5857, loss 0.387874, acc 0.875, learning_rate 0.0001
2017-10-10T13:42:23.968881: step 5858, loss 0.305067, acc 0.875, learning_rate 0.0001
2017-10-10T13:42:24.403225: step 5859, loss 0.209843, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:24.923304: step 5860, loss 0.308783, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:25.450750: step 5861, loss 0.354831, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:25.945074: step 5862, loss 0.210015, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:26.546689: step 5863, loss 0.29828, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:27.122636: step 5864, loss 0.225833, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:27.632889: step 5865, loss 0.264666, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:28.211497: step 5866, loss 0.361006, acc 0.875, learning_rate 0.0001
2017-10-10T13:42:28.857000: step 5867, loss 0.29502, acc 0.859375, learning_rate 0.0001
2017-10-10T13:42:29.329021: step 5868, loss 0.266319, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:29.800963: step 5869, loss 0.198845, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:30.364876: step 5870, loss 0.295632, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:30.972836: step 5871, loss 0.355562, acc 0.859375, learning_rate 0.0001
2017-10-10T13:42:31.443540: step 5872, loss 0.275715, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:31.921184: step 5873, loss 0.336954, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:32.458902: step 5874, loss 0.24333, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:33.056905: step 5875, loss 0.431112, acc 0.828125, learning_rate 0.0001
2017-10-10T13:42:33.625337: step 5876, loss 0.359409, acc 0.875, learning_rate 0.0001
2017-10-10T13:42:34.125043: step 5877, loss 0.280295, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:34.720994: step 5878, loss 0.109228, acc 1, learning_rate 0.0001
2017-10-10T13:42:35.209234: step 5879, loss 0.421618, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:35.670247: step 5880, loss 0.313198, acc 0.941176, learning_rate 0.0001

Evaluation:
2017-10-10T13:42:36.796997: step 5880, loss 0.242554, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5880

2017-10-10T13:42:38.303314: step 5881, loss 0.324169, acc 0.875, learning_rate 0.0001
2017-10-10T13:42:38.829110: step 5882, loss 0.273811, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:39.361275: step 5883, loss 0.220903, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:39.932980: step 5884, loss 0.401998, acc 0.875, learning_rate 0.0001
2017-10-10T13:42:40.426507: step 5885, loss 0.257684, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:41.003213: step 5886, loss 0.31009, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:41.620714: step 5887, loss 0.231692, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:42.021082: step 5888, loss 0.21985, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:42.494785: step 5889, loss 0.13447, acc 0.96875, learning_rate 0.0001
2017-10-10T13:42:42.996863: step 5890, loss 0.237886, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:43.488589: step 5891, loss 0.275943, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:43.942654: step 5892, loss 0.443454, acc 0.84375, learning_rate 0.0001
2017-10-10T13:42:44.483493: step 5893, loss 0.170493, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:45.000899: step 5894, loss 0.254613, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:45.536513: step 5895, loss 0.1238, acc 0.984375, learning_rate 0.0001
2017-10-10T13:42:46.080532: step 5896, loss 0.378641, acc 0.875, learning_rate 0.0001
2017-10-10T13:42:46.606326: step 5897, loss 0.254877, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:47.109875: step 5898, loss 0.250402, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:47.629041: step 5899, loss 0.243556, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:48.182596: step 5900, loss 0.180311, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:48.754905: step 5901, loss 0.255025, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:49.264408: step 5902, loss 0.584855, acc 0.859375, learning_rate 0.0001
2017-10-10T13:42:49.782795: step 5903, loss 0.30077, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:50.368878: step 5904, loss 0.313197, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:50.886124: step 5905, loss 0.201699, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:51.444226: step 5906, loss 0.337657, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:52.028867: step 5907, loss 0.344835, acc 0.84375, learning_rate 0.0001
2017-10-10T13:42:52.436855: step 5908, loss 0.241218, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:52.837007: step 5909, loss 0.168871, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:53.403376: step 5910, loss 0.190135, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:53.992102: step 5911, loss 0.359734, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:54.388976: step 5912, loss 0.274663, acc 0.890625, learning_rate 0.0001
2017-10-10T13:42:54.880869: step 5913, loss 0.181326, acc 0.953125, learning_rate 0.0001
2017-10-10T13:42:55.400884: step 5914, loss 0.283707, acc 0.90625, learning_rate 0.0001
2017-10-10T13:42:55.965101: step 5915, loss 0.308427, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:56.496043: step 5916, loss 0.401948, acc 0.828125, learning_rate 0.0001
2017-10-10T13:42:56.993035: step 5917, loss 0.26656, acc 0.921875, learning_rate 0.0001
2017-10-10T13:42:57.532971: step 5918, loss 0.370459, acc 0.875, learning_rate 0.0001
2017-10-10T13:42:58.044893: step 5919, loss 0.18087, acc 0.9375, learning_rate 0.0001
2017-10-10T13:42:58.654433: step 5920, loss 0.344007, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:42:59.793106: step 5920, loss 0.243708, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5920

2017-10-10T13:43:01.495701: step 5921, loss 0.325004, acc 0.890625, learning_rate 0.0001
2017-10-10T13:43:02.057161: step 5922, loss 0.481684, acc 0.875, learning_rate 0.0001
2017-10-10T13:43:02.656874: step 5923, loss 0.197936, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:03.184906: step 5924, loss 0.198341, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:03.824934: step 5925, loss 0.310795, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:04.307487: step 5926, loss 0.418375, acc 0.859375, learning_rate 0.0001
2017-10-10T13:43:04.761775: step 5927, loss 0.354088, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:05.231975: step 5928, loss 0.259954, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:05.799966: step 5929, loss 0.312634, acc 0.890625, learning_rate 0.0001
2017-10-10T13:43:06.333012: step 5930, loss 0.316439, acc 0.859375, learning_rate 0.0001
2017-10-10T13:43:06.872005: step 5931, loss 0.297974, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:07.426186: step 5932, loss 0.27394, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:08.024852: step 5933, loss 0.245716, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:08.528739: step 5934, loss 0.203663, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:09.053079: step 5935, loss 0.415518, acc 0.859375, learning_rate 0.0001
2017-10-10T13:43:09.576869: step 5936, loss 0.231637, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:10.104854: step 5937, loss 0.304612, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:10.584277: step 5938, loss 0.365425, acc 0.828125, learning_rate 0.0001
2017-10-10T13:43:11.116777: step 5939, loss 0.356668, acc 0.890625, learning_rate 0.0001
2017-10-10T13:43:11.686131: step 5940, loss 0.290819, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:12.135609: step 5941, loss 0.32174, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:12.665102: step 5942, loss 0.320266, acc 0.875, learning_rate 0.0001
2017-10-10T13:43:13.244913: step 5943, loss 0.320438, acc 0.828125, learning_rate 0.0001
2017-10-10T13:43:13.800546: step 5944, loss 0.229645, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:14.403368: step 5945, loss 0.517495, acc 0.796875, learning_rate 0.0001
2017-10-10T13:43:15.032998: step 5946, loss 0.167677, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:15.484932: step 5947, loss 0.417513, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:16.052844: step 5948, loss 0.404277, acc 0.828125, learning_rate 0.0001
2017-10-10T13:43:16.581020: step 5949, loss 0.383914, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:17.024868: step 5950, loss 0.247384, acc 0.890625, learning_rate 0.0001
2017-10-10T13:43:17.405422: step 5951, loss 0.226708, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:17.874527: step 5952, loss 0.287384, acc 0.875, learning_rate 0.0001
2017-10-10T13:43:18.349090: step 5953, loss 0.250661, acc 0.890625, learning_rate 0.0001
2017-10-10T13:43:18.904945: step 5954, loss 0.301792, acc 0.890625, learning_rate 0.0001
2017-10-10T13:43:19.376937: step 5955, loss 0.190677, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:19.917016: step 5956, loss 0.328338, acc 0.875, learning_rate 0.0001
2017-10-10T13:43:20.438520: step 5957, loss 0.394184, acc 0.84375, learning_rate 0.0001
2017-10-10T13:43:20.935609: step 5958, loss 0.322089, acc 0.84375, learning_rate 0.0001
2017-10-10T13:43:21.437411: step 5959, loss 0.206672, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:22.160009: step 5960, loss 0.128409, acc 0.984375, learning_rate 0.0001

Evaluation:
2017-10-10T13:43:23.394909: step 5960, loss 0.24256, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-5960

2017-10-10T13:43:24.861060: step 5961, loss 0.309655, acc 0.890625, learning_rate 0.0001
2017-10-10T13:43:25.460999: step 5962, loss 0.441877, acc 0.859375, learning_rate 0.0001
2017-10-10T13:43:25.968915: step 5963, loss 0.421841, acc 0.890625, learning_rate 0.0001
2017-10-10T13:43:26.456908: step 5964, loss 0.405434, acc 0.859375, learning_rate 0.0001
2017-10-10T13:43:27.088886: step 5965, loss 0.567944, acc 0.84375, learning_rate 0.0001
2017-10-10T13:43:27.756816: step 5966, loss 0.179139, acc 0.96875, learning_rate 0.0001
2017-10-10T13:43:28.199180: step 5967, loss 0.445032, acc 0.828125, learning_rate 0.0001
2017-10-10T13:43:28.736972: step 5968, loss 0.222722, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:29.275120: step 5969, loss 0.294229, acc 0.828125, learning_rate 0.0001
2017-10-10T13:43:29.861038: step 5970, loss 0.260512, acc 0.890625, learning_rate 0.0001
2017-10-10T13:43:30.437509: step 5971, loss 0.238239, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:31.028833: step 5972, loss 0.468432, acc 0.859375, learning_rate 0.0001
2017-10-10T13:43:31.550817: step 5973, loss 0.20393, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:32.130148: step 5974, loss 0.258059, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:32.706230: step 5975, loss 0.209531, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:33.268945: step 5976, loss 0.391529, acc 0.84375, learning_rate 0.0001
2017-10-10T13:43:33.860965: step 5977, loss 0.358556, acc 0.890625, learning_rate 0.0001
2017-10-10T13:43:34.304919: step 5978, loss 0.232127, acc 0.921569, learning_rate 0.0001
2017-10-10T13:43:34.881217: step 5979, loss 0.178168, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:35.412839: step 5980, loss 0.26203, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:35.934041: step 5981, loss 0.302739, acc 0.875, learning_rate 0.0001
2017-10-10T13:43:36.403436: step 5982, loss 0.187412, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:36.976768: step 5983, loss 0.253307, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:37.468923: step 5984, loss 0.455903, acc 0.875, learning_rate 0.0001
2017-10-10T13:43:38.055853: step 5985, loss 0.493821, acc 0.859375, learning_rate 0.0001
2017-10-10T13:43:38.496842: step 5986, loss 0.183195, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:38.929745: step 5987, loss 0.308807, acc 0.890625, learning_rate 0.0001
2017-10-10T13:43:39.446179: step 5988, loss 0.157693, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:39.948821: step 5989, loss 0.363172, acc 0.859375, learning_rate 0.0001
2017-10-10T13:43:40.500398: step 5990, loss 0.182312, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:40.980835: step 5991, loss 0.203605, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:41.536847: step 5992, loss 0.456442, acc 0.859375, learning_rate 0.0001
2017-10-10T13:43:42.080210: step 5993, loss 0.209274, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:42.604824: step 5994, loss 0.234768, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:43.163833: step 5995, loss 0.377885, acc 0.8125, learning_rate 0.0001
2017-10-10T13:43:43.696912: step 5996, loss 0.462265, acc 0.875, learning_rate 0.0001
2017-10-10T13:43:44.248119: step 5997, loss 0.339962, acc 0.875, learning_rate 0.0001
2017-10-10T13:43:44.770760: step 5998, loss 0.19842, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:45.309073: step 5999, loss 0.255331, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:45.859389: step 6000, loss 0.224943, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:43:47.100914: step 6000, loss 0.244086, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6000

2017-10-10T13:43:48.749715: step 6001, loss 0.319652, acc 0.859375, learning_rate 0.0001
2017-10-10T13:43:49.280880: step 6002, loss 0.285403, acc 0.9375, learning_rate 0.0001
2017-10-10T13:43:49.752978: step 6003, loss 0.309694, acc 0.859375, learning_rate 0.0001
2017-10-10T13:43:50.273076: step 6004, loss 0.380155, acc 0.875, learning_rate 0.0001
2017-10-10T13:43:50.725842: step 6005, loss 0.250696, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:51.174610: step 6006, loss 0.183997, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:51.764861: step 6007, loss 0.219789, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:52.325987: step 6008, loss 0.404778, acc 0.859375, learning_rate 0.0001
2017-10-10T13:43:52.876855: step 6009, loss 0.283205, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:53.450941: step 6010, loss 0.458774, acc 0.84375, learning_rate 0.0001
2017-10-10T13:43:53.996872: step 6011, loss 0.319509, acc 0.875, learning_rate 0.0001
2017-10-10T13:43:54.531424: step 6012, loss 0.453646, acc 0.84375, learning_rate 0.0001
2017-10-10T13:43:55.096869: step 6013, loss 0.335133, acc 0.859375, learning_rate 0.0001
2017-10-10T13:43:55.627228: step 6014, loss 0.198118, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:56.163465: step 6015, loss 0.27945, acc 0.921875, learning_rate 0.0001
2017-10-10T13:43:56.698898: step 6016, loss 0.321746, acc 0.90625, learning_rate 0.0001
2017-10-10T13:43:57.248881: step 6017, loss 0.302769, acc 0.890625, learning_rate 0.0001
2017-10-10T13:43:57.795839: step 6018, loss 0.129671, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:58.337398: step 6019, loss 0.185574, acc 0.953125, learning_rate 0.0001
2017-10-10T13:43:58.877622: step 6020, loss 0.319569, acc 0.875, learning_rate 0.0001
2017-10-10T13:43:59.453197: step 6021, loss 0.353135, acc 0.859375, learning_rate 0.0001
2017-10-10T13:43:59.992591: step 6022, loss 0.313077, acc 0.875, learning_rate 0.0001
2017-10-10T13:44:00.553599: step 6023, loss 0.290875, acc 0.90625, learning_rate 0.0001
2017-10-10T13:44:01.178002: step 6024, loss 0.296719, acc 0.890625, learning_rate 0.0001
2017-10-10T13:44:01.712830: step 6025, loss 0.32443, acc 0.90625, learning_rate 0.0001
2017-10-10T13:44:02.088825: step 6026, loss 0.382594, acc 0.828125, learning_rate 0.0001
2017-10-10T13:44:02.410091: step 6027, loss 0.421315, acc 0.859375, learning_rate 0.0001
2017-10-10T13:44:02.888873: step 6028, loss 0.31668, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:03.456976: step 6029, loss 0.190878, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:04.004844: step 6030, loss 0.262018, acc 0.890625, learning_rate 0.0001
2017-10-10T13:44:04.512854: step 6031, loss 0.356162, acc 0.890625, learning_rate 0.0001
2017-10-10T13:44:05.044878: step 6032, loss 0.271213, acc 0.859375, learning_rate 0.0001
2017-10-10T13:44:05.469161: step 6033, loss 0.456024, acc 0.84375, learning_rate 0.0001
2017-10-10T13:44:05.929324: step 6034, loss 0.335268, acc 0.859375, learning_rate 0.0001
2017-10-10T13:44:06.425040: step 6035, loss 0.257492, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:06.928955: step 6036, loss 0.305077, acc 0.90625, learning_rate 0.0001
2017-10-10T13:44:07.476677: step 6037, loss 0.168034, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:08.000622: step 6038, loss 0.301996, acc 0.90625, learning_rate 0.0001
2017-10-10T13:44:08.453278: step 6039, loss 0.239862, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:09.017009: step 6040, loss 0.35151, acc 0.859375, learning_rate 0.0001

Evaluation:
2017-10-10T13:44:10.229227: step 6040, loss 0.242458, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6040

2017-10-10T13:44:12.097037: step 6041, loss 0.255163, acc 0.890625, learning_rate 0.0001
2017-10-10T13:44:12.772709: step 6042, loss 0.163154, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:13.386871: step 6043, loss 0.362673, acc 0.890625, learning_rate 0.0001
2017-10-10T13:44:13.847196: step 6044, loss 0.245321, acc 0.875, learning_rate 0.0001
2017-10-10T13:44:14.245712: step 6045, loss 0.235309, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:14.704987: step 6046, loss 0.244186, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:15.237562: step 6047, loss 0.178637, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:15.764950: step 6048, loss 0.402228, acc 0.875, learning_rate 0.0001
2017-10-10T13:44:16.269112: step 6049, loss 0.254251, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:16.861012: step 6050, loss 0.261654, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:17.399467: step 6051, loss 0.345277, acc 0.875, learning_rate 0.0001
2017-10-10T13:44:17.933010: step 6052, loss 0.265447, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:18.536319: step 6053, loss 0.419108, acc 0.859375, learning_rate 0.0001
2017-10-10T13:44:19.016870: step 6054, loss 0.160819, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:19.528231: step 6055, loss 0.223119, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:20.040868: step 6056, loss 0.37703, acc 0.875, learning_rate 0.0001
2017-10-10T13:44:20.619083: step 6057, loss 0.276886, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:21.089237: step 6058, loss 0.35551, acc 0.90625, learning_rate 0.0001
2017-10-10T13:44:21.616881: step 6059, loss 0.310376, acc 0.90625, learning_rate 0.0001
2017-10-10T13:44:22.176533: step 6060, loss 0.310694, acc 0.890625, learning_rate 0.0001
2017-10-10T13:44:22.677007: step 6061, loss 0.277778, acc 0.890625, learning_rate 0.0001
2017-10-10T13:44:23.226722: step 6062, loss 0.356926, acc 0.875, learning_rate 0.0001
2017-10-10T13:44:23.872101: step 6063, loss 0.285554, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:24.325747: step 6064, loss 0.390293, acc 0.875, learning_rate 0.0001
2017-10-10T13:44:30.746352: step 6065, loss 0.383381, acc 0.84375, learning_rate 0.0001
2017-10-10T13:44:30.911757: step 6066, loss 0.239488, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:31.078425: step 6067, loss 0.293625, acc 0.875, learning_rate 0.0001
2017-10-10T13:44:31.245731: step 6068, loss 0.197947, acc 0.890625, learning_rate 0.0001
2017-10-10T13:44:31.410951: step 6069, loss 0.215584, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:31.578404: step 6070, loss 0.528575, acc 0.796875, learning_rate 0.0001
2017-10-10T13:44:31.744035: step 6071, loss 0.346594, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:31.911109: step 6072, loss 0.393475, acc 0.859375, learning_rate 0.0001
2017-10-10T13:44:32.080292: step 6073, loss 0.145218, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:32.245547: step 6074, loss 0.239374, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:32.410730: step 6075, loss 0.141272, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:32.548772: step 6076, loss 0.258511, acc 0.901961, learning_rate 0.0001
2017-10-10T13:44:32.715909: step 6077, loss 0.256892, acc 0.890625, learning_rate 0.0001
2017-10-10T13:44:32.879282: step 6078, loss 0.280556, acc 0.890625, learning_rate 0.0001
2017-10-10T13:44:33.043969: step 6079, loss 0.204161, acc 0.9375, learning_rate 0.0001
2017-10-10T13:44:33.221619: step 6080, loss 0.138504, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:44:36.253482: step 6080, loss 0.241392, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6080

2017-10-10T13:44:56.403054: step 6081, loss 0.232146, acc 0.921875, learning_rate 0.0001
2017-10-10T13:44:56.657944: step 6082, loss 0.545847, acc 0.859375, learning_rate 0.0001
2017-10-10T13:44:56.851381: step 6083, loss 0.401936, acc 0.890625, learning_rate 0.0001
2017-10-10T13:44:57.121617: step 6084, loss 0.323137, acc 0.890625, learning_rate 0.0001
2017-10-10T13:44:57.518965: step 6085, loss 0.352657, acc 0.890625, learning_rate 0.0001
2017-10-10T13:44:57.774120: step 6086, loss 0.294443, acc 0.859375, learning_rate 0.0001
2017-10-10T13:44:58.147799: step 6087, loss 0.311673, acc 0.84375, learning_rate 0.0001
2017-10-10T13:44:58.363061: step 6088, loss 0.16792, acc 0.953125, learning_rate 0.0001
2017-10-10T13:44:58.651192: step 6089, loss 0.138317, acc 0.984375, learning_rate 0.0001
2017-10-10T13:44:59.212249: step 6090, loss 0.353253, acc 0.875, learning_rate 0.0001
2017-10-10T13:44:59.525999: step 6091, loss 0.177745, acc 0.96875, learning_rate 0.0001
2017-10-10T13:44:59.722629: step 6092, loss 0.262398, acc 0.859375, learning_rate 0.0001
2017-10-10T13:44:59.931794: step 6093, loss 0.24914, acc 0.921875, learning_rate 0.0001
2017-10-10T13:45:00.538717: step 6094, loss 0.407425, acc 0.828125, learning_rate 0.0001
2017-10-10T13:45:01.164996: step 6095, loss 0.290901, acc 0.875, learning_rate 0.0001
2017-10-10T13:45:01.484189: step 6096, loss 0.296943, acc 0.890625, learning_rate 0.0001
2017-10-10T13:45:01.794027: step 6097, loss 0.227457, acc 0.921875, learning_rate 0.0001
2017-10-10T13:45:02.159291: step 6098, loss 0.307555, acc 0.875, learning_rate 0.0001
2017-10-10T13:45:02.546451: step 6099, loss 0.447385, acc 0.828125, learning_rate 0.0001
2017-10-10T13:45:02.867518: step 6100, loss 0.186298, acc 0.9375, learning_rate 0.0001
2017-10-10T13:45:03.073685: step 6101, loss 0.256856, acc 0.90625, learning_rate 0.0001
2017-10-10T13:45:03.237795: step 6102, loss 0.272118, acc 0.90625, learning_rate 0.0001
2017-10-10T13:45:03.401839: step 6103, loss 0.339534, acc 0.890625, learning_rate 0.0001
2017-10-10T13:45:03.643783: step 6104, loss 0.237487, acc 0.921875, learning_rate 0.0001
2017-10-10T13:45:03.938683: step 6105, loss 0.338841, acc 0.90625, learning_rate 0.0001
2017-10-10T13:45:04.283616: step 6106, loss 0.204225, acc 0.9375, learning_rate 0.0001
2017-10-10T13:45:04.609924: step 6107, loss 0.391008, acc 0.875, learning_rate 0.0001
2017-10-10T13:45:05.236028: step 6108, loss 0.21741, acc 0.921875, learning_rate 0.0001
2017-10-10T13:45:06.820809: step 6109, loss 0.350333, acc 0.875, learning_rate 0.0001
2017-10-10T13:45:07.287300: step 6110, loss 0.296479, acc 0.890625, learning_rate 0.0001
2017-10-10T13:45:07.650805: step 6111, loss 0.244481, acc 0.9375, learning_rate 0.0001
2017-10-10T13:45:08.060012: step 6112, loss 0.231863, acc 0.90625, learning_rate 0.0001
2017-10-10T13:45:08.339216: step 6113, loss 0.269913, acc 0.9375, learning_rate 0.0001
2017-10-10T13:45:08.506347: step 6114, loss 0.486406, acc 0.84375, learning_rate 0.0001
2017-10-10T13:45:08.979390: step 6115, loss 0.42653, acc 0.84375, learning_rate 0.0001
2017-10-10T13:45:09.400808: step 6116, loss 0.202228, acc 0.921875, learning_rate 0.0001
2017-10-10T13:45:10.286733: step 6117, loss 0.372268, acc 0.90625, learning_rate 0.0001
2017-10-10T13:45:10.728262: step 6118, loss 0.433078, acc 0.875, learning_rate 0.0001
2017-10-10T13:45:11.049664: step 6119, loss 0.325202, acc 0.859375, learning_rate 0.0001
2017-10-10T13:45:11.635063: step 6120, loss 0.4091, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:45:12.186052: step 6120, loss 0.242728, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6120

2017-10-10T13:46:12.472395: step 6121, loss 0.289168, acc 0.859375, learning_rate 0.0001
2017-10-10T13:46:12.710116: step 6122, loss 0.2698, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:13.011890: step 6123, loss 0.181608, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:13.465342: step 6124, loss 0.164624, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:13.828793: step 6125, loss 0.32126, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:14.002982: step 6126, loss 0.346514, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:14.303478: step 6127, loss 0.299635, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:14.554686: step 6128, loss 0.241308, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:14.780796: step 6129, loss 0.115008, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:15.079774: step 6130, loss 0.272291, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:15.304978: step 6131, loss 0.217174, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:15.588867: step 6132, loss 0.343718, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:15.800405: step 6133, loss 0.245821, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:16.158462: step 6134, loss 0.18717, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:16.441995: step 6135, loss 0.365316, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:16.625896: step 6136, loss 0.231923, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:16.931042: step 6137, loss 0.394199, acc 0.859375, learning_rate 0.0001
2017-10-10T13:46:17.437680: step 6138, loss 0.264901, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:17.617486: step 6139, loss 0.205191, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:17.898397: step 6140, loss 0.370872, acc 0.84375, learning_rate 0.0001
2017-10-10T13:46:18.170485: step 6141, loss 0.166519, acc 0.984375, learning_rate 0.0001
2017-10-10T13:46:18.499674: step 6142, loss 0.341653, acc 0.796875, learning_rate 0.0001
2017-10-10T13:46:18.768842: step 6143, loss 0.239354, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:19.056257: step 6144, loss 0.411436, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:19.303161: step 6145, loss 0.362397, acc 0.859375, learning_rate 0.0001
2017-10-10T13:46:19.764374: step 6146, loss 0.423396, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:20.112982: step 6147, loss 0.322321, acc 0.859375, learning_rate 0.0001
2017-10-10T13:46:20.616892: step 6148, loss 0.347391, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:20.978964: step 6149, loss 0.310793, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:21.368041: step 6150, loss 0.270947, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:21.624868: step 6151, loss 0.326151, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:21.888913: step 6152, loss 0.292567, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:22.214450: step 6153, loss 0.147207, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:22.491387: step 6154, loss 0.151881, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:22.657024: step 6155, loss 0.39672, acc 0.859375, learning_rate 0.0001
2017-10-10T13:46:23.111730: step 6156, loss 0.344194, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:23.381026: step 6157, loss 0.314769, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:23.705947: step 6158, loss 0.318689, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:23.908804: step 6159, loss 0.341925, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:24.145571: step 6160, loss 0.216302, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:46:24.703195: step 6160, loss 0.242579, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6160

2017-10-10T13:46:25.891386: step 6161, loss 0.204944, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:26.185166: step 6162, loss 0.179377, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:26.485002: step 6163, loss 0.196053, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:26.769164: step 6164, loss 0.169661, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:27.071739: step 6165, loss 0.451859, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:27.367317: step 6166, loss 0.231505, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:27.585036: step 6167, loss 0.257714, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:27.834969: step 6168, loss 0.187449, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:28.089291: step 6169, loss 0.503919, acc 0.828125, learning_rate 0.0001
2017-10-10T13:46:28.337907: step 6170, loss 0.393958, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:28.625754: step 6171, loss 0.480134, acc 0.828125, learning_rate 0.0001
2017-10-10T13:46:28.921013: step 6172, loss 0.123863, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:29.189330: step 6173, loss 0.326302, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:29.449944: step 6174, loss 0.140542, acc 0.980392, learning_rate 0.0001
2017-10-10T13:46:29.743750: step 6175, loss 0.26912, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:30.025775: step 6176, loss 0.246359, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:30.308368: step 6177, loss 0.213773, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:30.590080: step 6178, loss 0.35036, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:30.877709: step 6179, loss 0.292078, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:31.151917: step 6180, loss 0.370319, acc 0.828125, learning_rate 0.0001
2017-10-10T13:46:31.434764: step 6181, loss 0.211065, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:31.719837: step 6182, loss 0.278542, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:31.977803: step 6183, loss 0.26711, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:32.256142: step 6184, loss 0.412253, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:32.542697: step 6185, loss 0.346676, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:32.904938: step 6186, loss 0.194715, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:33.216134: step 6187, loss 0.243694, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:33.392771: step 6188, loss 0.382817, acc 0.859375, learning_rate 0.0001
2017-10-10T13:46:33.575517: step 6189, loss 0.319272, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:33.753434: step 6190, loss 0.306616, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:33.926589: step 6191, loss 0.140172, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:34.112270: step 6192, loss 0.276673, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:34.409317: step 6193, loss 0.159109, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:34.699068: step 6194, loss 0.322526, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:34.999367: step 6195, loss 0.268257, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:35.285095: step 6196, loss 0.330777, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:35.591866: step 6197, loss 0.427031, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:35.884603: step 6198, loss 0.301069, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:36.180272: step 6199, loss 0.211328, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:36.483152: step 6200, loss 0.325085, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:46:37.092850: step 6200, loss 0.242071, acc 0.906475

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6200

2017-10-10T13:46:38.360038: step 6201, loss 0.294625, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:38.636908: step 6202, loss 0.325012, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:38.928933: step 6203, loss 0.358844, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:39.222662: step 6204, loss 0.32879, acc 0.84375, learning_rate 0.0001
2017-10-10T13:46:39.523867: step 6205, loss 0.145848, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:39.803213: step 6206, loss 0.286237, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:40.082325: step 6207, loss 0.329103, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:40.364895: step 6208, loss 0.258155, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:40.642328: step 6209, loss 0.510799, acc 0.859375, learning_rate 0.0001
2017-10-10T13:46:40.925496: step 6210, loss 0.213522, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:41.209406: step 6211, loss 0.165632, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:41.493415: step 6212, loss 0.177141, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:41.771137: step 6213, loss 0.235433, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:42.066434: step 6214, loss 0.248063, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:42.346122: step 6215, loss 0.191169, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:42.647410: step 6216, loss 0.306987, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:42.946267: step 6217, loss 0.347862, acc 0.859375, learning_rate 0.0001
2017-10-10T13:46:43.225839: step 6218, loss 0.183455, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:43.506754: step 6219, loss 0.356474, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:43.786673: step 6220, loss 0.281912, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:44.075623: step 6221, loss 0.247661, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:44.359458: step 6222, loss 0.178029, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:44.637234: step 6223, loss 0.324797, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:44.896265: step 6224, loss 0.422145, acc 0.828125, learning_rate 0.0001
2017-10-10T13:46:45.060549: step 6225, loss 0.366464, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:45.250807: step 6226, loss 0.196899, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:45.520698: step 6227, loss 0.319111, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:45.855718: step 6228, loss 0.332638, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:46.187573: step 6229, loss 0.420254, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:46.359079: step 6230, loss 0.239292, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:46.547058: step 6231, loss 0.467345, acc 0.8125, learning_rate 0.0001
2017-10-10T13:46:46.724775: step 6232, loss 0.240599, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:46.984858: step 6233, loss 0.220232, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:47.289804: step 6234, loss 0.265912, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:47.575570: step 6235, loss 0.241819, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:47.882058: step 6236, loss 0.353564, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:48.182923: step 6237, loss 0.239969, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:48.477106: step 6238, loss 0.380603, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:48.752853: step 6239, loss 0.162247, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:49.033665: step 6240, loss 0.297812, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:46:49.692469: step 6240, loss 0.240069, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6240

2017-10-10T13:46:50.732758: step 6241, loss 0.211742, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:51.034837: step 6242, loss 0.331816, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:51.341562: step 6243, loss 0.146029, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:51.635272: step 6244, loss 0.2742, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:51.916309: step 6245, loss 0.22666, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:52.209267: step 6246, loss 0.170525, acc 0.96875, learning_rate 0.0001
2017-10-10T13:46:52.494200: step 6247, loss 0.286121, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:52.764162: step 6248, loss 0.180749, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:53.093751: step 6249, loss 0.274229, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:53.424114: step 6250, loss 0.183412, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:53.804870: step 6251, loss 0.219332, acc 0.953125, learning_rate 0.0001
2017-10-10T13:46:54.212893: step 6252, loss 0.292494, acc 0.875, learning_rate 0.0001
2017-10-10T13:46:54.576594: step 6253, loss 0.301589, acc 0.890625, learning_rate 0.0001
2017-10-10T13:46:54.916993: step 6254, loss 0.266995, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:55.338832: step 6255, loss 0.390939, acc 0.859375, learning_rate 0.0001
2017-10-10T13:46:55.734597: step 6256, loss 0.385069, acc 0.78125, learning_rate 0.0001
2017-10-10T13:46:56.150569: step 6257, loss 0.231401, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:56.535596: step 6258, loss 0.204715, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:56.954146: step 6259, loss 0.210682, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:57.354098: step 6260, loss 0.322346, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:57.696961: step 6261, loss 0.215238, acc 0.921875, learning_rate 0.0001
2017-10-10T13:46:58.053326: step 6262, loss 0.350884, acc 0.828125, learning_rate 0.0001
2017-10-10T13:46:58.431461: step 6263, loss 0.357801, acc 0.90625, learning_rate 0.0001
2017-10-10T13:46:58.829831: step 6264, loss 0.39818, acc 0.859375, learning_rate 0.0001
2017-10-10T13:46:59.252355: step 6265, loss 0.261713, acc 0.9375, learning_rate 0.0001
2017-10-10T13:46:59.726511: step 6266, loss 0.272603, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:00.172186: step 6267, loss 0.219352, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:00.497061: step 6268, loss 0.341942, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:00.808990: step 6269, loss 0.313489, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:01.100182: step 6270, loss 0.339395, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:01.479133: step 6271, loss 0.252576, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:01.765378: step 6272, loss 0.28352, acc 0.941176, learning_rate 0.0001
2017-10-10T13:47:02.126440: step 6273, loss 0.276435, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:02.517752: step 6274, loss 0.327564, acc 0.859375, learning_rate 0.0001
2017-10-10T13:47:02.897021: step 6275, loss 0.289097, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:03.207175: step 6276, loss 0.377168, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:03.541148: step 6277, loss 0.325533, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:03.966795: step 6278, loss 0.284024, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:04.385749: step 6279, loss 0.303935, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:04.825169: step 6280, loss 0.312711, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:47:05.745655: step 6280, loss 0.240553, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6280

2017-10-10T13:47:07.165204: step 6281, loss 0.382776, acc 0.859375, learning_rate 0.0001
2017-10-10T13:47:07.561811: step 6282, loss 0.241672, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:07.986713: step 6283, loss 0.283459, acc 0.859375, learning_rate 0.0001
2017-10-10T13:47:08.412860: step 6284, loss 0.106184, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:08.817569: step 6285, loss 0.305283, acc 0.84375, learning_rate 0.0001
2017-10-10T13:47:09.219565: step 6286, loss 0.218925, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:09.630918: step 6287, loss 0.408685, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:10.066303: step 6288, loss 0.307142, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:10.505021: step 6289, loss 0.29122, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:10.939548: step 6290, loss 0.245459, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:11.220881: step 6291, loss 0.262227, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:11.563165: step 6292, loss 0.304203, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:11.844033: step 6293, loss 0.239558, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:12.232945: step 6294, loss 0.47872, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:12.625800: step 6295, loss 0.189249, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:13.092861: step 6296, loss 0.370844, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:13.464723: step 6297, loss 0.474457, acc 0.796875, learning_rate 0.0001
2017-10-10T13:47:13.872926: step 6298, loss 0.4337, acc 0.859375, learning_rate 0.0001
2017-10-10T13:47:14.281198: step 6299, loss 0.300414, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:14.653147: step 6300, loss 0.277522, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:15.028600: step 6301, loss 0.320366, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:15.416969: step 6302, loss 0.299235, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:15.796955: step 6303, loss 0.157807, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:16.198120: step 6304, loss 0.298032, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:16.559910: step 6305, loss 0.125682, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:16.980892: step 6306, loss 0.37163, acc 0.859375, learning_rate 0.0001
2017-10-10T13:47:17.458502: step 6307, loss 0.213597, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:17.740853: step 6308, loss 0.222411, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:18.035173: step 6309, loss 0.309169, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:18.305050: step 6310, loss 0.306777, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:18.594108: step 6311, loss 0.29836, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:19.021119: step 6312, loss 0.221317, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:19.428041: step 6313, loss 0.310622, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:19.811428: step 6314, loss 0.287859, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:20.265066: step 6315, loss 0.282771, acc 0.859375, learning_rate 0.0001
2017-10-10T13:47:20.652888: step 6316, loss 0.336736, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:21.081905: step 6317, loss 0.258305, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:21.500863: step 6318, loss 0.216592, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:21.933010: step 6319, loss 0.312872, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:22.308878: step 6320, loss 0.236091, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:47:23.281067: step 6320, loss 0.238402, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6320

2017-10-10T13:47:24.945112: step 6321, loss 0.339707, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:25.312547: step 6322, loss 0.184631, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:25.727259: step 6323, loss 0.258053, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:26.155652: step 6324, loss 0.0971039, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:26.539137: step 6325, loss 0.250566, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:26.939510: step 6326, loss 0.212131, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:27.352552: step 6327, loss 0.306223, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:27.766428: step 6328, loss 0.25526, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:28.217840: step 6329, loss 0.191909, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:28.672913: step 6330, loss 0.18173, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:28.964522: step 6331, loss 0.347197, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:29.263655: step 6332, loss 0.26457, acc 0.859375, learning_rate 0.0001
2017-10-10T13:47:29.590131: step 6333, loss 0.232919, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:29.975580: step 6334, loss 0.345748, acc 0.859375, learning_rate 0.0001
2017-10-10T13:47:30.339294: step 6335, loss 0.323496, acc 0.859375, learning_rate 0.0001
2017-10-10T13:47:30.717735: step 6336, loss 0.241815, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:31.180842: step 6337, loss 0.25791, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:31.624896: step 6338, loss 0.226909, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:32.035521: step 6339, loss 0.138363, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:32.531571: step 6340, loss 0.454207, acc 0.828125, learning_rate 0.0001
2017-10-10T13:47:32.940847: step 6341, loss 0.150329, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:33.319080: step 6342, loss 0.298895, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:33.725670: step 6343, loss 0.287426, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:34.149480: step 6344, loss 0.307396, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:34.630840: step 6345, loss 0.24691, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:35.095063: step 6346, loss 0.273929, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:35.413613: step 6347, loss 0.228649, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:35.722863: step 6348, loss 0.280478, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:36.088350: step 6349, loss 0.375705, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:36.504472: step 6350, loss 0.430877, acc 0.84375, learning_rate 0.0001
2017-10-10T13:47:36.864505: step 6351, loss 0.249642, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:37.181142: step 6352, loss 0.378622, acc 0.859375, learning_rate 0.0001
2017-10-10T13:47:37.545014: step 6353, loss 0.35928, acc 0.84375, learning_rate 0.0001
2017-10-10T13:47:37.918653: step 6354, loss 0.409567, acc 0.859375, learning_rate 0.0001
2017-10-10T13:47:38.393186: step 6355, loss 0.288187, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:38.756574: step 6356, loss 0.409858, acc 0.828125, learning_rate 0.0001
2017-10-10T13:47:39.141163: step 6357, loss 0.18891, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:39.505056: step 6358, loss 0.216437, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:39.921012: step 6359, loss 0.201602, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:40.316138: step 6360, loss 0.162146, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:47:41.084972: step 6360, loss 0.238631, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6360

2017-10-10T13:47:42.470512: step 6361, loss 0.273155, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:42.884895: step 6362, loss 0.334235, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:43.247482: step 6363, loss 0.396805, acc 0.859375, learning_rate 0.0001
2017-10-10T13:47:43.633947: step 6364, loss 0.247234, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:43.995815: step 6365, loss 0.352496, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:44.336898: step 6366, loss 0.126903, acc 0.984375, learning_rate 0.0001
2017-10-10T13:47:44.740940: step 6367, loss 0.203777, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:45.126026: step 6368, loss 0.253887, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:45.543542: step 6369, loss 0.299258, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:45.962410: step 6370, loss 0.198517, acc 0.921569, learning_rate 0.0001
2017-10-10T13:47:46.437803: step 6371, loss 0.277954, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:46.767969: step 6372, loss 0.261774, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:47.092833: step 6373, loss 0.168296, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:47.500864: step 6374, loss 0.260242, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:47.973495: step 6375, loss 0.248016, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:48.414739: step 6376, loss 0.355997, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:48.788599: step 6377, loss 0.303177, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:49.189229: step 6378, loss 0.366023, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:49.586923: step 6379, loss 0.272663, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:49.986423: step 6380, loss 0.270005, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:50.399382: step 6381, loss 0.252235, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:50.792829: step 6382, loss 0.335547, acc 0.890625, learning_rate 0.0001
2017-10-10T13:47:51.218305: step 6383, loss 0.312546, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:51.653808: step 6384, loss 0.172024, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:52.132874: step 6385, loss 0.345465, acc 0.828125, learning_rate 0.0001
2017-10-10T13:47:52.606272: step 6386, loss 0.208722, acc 0.96875, learning_rate 0.0001
2017-10-10T13:47:52.933202: step 6387, loss 0.201978, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:53.232819: step 6388, loss 0.25207, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:53.569656: step 6389, loss 0.448057, acc 0.828125, learning_rate 0.0001
2017-10-10T13:47:53.980844: step 6390, loss 0.243512, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:54.384596: step 6391, loss 0.231123, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:54.777618: step 6392, loss 0.335446, acc 0.9375, learning_rate 0.0001
2017-10-10T13:47:55.184300: step 6393, loss 0.19724, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:55.600870: step 6394, loss 0.214212, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:56.004831: step 6395, loss 0.310138, acc 0.875, learning_rate 0.0001
2017-10-10T13:47:56.420856: step 6396, loss 0.465677, acc 0.84375, learning_rate 0.0001
2017-10-10T13:47:56.804855: step 6397, loss 0.214144, acc 0.953125, learning_rate 0.0001
2017-10-10T13:47:57.220787: step 6398, loss 0.271623, acc 0.921875, learning_rate 0.0001
2017-10-10T13:47:57.617340: step 6399, loss 0.239126, acc 0.90625, learning_rate 0.0001
2017-10-10T13:47:58.051796: step 6400, loss 0.405981, acc 0.828125, learning_rate 0.0001

Evaluation:
2017-10-10T13:47:58.944326: step 6400, loss 0.238682, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6400

2017-10-10T13:48:00.309488: step 6401, loss 0.243487, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:00.725054: step 6402, loss 0.236336, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:01.147444: step 6403, loss 0.321421, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:01.569908: step 6404, loss 0.400059, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:01.986072: step 6405, loss 0.382276, acc 0.859375, learning_rate 0.0001
2017-10-10T13:48:02.412880: step 6406, loss 0.219194, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:02.816443: step 6407, loss 0.365431, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:03.244860: step 6408, loss 0.211943, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:03.787667: step 6409, loss 0.370602, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:04.161446: step 6410, loss 0.39564, acc 0.84375, learning_rate 0.0001
2017-10-10T13:48:04.484527: step 6411, loss 0.453488, acc 0.859375, learning_rate 0.0001
2017-10-10T13:48:04.796929: step 6412, loss 0.233749, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:05.192842: step 6413, loss 0.35718, acc 0.859375, learning_rate 0.0001
2017-10-10T13:48:05.639612: step 6414, loss 0.290999, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:06.057225: step 6415, loss 0.338692, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:06.492994: step 6416, loss 0.2051, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:06.944963: step 6417, loss 0.155384, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:07.307485: step 6418, loss 0.463778, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:07.668943: step 6419, loss 0.231892, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:08.041249: step 6420, loss 0.430284, acc 0.796875, learning_rate 0.0001
2017-10-10T13:48:08.548722: step 6421, loss 0.398855, acc 0.84375, learning_rate 0.0001
2017-10-10T13:48:08.949309: step 6422, loss 0.230526, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:09.363721: step 6423, loss 0.384899, acc 0.859375, learning_rate 0.0001
2017-10-10T13:48:09.882477: step 6424, loss 0.290058, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:10.325100: step 6425, loss 0.355948, acc 0.84375, learning_rate 0.0001
2017-10-10T13:48:10.619668: step 6426, loss 0.27519, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:10.907167: step 6427, loss 0.154026, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:11.208926: step 6428, loss 0.102944, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:11.675193: step 6429, loss 0.151413, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:12.087778: step 6430, loss 0.289844, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:12.519644: step 6431, loss 0.378294, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:12.953897: step 6432, loss 0.139878, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:13.322660: step 6433, loss 0.222602, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:13.711981: step 6434, loss 0.142885, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:14.119656: step 6435, loss 0.375086, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:14.548951: step 6436, loss 0.291915, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:14.951468: step 6437, loss 0.383095, acc 0.828125, learning_rate 0.0001
2017-10-10T13:48:15.348544: step 6438, loss 0.186169, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:15.789038: step 6439, loss 0.225434, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:16.225203: step 6440, loss 0.219283, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:48:17.143673: step 6440, loss 0.237654, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6440

2017-10-10T13:48:18.712904: step 6441, loss 0.313284, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:19.116986: step 6442, loss 0.197575, acc 0.984375, learning_rate 0.0001
2017-10-10T13:48:19.531137: step 6443, loss 0.274327, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:19.843340: step 6444, loss 0.210522, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:20.244850: step 6445, loss 0.327016, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:20.756858: step 6446, loss 0.313949, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:21.200978: step 6447, loss 0.196165, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:21.527043: step 6448, loss 0.234173, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:21.816929: step 6449, loss 0.288955, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:22.152830: step 6450, loss 0.277942, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:22.500912: step 6451, loss 0.358226, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:22.908877: step 6452, loss 0.197894, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:23.379436: step 6453, loss 0.178804, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:23.806245: step 6454, loss 0.231783, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:24.210236: step 6455, loss 0.373847, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:24.632994: step 6456, loss 0.177054, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:25.063269: step 6457, loss 0.221166, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:25.477834: step 6458, loss 0.225506, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:25.912898: step 6459, loss 0.315562, acc 0.859375, learning_rate 0.0001
2017-10-10T13:48:26.353181: step 6460, loss 0.343805, acc 0.859375, learning_rate 0.0001
2017-10-10T13:48:26.744442: step 6461, loss 0.367956, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:27.257212: step 6462, loss 0.311965, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:27.622751: step 6463, loss 0.31987, acc 0.859375, learning_rate 0.0001
2017-10-10T13:48:27.931154: step 6464, loss 0.179448, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:28.249703: step 6465, loss 0.236651, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:28.570171: step 6466, loss 0.216265, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:28.989017: step 6467, loss 0.299939, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:29.360849: step 6468, loss 0.334605, acc 0.921569, learning_rate 0.0001
2017-10-10T13:48:29.794194: step 6469, loss 0.148843, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:30.192841: step 6470, loss 0.414454, acc 0.859375, learning_rate 0.0001
2017-10-10T13:48:30.624042: step 6471, loss 0.173472, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:31.039276: step 6472, loss 0.398058, acc 0.84375, learning_rate 0.0001
2017-10-10T13:48:31.443842: step 6473, loss 0.352546, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:31.838775: step 6474, loss 0.238614, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:32.270312: step 6475, loss 0.339191, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:32.697082: step 6476, loss 0.326759, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:33.109655: step 6477, loss 0.30838, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:33.530394: step 6478, loss 0.215675, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:33.956852: step 6479, loss 0.112526, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:34.364860: step 6480, loss 0.294049, acc 0.859375, learning_rate 0.0001

Evaluation:
2017-10-10T13:48:35.286225: step 6480, loss 0.238168, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6480

2017-10-10T13:48:36.571910: step 6481, loss 0.280372, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:36.968924: step 6482, loss 0.286908, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:37.366912: step 6483, loss 0.381698, acc 0.859375, learning_rate 0.0001
2017-10-10T13:48:37.767063: step 6484, loss 0.244883, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:38.164851: step 6485, loss 0.242902, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:38.584854: step 6486, loss 0.309515, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:39.036983: step 6487, loss 0.177047, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:39.352710: step 6488, loss 0.256013, acc 0.96875, learning_rate 0.0001
2017-10-10T13:48:39.696654: step 6489, loss 0.223364, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:40.029305: step 6490, loss 0.332541, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:40.477355: step 6491, loss 0.321124, acc 0.859375, learning_rate 0.0001
2017-10-10T13:48:40.823033: step 6492, loss 0.191099, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:41.211041: step 6493, loss 0.263552, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:41.544873: step 6494, loss 0.302362, acc 0.859375, learning_rate 0.0001
2017-10-10T13:48:41.989122: step 6495, loss 0.382486, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:42.359340: step 6496, loss 0.414026, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:42.744828: step 6497, loss 0.310254, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:43.184823: step 6498, loss 0.185897, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:43.592918: step 6499, loss 0.309451, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:44.010662: step 6500, loss 0.391284, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:44.421831: step 6501, loss 0.218559, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:44.864855: step 6502, loss 0.250445, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:45.345004: step 6503, loss 0.355781, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:45.654978: step 6504, loss 0.322452, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:45.963061: step 6505, loss 0.258752, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:46.284860: step 6506, loss 0.349455, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:46.688901: step 6507, loss 0.261285, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:47.081014: step 6508, loss 0.2648, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:47.476405: step 6509, loss 0.370803, acc 0.84375, learning_rate 0.0001
2017-10-10T13:48:47.833963: step 6510, loss 0.264541, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:48.235971: step 6511, loss 0.188183, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:48.565188: step 6512, loss 0.232651, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:48.948858: step 6513, loss 0.253255, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:49.357002: step 6514, loss 0.263515, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:49.758518: step 6515, loss 0.248047, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:50.162022: step 6516, loss 0.237938, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:50.578866: step 6517, loss 0.260667, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:50.987234: step 6518, loss 0.224656, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:51.407607: step 6519, loss 0.176316, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:51.838467: step 6520, loss 0.329432, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:48:52.759663: step 6520, loss 0.237883, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6520

2017-10-10T13:48:54.177303: step 6521, loss 0.257351, acc 0.890625, learning_rate 0.0001
2017-10-10T13:48:54.576892: step 6522, loss 0.257585, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:54.926306: step 6523, loss 0.310028, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:55.331612: step 6524, loss 0.478605, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:55.746073: step 6525, loss 0.371761, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:56.128677: step 6526, loss 0.279214, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:56.564565: step 6527, loss 0.235239, acc 0.90625, learning_rate 0.0001
2017-10-10T13:48:57.070630: step 6528, loss 0.223222, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:57.389778: step 6529, loss 0.240791, acc 0.921875, learning_rate 0.0001
2017-10-10T13:48:57.702072: step 6530, loss 0.420871, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:58.003203: step 6531, loss 0.358394, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:58.426201: step 6532, loss 0.352967, acc 0.875, learning_rate 0.0001
2017-10-10T13:48:58.839033: step 6533, loss 0.222071, acc 0.953125, learning_rate 0.0001
2017-10-10T13:48:59.247407: step 6534, loss 0.374109, acc 0.9375, learning_rate 0.0001
2017-10-10T13:48:59.669803: step 6535, loss 0.106574, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:00.109506: step 6536, loss 0.191335, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:00.532365: step 6537, loss 0.159055, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:00.952673: step 6538, loss 0.252972, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:01.384863: step 6539, loss 0.227467, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:01.792080: step 6540, loss 0.491437, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:02.190687: step 6541, loss 0.197434, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:02.661023: step 6542, loss 0.0797681, acc 1, learning_rate 0.0001
2017-10-10T13:49:03.150485: step 6543, loss 0.274532, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:03.488832: step 6544, loss 0.245014, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:03.801723: step 6545, loss 0.329225, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:04.080840: step 6546, loss 0.183134, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:04.503016: step 6547, loss 0.319204, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:04.920698: step 6548, loss 0.186272, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:05.337357: step 6549, loss 0.238837, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:05.751321: step 6550, loss 0.287141, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:06.178824: step 6551, loss 0.233711, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:06.603048: step 6552, loss 0.363407, acc 0.84375, learning_rate 0.0001
2017-10-10T13:49:07.020405: step 6553, loss 0.269285, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:07.427073: step 6554, loss 0.257026, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:07.808019: step 6555, loss 0.392714, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:08.216840: step 6556, loss 0.142438, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:08.632480: step 6557, loss 0.272364, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:09.062489: step 6558, loss 0.362341, acc 0.84375, learning_rate 0.0001
2017-10-10T13:49:09.478379: step 6559, loss 0.308968, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:09.854123: step 6560, loss 0.164936, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:49:10.704988: step 6560, loss 0.238381, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6560

2017-10-10T13:49:12.044930: step 6561, loss 0.282607, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:12.417198: step 6562, loss 0.414469, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:12.680932: step 6563, loss 0.274367, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:13.029677: step 6564, loss 0.276615, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:13.425268: step 6565, loss 0.198717, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:13.787862: step 6566, loss 0.314011, acc 0.901961, learning_rate 0.0001
2017-10-10T13:49:14.162006: step 6567, loss 0.271225, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:14.677464: step 6568, loss 0.192582, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:15.080930: step 6569, loss 0.204101, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:15.392989: step 6570, loss 0.328235, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:15.702845: step 6571, loss 0.291033, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:16.008828: step 6572, loss 0.27314, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:16.420870: step 6573, loss 0.314656, acc 0.84375, learning_rate 0.0001
2017-10-10T13:49:16.861139: step 6574, loss 0.275985, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:17.265831: step 6575, loss 0.192398, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:17.675000: step 6576, loss 0.355018, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:18.091744: step 6577, loss 0.289064, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:18.506541: step 6578, loss 0.309405, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:18.908837: step 6579, loss 0.24266, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:19.320010: step 6580, loss 0.276138, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:19.739008: step 6581, loss 0.177622, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:20.164665: step 6582, loss 0.206376, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:20.657697: step 6583, loss 0.290602, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:20.967473: step 6584, loss 0.256852, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:21.285470: step 6585, loss 0.17712, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:21.599926: step 6586, loss 0.132139, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:21.902879: step 6587, loss 0.321488, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:22.291130: step 6588, loss 0.237054, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:22.697434: step 6589, loss 0.311285, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:23.108339: step 6590, loss 0.34253, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:23.517457: step 6591, loss 0.36774, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:23.920947: step 6592, loss 0.389573, acc 0.84375, learning_rate 0.0001
2017-10-10T13:49:24.355249: step 6593, loss 0.17149, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:24.744549: step 6594, loss 0.246982, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:25.136774: step 6595, loss 0.462785, acc 0.8125, learning_rate 0.0001
2017-10-10T13:49:25.564836: step 6596, loss 0.268611, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:25.972829: step 6597, loss 0.266436, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:26.372304: step 6598, loss 0.341946, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:26.797994: step 6599, loss 0.247796, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:27.229139: step 6600, loss 0.316372, acc 0.859375, learning_rate 0.0001

Evaluation:
2017-10-10T13:49:28.113194: step 6600, loss 0.237584, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6600

2017-10-10T13:49:29.682198: step 6601, loss 0.241456, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:30.084910: step 6602, loss 0.22362, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:30.427272: step 6603, loss 0.267572, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:30.812586: step 6604, loss 0.234324, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:31.202316: step 6605, loss 0.22447, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:31.601006: step 6606, loss 0.364982, acc 0.859375, learning_rate 0.0001
2017-10-10T13:49:32.002097: step 6607, loss 0.256957, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:32.485314: step 6608, loss 0.346074, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:32.836849: step 6609, loss 0.239371, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:33.128828: step 6610, loss 0.30053, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:33.455326: step 6611, loss 0.392642, acc 0.8125, learning_rate 0.0001
2017-10-10T13:49:33.849622: step 6612, loss 0.294139, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:34.235784: step 6613, loss 0.328261, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:34.664779: step 6614, loss 0.316969, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:35.057060: step 6615, loss 0.204689, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:35.478031: step 6616, loss 0.295921, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:35.891832: step 6617, loss 0.357609, acc 0.84375, learning_rate 0.0001
2017-10-10T13:49:36.276386: step 6618, loss 0.15716, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:36.704534: step 6619, loss 0.179189, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:37.121445: step 6620, loss 0.16825, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:37.551998: step 6621, loss 0.229062, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:37.977049: step 6622, loss 0.205547, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:38.469173: step 6623, loss 0.260713, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:38.797812: step 6624, loss 0.242835, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:39.122971: step 6625, loss 0.319162, acc 0.8125, learning_rate 0.0001
2017-10-10T13:49:39.538139: step 6626, loss 0.211642, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:39.958203: step 6627, loss 0.293304, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:40.336879: step 6628, loss 0.301781, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:40.754273: step 6629, loss 0.145032, acc 1, learning_rate 0.0001
2017-10-10T13:49:41.110737: step 6630, loss 0.203576, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:41.548890: step 6631, loss 0.446063, acc 0.84375, learning_rate 0.0001
2017-10-10T13:49:41.951822: step 6632, loss 0.191684, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:42.397060: step 6633, loss 0.276553, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:42.790803: step 6634, loss 0.444351, acc 0.828125, learning_rate 0.0001
2017-10-10T13:49:43.205041: step 6635, loss 0.290507, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:43.580940: step 6636, loss 0.291547, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:43.983893: step 6637, loss 0.22511, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:44.408377: step 6638, loss 0.437048, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:44.811577: step 6639, loss 0.178216, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:45.240043: step 6640, loss 0.143324, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:49:46.076950: step 6640, loss 0.23667, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6640

2017-10-10T13:49:47.435140: step 6641, loss 0.255651, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:47.819436: step 6642, loss 0.420169, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:48.228870: step 6643, loss 0.49206, acc 0.78125, learning_rate 0.0001
2017-10-10T13:49:48.646490: step 6644, loss 0.306974, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:49.068755: step 6645, loss 0.245869, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:49.484551: step 6646, loss 0.360958, acc 0.84375, learning_rate 0.0001
2017-10-10T13:49:49.952970: step 6647, loss 0.30203, acc 0.859375, learning_rate 0.0001
2017-10-10T13:49:50.416979: step 6648, loss 0.179529, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:50.744243: step 6649, loss 0.36829, acc 0.828125, learning_rate 0.0001
2017-10-10T13:49:51.071256: step 6650, loss 0.223627, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:51.396876: step 6651, loss 0.332649, acc 0.859375, learning_rate 0.0001
2017-10-10T13:49:51.833128: step 6652, loss 0.120665, acc 0.984375, learning_rate 0.0001
2017-10-10T13:49:52.236722: step 6653, loss 0.312742, acc 0.875, learning_rate 0.0001
2017-10-10T13:49:52.608828: step 6654, loss 0.306785, acc 0.859375, learning_rate 0.0001
2017-10-10T13:49:52.999890: step 6655, loss 0.207569, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:53.398357: step 6656, loss 0.318288, acc 0.890625, learning_rate 0.0001
2017-10-10T13:49:53.811555: step 6657, loss 0.174559, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:54.224338: step 6658, loss 0.339049, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:54.652879: step 6659, loss 0.284235, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:55.068108: step 6660, loss 0.178938, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:55.589027: step 6661, loss 0.39173, acc 0.90625, learning_rate 0.0001
2017-10-10T13:49:55.984912: step 6662, loss 0.218166, acc 0.9375, learning_rate 0.0001
2017-10-10T13:49:56.280928: step 6663, loss 0.359713, acc 0.84375, learning_rate 0.0001
2017-10-10T13:49:56.563074: step 6664, loss 0.331626, acc 0.921569, learning_rate 0.0001
2017-10-10T13:49:56.891312: step 6665, loss 0.396192, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:57.308842: step 6666, loss 0.187667, acc 0.96875, learning_rate 0.0001
2017-10-10T13:49:57.724300: step 6667, loss 0.199607, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:58.144952: step 6668, loss 0.278715, acc 0.921875, learning_rate 0.0001
2017-10-10T13:49:58.571337: step 6669, loss 0.106809, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:58.993332: step 6670, loss 0.33319, acc 0.859375, learning_rate 0.0001
2017-10-10T13:49:59.417689: step 6671, loss 0.162366, acc 0.953125, learning_rate 0.0001
2017-10-10T13:49:59.811164: step 6672, loss 0.321477, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:00.212841: step 6673, loss 0.126491, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:00.618849: step 6674, loss 0.420349, acc 0.859375, learning_rate 0.0001
2017-10-10T13:50:01.073025: step 6675, loss 0.345831, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:01.488839: step 6676, loss 0.186657, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:01.913065: step 6677, loss 0.249763, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:02.314708: step 6678, loss 0.331093, acc 0.875, learning_rate 0.0001
2017-10-10T13:50:02.721139: step 6679, loss 0.280304, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:03.048845: step 6680, loss 0.222023, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:50:03.717370: step 6680, loss 0.23752, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6680

2017-10-10T13:50:05.227298: step 6681, loss 0.271877, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:05.573106: step 6682, loss 0.381897, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:05.940885: step 6683, loss 0.351231, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:06.373221: step 6684, loss 0.353154, acc 0.828125, learning_rate 0.0001
2017-10-10T13:50:06.772938: step 6685, loss 0.339981, acc 0.859375, learning_rate 0.0001
2017-10-10T13:50:07.268977: step 6686, loss 0.299149, acc 0.875, learning_rate 0.0001
2017-10-10T13:50:07.693571: step 6687, loss 0.325262, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:08.004563: step 6688, loss 0.145076, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:08.314586: step 6689, loss 0.310984, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:08.629659: step 6690, loss 0.227869, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:09.062442: step 6691, loss 0.23483, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:09.489718: step 6692, loss 0.426145, acc 0.84375, learning_rate 0.0001
2017-10-10T13:50:09.880538: step 6693, loss 0.139117, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:10.290151: step 6694, loss 0.173557, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:10.714362: step 6695, loss 0.249844, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:11.131533: step 6696, loss 0.28198, acc 0.859375, learning_rate 0.0001
2017-10-10T13:50:11.516839: step 6697, loss 0.345708, acc 0.875, learning_rate 0.0001
2017-10-10T13:50:11.910175: step 6698, loss 0.273868, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:12.276854: step 6699, loss 0.289053, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:12.581486: step 6700, loss 0.312699, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:12.977125: step 6701, loss 0.196128, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:13.329017: step 6702, loss 0.166358, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:13.833046: step 6703, loss 0.227862, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:14.230027: step 6704, loss 0.299198, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:14.481083: step 6705, loss 0.254858, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:14.762592: step 6706, loss 0.401014, acc 0.859375, learning_rate 0.0001
2017-10-10T13:50:15.073372: step 6707, loss 0.161797, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:15.437017: step 6708, loss 0.231464, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:15.826891: step 6709, loss 0.453074, acc 0.84375, learning_rate 0.0001
2017-10-10T13:50:16.299462: step 6710, loss 0.173784, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:16.685016: step 6711, loss 0.212268, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:17.095408: step 6712, loss 0.223864, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:17.433023: step 6713, loss 0.25198, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:17.789051: step 6714, loss 0.274938, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:18.200927: step 6715, loss 0.237103, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:18.624883: step 6716, loss 0.158027, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:19.039935: step 6717, loss 0.470599, acc 0.859375, learning_rate 0.0001
2017-10-10T13:50:19.473673: step 6718, loss 0.223617, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:19.855882: step 6719, loss 0.231489, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:20.251046: step 6720, loss 0.327442, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:50:21.054383: step 6720, loss 0.237031, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6720

2017-10-10T13:50:22.461173: step 6721, loss 0.125931, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:22.836976: step 6722, loss 0.225297, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:23.228889: step 6723, loss 0.206171, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:23.660882: step 6724, loss 0.454493, acc 0.875, learning_rate 0.0001
2017-10-10T13:50:23.984856: step 6725, loss 0.130715, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:24.405130: step 6726, loss 0.189744, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:24.932877: step 6727, loss 0.145179, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:25.354519: step 6728, loss 0.134831, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:25.679638: step 6729, loss 0.241837, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:26.004535: step 6730, loss 0.237706, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:26.448954: step 6731, loss 0.344945, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:26.877388: step 6732, loss 0.331198, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:27.274116: step 6733, loss 0.288392, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:27.696925: step 6734, loss 0.337906, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:28.121359: step 6735, loss 0.381706, acc 0.84375, learning_rate 0.0001
2017-10-10T13:50:28.532133: step 6736, loss 0.500295, acc 0.875, learning_rate 0.0001
2017-10-10T13:50:28.944855: step 6737, loss 0.443683, acc 0.859375, learning_rate 0.0001
2017-10-10T13:50:29.335962: step 6738, loss 0.216668, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:29.748945: step 6739, loss 0.205034, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:30.162861: step 6740, loss 0.212162, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:30.604833: step 6741, loss 0.138043, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:31.084921: step 6742, loss 0.229294, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:31.581986: step 6743, loss 0.189272, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:31.878860: step 6744, loss 0.288854, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:32.196674: step 6745, loss 0.245747, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:32.507113: step 6746, loss 0.291067, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:32.932213: step 6747, loss 0.359785, acc 0.84375, learning_rate 0.0001
2017-10-10T13:50:33.354763: step 6748, loss 0.312045, acc 0.875, learning_rate 0.0001
2017-10-10T13:50:33.783303: step 6749, loss 0.431648, acc 0.859375, learning_rate 0.0001
2017-10-10T13:50:34.207973: step 6750, loss 0.277694, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:34.628379: step 6751, loss 0.324165, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:35.044818: step 6752, loss 0.199688, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:35.444175: step 6753, loss 0.298736, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:35.846138: step 6754, loss 0.283155, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:36.261653: step 6755, loss 0.202448, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:36.666578: step 6756, loss 0.12469, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:37.070606: step 6757, loss 0.301633, acc 0.875, learning_rate 0.0001
2017-10-10T13:50:37.469051: step 6758, loss 0.276909, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:37.848864: step 6759, loss 0.350991, acc 0.859375, learning_rate 0.0001
2017-10-10T13:50:38.258803: step 6760, loss 0.318014, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:50:39.033167: step 6760, loss 0.236556, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6760

2017-10-10T13:50:40.293383: step 6761, loss 0.35177, acc 0.875, learning_rate 0.0001
2017-10-10T13:50:40.682154: step 6762, loss 0.42182, acc 0.882353, learning_rate 0.0001
2017-10-10T13:50:41.104717: step 6763, loss 0.185291, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:41.519981: step 6764, loss 0.241521, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:41.924858: step 6765, loss 0.287287, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:42.344462: step 6766, loss 0.23761, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:42.768869: step 6767, loss 0.272091, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:43.269870: step 6768, loss 0.198107, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:43.612845: step 6769, loss 0.381355, acc 0.84375, learning_rate 0.0001
2017-10-10T13:50:43.904172: step 6770, loss 0.290759, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:44.095601: step 6771, loss 0.232902, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:44.417763: step 6772, loss 0.281521, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:44.813730: step 6773, loss 0.214581, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:45.161958: step 6774, loss 0.261765, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:45.524971: step 6775, loss 0.218407, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:45.905084: step 6776, loss 0.23079, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:46.384874: step 6777, loss 0.218945, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:46.771816: step 6778, loss 0.178219, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:47.192866: step 6779, loss 0.167887, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:47.600673: step 6780, loss 0.350484, acc 0.84375, learning_rate 0.0001
2017-10-10T13:50:48.027912: step 6781, loss 0.235565, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:48.441227: step 6782, loss 0.260517, acc 0.859375, learning_rate 0.0001
2017-10-10T13:50:48.881189: step 6783, loss 0.342487, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:49.393082: step 6784, loss 0.255768, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:49.656867: step 6785, loss 0.37101, acc 0.859375, learning_rate 0.0001
2017-10-10T13:50:49.929826: step 6786, loss 0.303817, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:50.249991: step 6787, loss 0.212284, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:50.599289: step 6788, loss 0.27007, acc 0.875, learning_rate 0.0001
2017-10-10T13:50:51.076929: step 6789, loss 0.167998, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:51.444918: step 6790, loss 0.338147, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:51.852845: step 6791, loss 0.293398, acc 0.859375, learning_rate 0.0001
2017-10-10T13:50:52.248979: step 6792, loss 0.295448, acc 0.875, learning_rate 0.0001
2017-10-10T13:50:52.600149: step 6793, loss 0.265998, acc 0.90625, learning_rate 0.0001
2017-10-10T13:50:53.002291: step 6794, loss 0.267563, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:53.404243: step 6795, loss 0.245104, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:53.797024: step 6796, loss 0.15817, acc 0.953125, learning_rate 0.0001
2017-10-10T13:50:54.216949: step 6797, loss 0.321192, acc 0.875, learning_rate 0.0001
2017-10-10T13:50:54.596890: step 6798, loss 0.204252, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:54.961082: step 6799, loss 0.188417, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:55.345286: step 6800, loss 0.26474, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:50:56.154934: step 6800, loss 0.237497, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6800

2017-10-10T13:50:57.518795: step 6801, loss 0.219524, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:57.948913: step 6802, loss 0.207244, acc 0.921875, learning_rate 0.0001
2017-10-10T13:50:58.352238: step 6803, loss 0.281443, acc 0.890625, learning_rate 0.0001
2017-10-10T13:50:58.772208: step 6804, loss 0.138391, acc 0.96875, learning_rate 0.0001
2017-10-10T13:50:59.194522: step 6805, loss 0.353759, acc 0.9375, learning_rate 0.0001
2017-10-10T13:50:59.615811: step 6806, loss 0.252413, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:00.036556: step 6807, loss 0.281188, acc 0.875, learning_rate 0.0001
2017-10-10T13:51:00.527031: step 6808, loss 0.25311, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:00.972809: step 6809, loss 0.271573, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:01.257511: step 6810, loss 0.26321, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:01.585241: step 6811, loss 0.166584, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:01.906990: step 6812, loss 0.204201, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:02.264940: step 6813, loss 0.139229, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:02.709395: step 6814, loss 0.185359, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:03.136906: step 6815, loss 0.235728, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:03.524575: step 6816, loss 0.305869, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:03.912850: step 6817, loss 0.271136, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:04.343355: step 6818, loss 0.283721, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:04.772104: step 6819, loss 0.503891, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:05.185163: step 6820, loss 0.20169, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:05.605131: step 6821, loss 0.408953, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:06.013655: step 6822, loss 0.247129, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:06.459738: step 6823, loss 0.186046, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:06.892556: step 6824, loss 0.247162, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:07.388902: step 6825, loss 0.234171, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:07.730387: step 6826, loss 0.311522, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:08.029310: step 6827, loss 0.260285, acc 0.875, learning_rate 0.0001
2017-10-10T13:51:08.280842: step 6828, loss 0.165759, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:08.703218: step 6829, loss 0.342441, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:09.092287: step 6830, loss 0.130417, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:09.497058: step 6831, loss 0.215703, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:09.872685: step 6832, loss 0.401056, acc 0.84375, learning_rate 0.0001
2017-10-10T13:51:10.253157: step 6833, loss 0.228088, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:10.605122: step 6834, loss 0.320702, acc 0.875, learning_rate 0.0001
2017-10-10T13:51:11.085071: step 6835, loss 0.329663, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:11.485099: step 6836, loss 0.237096, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:11.900909: step 6837, loss 0.227539, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:12.346316: step 6838, loss 0.274486, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:12.760802: step 6839, loss 0.340498, acc 0.859375, learning_rate 0.0001
2017-10-10T13:51:13.182079: step 6840, loss 0.222241, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:51:14.079376: step 6840, loss 0.237512, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6840

2017-10-10T13:51:15.601122: step 6841, loss 0.225494, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:16.003644: step 6842, loss 0.246456, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:16.377114: step 6843, loss 0.26594, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:16.738257: step 6844, loss 0.270714, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:17.173035: step 6845, loss 0.335754, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:17.620949: step 6846, loss 0.412698, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:18.132910: step 6847, loss 0.222902, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:18.469553: step 6848, loss 0.238659, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:18.744011: step 6849, loss 0.404528, acc 0.875, learning_rate 0.0001
2017-10-10T13:51:19.052012: step 6850, loss 0.382094, acc 0.875, learning_rate 0.0001
2017-10-10T13:51:19.460839: step 6851, loss 0.430052, acc 0.859375, learning_rate 0.0001
2017-10-10T13:51:19.898544: step 6852, loss 0.218884, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:20.327716: step 6853, loss 0.195541, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:20.748853: step 6854, loss 0.338174, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:21.174187: step 6855, loss 0.219386, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:21.617032: step 6856, loss 0.308081, acc 0.875, learning_rate 0.0001
2017-10-10T13:51:22.000527: step 6857, loss 0.306749, acc 0.875, learning_rate 0.0001
2017-10-10T13:51:22.319464: step 6858, loss 0.225097, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:22.739767: step 6859, loss 0.379216, acc 0.875, learning_rate 0.0001
2017-10-10T13:51:23.098861: step 6860, loss 0.25292, acc 0.921569, learning_rate 0.0001
2017-10-10T13:51:23.508327: step 6861, loss 0.267708, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:23.891614: step 6862, loss 0.367783, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:24.340829: step 6863, loss 0.275025, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:24.809051: step 6864, loss 0.361775, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:25.176418: step 6865, loss 0.245792, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:25.494511: step 6866, loss 0.091596, acc 0.984375, learning_rate 0.0001
2017-10-10T13:51:25.809483: step 6867, loss 0.20648, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:26.132297: step 6868, loss 0.22082, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:26.557572: step 6869, loss 0.24019, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:26.954262: step 6870, loss 0.276197, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:27.355042: step 6871, loss 0.196648, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:27.751954: step 6872, loss 0.257846, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:28.165599: step 6873, loss 0.233946, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:28.588418: step 6874, loss 0.25706, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:29.014682: step 6875, loss 0.395933, acc 0.875, learning_rate 0.0001
2017-10-10T13:51:29.422040: step 6876, loss 0.260669, acc 0.859375, learning_rate 0.0001
2017-10-10T13:51:29.813667: step 6877, loss 0.299561, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:30.148988: step 6878, loss 0.262255, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:30.612860: step 6879, loss 0.267086, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:31.060817: step 6880, loss 0.294236, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:51:31.926768: step 6880, loss 0.236667, acc 0.907914

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6880

2017-10-10T13:51:33.137038: step 6881, loss 0.254288, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:33.513368: step 6882, loss 0.616676, acc 0.765625, learning_rate 0.0001
2017-10-10T13:51:33.945104: step 6883, loss 0.268874, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:34.378189: step 6884, loss 0.325111, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:34.707489: step 6885, loss 0.263051, acc 0.84375, learning_rate 0.0001
2017-10-10T13:51:35.085369: step 6886, loss 0.368274, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:35.553114: step 6887, loss 0.385276, acc 0.875, learning_rate 0.0001
2017-10-10T13:51:35.950686: step 6888, loss 0.313085, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:36.271318: step 6889, loss 0.26541, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:36.571509: step 6890, loss 0.269426, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:36.884185: step 6891, loss 0.176328, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:37.307798: step 6892, loss 0.139352, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:37.718286: step 6893, loss 0.387761, acc 0.84375, learning_rate 0.0001
2017-10-10T13:51:38.104844: step 6894, loss 0.288268, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:38.525499: step 6895, loss 0.208599, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:38.889398: step 6896, loss 0.232378, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:39.255159: step 6897, loss 0.319445, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:39.647963: step 6898, loss 0.178907, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:39.999097: step 6899, loss 0.26721, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:40.359172: step 6900, loss 0.187211, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:40.753124: step 6901, loss 0.319627, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:41.171830: step 6902, loss 0.495447, acc 0.859375, learning_rate 0.0001
2017-10-10T13:51:41.577354: step 6903, loss 0.388009, acc 0.859375, learning_rate 0.0001
2017-10-10T13:51:41.997776: step 6904, loss 0.315243, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:42.477547: step 6905, loss 0.443889, acc 0.875, learning_rate 0.0001
2017-10-10T13:51:42.952241: step 6906, loss 0.240505, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:43.252162: step 6907, loss 0.241732, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:43.525543: step 6908, loss 0.259869, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:43.797374: step 6909, loss 0.258673, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:44.201856: step 6910, loss 0.40428, acc 0.875, learning_rate 0.0001
2017-10-10T13:51:44.626546: step 6911, loss 0.253054, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:45.047225: step 6912, loss 0.272235, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:45.442889: step 6913, loss 0.366762, acc 0.859375, learning_rate 0.0001
2017-10-10T13:51:45.856923: step 6914, loss 0.166978, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:46.263856: step 6915, loss 0.243268, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:46.683708: step 6916, loss 0.336264, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:47.106686: step 6917, loss 0.277646, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:47.520331: step 6918, loss 0.257774, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:47.945003: step 6919, loss 0.365088, acc 0.859375, learning_rate 0.0001
2017-10-10T13:51:48.361828: step 6920, loss 0.190098, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:51:49.277667: step 6920, loss 0.235443, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6920

2017-10-10T13:51:50.706696: step 6921, loss 0.227045, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:51.041674: step 6922, loss 0.16315, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:51.451432: step 6923, loss 0.372784, acc 0.859375, learning_rate 0.0001
2017-10-10T13:51:51.859829: step 6924, loss 0.189854, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:52.291915: step 6925, loss 0.236394, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:52.710872: step 6926, loss 0.277664, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:53.107686: step 6927, loss 0.30757, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:53.505015: step 6928, loss 0.320841, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:54.062809: step 6929, loss 0.168913, acc 0.96875, learning_rate 0.0001
2017-10-10T13:51:54.379590: step 6930, loss 0.23448, acc 0.921875, learning_rate 0.0001
2017-10-10T13:51:54.708397: step 6931, loss 0.14601, acc 0.9375, learning_rate 0.0001
2017-10-10T13:51:55.041033: step 6932, loss 0.343968, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:55.465011: step 6933, loss 0.353748, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:55.892999: step 6934, loss 0.531086, acc 0.84375, learning_rate 0.0001
2017-10-10T13:51:56.340342: step 6935, loss 0.305941, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:56.733193: step 6936, loss 0.439195, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:57.188906: step 6937, loss 0.282701, acc 0.875, learning_rate 0.0001
2017-10-10T13:51:57.563799: step 6938, loss 0.263465, acc 0.953125, learning_rate 0.0001
2017-10-10T13:51:57.944892: step 6939, loss 0.493628, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:58.335691: step 6940, loss 0.242658, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:58.729162: step 6941, loss 0.301833, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:59.164470: step 6942, loss 0.255756, acc 0.890625, learning_rate 0.0001
2017-10-10T13:51:59.547639: step 6943, loss 0.320263, acc 0.90625, learning_rate 0.0001
2017-10-10T13:51:59.984840: step 6944, loss 0.145815, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:00.529102: step 6945, loss 0.190246, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:00.865946: step 6946, loss 0.205853, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:01.163425: step 6947, loss 0.30236, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:01.480933: step 6948, loss 0.13574, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:01.852409: step 6949, loss 0.258715, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:02.260763: step 6950, loss 0.374078, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:02.663944: step 6951, loss 0.365667, acc 0.875, learning_rate 0.0001
2017-10-10T13:52:03.062002: step 6952, loss 0.19852, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:03.478714: step 6953, loss 0.244821, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:03.901513: step 6954, loss 0.191398, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:04.327428: step 6955, loss 0.213429, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:04.739561: step 6956, loss 0.11499, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:05.152855: step 6957, loss 0.308079, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:05.443491: step 6958, loss 0.261949, acc 0.941176, learning_rate 0.0001
2017-10-10T13:52:05.779264: step 6959, loss 0.167382, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:06.180625: step 6960, loss 0.30028, acc 0.875, learning_rate 0.0001

Evaluation:
2017-10-10T13:52:07.049718: step 6960, loss 0.234632, acc 0.909353

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-6960

2017-10-10T13:52:08.647510: step 6961, loss 0.21859, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:09.022439: step 6962, loss 0.201062, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:09.394278: step 6963, loss 0.257484, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:09.798909: step 6964, loss 0.386844, acc 0.859375, learning_rate 0.0001
2017-10-10T13:52:10.190869: step 6965, loss 0.186906, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:10.611618: step 6966, loss 0.411096, acc 0.859375, learning_rate 0.0001
2017-10-10T13:52:11.018612: step 6967, loss 0.289402, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:11.488848: step 6968, loss 0.241086, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:11.968390: step 6969, loss 0.195586, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:12.277382: step 6970, loss 0.35378, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:12.610808: step 6971, loss 0.181931, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:12.910546: step 6972, loss 0.228576, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:13.275158: step 6973, loss 0.24784, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:13.746570: step 6974, loss 0.218715, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:14.173216: step 6975, loss 0.28031, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:14.564943: step 6976, loss 0.259483, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:14.944868: step 6977, loss 0.17296, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:15.330426: step 6978, loss 0.134617, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:15.748848: step 6979, loss 0.409248, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:16.158799: step 6980, loss 0.277471, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:16.584372: step 6981, loss 0.351766, acc 0.875, learning_rate 0.0001
2017-10-10T13:52:16.995970: step 6982, loss 0.367215, acc 0.84375, learning_rate 0.0001
2017-10-10T13:52:17.360910: step 6983, loss 0.345099, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:17.837152: step 6984, loss 0.195738, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:18.222266: step 6985, loss 0.439607, acc 0.859375, learning_rate 0.0001
2017-10-10T13:52:18.542549: step 6986, loss 0.212143, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:18.868266: step 6987, loss 0.196853, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:19.197166: step 6988, loss 0.321803, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:19.616018: step 6989, loss 0.325734, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:20.040520: step 6990, loss 0.278762, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:20.453884: step 6991, loss 0.218978, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:20.888943: step 6992, loss 0.298983, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:21.301337: step 6993, loss 0.142796, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:21.725120: step 6994, loss 0.367592, acc 0.859375, learning_rate 0.0001
2017-10-10T13:52:22.109785: step 6995, loss 0.20373, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:22.529025: step 6996, loss 0.267039, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:22.917092: step 6997, loss 0.34907, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:23.325348: step 6998, loss 0.344859, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:23.695938: step 6999, loss 0.257476, acc 0.875, learning_rate 0.0001
2017-10-10T13:52:24.123738: step 7000, loss 0.247488, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:52:24.997390: step 7000, loss 0.23348, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7000

2017-10-10T13:52:26.268797: step 7001, loss 0.235583, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:26.683042: step 7002, loss 0.130223, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:27.072941: step 7003, loss 0.250121, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:27.432940: step 7004, loss 0.334737, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:27.892507: step 7005, loss 0.227405, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:28.283139: step 7006, loss 0.414596, acc 0.8125, learning_rate 0.0001
2017-10-10T13:52:28.697042: step 7007, loss 0.187351, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:29.092894: step 7008, loss 0.23176, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:29.574903: step 7009, loss 0.246131, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:29.916916: step 7010, loss 0.260714, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:30.176889: step 7011, loss 0.298284, acc 0.859375, learning_rate 0.0001
2017-10-10T13:52:30.452825: step 7012, loss 0.230564, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:30.816867: step 7013, loss 0.228164, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:31.230196: step 7014, loss 0.401678, acc 0.796875, learning_rate 0.0001
2017-10-10T13:52:31.660974: step 7015, loss 0.277154, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:32.040827: step 7016, loss 0.195216, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:32.477077: step 7017, loss 0.183073, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:32.920511: step 7018, loss 0.282538, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:33.252995: step 7019, loss 0.317292, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:33.590005: step 7020, loss 0.299758, acc 0.875, learning_rate 0.0001
2017-10-10T13:52:33.940123: step 7021, loss 0.349027, acc 0.875, learning_rate 0.0001
2017-10-10T13:52:34.319326: step 7022, loss 0.432339, acc 0.859375, learning_rate 0.0001
2017-10-10T13:52:34.741346: step 7023, loss 0.3163, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:35.260993: step 7024, loss 0.240371, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:35.704874: step 7025, loss 0.334697, acc 0.875, learning_rate 0.0001
2017-10-10T13:52:36.004833: step 7026, loss 0.292006, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:36.292837: step 7027, loss 0.271878, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:36.637093: step 7028, loss 0.337369, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:37.060961: step 7029, loss 0.202481, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:37.479160: step 7030, loss 0.290855, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:37.895887: step 7031, loss 0.148856, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:38.331061: step 7032, loss 0.24363, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:38.754424: step 7033, loss 0.422635, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:39.164321: step 7034, loss 0.326182, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:39.592444: step 7035, loss 0.177433, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:39.997893: step 7036, loss 0.168406, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:40.415697: step 7037, loss 0.26816, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:40.843301: step 7038, loss 0.366645, acc 0.875, learning_rate 0.0001
2017-10-10T13:52:41.294725: step 7039, loss 0.212209, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:41.728845: step 7040, loss 0.162487, acc 0.96875, learning_rate 0.0001

Evaluation:
2017-10-10T13:52:42.744899: step 7040, loss 0.232329, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7040

2017-10-10T13:52:44.056756: step 7041, loss 0.254099, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:44.456964: step 7042, loss 0.182827, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:44.839419: step 7043, loss 0.266878, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:45.221778: step 7044, loss 0.19894, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:45.558017: step 7045, loss 0.254265, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:45.962826: step 7046, loss 0.276767, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:46.388822: step 7047, loss 0.278846, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:46.784883: step 7048, loss 0.236834, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:47.264930: step 7049, loss 0.304978, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:47.710605: step 7050, loss 0.223153, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:47.992470: step 7051, loss 0.353047, acc 0.859375, learning_rate 0.0001
2017-10-10T13:52:48.299096: step 7052, loss 0.131839, acc 1, learning_rate 0.0001
2017-10-10T13:52:48.604586: step 7053, loss 0.121535, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:48.964896: step 7054, loss 0.393869, acc 0.84375, learning_rate 0.0001
2017-10-10T13:52:49.342932: step 7055, loss 0.370502, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:49.687644: step 7056, loss 0.220736, acc 0.960784, learning_rate 0.0001
2017-10-10T13:52:50.059525: step 7057, loss 0.272356, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:50.469206: step 7058, loss 0.245041, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:50.825665: step 7059, loss 0.273983, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:51.149078: step 7060, loss 0.337135, acc 0.84375, learning_rate 0.0001
2017-10-10T13:52:51.568611: step 7061, loss 0.0972051, acc 0.984375, learning_rate 0.0001
2017-10-10T13:52:51.944297: step 7062, loss 0.311585, acc 0.859375, learning_rate 0.0001
2017-10-10T13:52:52.360979: step 7063, loss 0.206599, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:52.914416: step 7064, loss 0.155928, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:53.239166: step 7065, loss 0.236837, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:53.544405: step 7066, loss 0.385108, acc 0.859375, learning_rate 0.0001
2017-10-10T13:52:53.861799: step 7067, loss 0.292394, acc 0.875, learning_rate 0.0001
2017-10-10T13:52:54.171381: step 7068, loss 0.260716, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:54.471954: step 7069, loss 0.256985, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:54.773870: step 7070, loss 0.282315, acc 0.921875, learning_rate 0.0001
2017-10-10T13:52:55.019495: step 7071, loss 0.265449, acc 0.859375, learning_rate 0.0001
2017-10-10T13:52:55.319417: step 7072, loss 0.133579, acc 0.96875, learning_rate 0.0001
2017-10-10T13:52:55.624500: step 7073, loss 0.334763, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:55.921898: step 7074, loss 0.179277, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:56.217757: step 7075, loss 0.176787, acc 0.953125, learning_rate 0.0001
2017-10-10T13:52:56.512882: step 7076, loss 0.283643, acc 0.890625, learning_rate 0.0001
2017-10-10T13:52:56.813117: step 7077, loss 0.276189, acc 0.90625, learning_rate 0.0001
2017-10-10T13:52:57.105294: step 7078, loss 0.357947, acc 0.84375, learning_rate 0.0001
2017-10-10T13:52:57.394608: step 7079, loss 0.260361, acc 0.9375, learning_rate 0.0001
2017-10-10T13:52:57.681528: step 7080, loss 0.476845, acc 0.84375, learning_rate 0.0001

Evaluation:
2017-10-10T13:52:58.323758: step 7080, loss 0.231884, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7080

2017-10-10T13:52:59.576836: step 7081, loss 0.370792, acc 0.875, learning_rate 0.0001
2017-10-10T13:52:59.819770: step 7082, loss 0.296149, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:00.144853: step 7083, loss 0.291764, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:00.430560: step 7084, loss 0.239367, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:00.718338: step 7085, loss 0.304027, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:01.008398: step 7086, loss 0.100459, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:01.360849: step 7087, loss 0.159731, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:01.685536: step 7088, loss 0.185269, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:01.861415: step 7089, loss 0.293664, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:02.036227: step 7090, loss 0.300154, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:02.209490: step 7091, loss 0.315023, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:02.385754: step 7092, loss 0.212827, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:02.569215: step 7093, loss 0.257174, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:02.776371: step 7094, loss 0.286469, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:03.020855: step 7095, loss 0.312863, acc 0.859375, learning_rate 0.0001
2017-10-10T13:53:03.272461: step 7096, loss 0.190274, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:03.548365: step 7097, loss 0.250639, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:03.826567: step 7098, loss 0.398456, acc 0.859375, learning_rate 0.0001
2017-10-10T13:53:04.115944: step 7099, loss 0.372654, acc 0.84375, learning_rate 0.0001
2017-10-10T13:53:04.411471: step 7100, loss 0.183756, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:04.704904: step 7101, loss 0.282604, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:04.978251: step 7102, loss 0.317516, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:05.266609: step 7103, loss 0.378653, acc 0.828125, learning_rate 0.0001
2017-10-10T13:53:05.551775: step 7104, loss 0.270675, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:05.843443: step 7105, loss 0.254895, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:06.107760: step 7106, loss 0.410547, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:06.384107: step 7107, loss 0.348722, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:06.665174: step 7108, loss 0.223244, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:06.945665: step 7109, loss 0.18489, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:07.243329: step 7110, loss 0.129986, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:07.533728: step 7111, loss 0.322051, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:07.812966: step 7112, loss 0.27403, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:08.113907: step 7113, loss 0.318131, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:08.407310: step 7114, loss 0.365709, acc 0.84375, learning_rate 0.0001
2017-10-10T13:53:08.694645: step 7115, loss 0.282485, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:08.974574: step 7116, loss 0.356934, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:09.255953: step 7117, loss 0.206515, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:09.539869: step 7118, loss 0.275024, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:09.822068: step 7119, loss 0.320416, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:10.116886: step 7120, loss 0.366557, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:53:10.770819: step 7120, loss 0.230834, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7120

2017-10-10T13:53:11.799481: step 7121, loss 0.247096, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:12.091857: step 7122, loss 0.300557, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:12.395812: step 7123, loss 0.308561, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:12.684039: step 7124, loss 0.241434, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:12.966692: step 7125, loss 0.369745, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:13.252868: step 7126, loss 0.339001, acc 0.859375, learning_rate 0.0001
2017-10-10T13:53:13.522039: step 7127, loss 0.169663, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:13.793784: step 7128, loss 0.261527, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:14.140835: step 7129, loss 0.176325, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:14.465904: step 7130, loss 0.284858, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:14.654189: step 7131, loss 0.126285, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:14.840207: step 7132, loss 0.513453, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:15.029840: step 7133, loss 0.288499, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:15.199706: step 7134, loss 0.235524, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:15.475416: step 7135, loss 0.453424, acc 0.796875, learning_rate 0.0001
2017-10-10T13:53:15.756375: step 7136, loss 0.28316, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:16.041925: step 7137, loss 0.288415, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:16.327304: step 7138, loss 0.32421, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:16.601607: step 7139, loss 0.27387, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:16.896904: step 7140, loss 0.210243, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:17.184334: step 7141, loss 0.194141, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:17.480290: step 7142, loss 0.262014, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:17.760526: step 7143, loss 0.233785, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:18.063537: step 7144, loss 0.282799, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:18.355758: step 7145, loss 0.169686, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:18.622876: step 7146, loss 0.349372, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:18.901173: step 7147, loss 0.223163, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:19.194182: step 7148, loss 0.270556, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:19.504777: step 7149, loss 0.24114, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:19.796858: step 7150, loss 0.252275, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:20.092099: step 7151, loss 0.220185, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:20.372913: step 7152, loss 0.53209, acc 0.8125, learning_rate 0.0001
2017-10-10T13:53:20.659029: step 7153, loss 0.229597, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:20.901045: step 7154, loss 0.297852, acc 0.921569, learning_rate 0.0001
2017-10-10T13:53:21.202044: step 7155, loss 0.296293, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:21.483115: step 7156, loss 0.184651, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:21.771021: step 7157, loss 0.316538, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:22.058124: step 7158, loss 0.175987, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:22.348881: step 7159, loss 0.196713, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:22.647384: step 7160, loss 0.315731, acc 0.84375, learning_rate 0.0001

Evaluation:
2017-10-10T13:53:23.278636: step 7160, loss 0.232329, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7160

2017-10-10T13:53:24.445103: step 7161, loss 0.285402, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:24.740871: step 7162, loss 0.21893, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:25.030145: step 7163, loss 0.367724, acc 0.84375, learning_rate 0.0001
2017-10-10T13:53:25.325211: step 7164, loss 0.127797, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:25.621398: step 7165, loss 0.205102, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:25.920566: step 7166, loss 0.362252, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:26.212855: step 7167, loss 0.272638, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:26.550977: step 7168, loss 0.274553, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:26.891366: step 7169, loss 0.33948, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:27.068295: step 7170, loss 0.260297, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:27.245407: step 7171, loss 0.256827, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:27.420017: step 7172, loss 0.374291, acc 0.828125, learning_rate 0.0001
2017-10-10T13:53:27.593189: step 7173, loss 0.139898, acc 0.984375, learning_rate 0.0001
2017-10-10T13:53:27.777185: step 7174, loss 0.301686, acc 0.859375, learning_rate 0.0001
2017-10-10T13:53:28.036251: step 7175, loss 0.301994, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:28.319296: step 7176, loss 0.169786, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:28.616370: step 7177, loss 0.256786, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:28.907337: step 7178, loss 0.268553, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:29.207210: step 7179, loss 0.189263, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:29.480030: step 7180, loss 0.231891, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:29.763533: step 7181, loss 0.345693, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:30.017095: step 7182, loss 0.200897, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:30.286283: step 7183, loss 0.279156, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:30.567343: step 7184, loss 0.331212, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:30.849759: step 7185, loss 0.220533, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:31.015538: step 7186, loss 0.238514, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:31.264846: step 7187, loss 0.250602, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:31.563425: step 7188, loss 0.186315, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:31.853694: step 7189, loss 0.242208, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:32.144364: step 7190, loss 0.232252, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:32.432499: step 7191, loss 0.171391, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:32.730428: step 7192, loss 0.293181, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:33.014120: step 7193, loss 0.354641, acc 0.859375, learning_rate 0.0001
2017-10-10T13:53:33.315557: step 7194, loss 0.131113, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:33.609735: step 7195, loss 0.193129, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:33.924797: step 7196, loss 0.272892, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:34.222651: step 7197, loss 0.270242, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:34.512121: step 7198, loss 0.178486, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:34.791458: step 7199, loss 0.240782, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:35.086802: step 7200, loss 0.213139, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:53:35.710747: step 7200, loss 0.231573, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7200

2017-10-10T13:53:36.912362: step 7201, loss 0.320132, acc 0.84375, learning_rate 0.0001
2017-10-10T13:53:37.160933: step 7202, loss 0.365139, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:37.474390: step 7203, loss 0.27676, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:37.764442: step 7204, loss 0.214979, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:38.049805: step 7205, loss 0.410392, acc 0.859375, learning_rate 0.0001
2017-10-10T13:53:38.332015: step 7206, loss 0.38384, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:38.608464: step 7207, loss 0.226403, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:38.924475: step 7208, loss 0.24291, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:39.297153: step 7209, loss 0.18599, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:39.615919: step 7210, loss 0.344582, acc 0.828125, learning_rate 0.0001
2017-10-10T13:53:39.810261: step 7211, loss 0.228384, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:39.990953: step 7212, loss 0.250626, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:40.177196: step 7213, loss 0.26894, acc 0.859375, learning_rate 0.0001
2017-10-10T13:53:40.367923: step 7214, loss 0.229442, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:40.656657: step 7215, loss 0.222329, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:40.939183: step 7216, loss 0.283145, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:41.230665: step 7217, loss 0.300045, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:41.518027: step 7218, loss 0.189413, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:41.804807: step 7219, loss 0.339309, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:42.094075: step 7220, loss 0.281536, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:42.382754: step 7221, loss 0.288366, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:42.658978: step 7222, loss 0.315154, acc 0.859375, learning_rate 0.0001
2017-10-10T13:53:42.948167: step 7223, loss 0.184498, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:43.225784: step 7224, loss 0.359705, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:43.522704: step 7225, loss 0.166341, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:43.810056: step 7226, loss 0.330838, acc 0.84375, learning_rate 0.0001
2017-10-10T13:53:44.097727: step 7227, loss 0.248506, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:44.386916: step 7228, loss 0.253744, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:44.677813: step 7229, loss 0.311721, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:44.960971: step 7230, loss 0.324847, acc 0.84375, learning_rate 0.0001
2017-10-10T13:53:45.242756: step 7231, loss 0.182867, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:45.533333: step 7232, loss 0.221481, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:45.814015: step 7233, loss 0.262279, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:46.115819: step 7234, loss 0.275141, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:46.414759: step 7235, loss 0.231415, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:46.710199: step 7236, loss 0.569844, acc 0.828125, learning_rate 0.0001
2017-10-10T13:53:47.006227: step 7237, loss 0.59653, acc 0.828125, learning_rate 0.0001
2017-10-10T13:53:47.295340: step 7238, loss 0.247493, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:47.589729: step 7239, loss 0.350904, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:47.880500: step 7240, loss 0.224965, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:53:48.530817: step 7240, loss 0.230536, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7240

2017-10-10T13:53:49.800964: step 7241, loss 0.202674, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:50.110327: step 7242, loss 0.246182, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:50.394821: step 7243, loss 0.260204, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:50.687220: step 7244, loss 0.337599, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:50.987497: step 7245, loss 0.202053, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:51.284427: step 7246, loss 0.320713, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:51.650489: step 7247, loss 0.0915761, acc 1, learning_rate 0.0001
2017-10-10T13:53:51.980601: step 7248, loss 0.206514, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:52.158361: step 7249, loss 0.243016, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:52.335507: step 7250, loss 0.305425, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:52.518638: step 7251, loss 0.142509, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:52.672914: step 7252, loss 0.252918, acc 0.941176, learning_rate 0.0001
2017-10-10T13:53:52.942959: step 7253, loss 0.364699, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:53.225882: step 7254, loss 0.269584, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:53.511852: step 7255, loss 0.250217, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:53.789109: step 7256, loss 0.202379, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:54.067673: step 7257, loss 0.525106, acc 0.84375, learning_rate 0.0001
2017-10-10T13:53:54.352079: step 7258, loss 0.185983, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:54.640654: step 7259, loss 0.316801, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:54.927950: step 7260, loss 0.218647, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:55.216031: step 7261, loss 0.186137, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:55.500084: step 7262, loss 0.142657, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:55.791344: step 7263, loss 0.200173, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:56.089473: step 7264, loss 0.0577352, acc 1, learning_rate 0.0001
2017-10-10T13:53:56.390654: step 7265, loss 0.234021, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:56.675506: step 7266, loss 0.274077, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:56.967863: step 7267, loss 0.222953, acc 0.890625, learning_rate 0.0001
2017-10-10T13:53:57.262675: step 7268, loss 0.255411, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:57.554832: step 7269, loss 0.216247, acc 0.96875, learning_rate 0.0001
2017-10-10T13:53:57.852151: step 7270, loss 0.164274, acc 0.9375, learning_rate 0.0001
2017-10-10T13:53:58.148962: step 7271, loss 0.307364, acc 0.953125, learning_rate 0.0001
2017-10-10T13:53:58.446497: step 7272, loss 0.176987, acc 0.921875, learning_rate 0.0001
2017-10-10T13:53:58.748428: step 7273, loss 0.339282, acc 0.84375, learning_rate 0.0001
2017-10-10T13:53:59.036799: step 7274, loss 0.310235, acc 0.828125, learning_rate 0.0001
2017-10-10T13:53:59.314262: step 7275, loss 0.253191, acc 0.875, learning_rate 0.0001
2017-10-10T13:53:59.604060: step 7276, loss 0.305101, acc 0.90625, learning_rate 0.0001
2017-10-10T13:53:59.877069: step 7277, loss 0.298418, acc 0.859375, learning_rate 0.0001
2017-10-10T13:54:00.127798: step 7278, loss 0.19421, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:00.403163: step 7279, loss 0.154184, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:00.682341: step 7280, loss 0.390251, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:54:01.315779: step 7280, loss 0.228651, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7280

2017-10-10T13:54:02.390648: step 7281, loss 0.209305, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:02.682965: step 7282, loss 0.183551, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:02.970243: step 7283, loss 0.218634, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:03.263395: step 7284, loss 0.238083, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:03.545356: step 7285, loss 0.322708, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:03.833943: step 7286, loss 0.50779, acc 0.875, learning_rate 0.0001
2017-10-10T13:54:04.216882: step 7287, loss 0.260607, acc 0.875, learning_rate 0.0001
2017-10-10T13:54:04.522627: step 7288, loss 0.18309, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:04.724628: step 7289, loss 0.37014, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:04.910260: step 7290, loss 0.32599, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:05.099703: step 7291, loss 0.284878, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:05.283717: step 7292, loss 0.20904, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:05.552445: step 7293, loss 0.343158, acc 0.859375, learning_rate 0.0001
2017-10-10T13:54:05.832455: step 7294, loss 0.12493, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:06.119860: step 7295, loss 0.359143, acc 0.84375, learning_rate 0.0001
2017-10-10T13:54:06.351364: step 7296, loss 0.178847, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:06.519065: step 7297, loss 0.255965, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:06.776096: step 7298, loss 0.306318, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:07.048848: step 7299, loss 0.194993, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:07.343874: step 7300, loss 0.242253, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:07.641308: step 7301, loss 0.266834, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:07.922774: step 7302, loss 0.195542, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:08.218902: step 7303, loss 0.126749, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:08.504933: step 7304, loss 0.154604, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:08.796262: step 7305, loss 0.397557, acc 0.84375, learning_rate 0.0001
2017-10-10T13:54:09.077298: step 7306, loss 0.30259, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:09.371277: step 7307, loss 0.367086, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:09.675636: step 7308, loss 0.266978, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:09.969764: step 7309, loss 0.206655, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:10.253295: step 7310, loss 0.259665, acc 0.875, learning_rate 0.0001
2017-10-10T13:54:10.529341: step 7311, loss 0.262726, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:10.817255: step 7312, loss 0.171166, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:11.112245: step 7313, loss 0.298639, acc 0.875, learning_rate 0.0001
2017-10-10T13:54:11.390587: step 7314, loss 0.311043, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:11.679848: step 7315, loss 0.225314, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:11.978140: step 7316, loss 0.177167, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:12.262340: step 7317, loss 0.306961, acc 0.875, learning_rate 0.0001
2017-10-10T13:54:12.555024: step 7318, loss 0.179862, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:12.844473: step 7319, loss 0.190847, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:13.131419: step 7320, loss 0.234323, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:54:13.768367: step 7320, loss 0.229893, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7320

2017-10-10T13:54:14.908882: step 7321, loss 0.221803, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:15.171462: step 7322, loss 0.199565, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:15.424267: step 7323, loss 0.274959, acc 0.859375, learning_rate 0.0001
2017-10-10T13:54:15.681875: step 7324, loss 0.189435, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:16.016890: step 7325, loss 0.13934, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:16.321176: step 7326, loss 0.158807, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:16.676898: step 7327, loss 0.170728, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:17.012970: step 7328, loss 0.177464, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:17.192409: step 7329, loss 0.335943, acc 0.84375, learning_rate 0.0001
2017-10-10T13:54:17.381809: step 7330, loss 0.214023, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:17.553944: step 7331, loss 0.348291, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:17.740412: step 7332, loss 0.392928, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:18.043902: step 7333, loss 0.309105, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:18.337366: step 7334, loss 0.310089, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:18.623677: step 7335, loss 0.270664, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:18.914714: step 7336, loss 0.289681, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:19.201887: step 7337, loss 0.266844, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:19.497192: step 7338, loss 0.25328, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:19.789905: step 7339, loss 0.336321, acc 0.875, learning_rate 0.0001
2017-10-10T13:54:20.077791: step 7340, loss 0.260431, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:20.357806: step 7341, loss 0.270525, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:20.648473: step 7342, loss 0.447627, acc 0.828125, learning_rate 0.0001
2017-10-10T13:54:20.939051: step 7343, loss 0.27848, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:21.229511: step 7344, loss 0.166406, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:21.519964: step 7345, loss 0.24903, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:21.817605: step 7346, loss 0.19504, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:22.113258: step 7347, loss 0.46864, acc 0.796875, learning_rate 0.0001
2017-10-10T13:54:22.405026: step 7348, loss 0.160706, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:22.701660: step 7349, loss 0.151045, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:22.953488: step 7350, loss 0.15718, acc 0.980392, learning_rate 0.0001
2017-10-10T13:54:23.257010: step 7351, loss 0.270767, acc 0.875, learning_rate 0.0001
2017-10-10T13:54:23.546412: step 7352, loss 0.355488, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:23.830087: step 7353, loss 0.193974, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:24.124204: step 7354, loss 0.319762, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:24.402465: step 7355, loss 0.207472, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:24.675856: step 7356, loss 0.234964, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:24.948139: step 7357, loss 0.412514, acc 0.859375, learning_rate 0.0001
2017-10-10T13:54:25.233508: step 7358, loss 0.152509, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:25.502303: step 7359, loss 0.558419, acc 0.84375, learning_rate 0.0001
2017-10-10T13:54:25.781856: step 7360, loss 0.196421, acc 0.953125, learning_rate 0.0001

Evaluation:
2017-10-10T13:54:26.396021: step 7360, loss 0.231497, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7360

2017-10-10T13:54:27.518218: step 7361, loss 0.326743, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:27.808807: step 7362, loss 0.243031, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:28.091440: step 7363, loss 0.374142, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:28.392053: step 7364, loss 0.294315, acc 0.875, learning_rate 0.0001
2017-10-10T13:54:28.689666: step 7365, loss 0.205377, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:28.987006: step 7366, loss 0.370048, acc 0.859375, learning_rate 0.0001
2017-10-10T13:54:29.369067: step 7367, loss 0.228358, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:29.662708: step 7368, loss 0.197214, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:29.842159: step 7369, loss 0.0784941, acc 0.984375, learning_rate 0.0001
2017-10-10T13:54:30.016597: step 7370, loss 0.153995, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:30.195000: step 7371, loss 0.320242, acc 0.859375, learning_rate 0.0001
2017-10-10T13:54:30.371852: step 7372, loss 0.340059, acc 0.875, learning_rate 0.0001
2017-10-10T13:54:30.561563: step 7373, loss 0.310029, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:30.863877: step 7374, loss 0.301897, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:31.164118: step 7375, loss 0.209413, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:31.447450: step 7376, loss 0.210723, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:31.701550: step 7377, loss 0.31578, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:31.987288: step 7378, loss 0.351183, acc 0.875, learning_rate 0.0001
2017-10-10T13:54:32.268591: step 7379, loss 0.195427, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:32.548262: step 7380, loss 0.193169, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:32.845699: step 7381, loss 0.393857, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:33.123443: step 7382, loss 0.138234, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:33.404969: step 7383, loss 0.357567, acc 0.859375, learning_rate 0.0001
2017-10-10T13:54:33.710522: step 7384, loss 0.231765, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:33.993933: step 7385, loss 0.115924, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:34.270531: step 7386, loss 0.275143, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:34.568891: step 7387, loss 0.437735, acc 0.859375, learning_rate 0.0001
2017-10-10T13:54:34.860170: step 7388, loss 0.285844, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:35.147217: step 7389, loss 0.125224, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:35.432612: step 7390, loss 0.188481, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:35.717146: step 7391, loss 0.450768, acc 0.828125, learning_rate 0.0001
2017-10-10T13:54:36.021066: step 7392, loss 0.338801, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:36.318308: step 7393, loss 0.368622, acc 0.859375, learning_rate 0.0001
2017-10-10T13:54:36.591881: step 7394, loss 0.26249, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:36.877381: step 7395, loss 0.146086, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:37.176854: step 7396, loss 0.328144, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:37.474158: step 7397, loss 0.195134, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:37.759566: step 7398, loss 0.423468, acc 0.84375, learning_rate 0.0001
2017-10-10T13:54:38.044802: step 7399, loss 0.304671, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:38.335067: step 7400, loss 0.262466, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:54:38.978436: step 7400, loss 0.229651, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7400

2017-10-10T13:54:40.036715: step 7401, loss 0.223099, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:40.328209: step 7402, loss 0.239856, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:40.616529: step 7403, loss 0.135335, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:40.905089: step 7404, loss 0.212269, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:41.177877: step 7405, loss 0.238704, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:41.456760: step 7406, loss 0.140274, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:41.735974: step 7407, loss 0.229372, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:42.092088: step 7408, loss 0.203773, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:42.421598: step 7409, loss 0.185286, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:42.599231: step 7410, loss 0.217381, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:42.773770: step 7411, loss 0.293618, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:42.948537: step 7412, loss 0.29907, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:43.124933: step 7413, loss 0.196785, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:43.424739: step 7414, loss 0.232719, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:43.718591: step 7415, loss 0.310416, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:44.011578: step 7416, loss 0.239298, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:44.300841: step 7417, loss 0.239649, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:44.578781: step 7418, loss 0.217467, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:44.877540: step 7419, loss 0.282796, acc 0.859375, learning_rate 0.0001
2017-10-10T13:54:45.165786: step 7420, loss 0.203521, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:45.447565: step 7421, loss 0.318594, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:45.726995: step 7422, loss 0.257406, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:46.014185: step 7423, loss 0.371454, acc 0.859375, learning_rate 0.0001
2017-10-10T13:54:46.296415: step 7424, loss 0.136302, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:46.579297: step 7425, loss 0.248661, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:46.875500: step 7426, loss 0.193271, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:47.153363: step 7427, loss 0.169675, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:47.443695: step 7428, loss 0.231203, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:47.748825: step 7429, loss 0.146353, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:48.038521: step 7430, loss 0.178372, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:48.316485: step 7431, loss 0.277256, acc 0.859375, learning_rate 0.0001
2017-10-10T13:54:48.598860: step 7432, loss 0.171181, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:48.880726: step 7433, loss 0.255444, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:49.166198: step 7434, loss 0.343914, acc 0.875, learning_rate 0.0001
2017-10-10T13:54:49.452326: step 7435, loss 0.124021, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:49.717193: step 7436, loss 0.385522, acc 0.84375, learning_rate 0.0001
2017-10-10T13:54:50.001698: step 7437, loss 0.273228, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:50.282871: step 7438, loss 0.184543, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:50.559872: step 7439, loss 0.261663, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:50.840981: step 7440, loss 0.268382, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:54:51.470112: step 7440, loss 0.230932, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7440

2017-10-10T13:54:52.580933: step 7441, loss 0.149884, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:52.843133: step 7442, loss 0.147333, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:53.097207: step 7443, loss 0.362421, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:53.396986: step 7444, loss 0.288049, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:53.692860: step 7445, loss 0.21757, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:53.968858: step 7446, loss 0.253964, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:54.304905: step 7447, loss 0.299072, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:54.647526: step 7448, loss 0.30383, acc 0.882353, learning_rate 0.0001
2017-10-10T13:54:54.828540: step 7449, loss 0.210866, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:55.004225: step 7450, loss 0.355045, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:55.179660: step 7451, loss 0.141821, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:55.356664: step 7452, loss 0.268507, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:55.542468: step 7453, loss 0.368572, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:55.789669: step 7454, loss 0.258841, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:56.076556: step 7455, loss 0.33942, acc 0.859375, learning_rate 0.0001
2017-10-10T13:54:56.374986: step 7456, loss 0.199847, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:56.672404: step 7457, loss 0.239044, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:56.967541: step 7458, loss 0.311817, acc 0.90625, learning_rate 0.0001
2017-10-10T13:54:57.264082: step 7459, loss 0.124536, acc 0.96875, learning_rate 0.0001
2017-10-10T13:54:57.555822: step 7460, loss 0.0866315, acc 0.953125, learning_rate 0.0001
2017-10-10T13:54:57.838152: step 7461, loss 0.273478, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:58.119340: step 7462, loss 0.19871, acc 0.921875, learning_rate 0.0001
2017-10-10T13:54:58.405403: step 7463, loss 0.346279, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:58.686280: step 7464, loss 0.429599, acc 0.859375, learning_rate 0.0001
2017-10-10T13:54:58.971852: step 7465, loss 0.194599, acc 0.9375, learning_rate 0.0001
2017-10-10T13:54:59.256728: step 7466, loss 0.395283, acc 0.875, learning_rate 0.0001
2017-10-10T13:54:59.541653: step 7467, loss 0.326411, acc 0.890625, learning_rate 0.0001
2017-10-10T13:54:59.834163: step 7468, loss 0.136796, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:00.136516: step 7469, loss 0.151586, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:00.420065: step 7470, loss 0.0994993, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:00.702733: step 7471, loss 0.171741, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:00.995199: step 7472, loss 0.251676, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:01.277852: step 7473, loss 0.226095, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:01.564227: step 7474, loss 0.252079, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:01.824833: step 7475, loss 0.208404, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:02.112877: step 7476, loss 0.159063, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:02.405193: step 7477, loss 0.170851, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:02.702949: step 7478, loss 0.357131, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:02.984195: step 7479, loss 0.174992, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:03.254289: step 7480, loss 0.296144, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:55:03.802732: step 7480, loss 0.232127, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7480

2017-10-10T13:55:05.004980: step 7481, loss 0.210071, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:05.288143: step 7482, loss 0.240335, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:05.568691: step 7483, loss 0.282845, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:05.871945: step 7484, loss 0.191055, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:06.164854: step 7485, loss 0.136452, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:06.463452: step 7486, loss 0.250983, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:06.760735: step 7487, loss 0.219906, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:07.108870: step 7488, loss 0.272455, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:07.452383: step 7489, loss 0.518723, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:07.632631: step 7490, loss 0.342186, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:07.817972: step 7491, loss 0.33717, acc 0.859375, learning_rate 0.0001
2017-10-10T13:55:07.999922: step 7492, loss 0.425822, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:08.177784: step 7493, loss 0.175243, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:08.353587: step 7494, loss 0.344232, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:08.654492: step 7495, loss 0.273798, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:08.954757: step 7496, loss 0.232719, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:09.250624: step 7497, loss 0.150698, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:09.543182: step 7498, loss 0.277798, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:09.848996: step 7499, loss 0.163249, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:10.142550: step 7500, loss 0.337223, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:10.433730: step 7501, loss 0.18078, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:10.718323: step 7502, loss 0.296418, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:10.995197: step 7503, loss 0.360312, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:11.277861: step 7504, loss 0.256864, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:11.560562: step 7505, loss 0.188213, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:11.840928: step 7506, loss 0.247325, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:12.132213: step 7507, loss 0.377861, acc 0.8125, learning_rate 0.0001
2017-10-10T13:55:12.415101: step 7508, loss 0.426118, acc 0.8125, learning_rate 0.0001
2017-10-10T13:55:12.692355: step 7509, loss 0.268142, acc 0.859375, learning_rate 0.0001
2017-10-10T13:55:12.972559: step 7510, loss 0.264484, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:13.252190: step 7511, loss 0.319157, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:13.550053: step 7512, loss 0.222309, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:13.829448: step 7513, loss 0.227226, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:14.116571: step 7514, loss 0.235581, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:14.392889: step 7515, loss 0.269112, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:14.653945: step 7516, loss 0.403655, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:14.935585: step 7517, loss 0.396536, acc 0.8125, learning_rate 0.0001
2017-10-10T13:55:15.214553: step 7518, loss 0.18519, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:15.504936: step 7519, loss 0.196123, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:15.783269: step 7520, loss 0.215285, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:55:16.397253: step 7520, loss 0.230349, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7520

2017-10-10T13:55:17.483125: step 7521, loss 0.178178, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:17.773435: step 7522, loss 0.339893, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:18.060416: step 7523, loss 0.357106, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:18.342371: step 7524, loss 0.327676, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:18.636684: step 7525, loss 0.25218, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:18.930603: step 7526, loss 0.187795, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:19.220202: step 7527, loss 0.273929, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:19.500511: step 7528, loss 0.304256, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:19.828934: step 7529, loss 0.276326, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:20.178126: step 7530, loss 0.379454, acc 0.796875, learning_rate 0.0001
2017-10-10T13:55:20.355214: step 7531, loss 0.342703, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:20.531634: step 7532, loss 0.199371, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:20.704614: step 7533, loss 0.250157, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:20.905034: step 7534, loss 0.313054, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:21.176920: step 7535, loss 0.287625, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:21.471190: step 7536, loss 0.282199, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:21.721794: step 7537, loss 0.379883, acc 0.8125, learning_rate 0.0001
2017-10-10T13:55:21.991143: step 7538, loss 0.221306, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:22.250976: step 7539, loss 0.275702, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:22.525523: step 7540, loss 0.393406, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:22.811380: step 7541, loss 0.331173, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:23.090499: step 7542, loss 0.243762, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:23.377308: step 7543, loss 0.295827, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:23.660342: step 7544, loss 0.174444, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:23.938877: step 7545, loss 0.345993, acc 0.8125, learning_rate 0.0001
2017-10-10T13:55:24.186550: step 7546, loss 0.284507, acc 0.862745, learning_rate 0.0001
2017-10-10T13:55:24.468301: step 7547, loss 0.310923, acc 0.84375, learning_rate 0.0001
2017-10-10T13:55:24.717981: step 7548, loss 0.216698, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:25.011519: step 7549, loss 0.294169, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:25.284431: step 7550, loss 0.139177, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:25.550422: step 7551, loss 0.266512, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:25.824999: step 7552, loss 0.32144, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:26.100638: step 7553, loss 0.422479, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:26.384460: step 7554, loss 0.264029, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:26.674104: step 7555, loss 0.313944, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:26.963603: step 7556, loss 0.315322, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:27.262313: step 7557, loss 0.483137, acc 0.859375, learning_rate 0.0001
2017-10-10T13:55:27.541614: step 7558, loss 0.167169, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:27.830228: step 7559, loss 0.2397, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:28.120057: step 7560, loss 0.32003, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:55:28.736831: step 7560, loss 0.229286, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7560

2017-10-10T13:55:29.892918: step 7561, loss 0.3782, acc 0.84375, learning_rate 0.0001
2017-10-10T13:55:30.164890: step 7562, loss 0.244603, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:30.395282: step 7563, loss 0.17812, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:30.636830: step 7564, loss 0.313409, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:30.900122: step 7565, loss 0.318123, acc 0.859375, learning_rate 0.0001
2017-10-10T13:55:31.184224: step 7566, loss 0.194128, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:31.467291: step 7567, loss 0.207687, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:31.738582: step 7568, loss 0.234534, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:32.087208: step 7569, loss 0.266662, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:32.422244: step 7570, loss 0.329773, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:32.606150: step 7571, loss 0.276718, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:32.781404: step 7572, loss 0.208416, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:32.967992: step 7573, loss 0.129225, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:33.141468: step 7574, loss 0.245475, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:33.314250: step 7575, loss 0.151329, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:33.492291: step 7576, loss 0.222943, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:33.655314: step 7577, loss 0.137493, acc 1, learning_rate 0.0001
2017-10-10T13:55:33.819909: step 7578, loss 0.238266, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:33.985927: step 7579, loss 0.197625, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:34.161805: step 7580, loss 0.220402, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:34.327004: step 7581, loss 0.235242, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:34.491189: step 7582, loss 0.0896904, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:34.656541: step 7583, loss 0.266199, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:34.819795: step 7584, loss 0.335225, acc 0.859375, learning_rate 0.0001
2017-10-10T13:55:34.983608: step 7585, loss 0.276642, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:35.147364: step 7586, loss 0.277826, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:35.315804: step 7587, loss 0.33582, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:35.481019: step 7588, loss 0.270326, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:35.641777: step 7589, loss 0.137444, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:35.807156: step 7590, loss 0.214417, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:36.001181: step 7591, loss 0.288981, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:36.168375: step 7592, loss 0.222602, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:36.336461: step 7593, loss 0.184205, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:36.500141: step 7594, loss 0.227997, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:36.670372: step 7595, loss 0.298283, acc 0.828125, learning_rate 0.0001
2017-10-10T13:55:36.836132: step 7596, loss 0.441332, acc 0.8125, learning_rate 0.0001
2017-10-10T13:55:37.005221: step 7597, loss 0.22336, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:37.172111: step 7598, loss 0.185413, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:37.341497: step 7599, loss 0.152923, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:37.506420: step 7600, loss 0.217791, acc 0.890625, learning_rate 0.0001

Evaluation:
2017-10-10T13:55:37.923253: step 7600, loss 0.228467, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7600

2017-10-10T13:55:38.571356: step 7601, loss 0.223585, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:38.735212: step 7602, loss 0.404979, acc 0.84375, learning_rate 0.0001
2017-10-10T13:55:38.903610: step 7603, loss 0.202825, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:39.069982: step 7604, loss 0.350652, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:39.233387: step 7605, loss 0.177784, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:39.400356: step 7606, loss 0.263904, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:39.563738: step 7607, loss 0.241454, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:39.730012: step 7608, loss 0.158437, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:39.895206: step 7609, loss 0.224897, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:40.061436: step 7610, loss 0.133003, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:40.223077: step 7611, loss 0.175214, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:40.390142: step 7612, loss 0.242566, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:40.552503: step 7613, loss 0.337116, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:40.715710: step 7614, loss 0.187915, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:40.880793: step 7615, loss 0.215279, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:41.046469: step 7616, loss 0.211396, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:41.213406: step 7617, loss 0.110164, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:41.380105: step 7618, loss 0.387478, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:41.545194: step 7619, loss 0.155805, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:41.709998: step 7620, loss 0.244688, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:41.879847: step 7621, loss 0.213893, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:42.042948: step 7622, loss 0.372499, acc 0.828125, learning_rate 0.0001
2017-10-10T13:55:42.209852: step 7623, loss 0.372455, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:42.373446: step 7624, loss 0.154512, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:42.537737: step 7625, loss 0.108917, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:42.704978: step 7626, loss 0.237271, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:42.871762: step 7627, loss 0.285017, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:43.042620: step 7628, loss 0.265372, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:43.205207: step 7629, loss 0.168074, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:43.368747: step 7630, loss 0.169087, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:43.535736: step 7631, loss 0.28381, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:43.699185: step 7632, loss 0.148838, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:43.862409: step 7633, loss 0.33469, acc 0.859375, learning_rate 0.0001
2017-10-10T13:55:44.027744: step 7634, loss 0.367656, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:44.191454: step 7635, loss 0.324327, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:44.362672: step 7636, loss 0.263875, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:44.529260: step 7637, loss 0.271319, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:44.694648: step 7638, loss 0.234666, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:44.866016: step 7639, loss 0.252107, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:45.032531: step 7640, loss 0.261776, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:55:45.428859: step 7640, loss 0.228725, acc 0.91223

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7640

2017-10-10T13:55:46.144196: step 7641, loss 0.266663, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:46.309766: step 7642, loss 0.158199, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:46.475826: step 7643, loss 0.161008, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:46.612140: step 7644, loss 0.187808, acc 0.960784, learning_rate 0.0001
2017-10-10T13:55:46.776034: step 7645, loss 0.248044, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:46.942746: step 7646, loss 0.160877, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:47.106247: step 7647, loss 0.219709, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:47.270805: step 7648, loss 0.13252, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:47.436864: step 7649, loss 0.346883, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:47.599217: step 7650, loss 0.325726, acc 0.8125, learning_rate 0.0001
2017-10-10T13:55:47.762712: step 7651, loss 0.197092, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:47.931399: step 7652, loss 0.164668, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:48.096697: step 7653, loss 0.17938, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:48.262350: step 7654, loss 0.294228, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:48.428100: step 7655, loss 0.245708, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:48.593353: step 7656, loss 0.189641, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:48.756562: step 7657, loss 0.195027, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:48.922611: step 7658, loss 0.276661, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:49.088639: step 7659, loss 0.326962, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:49.251862: step 7660, loss 0.148729, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:49.415193: step 7661, loss 0.430036, acc 0.84375, learning_rate 0.0001
2017-10-10T13:55:49.577769: step 7662, loss 0.193642, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:49.740745: step 7663, loss 0.353614, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:49.907861: step 7664, loss 0.111344, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:50.074019: step 7665, loss 0.332448, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:50.239285: step 7666, loss 0.264861, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:50.404753: step 7667, loss 0.154399, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:50.569766: step 7668, loss 0.402629, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:50.734896: step 7669, loss 0.275176, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:50.898302: step 7670, loss 0.200054, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:51.066075: step 7671, loss 0.269973, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:51.228173: step 7672, loss 0.342391, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:51.390982: step 7673, loss 0.280892, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:51.555889: step 7674, loss 0.263376, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:51.719956: step 7675, loss 0.227582, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:51.881806: step 7676, loss 0.19554, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:52.045151: step 7677, loss 0.269908, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:52.211477: step 7678, loss 0.240939, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:52.376901: step 7679, loss 0.323405, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:52.538994: step 7680, loss 0.341369, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:55:52.954826: step 7680, loss 0.229313, acc 0.910791

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7680

2017-10-10T13:55:53.530422: step 7681, loss 0.306796, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:53.693152: step 7682, loss 0.348184, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:53.865640: step 7683, loss 0.329121, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:54.030568: step 7684, loss 0.215454, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:54.195150: step 7685, loss 0.10945, acc 0.984375, learning_rate 0.0001
2017-10-10T13:55:54.356124: step 7686, loss 0.179837, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:54.521954: step 7687, loss 0.197595, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:54.689172: step 7688, loss 0.164954, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:54.853495: step 7689, loss 0.365331, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:55.017139: step 7690, loss 0.321974, acc 0.859375, learning_rate 0.0001
2017-10-10T13:55:55.182332: step 7691, loss 0.263393, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:55.351099: step 7692, loss 0.21772, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:55.519984: step 7693, loss 0.450264, acc 0.828125, learning_rate 0.0001
2017-10-10T13:55:55.684188: step 7694, loss 0.314687, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:55.849978: step 7695, loss 0.360438, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:56.013609: step 7696, loss 0.195267, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:56.177984: step 7697, loss 0.1842, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:56.341227: step 7698, loss 0.273135, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:56.507429: step 7699, loss 0.388499, acc 0.8125, learning_rate 0.0001
2017-10-10T13:55:56.667334: step 7700, loss 0.249621, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:56.832363: step 7701, loss 0.252241, acc 0.953125, learning_rate 0.0001
2017-10-10T13:55:57.000629: step 7702, loss 0.298749, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:57.164896: step 7703, loss 0.28587, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:57.329660: step 7704, loss 0.268748, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:57.489280: step 7705, loss 0.362248, acc 0.859375, learning_rate 0.0001
2017-10-10T13:55:57.654373: step 7706, loss 0.279135, acc 0.875, learning_rate 0.0001
2017-10-10T13:55:57.820075: step 7707, loss 0.277733, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:57.989029: step 7708, loss 0.293662, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:58.156569: step 7709, loss 0.212181, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:58.321373: step 7710, loss 0.386936, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:58.486109: step 7711, loss 0.197988, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:58.652616: step 7712, loss 0.290961, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:58.818649: step 7713, loss 0.180963, acc 0.9375, learning_rate 0.0001
2017-10-10T13:55:58.985394: step 7714, loss 0.270749, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:59.147346: step 7715, loss 0.249495, acc 0.90625, learning_rate 0.0001
2017-10-10T13:55:59.309950: step 7716, loss 0.181316, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:59.472517: step 7717, loss 0.190043, acc 0.921875, learning_rate 0.0001
2017-10-10T13:55:59.633772: step 7718, loss 0.118793, acc 0.96875, learning_rate 0.0001
2017-10-10T13:55:59.798980: step 7719, loss 0.244312, acc 0.890625, learning_rate 0.0001
2017-10-10T13:55:59.971005: step 7720, loss 0.306332, acc 0.90625, learning_rate 0.0001

Evaluation:
2017-10-10T13:56:00.383531: step 7720, loss 0.228075, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7720

2017-10-10T13:56:01.028065: step 7721, loss 0.279125, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:01.195557: step 7722, loss 0.104356, acc 0.96875, learning_rate 0.0001
2017-10-10T13:56:01.360131: step 7723, loss 0.266938, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:01.524068: step 7724, loss 0.378421, acc 0.859375, learning_rate 0.0001
2017-10-10T13:56:01.687807: step 7725, loss 0.169433, acc 0.953125, learning_rate 0.0001
2017-10-10T13:56:01.854543: step 7726, loss 0.346866, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:02.022189: step 7727, loss 0.269526, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:02.184472: step 7728, loss 0.180393, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:02.351617: step 7729, loss 0.293739, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:02.518301: step 7730, loss 0.150808, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:02.683086: step 7731, loss 0.248561, acc 0.953125, learning_rate 0.0001
2017-10-10T13:56:02.850798: step 7732, loss 0.175041, acc 0.953125, learning_rate 0.0001
2017-10-10T13:56:03.013636: step 7733, loss 0.230652, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:03.175614: step 7734, loss 0.239253, acc 0.953125, learning_rate 0.0001
2017-10-10T13:56:03.337520: step 7735, loss 0.313047, acc 0.875, learning_rate 0.0001
2017-10-10T13:56:03.501708: step 7736, loss 0.257845, acc 0.890625, learning_rate 0.0001
2017-10-10T13:56:03.666050: step 7737, loss 0.178597, acc 0.953125, learning_rate 0.0001
2017-10-10T13:56:03.832847: step 7738, loss 0.14674, acc 0.96875, learning_rate 0.0001
2017-10-10T13:56:03.999125: step 7739, loss 0.152646, acc 0.96875, learning_rate 0.0001
2017-10-10T13:56:04.162095: step 7740, loss 0.2939, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:04.331471: step 7741, loss 0.190274, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:04.471761: step 7742, loss 0.22222, acc 0.941176, learning_rate 0.0001
2017-10-10T13:56:04.639045: step 7743, loss 0.299561, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:04.805246: step 7744, loss 0.217359, acc 0.875, learning_rate 0.0001
2017-10-10T13:56:04.976909: step 7745, loss 0.388243, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:05.141995: step 7746, loss 0.355855, acc 0.890625, learning_rate 0.0001
2017-10-10T13:56:05.307962: step 7747, loss 0.250266, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:05.474476: step 7748, loss 0.254179, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:05.648100: step 7749, loss 0.256644, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:05.812165: step 7750, loss 0.220422, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:05.979369: step 7751, loss 0.276612, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:06.144198: step 7752, loss 0.165116, acc 0.953125, learning_rate 0.0001
2017-10-10T13:56:06.311090: step 7753, loss 0.182048, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:06.473704: step 7754, loss 0.177339, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:06.634185: step 7755, loss 0.2629, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:06.799330: step 7756, loss 0.295228, acc 0.890625, learning_rate 0.0001
2017-10-10T13:56:06.965789: step 7757, loss 0.462595, acc 0.828125, learning_rate 0.0001
2017-10-10T13:56:07.132128: step 7758, loss 0.131597, acc 0.96875, learning_rate 0.0001
2017-10-10T13:56:07.296958: step 7759, loss 0.250092, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:07.460338: step 7760, loss 0.206118, acc 0.921875, learning_rate 0.0001

Evaluation:
2017-10-10T13:56:07.871690: step 7760, loss 0.228765, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7760

2017-10-10T13:56:08.590218: step 7761, loss 0.26903, acc 0.875, learning_rate 0.0001
2017-10-10T13:56:08.751624: step 7762, loss 0.185352, acc 0.953125, learning_rate 0.0001
2017-10-10T13:56:08.917435: step 7763, loss 0.264924, acc 0.890625, learning_rate 0.0001
2017-10-10T13:56:09.079866: step 7764, loss 0.326754, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:09.244264: step 7765, loss 0.321992, acc 0.890625, learning_rate 0.0001
2017-10-10T13:56:09.408877: step 7766, loss 0.177584, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:09.573338: step 7767, loss 0.175305, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:09.739285: step 7768, loss 0.318948, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:09.903484: step 7769, loss 0.262841, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:10.071050: step 7770, loss 0.231914, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:10.236515: step 7771, loss 0.110255, acc 0.96875, learning_rate 0.0001
2017-10-10T13:56:10.398368: step 7772, loss 0.105256, acc 0.984375, learning_rate 0.0001
2017-10-10T13:56:10.563902: step 7773, loss 0.229921, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:10.729219: step 7774, loss 0.334014, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:10.900538: step 7775, loss 0.38442, acc 0.875, learning_rate 0.0001
2017-10-10T13:56:11.064439: step 7776, loss 0.342108, acc 0.859375, learning_rate 0.0001
2017-10-10T13:56:11.230695: step 7777, loss 0.227025, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:11.393371: step 7778, loss 0.509199, acc 0.8125, learning_rate 0.0001
2017-10-10T13:56:11.556974: step 7779, loss 0.356974, acc 0.84375, learning_rate 0.0001
2017-10-10T13:56:11.722460: step 7780, loss 0.223057, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:11.889692: step 7781, loss 0.193689, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:12.055182: step 7782, loss 0.352037, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:12.220158: step 7783, loss 0.233694, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:12.383581: step 7784, loss 0.248343, acc 0.890625, learning_rate 0.0001
2017-10-10T13:56:12.549262: step 7785, loss 0.185665, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:12.717226: step 7786, loss 0.308969, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:12.878517: step 7787, loss 0.260621, acc 0.953125, learning_rate 0.0001
2017-10-10T13:56:13.041344: step 7788, loss 0.243156, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:13.204595: step 7789, loss 0.174224, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:13.370971: step 7790, loss 0.297862, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:13.538455: step 7791, loss 0.297442, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:13.700180: step 7792, loss 0.288924, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:13.865079: step 7793, loss 0.180966, acc 0.96875, learning_rate 0.0001
2017-10-10T13:56:14.027547: step 7794, loss 0.223415, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:14.193856: step 7795, loss 0.215914, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:14.359123: step 7796, loss 0.25634, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:14.525002: step 7797, loss 0.21647, acc 0.890625, learning_rate 0.0001
2017-10-10T13:56:14.691868: step 7798, loss 0.276058, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:14.854128: step 7799, loss 0.220169, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:15.018903: step 7800, loss 0.232234, acc 0.9375, learning_rate 0.0001

Evaluation:
2017-10-10T13:56:15.430906: step 7800, loss 0.228395, acc 0.915108

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7800

2017-10-10T13:56:16.006163: step 7801, loss 0.192639, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:16.172355: step 7802, loss 0.144721, acc 0.96875, learning_rate 0.0001
2017-10-10T13:56:16.337191: step 7803, loss 0.222166, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:16.502818: step 7804, loss 0.219057, acc 0.953125, learning_rate 0.0001
2017-10-10T13:56:16.668746: step 7805, loss 0.26155, acc 0.890625, learning_rate 0.0001
2017-10-10T13:56:16.831905: step 7806, loss 0.164879, acc 0.953125, learning_rate 0.0001
2017-10-10T13:56:17.000521: step 7807, loss 0.162284, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:17.165915: step 7808, loss 0.274168, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:17.328404: step 7809, loss 0.26158, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:17.492268: step 7810, loss 0.150227, acc 0.96875, learning_rate 0.0001
2017-10-10T13:56:17.657587: step 7811, loss 0.41332, acc 0.875, learning_rate 0.0001
2017-10-10T13:56:17.822221: step 7812, loss 0.25333, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:17.991130: step 7813, loss 0.254232, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:18.156668: step 7814, loss 0.155217, acc 0.953125, learning_rate 0.0001
2017-10-10T13:56:18.319265: step 7815, loss 0.122647, acc 0.953125, learning_rate 0.0001
2017-10-10T13:56:18.479351: step 7816, loss 0.303422, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:18.646498: step 7817, loss 0.145968, acc 0.96875, learning_rate 0.0001
2017-10-10T13:56:18.812384: step 7818, loss 0.144546, acc 0.96875, learning_rate 0.0001
2017-10-10T13:56:18.976913: step 7819, loss 0.249827, acc 0.96875, learning_rate 0.0001
2017-10-10T13:56:19.143562: step 7820, loss 0.316756, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:19.311771: step 7821, loss 0.287843, acc 0.875, learning_rate 0.0001
2017-10-10T13:56:19.474093: step 7822, loss 0.320747, acc 0.890625, learning_rate 0.0001
2017-10-10T13:56:19.639385: step 7823, loss 0.168671, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:19.810904: step 7824, loss 0.252401, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:19.977414: step 7825, loss 0.189535, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:20.143644: step 7826, loss 0.302009, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:20.308540: step 7827, loss 0.19209, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:20.471842: step 7828, loss 0.278851, acc 0.890625, learning_rate 0.0001
2017-10-10T13:56:20.637507: step 7829, loss 0.0974403, acc 0.984375, learning_rate 0.0001
2017-10-10T13:56:20.803421: step 7830, loss 0.182598, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:20.967116: step 7831, loss 0.242504, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:21.133109: step 7832, loss 0.171878, acc 0.921875, learning_rate 0.0001
2017-10-10T13:56:21.297941: step 7833, loss 0.115022, acc 0.96875, learning_rate 0.0001
2017-10-10T13:56:21.461108: step 7834, loss 0.239027, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:21.623309: step 7835, loss 0.328752, acc 0.90625, learning_rate 0.0001
2017-10-10T13:56:21.786461: step 7836, loss 0.258166, acc 0.890625, learning_rate 0.0001
2017-10-10T13:56:21.954934: step 7837, loss 0.1826, acc 0.9375, learning_rate 0.0001
2017-10-10T13:56:22.118933: step 7838, loss 0.157057, acc 0.984375, learning_rate 0.0001
2017-10-10T13:56:22.283857: step 7839, loss 0.386536, acc 0.890625, learning_rate 0.0001
2017-10-10T13:56:22.424063: step 7840, loss 0.432184, acc 0.823529, learning_rate 0.0001

Evaluation:
2017-10-10T13:56:22.826245: step 7840, loss 0.228568, acc 0.913669

Saved model checkpoint to /home/sheep/bigdata/runs/1507657518/checkpoints/model-7840

